{
  
    
        "post0": {
            "title": "Title",
            "content": "Tools - pandas . permalink: /tools_pandas | . The pandas library provides high-performance, easy-to-use data structures and data analysis tools. The main data structure is the DataFrame, which you can think of as an in-memory 2D table (like a spreadsheet, with column names and row labels). Many features available in Excel are available programmatically, such as creating pivot tables, computing columns based on other columns, plotting graphs, etc. You can also group rows by column value, or join tables much like in SQL. Pandas is also great at handling time series. . Prerequisites: . NumPy â€“ if you are not familiar with NumPy, we recommend that you go through the NumPy tutorial now. | . Setup . First, let&#39;s import pandas. People usually import it as pd: . import pandas as pd . Series objects . The pandas library contains these useful data structures: . Series objects, that we will discuss now. A Series object is 1D array, similar to a column in a spreadsheet (with a column name and row labels). | DataFrame objects. This is a 2D table, similar to a spreadsheet (with column names and row labels). | Panel objects. You can see a Panel as a dictionary of DataFrames. These are less used, so we will not discuss them here. | . Creating a Series . Let&#39;s start by creating our first Series object! . s = pd.Series([2,-1,3,5]) s . 0 2 1 -1 2 3 3 5 dtype: int64 . Similar to a 1D ndarray . Series objects behave much like one-dimensional NumPy ndarrays, and you can often pass them as parameters to NumPy functions: . import numpy as np np.exp(s) . 0 7.389056 1 0.367879 2 20.085537 3 148.413159 dtype: float64 . Arithmetic operations on Series are also possible, and they apply elementwise, just like for ndarrays: . s + [1000,2000,3000,4000] . 0 1002 1 1999 2 3003 3 4005 dtype: int64 . Similar to NumPy, if you add a single number to a Series, that number is added to all items in the Series. This is called broadcasting: . s + 1000 . 0 1002 1 999 2 1003 3 1005 dtype: int64 . The same is true for all binary operations such as * or /, and even conditional operations: . s &lt; 0 . 0 False 1 True 2 False 3 False dtype: bool . Index labels . Each item in a Series object has a unique identifier called the index label. By default, it is simply the rank of the item in the Series (starting at 0) but you can also set the index labels manually: . s2 = pd.Series([68, 83, 112, 68], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;, &quot;darwin&quot;]) s2 . alice 68 bob 83 charles 112 darwin 68 dtype: int64 . You can then use the Series just like a dict: . s2[&quot;bob&quot;] . 83 . You can still access the items by integer location, like in a regular array: . s2[1] . 83 . To make it clear when you are accessing by label or by integer location, it is recommended to always use the loc attribute when accessing by label, and the iloc attribute when accessing by integer location: . s2.loc[&quot;bob&quot;] . 83 . s2.iloc[1] . 83 . Slicing a Series also slices the index labels: . s2.iloc[1:3] . bob 83 charles 112 dtype: int64 . This can lead to unexpected results when using the default numeric labels, so be careful: . surprise = pd.Series([1000, 1001, 1002, 1003]) surprise . 0 1000 1 1001 2 1002 3 1003 dtype: int64 . surprise_slice = surprise[2:] surprise_slice . 2 1002 3 1003 dtype: int64 . Oh look! The first element has index label 2. The element with index label 0 is absent from the slice: . try: surprise_slice[0] except KeyError as e: print(&quot;Key error:&quot;, e) . Key error: 0 . But remember that you can access elements by integer location using the iloc attribute. This illustrates another reason why it&#39;s always better to use loc and iloc to access Series objects: . surprise_slice.iloc[0] . 1002 . Init from dict . You can create a Series object from a dict. The keys will be used as index labels: . weights = {&quot;alice&quot;: 68, &quot;bob&quot;: 83, &quot;colin&quot;: 86, &quot;darwin&quot;: 68} s3 = pd.Series(weights) s3 . alice 68 bob 83 colin 86 darwin 68 dtype: int64 . You can control which elements you want to include in the Series and in what order by explicitly specifying the desired index: . s4 = pd.Series(weights, index = [&quot;colin&quot;, &quot;alice&quot;]) s4 . colin 86 alice 68 dtype: int64 . Automatic alignment . When an operation involves multiple Series objects, pandas automatically aligns items by matching index labels. . print(s2.keys()) print(s3.keys()) s2 + s3 . Index([&#39;alice&#39;, &#39;bob&#39;, &#39;charles&#39;, &#39;darwin&#39;], dtype=&#39;object&#39;) Index([&#39;alice&#39;, &#39;bob&#39;, &#39;colin&#39;, &#39;darwin&#39;], dtype=&#39;object&#39;) . alice 136.0 bob 166.0 charles NaN colin NaN darwin 136.0 dtype: float64 . The resulting Series contains the union of index labels from s2 and s3. Since &quot;colin&quot; is missing from s2 and &quot;charles&quot; is missing from s3, these items have a NaN result value. (ie. Not-a-Number means missing). . Automatic alignment is very handy when working with data that may come from various sources with varying structure and missing items. But if you forget to set the right index labels, you can have surprising results: . s5 = pd.Series([1000,1000,1000,1000]) print(&quot;s2 =&quot;, s2.values) print(&quot;s5 =&quot;, s5.values) s2 + s5 . s2 = [ 68 83 112 68] s5 = [1000 1000 1000 1000] . alice NaN bob NaN charles NaN darwin NaN 0 NaN 1 NaN 2 NaN 3 NaN dtype: float64 . Pandas could not align the Series, since their labels do not match at all, hence the full NaN result. . Init with a scalar . You can also initialize a Series object using a scalar and a list of index labels: all items will be set to the scalar. . meaning = pd.Series(42, [&quot;life&quot;, &quot;universe&quot;, &quot;everything&quot;]) meaning . life 42 universe 42 everything 42 dtype: int64 . Series name . A Series can have a name: . s6 = pd.Series([83, 68], index=[&quot;bob&quot;, &quot;alice&quot;], name=&quot;weights&quot;) s6 . bob 83 alice 68 Name: weights, dtype: int64 . Plotting a Series . Pandas makes it easy to plot Series data using matplotlib (for more details on matplotlib, check out the matplotlib tutorial). Just import matplotlib and call the plot() method: . %matplotlib inline import matplotlib.pyplot as plt temperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5] s7 = pd.Series(temperatures, name=&quot;Temperature&quot;) s7.plot() plt.show() . There are many options for plotting your data. It is not necessary to list them all here: if you need a particular type of plot (histograms, pie charts, etc.), just look for it in the excellent Visualization section of pandas&#39; documentation, and look at the example code. . Handling time . Many datasets have timestamps, and pandas is awesome at manipulating such data: . it can represent periods (such as 2016Q3) and frequencies (such as &quot;monthly&quot;), | it can convert periods to actual timestamps, and vice versa, | it can resample data and aggregate values any way you like, | it can handle timezones. | . Time range . Let&#39;s start by creating a time series using pd.date_range(). This returns a DatetimeIndex containing one datetime per hour for 12 hours starting on October 29th 2016 at 5:30pm. . dates = pd.date_range(&#39;2016/10/29 5:30pm&#39;, periods=12, freq=&#39;H&#39;) dates . DatetimeIndex([&#39;2016-10-29 17:30:00&#39;, &#39;2016-10-29 18:30:00&#39;, &#39;2016-10-29 19:30:00&#39;, &#39;2016-10-29 20:30:00&#39;, &#39;2016-10-29 21:30:00&#39;, &#39;2016-10-29 22:30:00&#39;, &#39;2016-10-29 23:30:00&#39;, &#39;2016-10-30 00:30:00&#39;, &#39;2016-10-30 01:30:00&#39;, &#39;2016-10-30 02:30:00&#39;, &#39;2016-10-30 03:30:00&#39;, &#39;2016-10-30 04:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;H&#39;) . This DatetimeIndex may be used as an index in a Series: . temp_series = pd.Series(temperatures, dates) temp_series . 2016-10-29 17:30:00 4.4 2016-10-29 18:30:00 5.1 2016-10-29 19:30:00 6.1 2016-10-29 20:30:00 6.2 2016-10-29 21:30:00 6.1 2016-10-29 22:30:00 6.1 2016-10-29 23:30:00 5.7 2016-10-30 00:30:00 5.2 2016-10-30 01:30:00 4.7 2016-10-30 02:30:00 4.1 2016-10-30 03:30:00 3.9 2016-10-30 04:30:00 3.5 Freq: H, dtype: float64 . Let&#39;s plot this series: . temp_series.plot(kind=&quot;bar&quot;) plt.grid(True) plt.show() . Resampling . Pandas lets us resample a time series very simply. Just call the resample() method and specify a new frequency: . temp_series_freq_2H = temp_series.resample(&quot;2H&quot;) temp_series_freq_2H . DatetimeIndexResampler [freq=&lt;2 * Hours&gt;, axis=0, closed=left, label=left, convention=start, base=0] . The resampling operation is actually a deferred operation, which is why we did not get a Series object, but a DatetimeIndexResampler object instead. To actually perform the resampling operation, we can simply call the mean() method: Pandas will compute the mean of every pair of consecutive hours: . temp_series_freq_2H = temp_series_freq_2H.mean() . Let&#39;s plot the result: . temp_series_freq_2H.plot(kind=&quot;bar&quot;) plt.show() . Note how the values have automatically been aggregated into 2-hour periods. If we look at the 6-8pm period, for example, we had a value of 5.1 at 6:30pm, and 6.1 at 7:30pm. After resampling, we just have one value of 5.6, which is the mean of 5.1 and 6.1. Rather than computing the mean, we could have used any other aggregation function, for example we can decide to keep the minimum value of each period: . temp_series_freq_2H = temp_series.resample(&quot;2H&quot;).min() temp_series_freq_2H . 2016-10-29 16:00:00 4.4 2016-10-29 18:00:00 5.1 2016-10-29 20:00:00 6.1 2016-10-29 22:00:00 5.7 2016-10-30 00:00:00 4.7 2016-10-30 02:00:00 3.9 2016-10-30 04:00:00 3.5 Freq: 2H, dtype: float64 . Or, equivalently, we could use the apply() method instead: . temp_series_freq_2H = temp_series.resample(&quot;2H&quot;).apply(np.min) temp_series_freq_2H . 2016-10-29 16:00:00 4.4 2016-10-29 18:00:00 5.1 2016-10-29 20:00:00 6.1 2016-10-29 22:00:00 5.7 2016-10-30 00:00:00 4.7 2016-10-30 02:00:00 3.9 2016-10-30 04:00:00 3.5 Freq: 2H, dtype: float64 . Upsampling and interpolation . This was an example of downsampling. We can also upsample (ie. increase the frequency), but this creates holes in our data: . temp_series_freq_15min = temp_series.resample(&quot;15Min&quot;).mean() temp_series_freq_15min.head(n=10) # `head` displays the top n values . 2016-10-29 17:30:00 4.4 2016-10-29 17:45:00 NaN 2016-10-29 18:00:00 NaN 2016-10-29 18:15:00 NaN 2016-10-29 18:30:00 5.1 2016-10-29 18:45:00 NaN 2016-10-29 19:00:00 NaN 2016-10-29 19:15:00 NaN 2016-10-29 19:30:00 6.1 2016-10-29 19:45:00 NaN Freq: 15T, dtype: float64 . One solution is to fill the gaps by interpolating. We just call the interpolate() method. The default is to use linear interpolation, but we can also select another method, such as cubic interpolation: . temp_series_freq_15min = temp_series.resample(&quot;15Min&quot;).interpolate(method=&quot;cubic&quot;) temp_series_freq_15min.head(n=10) . 2016-10-29 17:30:00 4.400000 2016-10-29 17:45:00 4.452911 2016-10-29 18:00:00 4.605113 2016-10-29 18:15:00 4.829758 2016-10-29 18:30:00 5.100000 2016-10-29 18:45:00 5.388992 2016-10-29 19:00:00 5.669887 2016-10-29 19:15:00 5.915839 2016-10-29 19:30:00 6.100000 2016-10-29 19:45:00 6.203621 Freq: 15T, dtype: float64 . temp_series.plot(label=&quot;Period: 1 hour&quot;) temp_series_freq_15min.plot(label=&quot;Period: 15 minutes&quot;) plt.legend() plt.show() . Timezones . By default datetimes are naive: they are not aware of timezones, so 2016-10-30 02:30 might mean October 30th 2016 at 2:30am in Paris or in New York. We can make datetimes timezone aware by calling the tz_localize() method: . temp_series_ny = temp_series.tz_localize(&quot;America/New_York&quot;) temp_series_ny . 2016-10-29 17:30:00-04:00 4.4 2016-10-29 18:30:00-04:00 5.1 2016-10-29 19:30:00-04:00 6.1 2016-10-29 20:30:00-04:00 6.2 2016-10-29 21:30:00-04:00 6.1 2016-10-29 22:30:00-04:00 6.1 2016-10-29 23:30:00-04:00 5.7 2016-10-30 00:30:00-04:00 5.2 2016-10-30 01:30:00-04:00 4.7 2016-10-30 02:30:00-04:00 4.1 2016-10-30 03:30:00-04:00 3.9 2016-10-30 04:30:00-04:00 3.5 Freq: H, dtype: float64 . Note that -04:00 is now appended to all the datetimes. This means that these datetimes refer to UTC - 4 hours. . We can convert these datetimes to Paris time like this: . temp_series_paris = temp_series_ny.tz_convert(&quot;Europe/Paris&quot;) temp_series_paris . 2016-10-29 23:30:00+02:00 4.4 2016-10-30 00:30:00+02:00 5.1 2016-10-30 01:30:00+02:00 6.1 2016-10-30 02:30:00+02:00 6.2 2016-10-30 02:30:00+01:00 6.1 2016-10-30 03:30:00+01:00 6.1 2016-10-30 04:30:00+01:00 5.7 2016-10-30 05:30:00+01:00 5.2 2016-10-30 06:30:00+01:00 4.7 2016-10-30 07:30:00+01:00 4.1 2016-10-30 08:30:00+01:00 3.9 2016-10-30 09:30:00+01:00 3.5 Freq: H, dtype: float64 . You may have noticed that the UTC offset changes from +02:00 to +01:00: this is because France switches to winter time at 3am that particular night (time goes back to 2am). Notice that 2:30am occurs twice! Let&#39;s go back to a naive representation (if you log some data hourly using local time, without storing the timezone, you might get something like this): . temp_series_paris_naive = temp_series_paris.tz_localize(None) temp_series_paris_naive . 2016-10-29 23:30:00 4.4 2016-10-30 00:30:00 5.1 2016-10-30 01:30:00 6.1 2016-10-30 02:30:00 6.2 2016-10-30 02:30:00 6.1 2016-10-30 03:30:00 6.1 2016-10-30 04:30:00 5.7 2016-10-30 05:30:00 5.2 2016-10-30 06:30:00 4.7 2016-10-30 07:30:00 4.1 2016-10-30 08:30:00 3.9 2016-10-30 09:30:00 3.5 Freq: H, dtype: float64 . Now 02:30 is really ambiguous. If we try to localize these naive datetimes to the Paris timezone, we get an error: . try: temp_series_paris_naive.tz_localize(&quot;Europe/Paris&quot;) except Exception as e: print(type(e)) print(e) . &lt;class &#39;pytz.exceptions.AmbiguousTimeError&#39;&gt; Cannot infer dst time from Timestamp(&#39;2016-10-30 02:30:00&#39;), try using the &#39;ambiguous&#39; argument . Fortunately using the ambiguous argument we can tell pandas to infer the right DST (Daylight Saving Time) based on the order of the ambiguous timestamps: . temp_series_paris_naive.tz_localize(&quot;Europe/Paris&quot;, ambiguous=&quot;infer&quot;) . 2016-10-29 23:30:00+02:00 4.4 2016-10-30 00:30:00+02:00 5.1 2016-10-30 01:30:00+02:00 6.1 2016-10-30 02:30:00+02:00 6.2 2016-10-30 02:30:00+01:00 6.1 2016-10-30 03:30:00+01:00 6.1 2016-10-30 04:30:00+01:00 5.7 2016-10-30 05:30:00+01:00 5.2 2016-10-30 06:30:00+01:00 4.7 2016-10-30 07:30:00+01:00 4.1 2016-10-30 08:30:00+01:00 3.9 2016-10-30 09:30:00+01:00 3.5 Freq: H, dtype: float64 . Periods . The pd.period_range() function returns a PeriodIndex instead of a DatetimeIndex. For example, let&#39;s get all quarters in 2016 and 2017: . quarters = pd.period_range(&#39;2016Q1&#39;, periods=8, freq=&#39;Q&#39;) quarters . PeriodIndex([&#39;2016Q1&#39;, &#39;2016Q2&#39;, &#39;2016Q3&#39;, &#39;2016Q4&#39;, &#39;2017Q1&#39;, &#39;2017Q2&#39;, &#39;2017Q3&#39;, &#39;2017Q4&#39;], dtype=&#39;period[Q-DEC]&#39;, freq=&#39;Q-DEC&#39;) . Adding a number N to a PeriodIndex shifts the periods by N times the PeriodIndex&#39;s frequency: . quarters + 3 . PeriodIndex([&#39;2016Q4&#39;, &#39;2017Q1&#39;, &#39;2017Q2&#39;, &#39;2017Q3&#39;, &#39;2017Q4&#39;, &#39;2018Q1&#39;, &#39;2018Q2&#39;, &#39;2018Q3&#39;], dtype=&#39;period[Q-DEC]&#39;, freq=&#39;Q-DEC&#39;) . The asfreq() method lets us change the frequency of the PeriodIndex. All periods are lengthened or shortened accordingly. For example, let&#39;s convert all the quarterly periods to monthly periods (zooming in): . quarters.asfreq(&quot;M&quot;) . PeriodIndex([&#39;2016-03&#39;, &#39;2016-06&#39;, &#39;2016-09&#39;, &#39;2016-12&#39;, &#39;2017-03&#39;, &#39;2017-06&#39;, &#39;2017-09&#39;, &#39;2017-12&#39;], dtype=&#39;period[M]&#39;, freq=&#39;M&#39;) . By default, the asfreq zooms on the end of each period. We can tell it to zoom on the start of each period instead: . quarters.asfreq(&quot;M&quot;, how=&quot;start&quot;) . PeriodIndex([&#39;2016-01&#39;, &#39;2016-04&#39;, &#39;2016-07&#39;, &#39;2016-10&#39;, &#39;2017-01&#39;, &#39;2017-04&#39;, &#39;2017-07&#39;, &#39;2017-10&#39;], dtype=&#39;period[M]&#39;, freq=&#39;M&#39;) . And we can zoom out: . quarters.asfreq(&quot;A&quot;) . PeriodIndex([&#39;2016&#39;, &#39;2016&#39;, &#39;2016&#39;, &#39;2016&#39;, &#39;2017&#39;, &#39;2017&#39;, &#39;2017&#39;, &#39;2017&#39;], dtype=&#39;period[A-DEC]&#39;, freq=&#39;A-DEC&#39;) . Of course we can create a Series with a PeriodIndex: . quarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters) quarterly_revenue . 2016Q1 300 2016Q2 320 2016Q3 290 2016Q4 390 2017Q1 320 2017Q2 360 2017Q3 310 2017Q4 410 Freq: Q-DEC, dtype: int64 . quarterly_revenue.plot(kind=&quot;line&quot;) plt.show() . We can convert periods to timestamps by calling to_timestamp. By default this will give us the first day of each period, but by setting how and freq, we can get the last hour of each period: . last_hours = quarterly_revenue.to_timestamp(how=&quot;end&quot;, freq=&quot;H&quot;) last_hours . 2016-03-31 23:00:00 300 2016-06-30 23:00:00 320 2016-09-30 23:00:00 290 2016-12-31 23:00:00 390 2017-03-31 23:00:00 320 2017-06-30 23:00:00 360 2017-09-30 23:00:00 310 2017-12-31 23:00:00 410 Freq: Q-DEC, dtype: int64 . And back to periods by calling to_period: . last_hours.to_period() . 2016Q1 300 2016Q2 320 2016Q3 290 2016Q4 390 2017Q1 320 2017Q2 360 2017Q3 310 2017Q4 410 Freq: Q-DEC, dtype: int64 . Pandas also provides many other time-related functions that we recommend you check out in the documentation. To whet your appetite, here is one way to get the last business day of each month in 2016, at 9am: . months_2016 = pd.period_range(&quot;2016&quot;, periods=12, freq=&quot;M&quot;) one_day_after_last_days = months_2016.asfreq(&quot;D&quot;) + 1 last_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay() last_bdays.to_period(&quot;H&quot;) + 9 . PeriodIndex([&#39;2016-01-29 09:00&#39;, &#39;2016-02-29 09:00&#39;, &#39;2016-03-31 09:00&#39;, &#39;2016-04-29 09:00&#39;, &#39;2016-05-31 09:00&#39;, &#39;2016-06-30 09:00&#39;, &#39;2016-07-29 09:00&#39;, &#39;2016-08-31 09:00&#39;, &#39;2016-09-30 09:00&#39;, &#39;2016-10-31 09:00&#39;, &#39;2016-11-30 09:00&#39;, &#39;2016-12-30 09:00&#39;], dtype=&#39;period[H]&#39;, freq=&#39;H&#39;) . DataFrame objects . A DataFrame object represents a spreadsheet, with cell values, column names and row index labels. You can define expressions to compute columns based on other columns, create pivot-tables, group rows, draw graphs, etc. You can see DataFrames as dictionaries of Series. . Creating a DataFrame . You can create a DataFrame by passing a dictionary of Series objects: . people_dict = { &quot;weight&quot;: pd.Series([68, 83, 112], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;]), &quot;birthyear&quot;: pd.Series([1984, 1985, 1992], index=[&quot;bob&quot;, &quot;alice&quot;, &quot;charles&quot;], name=&quot;year&quot;), &quot;children&quot;: pd.Series([0, 3], index=[&quot;charles&quot;, &quot;bob&quot;]), &quot;hobby&quot;: pd.Series([&quot;Biking&quot;, &quot;Dancing&quot;], index=[&quot;alice&quot;, &quot;bob&quot;]), } people = pd.DataFrame(people_dict) people . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3.0 | Dancing | 83 | . charles 1992 | 0.0 | NaN | 112 | . A few things to note: . the Series were automatically aligned based on their index, | missing values are represented as NaN, | Series names are ignored (the name &quot;year&quot; was dropped), | DataFrames are displayed nicely in Jupyter notebooks, woohoo! | . You can access columns pretty much as you would expect. They are returned as Series objects: . people[&quot;birthyear&quot;] . alice 1985 bob 1984 charles 1992 Name: birthyear, dtype: int64 . You can also get multiple columns at once: . people[[&quot;birthyear&quot;, &quot;hobby&quot;]] . birthyear hobby . alice 1985 | Biking | . bob 1984 | Dancing | . charles 1992 | NaN | . If you pass a list of columns and/or index row labels to the DataFrame constructor, it will guarantee that these columns and/or rows will exist, in that order, and no other column/row will exist. For example: . d2 = pd.DataFrame( people_dict, columns=[&quot;birthyear&quot;, &quot;weight&quot;, &quot;height&quot;], index=[&quot;bob&quot;, &quot;alice&quot;, &quot;eugene&quot;] ) d2 . birthyear weight height . bob 1984.0 | 83.0 | NaN | . alice 1985.0 | 68.0 | NaN | . eugene NaN | NaN | NaN | . Another convenient way to create a DataFrame is to pass all the values to the constructor as an ndarray, or a list of lists, and specify the column names and row index labels separately: . values = [ [1985, np.nan, &quot;Biking&quot;, 68], [1984, 3, &quot;Dancing&quot;, 83], [1992, 0, np.nan, 112] ] d3 = pd.DataFrame( values, columns=[&quot;birthyear&quot;, &quot;children&quot;, &quot;hobby&quot;, &quot;weight&quot;], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;] ) d3 . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3.0 | Dancing | 83 | . charles 1992 | 0.0 | NaN | 112 | . To specify missing values, you can either use np.nan or NumPy&#39;s masked arrays: . masked_array = np.ma.asarray(values, dtype=np.object) masked_array[(0, 2), (1, 2)] = np.ma.masked d3 = pd.DataFrame( masked_array, columns=[&quot;birthyear&quot;, &quot;children&quot;, &quot;hobby&quot;, &quot;weight&quot;], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;] ) d3 . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3 | Dancing | 83 | . charles 1992 | 0 | NaN | 112 | . Instead of an ndarray, you can also pass a DataFrame object: . d4 = pd.DataFrame( d3, columns=[&quot;hobby&quot;, &quot;children&quot;], index=[&quot;alice&quot;, &quot;bob&quot;] ) d4 . hobby children . alice Biking | NaN | . bob Dancing | 3 | . It is also possible to create a DataFrame with a dictionary (or list) of dictionaries (or list): . people = pd.DataFrame({ &quot;birthyear&quot;: {&quot;alice&quot;:1985, &quot;bob&quot;: 1984, &quot;charles&quot;: 1992}, &quot;hobby&quot;: {&quot;alice&quot;:&quot;Biking&quot;, &quot;bob&quot;: &quot;Dancing&quot;}, &quot;weight&quot;: {&quot;alice&quot;:68, &quot;bob&quot;: 83, &quot;charles&quot;: 112}, &quot;children&quot;: {&quot;bob&quot;: 3, &quot;charles&quot;: 0} }) people . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3.0 | Dancing | 83 | . charles 1992 | 0.0 | NaN | 112 | . Multi-indexing . If all columns are tuples of the same size, then they are understood as a multi-index. The same goes for row index labels. For example: . d5 = pd.DataFrame( { (&quot;public&quot;, &quot;birthyear&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):1985, (&quot;Paris&quot;,&quot;bob&quot;): 1984, (&quot;London&quot;,&quot;charles&quot;): 1992}, (&quot;public&quot;, &quot;hobby&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):&quot;Biking&quot;, (&quot;Paris&quot;,&quot;bob&quot;): &quot;Dancing&quot;}, (&quot;private&quot;, &quot;weight&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):68, (&quot;Paris&quot;,&quot;bob&quot;): 83, (&quot;London&quot;,&quot;charles&quot;): 112}, (&quot;private&quot;, &quot;children&quot;): {(&quot;Paris&quot;, &quot;alice&quot;):np.nan, (&quot;Paris&quot;,&quot;bob&quot;): 3, (&quot;London&quot;,&quot;charles&quot;): 0} } ) d5 . private public . children weight birthyear hobby . London charles 0.0 | 112 | 1992 | NaN | . Paris alice NaN | 68 | 1985 | Biking | . bob 3.0 | 83 | 1984 | Dancing | . You can now get a DataFrame containing all the &quot;public&quot; columns very simply: . d5[&quot;public&quot;] . birthyear hobby . London charles 1992 | NaN | . Paris alice 1985 | Biking | . bob 1984 | Dancing | . d5[&quot;public&quot;, &quot;hobby&quot;] # Same result as d5[&quot;public&quot;][&quot;hobby&quot;] . London charles NaN Paris alice Biking bob Dancing Name: (public, hobby), dtype: object . Dropping a level . Let&#39;s look at d5 again: . d5 . private public . children weight birthyear hobby . London charles 0.0 | 112 | 1992 | NaN | . Paris alice NaN | 68 | 1985 | Biking | . bob 3.0 | 83 | 1984 | Dancing | . There are two levels of columns, and two levels of indices. We can drop a column level by calling droplevel() (the same goes for indices): . d5.columns = d5.columns.droplevel(level = 0) d5 . children weight birthyear hobby . London charles 0.0 | 112 | 1992 | NaN | . Paris alice NaN | 68 | 1985 | Biking | . bob 3.0 | 83 | 1984 | Dancing | . Transposing . You can swap columns and indices using the T attribute: . d6 = d5.T d6 . London Paris . charles alice bob . children 0 | NaN | 3 | . weight 112 | 68 | 83 | . birthyear 1992 | 1985 | 1984 | . hobby NaN | Biking | Dancing | . Stacking and unstacking levels . Calling the stack() method will push the lowest column level after the lowest index: . d7 = d6.stack() d7 . London Paris . children bob NaN | 3 | . charles 0 | NaN | . weight alice NaN | 68 | . bob NaN | 83 | . charles 112 | NaN | . birthyear alice NaN | 1985 | . bob NaN | 1984 | . charles 1992 | NaN | . hobby alice NaN | Biking | . bob NaN | Dancing | . Note that many NaN values appeared. This makes sense because many new combinations did not exist before (eg. there was no bob in London). . Calling unstack() will do the reverse, once again creating many NaN values. . d8 = d7.unstack() d8 . London Paris . alice bob charles alice bob charles . children None | NaN | 0 | None | 3 | NaN | . weight NaN | NaN | 112 | 68 | 83 | NaN | . birthyear NaN | NaN | 1992 | 1985 | 1984 | NaN | . hobby NaN | NaN | None | Biking | Dancing | None | . If we call unstack again, we end up with a Series object: . d9 = d8.unstack() d9 . London alice children None weight NaN birthyear NaN hobby NaN bob children NaN weight NaN birthyear NaN hobby NaN charles children 0 weight 112 birthyear 1992 hobby None Paris alice children None weight 68 birthyear 1985 hobby Biking bob children 3 weight 83 birthyear 1984 hobby Dancing charles children NaN weight NaN birthyear NaN hobby None dtype: object . The stack() and unstack() methods let you select the level to stack/unstack. You can even stack/unstack multiple levels at once: . d10 = d9.unstack(level = (0,1)) d10 . London Paris . alice bob charles alice bob charles . children None | NaN | 0 | None | 3 | NaN | . weight NaN | NaN | 112 | 68 | 83 | NaN | . birthyear NaN | NaN | 1992 | 1985 | 1984 | NaN | . hobby NaN | NaN | None | Biking | Dancing | None | . Most methods return modified copies . As you may have noticed, the stack() and unstack() methods do not modify the object they apply to. Instead, they work on a copy and return that copy. This is true of most methods in pandas. . Accessing rows . Let&#39;s go back to the people DataFrame: . people . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3.0 | Dancing | 83 | . charles 1992 | 0.0 | NaN | 112 | . The loc attribute lets you access rows instead of columns. The result is a Series object in which the DataFrame&#39;s column names are mapped to row index labels: . people.loc[&quot;charles&quot;] . birthyear 1992 children 0 hobby NaN weight 112 Name: charles, dtype: object . You can also access rows by integer location using the iloc attribute: . people.iloc[2] . birthyear 1992 children 0 hobby NaN weight 112 Name: charles, dtype: object . You can also get a slice of rows, and this returns a DataFrame object: . people.iloc[1:3] . birthyear children hobby weight . bob 1984 | 3.0 | Dancing | 83 | . charles 1992 | 0.0 | NaN | 112 | . Finally, you can pass a boolean array to get the matching rows: . people[np.array([True, False, True])] . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . charles 1992 | 0.0 | NaN | 112 | . This is most useful when combined with boolean expressions: . people[people[&quot;birthyear&quot;] &lt; 1990] . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3.0 | Dancing | 83 | . Adding and removing columns . You can generally treat DataFrame objects like dictionaries of Series, so the following work fine: . people . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3.0 | Dancing | 83 | . charles 1992 | 0.0 | NaN | 112 | . people[&quot;age&quot;] = 2018 - people[&quot;birthyear&quot;] # adds a new column &quot;age&quot; people[&quot;over 30&quot;] = people[&quot;age&quot;] &gt; 30 # adds another column &quot;over 30&quot; birthyears = people.pop(&quot;birthyear&quot;) del people[&quot;children&quot;] people . hobby weight age over 30 . alice Biking | 68 | 33 | True | . bob Dancing | 83 | 34 | True | . charles NaN | 112 | 26 | False | . birthyears . alice 1985 bob 1984 charles 1992 Name: birthyear, dtype: int64 . When you add a new colum, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored: . people[&quot;pets&quot;] = pd.Series({&quot;bob&quot;: 0, &quot;charles&quot;: 5, &quot;eugene&quot;:1}) # alice is missing, eugene is ignored people . hobby weight age over 30 pets . alice Biking | 68 | 33 | True | NaN | . bob Dancing | 83 | 34 | True | 0.0 | . charles NaN | 112 | 26 | False | 5.0 | . When adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the insert() method: . people.insert(1, &quot;height&quot;, [172, 181, 185]) people . hobby height weight age over 30 pets . alice Biking | 172 | 68 | 33 | True | NaN | . bob Dancing | 181 | 83 | 34 | True | 0.0 | . charles NaN | 185 | 112 | 26 | False | 5.0 | . Assigning new columns . You can also create new columns by calling the assign() method. Note that this returns a new DataFrame object, the original is not modified: . people.assign( body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2, has_pets = people[&quot;pets&quot;] &gt; 0 ) . hobby height weight age over 30 pets body_mass_index has_pets . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . Note that you cannot access columns created within the same assignment: . try: people.assign( body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2, overweight = people[&quot;body_mass_index&quot;] &gt; 25 ) except KeyError as e: print(&quot;Key error:&quot;, e) . Key error: &#39;body_mass_index&#39; . The solution is to split this assignment in two consecutive assignments: . d6 = people.assign(body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2) d6.assign(overweight = d6[&quot;body_mass_index&quot;] &gt; 25) . hobby height weight age over 30 pets body_mass_index overweight . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | True | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . Having to create a temporary variable d6 is not very convenient. You may want to just chain the assigment calls, but it does not work because the people object is not actually modified by the first assignment: . try: (people .assign(body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2) .assign(overweight = people[&quot;body_mass_index&quot;] &gt; 25) ) except KeyError as e: print(&quot;Key error:&quot;, e) . Key error: &#39;body_mass_index&#39; . But fear not, there is a simple solution. You can pass a function to the assign() method (typically a lambda function), and this function will be called with the DataFrame as a parameter: . (people .assign(body_mass_index = lambda df: df[&quot;weight&quot;] / (df[&quot;height&quot;] / 100) ** 2) .assign(overweight = lambda df: df[&quot;body_mass_index&quot;] &gt; 25) ) . hobby height weight age over 30 pets body_mass_index overweight . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | True | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . Problem solved! . Evaluating an expression . A great feature supported by pandas is expression evaluation. This relies on the numexpr library which must be installed. . people.eval(&quot;weight / (height/100) ** 2 &gt; 25&quot;) . alice False bob True charles True dtype: bool . Assignment expressions are also supported. Let&#39;s set inplace=True to directly modify the DataFrame rather than getting a modified copy: . people.eval(&quot;body_mass_index = weight / (height/100) ** 2&quot;, inplace=True) people . hobby height weight age over 30 pets body_mass_index . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | . You can use a local or global variable in an expression by prefixing it with &#39;@&#39;: . overweight_threshold = 30 people.eval(&quot;overweight = body_mass_index &gt; @overweight_threshold&quot;, inplace=True) people . hobby height weight age over 30 pets body_mass_index overweight . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . Querying a DataFrame . The query() method lets you filter a DataFrame based on a query expression: . people.query(&quot;age &gt; 30 and pets == 0&quot;) . hobby height weight age over 30 pets body_mass_index overweight . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . Sorting a DataFrame . You can sort a DataFrame by calling its sort_index method. By default it sorts the rows by their index label, in ascending order, but let&#39;s reverse the order: . people.sort_index(ascending=False) . hobby height weight age over 30 pets body_mass_index overweight . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . Note that sort_index returned a sorted copy of the DataFrame. To modify people directly, we can set the inplace argument to True. Also, we can sort the columns instead of the rows by setting axis=1: . people.sort_index(axis=1, inplace=True) people . age body_mass_index height hobby over 30 overweight pets weight . alice 33 | 22.985398 | 172 | Biking | True | False | NaN | 68 | . bob 34 | 25.335002 | 181 | Dancing | True | False | 0.0 | 83 | . charles 26 | 32.724617 | 185 | NaN | False | True | 5.0 | 112 | . To sort the DataFrame by the values instead of the labels, we can use sort_values and specify the column to sort by: . people.sort_values(by=&quot;age&quot;, inplace=True) people . age body_mass_index height hobby over 30 overweight pets weight . charles 26 | 32.724617 | 185 | NaN | False | True | 5.0 | 112 | . alice 33 | 22.985398 | 172 | Biking | True | False | NaN | 68 | . bob 34 | 25.335002 | 181 | Dancing | True | False | 0.0 | 83 | . Plotting a DataFrame . Just like for Series, pandas makes it easy to draw nice graphs based on a DataFrame. . For example, it is trivial to create a line plot from a DataFrame&#39;s data by calling its plot method: . people.plot(kind = &quot;line&quot;, x = &quot;body_mass_index&quot;, y = [&quot;height&quot;, &quot;weight&quot;]) plt.show() . You can pass extra arguments supported by matplotlib&#39;s functions. For example, we can create scatterplot and pass it a list of sizes using the s argument of matplotlib&#39;s scatter() function: . people.plot(kind = &quot;scatter&quot;, x = &quot;height&quot;, y = &quot;weight&quot;, s=[40, 120, 200]) plt.show() . Again, there are way too many options to list here: the best option is to scroll through the Visualization page in pandas&#39; documentation, find the plot you are interested in and look at the example code. . Operations on DataFrames . Although DataFrames do not try to mimick NumPy arrays, there are a few similarities. Let&#39;s create a DataFrame to demonstrate this: . grades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]]) grades = pd.DataFrame(grades_array, columns=[&quot;sep&quot;, &quot;oct&quot;, &quot;nov&quot;], index=[&quot;alice&quot;,&quot;bob&quot;,&quot;charles&quot;,&quot;darwin&quot;]) grades . sep oct nov . alice 8 | 8 | 9 | . bob 10 | 9 | 9 | . charles 4 | 8 | 2 | . darwin 9 | 10 | 10 | . You can apply NumPy mathematical functions on a DataFrame: the function is applied to all values: . np.sqrt(grades) . sep oct nov . alice 2.828427 | 2.828427 | 3.000000 | . bob 3.162278 | 3.000000 | 3.000000 | . charles 2.000000 | 2.828427 | 1.414214 | . darwin 3.000000 | 3.162278 | 3.162278 | . Similarly, adding a single value to a DataFrame will add that value to all elements in the DataFrame. This is called broadcasting: . grades + 1 . sep oct nov . alice 9 | 9 | 10 | . bob 11 | 10 | 10 | . charles 5 | 9 | 3 | . darwin 10 | 11 | 11 | . Of course, the same is true for all other binary operations, including arithmetic (*,/,**...) and conditional (&gt;, ==...) operations: . grades &gt;= 5 . sep oct nov . alice True | True | True | . bob True | True | True | . charles False | True | False | . darwin True | True | True | . Aggregation operations, such as computing the max, the sum or the mean of a DataFrame, apply to each column, and you get back a Series object: . grades.mean() . sep 7.75 oct 8.75 nov 7.50 dtype: float64 . The all method is also an aggregation operation: it checks whether all values are True or not. Let&#39;s see during which months all students got a grade greater than 5: . (grades &gt; 5).all() . sep False oct True nov False dtype: bool . Most of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let&#39;s find out which students had all grades greater than 5: . (grades &gt; 5).all(axis = 1) . alice True bob True charles False darwin True dtype: bool . The any method returns True if any value is True. Let&#39;s see who got at least one grade 10: . (grades == 10).any(axis = 1) . alice False bob True charles False darwin True dtype: bool . If you add a Series object to a DataFrame (or execute any other binary operation), pandas attempts to broadcast the operation to all rows in the DataFrame. This only works if the Series has the same size as the DataFrames rows. For example, let&#39;s substract the mean of the DataFrame (a Series object) from the DataFrame: . grades - grades.mean() # equivalent to: grades - [7.75, 8.75, 7.50] . sep oct nov . alice 0.25 | -0.75 | 1.5 | . bob 2.25 | 0.25 | 1.5 | . charles -3.75 | -0.75 | -5.5 | . darwin 1.25 | 1.25 | 2.5 | . We substracted 7.75 from all September grades, 8.75 from October grades and 7.50 from November grades. It is equivalent to substracting this DataFrame: . pd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns) . sep oct nov . alice 7.75 | 8.75 | 7.5 | . bob 7.75 | 8.75 | 7.5 | . charles 7.75 | 8.75 | 7.5 | . darwin 7.75 | 8.75 | 7.5 | . If you want to substract the global mean from every grade, here is one way to do it: . grades - grades.values.mean() # substracts the global mean (8.00) from all grades . sep oct nov . alice 0.0 | 0.0 | 1.0 | . bob 2.0 | 1.0 | 1.0 | . charles -4.0 | 0.0 | -6.0 | . darwin 1.0 | 2.0 | 2.0 | . Automatic alignment . Similar to Series, when operating on multiple DataFrames, pandas automatically aligns them by row index label, but also by column names. Let&#39;s create a DataFrame with bonus points for each person from October to December: . bonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]]) bonus_points = pd.DataFrame(bonus_array, columns=[&quot;oct&quot;, &quot;nov&quot;, &quot;dec&quot;], index=[&quot;bob&quot;,&quot;colin&quot;, &quot;darwin&quot;, &quot;charles&quot;]) bonus_points . oct nov dec . bob 0.0 | NaN | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . grades + bonus_points . dec nov oct sep . alice NaN | NaN | NaN | NaN | . bob NaN | NaN | 9.0 | NaN | . charles NaN | 5.0 | 11.0 | NaN | . colin NaN | NaN | NaN | NaN | . darwin NaN | 11.0 | 10.0 | NaN | . Looks like the addition worked in some cases but way too many elements are now empty. That&#39;s because when aligning the DataFrames, some columns and rows were only present on one side, and thus they were considered missing on the other side (NaN). Then adding NaN to a number results in NaN, hence the result. . Handling missing data . Dealing with missing data is a frequent task when working with real life data. Pandas offers a few tools to handle missing data. . Let&#39;s try to fix the problem above. For example, we can decide that missing data should result in a zero, instead of NaN. We can replace all NaN values by a any value using the fillna() method: . (grades + bonus_points).fillna(0) . dec nov oct sep . alice 0.0 | 0.0 | 0.0 | 0.0 | . bob 0.0 | 0.0 | 9.0 | 0.0 | . charles 0.0 | 5.0 | 11.0 | 0.0 | . colin 0.0 | 0.0 | 0.0 | 0.0 | . darwin 0.0 | 11.0 | 10.0 | 0.0 | . It&#39;s a bit unfair that we&#39;re setting grades to zero in September, though. Perhaps we should decide that missing grades are missing grades, but missing bonus points should be replaced by zeros: . fixed_bonus_points = bonus_points.fillna(0) fixed_bonus_points.insert(0, &quot;sep&quot;, 0) fixed_bonus_points.loc[&quot;alice&quot;] = 0 grades + fixed_bonus_points . dec nov oct sep . alice NaN | 9.0 | 8.0 | 8.0 | . bob NaN | 9.0 | 9.0 | 10.0 | . charles NaN | 5.0 | 11.0 | 4.0 | . colin NaN | NaN | NaN | NaN | . darwin NaN | 11.0 | 10.0 | 9.0 | . That&#39;s much better: although we made up some data, we have not been too unfair. . Another way to handle missing data is to interpolate. Let&#39;s look at the bonus_points DataFrame again: . bonus_points . oct nov dec . bob 0.0 | NaN | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . Now let&#39;s call the interpolate method. By default, it interpolates vertically (axis=0), so let&#39;s tell it to interpolate horizontally (axis=1). . bonus_points.interpolate(axis=1) . oct nov dec . bob 0.0 | 1.0 | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . Bob had 0 bonus points in October, and 2 in December. When we interpolate for November, we get the mean: 1 bonus point. Colin had 1 bonus point in November, but we do not know how many bonus points he had in September, so we cannot interpolate, this is why there is still a missing value in October after interpolation. To fix this, we can set the September bonus points to 0 before interpolation. . better_bonus_points = bonus_points.copy() better_bonus_points.insert(0, &quot;sep&quot;, 0) better_bonus_points.loc[&quot;alice&quot;] = 0 better_bonus_points = better_bonus_points.interpolate(axis=1) better_bonus_points . sep oct nov dec . bob 0.0 | 0.0 | 1.0 | 2.0 | . colin 0.0 | 0.5 | 1.0 | 0.0 | . darwin 0.0 | 0.0 | 1.0 | 0.0 | . charles 0.0 | 3.0 | 3.0 | 0.0 | . alice 0.0 | 0.0 | 0.0 | 0.0 | . Great, now we have reasonable bonus points everywhere. Let&#39;s find out the final grades: . grades + better_bonus_points . dec nov oct sep . alice NaN | 9.0 | 8.0 | 8.0 | . bob NaN | 10.0 | 9.0 | 10.0 | . charles NaN | 5.0 | 11.0 | 4.0 | . colin NaN | NaN | NaN | NaN | . darwin NaN | 11.0 | 10.0 | 9.0 | . It is slightly annoying that the September column ends up on the right. This is because the DataFrames we are adding do not have the exact same columns (the grades DataFrame is missing the &quot;dec&quot; column), so to make things predictable, pandas orders the final columns alphabetically. To fix this, we can simply add the missing column before adding: . grades[&quot;dec&quot;] = np.nan final_grades = grades + better_bonus_points final_grades . sep oct nov dec . alice 8.0 | 8.0 | 9.0 | NaN | . bob 10.0 | 9.0 | 10.0 | NaN | . charles 4.0 | 11.0 | 5.0 | NaN | . colin NaN | NaN | NaN | NaN | . darwin 9.0 | 10.0 | 11.0 | NaN | . There&#39;s not much we can do about December and Colin: it&#39;s bad enough that we are making up bonus points, but we can&#39;t reasonably make up grades (well I guess some teachers probably do). So let&#39;s call the dropna() method to get rid of rows that are full of NaNs: . final_grades_clean = final_grades.dropna(how=&quot;all&quot;) final_grades_clean . sep oct nov dec . alice 8.0 | 8.0 | 9.0 | NaN | . bob 10.0 | 9.0 | 10.0 | NaN | . charles 4.0 | 11.0 | 5.0 | NaN | . darwin 9.0 | 10.0 | 11.0 | NaN | . Now let&#39;s remove columns that are full of NaNs by setting the axis argument to 1: . final_grades_clean = final_grades_clean.dropna(axis=1, how=&quot;all&quot;) final_grades_clean . sep oct nov . alice 8.0 | 8.0 | 9.0 | . bob 10.0 | 9.0 | 10.0 | . charles 4.0 | 11.0 | 5.0 | . darwin 9.0 | 10.0 | 11.0 | . Aggregating with groupby . Similar to the SQL language, pandas allows grouping your data into groups to run calculations over each group. . First, let&#39;s add some extra data about each person so we can group them, and let&#39;s go back to the final_grades DataFrame so we can see how NaN values are handled: . final_grades[&quot;hobby&quot;] = [&quot;Biking&quot;, &quot;Dancing&quot;, np.nan, &quot;Dancing&quot;, &quot;Biking&quot;] final_grades . sep oct nov dec hobby . alice 8.0 | 8.0 | 9.0 | NaN | Biking | . bob 10.0 | 9.0 | 10.0 | NaN | Dancing | . charles 4.0 | 11.0 | 5.0 | NaN | NaN | . colin NaN | NaN | NaN | NaN | Dancing | . darwin 9.0 | 10.0 | 11.0 | NaN | Biking | . Now let&#39;s group data in this DataFrame by hobby: . grouped_grades = final_grades.groupby(&quot;hobby&quot;) grouped_grades . &lt;pandas.core.groupby.DataFrameGroupBy object at 0x10b680e10&gt; . We are ready to compute the average grade per hobby: . grouped_grades.mean() . sep oct nov dec . hobby . Biking 8.5 | 9.0 | 10.0 | NaN | . Dancing 10.0 | 9.0 | 10.0 | NaN | . That was easy! Note that the NaN values have simply been skipped when computing the means. . Pivot tables . Pandas supports spreadsheet-like pivot tables that allow quick data summarization. To illustrate this, let&#39;s create a simple DataFrame: . bonus_points . oct nov dec . bob 0.0 | NaN | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . more_grades = final_grades_clean.stack().reset_index() more_grades.columns = [&quot;name&quot;, &quot;month&quot;, &quot;grade&quot;] more_grades[&quot;bonus&quot;] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0] more_grades . name month grade bonus . 0 alice | sep | 8.0 | NaN | . 1 alice | oct | 8.0 | NaN | . 2 alice | nov | 9.0 | NaN | . 3 bob | sep | 10.0 | 0.0 | . 4 bob | oct | 9.0 | NaN | . 5 bob | nov | 10.0 | 2.0 | . 6 charles | sep | 4.0 | 3.0 | . 7 charles | oct | 11.0 | 3.0 | . 8 charles | nov | 5.0 | 0.0 | . 9 darwin | sep | 9.0 | 0.0 | . 10 darwin | oct | 10.0 | 1.0 | . 11 darwin | nov | 11.0 | 0.0 | . Now we can call the pd.pivot_table() function for this DataFrame, asking to group by the name column. By default, pivot_table() computes the mean of each numeric column: . pd.pivot_table(more_grades, index=&quot;name&quot;) . bonus grade . name . alice NaN | 8.333333 | . bob 1.000000 | 9.666667 | . charles 2.000000 | 6.666667 | . darwin 0.333333 | 10.000000 | . We can change the aggregation function by setting the aggfunc argument, and we can also specify the list of columns whose values will be aggregated: . pd.pivot_table(more_grades, index=&quot;name&quot;, values=[&quot;grade&quot;,&quot;bonus&quot;], aggfunc=np.max) . bonus grade . name . alice NaN | 9.0 | . bob 2.0 | 10.0 | . charles 3.0 | 11.0 | . darwin 1.0 | 11.0 | . We can also specify the columns to aggregate over horizontally, and request the grand totals for each row and column by setting margins=True: . pd.pivot_table(more_grades, index=&quot;name&quot;, values=&quot;grade&quot;, columns=&quot;month&quot;, margins=True) . month nov oct sep All . name . alice 9.00 | 8.0 | 8.00 | 8.333333 | . bob 10.00 | 9.0 | 10.00 | 9.666667 | . charles 5.00 | 11.0 | 4.00 | 6.666667 | . darwin 11.00 | 10.0 | 9.00 | 10.000000 | . All 8.75 | 9.5 | 7.75 | 8.666667 | . Finally, we can specify multiple index or column names, and pandas will create multi-level indices: . pd.pivot_table(more_grades, index=(&quot;name&quot;, &quot;month&quot;), margins=True) . bonus grade . name month . alice nov NaN | 9.00 | . oct NaN | 8.00 | . sep NaN | 8.00 | . bob nov 2.000 | 10.00 | . oct NaN | 9.00 | . sep 0.000 | 10.00 | . charles nov 0.000 | 5.00 | . oct 3.000 | 11.00 | . sep 3.000 | 4.00 | . darwin nov 0.000 | 11.00 | . oct 1.000 | 10.00 | . sep 0.000 | 9.00 | . All 1.125 | 8.75 | . Overview functions . When dealing with large DataFrames, it is useful to get a quick overview of its content. Pandas offers a few functions for this. First, let&#39;s create a large DataFrame with a mix of numeric values, missing values and text values. Notice how Jupyter displays only the corners of the DataFrame: . much_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26)) large_df = pd.DataFrame(much_data, columns=list(&quot;ABCDEFGHIJKLMNOPQRSTUVWXYZ&quot;)) large_df[large_df % 16 == 0] = np.nan large_df.insert(3,&quot;some_text&quot;, &quot;Blabla&quot;) large_df . A B C some_text D E F G H I ... Q R S T U V W X Y Z . 0 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 1 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 2 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 3 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 4 44.0 | 55.0 | 88.0 | Blabla | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | ... | 55.0 | 44.0 | 55.0 | 88.0 | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | . 5 55.0 | 66.0 | 99.0 | Blabla | 154.0 | 44.0 | 143.0 | 77.0 | 33.0 | 11.0 | ... | 66.0 | 55.0 | 66.0 | 99.0 | 154.0 | 44.0 | 143.0 | 77.0 | 33.0 | 11.0 | . 6 66.0 | 77.0 | 110.0 | Blabla | 165.0 | 55.0 | 154.0 | 88.0 | 44.0 | 22.0 | ... | 77.0 | 66.0 | 77.0 | 110.0 | 165.0 | 55.0 | 154.0 | 88.0 | 44.0 | 22.0 | . 7 77.0 | 88.0 | 121.0 | Blabla | NaN | 66.0 | 165.0 | 99.0 | 55.0 | 33.0 | ... | 88.0 | 77.0 | 88.0 | 121.0 | NaN | 66.0 | 165.0 | 99.0 | 55.0 | 33.0 | . 8 88.0 | 99.0 | 132.0 | Blabla | NaN | 77.0 | NaN | 110.0 | 66.0 | 44.0 | ... | 99.0 | 88.0 | 99.0 | 132.0 | NaN | 77.0 | NaN | 110.0 | 66.0 | 44.0 | . 9 99.0 | 110.0 | 143.0 | Blabla | 11.0 | 88.0 | NaN | 121.0 | 77.0 | 55.0 | ... | 110.0 | 99.0 | 110.0 | 143.0 | 11.0 | 88.0 | NaN | 121.0 | 77.0 | 55.0 | . 10 110.0 | 121.0 | 154.0 | Blabla | 22.0 | 99.0 | 11.0 | 132.0 | 88.0 | 66.0 | ... | 121.0 | 110.0 | 121.0 | 154.0 | 22.0 | 99.0 | 11.0 | 132.0 | 88.0 | 66.0 | . 11 121.0 | 132.0 | 165.0 | Blabla | 33.0 | 110.0 | 22.0 | 143.0 | 99.0 | 77.0 | ... | 132.0 | 121.0 | 132.0 | 165.0 | 33.0 | 110.0 | 22.0 | 143.0 | 99.0 | 77.0 | . 12 132.0 | 143.0 | NaN | Blabla | 44.0 | 121.0 | 33.0 | 154.0 | 110.0 | 88.0 | ... | 143.0 | 132.0 | 143.0 | NaN | 44.0 | 121.0 | 33.0 | 154.0 | 110.0 | 88.0 | . 13 143.0 | 154.0 | NaN | Blabla | 55.0 | 132.0 | 44.0 | 165.0 | 121.0 | 99.0 | ... | 154.0 | 143.0 | 154.0 | NaN | 55.0 | 132.0 | 44.0 | 165.0 | 121.0 | 99.0 | . 14 154.0 | 165.0 | 11.0 | Blabla | 66.0 | 143.0 | 55.0 | NaN | 132.0 | 110.0 | ... | 165.0 | 154.0 | 165.0 | 11.0 | 66.0 | 143.0 | 55.0 | NaN | 132.0 | 110.0 | . 15 165.0 | NaN | 22.0 | Blabla | 77.0 | 154.0 | 66.0 | NaN | 143.0 | 121.0 | ... | NaN | 165.0 | NaN | 22.0 | 77.0 | 154.0 | 66.0 | NaN | 143.0 | 121.0 | . 16 NaN | NaN | 33.0 | Blabla | 88.0 | 165.0 | 77.0 | 11.0 | 154.0 | 132.0 | ... | NaN | NaN | NaN | 33.0 | 88.0 | 165.0 | 77.0 | 11.0 | 154.0 | 132.0 | . 17 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 18 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 19 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 20 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 21 44.0 | 55.0 | 88.0 | Blabla | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | ... | 55.0 | 44.0 | 55.0 | 88.0 | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | . 22 55.0 | 66.0 | 99.0 | Blabla | 154.0 | 44.0 | 143.0 | 77.0 | 33.0 | 11.0 | ... | 66.0 | 55.0 | 66.0 | 99.0 | 154.0 | 44.0 | 143.0 | 77.0 | 33.0 | 11.0 | . 23 66.0 | 77.0 | 110.0 | Blabla | 165.0 | 55.0 | 154.0 | 88.0 | 44.0 | 22.0 | ... | 77.0 | 66.0 | 77.0 | 110.0 | 165.0 | 55.0 | 154.0 | 88.0 | 44.0 | 22.0 | . 24 77.0 | 88.0 | 121.0 | Blabla | NaN | 66.0 | 165.0 | 99.0 | 55.0 | 33.0 | ... | 88.0 | 77.0 | 88.0 | 121.0 | NaN | 66.0 | 165.0 | 99.0 | 55.0 | 33.0 | . 25 88.0 | 99.0 | 132.0 | Blabla | NaN | 77.0 | NaN | 110.0 | 66.0 | 44.0 | ... | 99.0 | 88.0 | 99.0 | 132.0 | NaN | 77.0 | NaN | 110.0 | 66.0 | 44.0 | . 26 99.0 | 110.0 | 143.0 | Blabla | 11.0 | 88.0 | NaN | 121.0 | 77.0 | 55.0 | ... | 110.0 | 99.0 | 110.0 | 143.0 | 11.0 | 88.0 | NaN | 121.0 | 77.0 | 55.0 | . 27 110.0 | 121.0 | 154.0 | Blabla | 22.0 | 99.0 | 11.0 | 132.0 | 88.0 | 66.0 | ... | 121.0 | 110.0 | 121.0 | 154.0 | 22.0 | 99.0 | 11.0 | 132.0 | 88.0 | 66.0 | . 28 121.0 | 132.0 | 165.0 | Blabla | 33.0 | 110.0 | 22.0 | 143.0 | 99.0 | 77.0 | ... | 132.0 | 121.0 | 132.0 | 165.0 | 33.0 | 110.0 | 22.0 | 143.0 | 99.0 | 77.0 | . 29 132.0 | 143.0 | NaN | Blabla | 44.0 | 121.0 | 33.0 | 154.0 | 110.0 | 88.0 | ... | 143.0 | 132.0 | 143.0 | NaN | 44.0 | 121.0 | 33.0 | 154.0 | 110.0 | 88.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 9970 88.0 | 99.0 | 132.0 | Blabla | NaN | 77.0 | NaN | 110.0 | 66.0 | 44.0 | ... | 99.0 | 88.0 | 99.0 | 132.0 | NaN | 77.0 | NaN | 110.0 | 66.0 | 44.0 | . 9971 99.0 | 110.0 | 143.0 | Blabla | 11.0 | 88.0 | NaN | 121.0 | 77.0 | 55.0 | ... | 110.0 | 99.0 | 110.0 | 143.0 | 11.0 | 88.0 | NaN | 121.0 | 77.0 | 55.0 | . 9972 110.0 | 121.0 | 154.0 | Blabla | 22.0 | 99.0 | 11.0 | 132.0 | 88.0 | 66.0 | ... | 121.0 | 110.0 | 121.0 | 154.0 | 22.0 | 99.0 | 11.0 | 132.0 | 88.0 | 66.0 | . 9973 121.0 | 132.0 | 165.0 | Blabla | 33.0 | 110.0 | 22.0 | 143.0 | 99.0 | 77.0 | ... | 132.0 | 121.0 | 132.0 | 165.0 | 33.0 | 110.0 | 22.0 | 143.0 | 99.0 | 77.0 | . 9974 132.0 | 143.0 | NaN | Blabla | 44.0 | 121.0 | 33.0 | 154.0 | 110.0 | 88.0 | ... | 143.0 | 132.0 | 143.0 | NaN | 44.0 | 121.0 | 33.0 | 154.0 | 110.0 | 88.0 | . 9975 143.0 | 154.0 | NaN | Blabla | 55.0 | 132.0 | 44.0 | 165.0 | 121.0 | 99.0 | ... | 154.0 | 143.0 | 154.0 | NaN | 55.0 | 132.0 | 44.0 | 165.0 | 121.0 | 99.0 | . 9976 154.0 | 165.0 | 11.0 | Blabla | 66.0 | 143.0 | 55.0 | NaN | 132.0 | 110.0 | ... | 165.0 | 154.0 | 165.0 | 11.0 | 66.0 | 143.0 | 55.0 | NaN | 132.0 | 110.0 | . 9977 165.0 | NaN | 22.0 | Blabla | 77.0 | 154.0 | 66.0 | NaN | 143.0 | 121.0 | ... | NaN | 165.0 | NaN | 22.0 | 77.0 | 154.0 | 66.0 | NaN | 143.0 | 121.0 | . 9978 NaN | NaN | 33.0 | Blabla | 88.0 | 165.0 | 77.0 | 11.0 | 154.0 | 132.0 | ... | NaN | NaN | NaN | 33.0 | 88.0 | 165.0 | 77.0 | 11.0 | 154.0 | 132.0 | . 9979 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 9980 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 9981 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 9982 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 9983 44.0 | 55.0 | 88.0 | Blabla | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | ... | 55.0 | 44.0 | 55.0 | 88.0 | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | . 9984 55.0 | 66.0 | 99.0 | Blabla | 154.0 | 44.0 | 143.0 | 77.0 | 33.0 | 11.0 | ... | 66.0 | 55.0 | 66.0 | 99.0 | 154.0 | 44.0 | 143.0 | 77.0 | 33.0 | 11.0 | . 9985 66.0 | 77.0 | 110.0 | Blabla | 165.0 | 55.0 | 154.0 | 88.0 | 44.0 | 22.0 | ... | 77.0 | 66.0 | 77.0 | 110.0 | 165.0 | 55.0 | 154.0 | 88.0 | 44.0 | 22.0 | . 9986 77.0 | 88.0 | 121.0 | Blabla | NaN | 66.0 | 165.0 | 99.0 | 55.0 | 33.0 | ... | 88.0 | 77.0 | 88.0 | 121.0 | NaN | 66.0 | 165.0 | 99.0 | 55.0 | 33.0 | . 9987 88.0 | 99.0 | 132.0 | Blabla | NaN | 77.0 | NaN | 110.0 | 66.0 | 44.0 | ... | 99.0 | 88.0 | 99.0 | 132.0 | NaN | 77.0 | NaN | 110.0 | 66.0 | 44.0 | . 9988 99.0 | 110.0 | 143.0 | Blabla | 11.0 | 88.0 | NaN | 121.0 | 77.0 | 55.0 | ... | 110.0 | 99.0 | 110.0 | 143.0 | 11.0 | 88.0 | NaN | 121.0 | 77.0 | 55.0 | . 9989 110.0 | 121.0 | 154.0 | Blabla | 22.0 | 99.0 | 11.0 | 132.0 | 88.0 | 66.0 | ... | 121.0 | 110.0 | 121.0 | 154.0 | 22.0 | 99.0 | 11.0 | 132.0 | 88.0 | 66.0 | . 9990 121.0 | 132.0 | 165.0 | Blabla | 33.0 | 110.0 | 22.0 | 143.0 | 99.0 | 77.0 | ... | 132.0 | 121.0 | 132.0 | 165.0 | 33.0 | 110.0 | 22.0 | 143.0 | 99.0 | 77.0 | . 9991 132.0 | 143.0 | NaN | Blabla | 44.0 | 121.0 | 33.0 | 154.0 | 110.0 | 88.0 | ... | 143.0 | 132.0 | 143.0 | NaN | 44.0 | 121.0 | 33.0 | 154.0 | 110.0 | 88.0 | . 9992 143.0 | 154.0 | NaN | Blabla | 55.0 | 132.0 | 44.0 | 165.0 | 121.0 | 99.0 | ... | 154.0 | 143.0 | 154.0 | NaN | 55.0 | 132.0 | 44.0 | 165.0 | 121.0 | 99.0 | . 9993 154.0 | 165.0 | 11.0 | Blabla | 66.0 | 143.0 | 55.0 | NaN | 132.0 | 110.0 | ... | 165.0 | 154.0 | 165.0 | 11.0 | 66.0 | 143.0 | 55.0 | NaN | 132.0 | 110.0 | . 9994 165.0 | NaN | 22.0 | Blabla | 77.0 | 154.0 | 66.0 | NaN | 143.0 | 121.0 | ... | NaN | 165.0 | NaN | 22.0 | 77.0 | 154.0 | 66.0 | NaN | 143.0 | 121.0 | . 9995 NaN | NaN | 33.0 | Blabla | 88.0 | 165.0 | 77.0 | 11.0 | 154.0 | 132.0 | ... | NaN | NaN | NaN | 33.0 | 88.0 | 165.0 | 77.0 | 11.0 | 154.0 | 132.0 | . 9996 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 9997 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 9998 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 9999 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 10000 rows Ã— 27 columns . The head() method returns the top 5 rows: . large_df.head() . A B C some_text D E F G H I ... Q R S T U V W X Y Z . 0 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 1 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 2 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 3 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 4 44.0 | 55.0 | 88.0 | Blabla | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | ... | 55.0 | 44.0 | 55.0 | 88.0 | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | . 5 rows Ã— 27 columns . Of course there&#39;s also a tail() function to view the bottom 5 rows. You can pass the number of rows you want: . large_df.tail(n=2) . A B C some_text D E F G H I ... Q R S T U V W X Y Z . 9998 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 9999 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 2 rows Ã— 27 columns . The info() method prints out a summary of each columns contents: . large_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10000 entries, 0 to 9999 Data columns (total 27 columns): A 8823 non-null float64 B 8824 non-null float64 C 8824 non-null float64 some_text 10000 non-null object D 8824 non-null float64 E 8822 non-null float64 F 8824 non-null float64 G 8824 non-null float64 H 8822 non-null float64 I 8823 non-null float64 J 8823 non-null float64 K 8822 non-null float64 L 8824 non-null float64 M 8824 non-null float64 N 8822 non-null float64 O 8824 non-null float64 P 8824 non-null float64 Q 8824 non-null float64 R 8823 non-null float64 S 8824 non-null float64 T 8824 non-null float64 U 8824 non-null float64 V 8822 non-null float64 W 8824 non-null float64 X 8824 non-null float64 Y 8822 non-null float64 Z 8823 non-null float64 dtypes: float64(26), object(1) memory usage: 2.1+ MB . Finally, the describe() method gives a nice overview of the main aggregated values over each column: . count: number of non-null (not NaN) values | mean: mean of non-null values | std: standard deviation of non-null values | min: minimum of non-null values | 25%, 50%, 75%: 25th, 50th and 75th percentile of non-null values | max: maximum of non-null values | . large_df.describe() . A B C D E F G H I J ... Q R S T U V W X Y Z . count 8823.000000 | 8824.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8823.000000 | 8823.000000 | ... | 8824.000000 | 8823.000000 | 8824.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8823.000000 | . mean 87.977559 | 87.972575 | 87.987534 | 88.012466 | 87.983791 | 88.007480 | 87.977561 | 88.000000 | 88.022441 | 88.022441 | ... | 87.972575 | 87.977559 | 87.972575 | 87.987534 | 88.012466 | 87.983791 | 88.007480 | 87.977561 | 88.000000 | 88.022441 | . std 47.535911 | 47.535523 | 47.521679 | 47.521679 | 47.535001 | 47.519371 | 47.529755 | 47.536879 | 47.535911 | 47.535911 | ... | 47.535523 | 47.535911 | 47.535523 | 47.521679 | 47.521679 | 47.535001 | 47.519371 | 47.529755 | 47.536879 | 47.535911 | . min 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | ... | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | . 25% 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | ... | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | . 50% 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | ... | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | . 75% 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | ... | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | . max 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | ... | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | . 8 rows Ã— 26 columns . Saving &amp; loading . Pandas can save DataFrames to various backends, including file formats such as CSV, Excel, JSON, HTML and HDF5, or to a SQL database. Let&#39;s create a DataFrame to demonstrate this: . my_df = pd.DataFrame( [[&quot;Biking&quot;, 68.5, 1985, np.nan], [&quot;Dancing&quot;, 83.1, 1984, 3]], columns=[&quot;hobby&quot;,&quot;weight&quot;,&quot;birthyear&quot;,&quot;children&quot;], index=[&quot;alice&quot;, &quot;bob&quot;] ) my_df . hobby weight birthyear children . alice Biking | 68.5 | 1985 | NaN | . bob Dancing | 83.1 | 1984 | 3.0 | . Saving . Let&#39;s save it to CSV, HTML and JSON: . my_df.to_csv(&quot;my_df.csv&quot;) my_df.to_html(&quot;my_df.html&quot;) my_df.to_json(&quot;my_df.json&quot;) . Done! Let&#39;s take a peek at what was saved: . for filename in (&quot;my_df.csv&quot;, &quot;my_df.html&quot;, &quot;my_df.json&quot;): print(&quot;#&quot;, filename) with open(filename, &quot;rt&quot;) as f: print(f.read()) print() . # my_df.csv ,hobby,weight,birthyear,children alice,Biking,68.5,1985, bob,Dancing,83.1,1984,3.0 # my_df.html &lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt; &lt;thead&gt; &lt;tr style=&#34;text-align: right;&#34;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;hobby&lt;/th&gt; &lt;th&gt;weight&lt;/th&gt; &lt;th&gt;birthyear&lt;/th&gt; &lt;th&gt;children&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;alice&lt;/th&gt; &lt;td&gt;Biking&lt;/td&gt; &lt;td&gt;68.5&lt;/td&gt; &lt;td&gt;1985&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;bob&lt;/th&gt; &lt;td&gt;Dancing&lt;/td&gt; &lt;td&gt;83.1&lt;/td&gt; &lt;td&gt;1984&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; # my_df.json {&#34;hobby&#34;:{&#34;alice&#34;:&#34;Biking&#34;,&#34;bob&#34;:&#34;Dancing&#34;},&#34;weight&#34;:{&#34;alice&#34;:68.5,&#34;bob&#34;:83.1},&#34;birthyear&#34;:{&#34;alice&#34;:1985,&#34;bob&#34;:1984},&#34;children&#34;:{&#34;alice&#34;:null,&#34;bob&#34;:3.0}} . Note that the index is saved as the first column (with no name) in a CSV file, as &lt;th&gt; tags in HTML and as keys in JSON. . Saving to other formats works very similarly, but some formats require extra libraries to be installed. For example, saving to Excel requires the openpyxl library: . try: my_df.to_excel(&quot;my_df.xlsx&quot;, sheet_name=&#39;People&#39;) except ImportError as e: print(e) . No module named &#39;openpyxl&#39; . Loading . Now let&#39;s load our CSV file back into a DataFrame: . my_df_loaded = pd.read_csv(&quot;my_df.csv&quot;, index_col=0) my_df_loaded . hobby weight birthyear children . alice Biking | 68.5 | 1985 | NaN | . bob Dancing | 83.1 | 1984 | 3.0 | . As you might guess, there are similar read_json, read_html, read_excel functions as well. We can also read data straight from the Internet. For example, let&#39;s load all U.S. cities from simplemaps.com: . us_cities = None try: csv_url = &quot;http://simplemaps.com/files/cities.csv&quot; us_cities = pd.read_csv(csv_url, index_col=0) us_cities = us_cities.head() except IOError as e: print(e) us_cities . &lt;urlopen error [Errno 8] nodename nor servname provided, or not known&gt; . There are more options available, in particular regarding datetime format. Check out the documentation for more details. . Combining DataFrames . SQL-like joins . One powerful feature of pandas is it&#39;s ability to perform SQL-like joins on DataFrames. Various types of joins are supported: inner joins, left/right outer joins and full joins. To illustrate this, let&#39;s start by creating a couple simple DataFrames: . city_loc = pd.DataFrame( [ [&quot;CA&quot;, &quot;San Francisco&quot;, 37.781334, -122.416728], [&quot;NY&quot;, &quot;New York&quot;, 40.705649, -74.008344], [&quot;FL&quot;, &quot;Miami&quot;, 25.791100, -80.320733], [&quot;OH&quot;, &quot;Cleveland&quot;, 41.473508, -81.739791], [&quot;UT&quot;, &quot;Salt Lake City&quot;, 40.755851, -111.896657] ], columns=[&quot;state&quot;, &quot;city&quot;, &quot;lat&quot;, &quot;lng&quot;]) city_loc . state city lat lng . 0 CA | San Francisco | 37.781334 | -122.416728 | . 1 NY | New York | 40.705649 | -74.008344 | . 2 FL | Miami | 25.791100 | -80.320733 | . 3 OH | Cleveland | 41.473508 | -81.739791 | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | . city_pop = pd.DataFrame( [ [808976, &quot;San Francisco&quot;, &quot;California&quot;], [8363710, &quot;New York&quot;, &quot;New-York&quot;], [413201, &quot;Miami&quot;, &quot;Florida&quot;], [2242193, &quot;Houston&quot;, &quot;Texas&quot;] ], index=[3,4,5,6], columns=[&quot;population&quot;, &quot;city&quot;, &quot;state&quot;]) city_pop . population city state . 3 808976 | San Francisco | California | . 4 8363710 | New York | New-York | . 5 413201 | Miami | Florida | . 6 2242193 | Houston | Texas | . Now let&#39;s join these DataFrames using the merge() function: . pd.merge(left=city_loc, right=city_pop, on=&quot;city&quot;) . state_x city lat lng population state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976 | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710 | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201 | Florida | . Note that both DataFrames have a column named state, so in the result they got renamed to state_x and state_y. . Also, note that Cleveland, Salt Lake City and Houston were dropped because they don&#39;t exist in both DataFrames. This is the equivalent of a SQL INNER JOIN. If you want a FULL OUTER JOIN, where no city gets dropped and NaN values are added, you must specify how=&quot;outer&quot;: . all_cities = pd.merge(left=city_loc, right=city_pop, on=&quot;city&quot;, how=&quot;outer&quot;) all_cities . state_x city lat lng population state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976.0 | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710.0 | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201.0 | Florida | . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | NaN | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | NaN | NaN | . 5 NaN | Houston | NaN | NaN | 2242193.0 | Texas | . Of course LEFT OUTER JOIN is also available by setting how=&quot;left&quot;: only the cities present in the left DataFrame end up in the result. Similarly, with how=&quot;right&quot; only cities in the right DataFrame appear in the result. For example: . pd.merge(left=city_loc, right=city_pop, on=&quot;city&quot;, how=&quot;right&quot;) . state_x city lat lng population state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976 | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710 | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201 | Florida | . 3 NaN | Houston | NaN | NaN | 2242193 | Texas | . If the key to join on is actually in one (or both) DataFrame&#39;s index, you must use left_index=True and/or right_index=True. If the key column names differ, you must use left_on and right_on. For example: . city_pop2 = city_pop.copy() city_pop2.columns = [&quot;population&quot;, &quot;name&quot;, &quot;state&quot;] pd.merge(left=city_loc, right=city_pop2, left_on=&quot;city&quot;, right_on=&quot;name&quot;) . state_x city lat lng population name state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976 | San Francisco | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710 | New York | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201 | Miami | Florida | . Concatenation . Rather than joining DataFrames, we may just want to concatenate them. That&#39;s what concat() is for: . result_concat = pd.concat([city_loc, city_pop]) result_concat . city lat lng population state . 0 San Francisco | 37.781334 | -122.416728 | NaN | CA | . 1 New York | 40.705649 | -74.008344 | NaN | NY | . 2 Miami | 25.791100 | -80.320733 | NaN | FL | . 3 Cleveland | 41.473508 | -81.739791 | NaN | OH | . 4 Salt Lake City | 40.755851 | -111.896657 | NaN | UT | . 3 San Francisco | NaN | NaN | 808976.0 | California | . 4 New York | NaN | NaN | 8363710.0 | New-York | . 5 Miami | NaN | NaN | 413201.0 | Florida | . 6 Houston | NaN | NaN | 2242193.0 | Texas | . Note that this operation aligned the data horizontally (by columns) but not vertically (by rows). In this example, we end up with multiple rows having the same index (eg. 3). Pandas handles this rather gracefully: . result_concat.loc[3] . city lat lng population state . 3 Cleveland | 41.473508 | -81.739791 | NaN | OH | . 3 San Francisco | NaN | NaN | 808976.0 | California | . Or you can tell pandas to just ignore the index: . pd.concat([city_loc, city_pop], ignore_index=True) . city lat lng population state . 0 San Francisco | 37.781334 | -122.416728 | NaN | CA | . 1 New York | 40.705649 | -74.008344 | NaN | NY | . 2 Miami | 25.791100 | -80.320733 | NaN | FL | . 3 Cleveland | 41.473508 | -81.739791 | NaN | OH | . 4 Salt Lake City | 40.755851 | -111.896657 | NaN | UT | . 5 San Francisco | NaN | NaN | 808976.0 | California | . 6 New York | NaN | NaN | 8363710.0 | New-York | . 7 Miami | NaN | NaN | 413201.0 | Florida | . 8 Houston | NaN | NaN | 2242193.0 | Texas | . Notice that when a column does not exist in a DataFrame, it acts as if it was filled with NaN values. If we set join=&quot;inner&quot;, then only columns that exist in both DataFrames are returned: . pd.concat([city_loc, city_pop], join=&quot;inner&quot;) . state city . 0 CA | San Francisco | . 1 NY | New York | . 2 FL | Miami | . 3 OH | Cleveland | . 4 UT | Salt Lake City | . 3 California | San Francisco | . 4 New-York | New York | . 5 Florida | Miami | . 6 Texas | Houston | . You can concatenate DataFrames horizontally instead of vertically by setting axis=1: . pd.concat([city_loc, city_pop], axis=1) . state city lat lng population city state . 0 CA | San Francisco | 37.781334 | -122.416728 | NaN | NaN | NaN | . 1 NY | New York | 40.705649 | -74.008344 | NaN | NaN | NaN | . 2 FL | Miami | 25.791100 | -80.320733 | NaN | NaN | NaN | . 3 OH | Cleveland | 41.473508 | -81.739791 | 808976.0 | San Francisco | California | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | 8363710.0 | New York | New-York | . 5 NaN | NaN | NaN | NaN | 413201.0 | Miami | Florida | . 6 NaN | NaN | NaN | NaN | 2242193.0 | Houston | Texas | . In this case it really does not make much sense because the indices do not align well (eg. Cleveland and San Francisco end up on the same row, because they shared the index label 3). So let&#39;s reindex the DataFrames by city name before concatenating: . pd.concat([city_loc.set_index(&quot;city&quot;), city_pop.set_index(&quot;city&quot;)], axis=1) . state lat lng population state . Cleveland OH | 41.473508 | -81.739791 | NaN | NaN | . Houston NaN | NaN | NaN | 2242193.0 | Texas | . Miami FL | 25.791100 | -80.320733 | 413201.0 | Florida | . New York NY | 40.705649 | -74.008344 | 8363710.0 | New-York | . Salt Lake City UT | 40.755851 | -111.896657 | NaN | NaN | . San Francisco CA | 37.781334 | -122.416728 | 808976.0 | California | . This looks a lot like a FULL OUTER JOIN, except that the state columns were not renamed to state_x and state_y, and the city column is now the index. . The append() method is a useful shorthand for concatenating DataFrames vertically: . city_loc.append(city_pop) . city lat lng population state . 0 San Francisco | 37.781334 | -122.416728 | NaN | CA | . 1 New York | 40.705649 | -74.008344 | NaN | NY | . 2 Miami | 25.791100 | -80.320733 | NaN | FL | . 3 Cleveland | 41.473508 | -81.739791 | NaN | OH | . 4 Salt Lake City | 40.755851 | -111.896657 | NaN | UT | . 3 San Francisco | NaN | NaN | 808976.0 | California | . 4 New York | NaN | NaN | 8363710.0 | New-York | . 5 Miami | NaN | NaN | 413201.0 | Florida | . 6 Houston | NaN | NaN | 2242193.0 | Texas | . As always in pandas, the append() method does not actually modify city_loc: it works on a copy and returns the modified copy. . Categories . It is quite frequent to have values that represent categories, for example 1 for female and 2 for male, or &quot;A&quot; for Good, &quot;B&quot; for Average, &quot;C&quot; for Bad. These categorical values can be hard to read and cumbersome to handle, but fortunately pandas makes it easy. To illustrate this, let&#39;s take the city_pop DataFrame we created earlier, and add a column that represents a category: . city_eco = city_pop.copy() city_eco[&quot;eco_code&quot;] = [17, 17, 34, 20] city_eco . population city state eco_code . 3 808976 | San Francisco | California | 17 | . 4 8363710 | New York | New-York | 17 | . 5 413201 | Miami | Florida | 34 | . 6 2242193 | Houston | Texas | 20 | . Right now the eco_code column is full of apparently meaningless codes. Let&#39;s fix that. First, we will create a new categorical column based on the eco_codes: . city_eco[&quot;economy&quot;] = city_eco[&quot;eco_code&quot;].astype(&#39;category&#39;) city_eco[&quot;economy&quot;].cat.categories . Int64Index([17, 20, 34], dtype=&#39;int64&#39;) . Now we can give each category a meaningful name: . city_eco[&quot;economy&quot;].cat.categories = [&quot;Finance&quot;, &quot;Energy&quot;, &quot;Tourism&quot;] city_eco . population city state eco_code economy . 3 808976 | San Francisco | California | 17 | Finance | . 4 8363710 | New York | New-York | 17 | Finance | . 5 413201 | Miami | Florida | 34 | Tourism | . 6 2242193 | Houston | Texas | 20 | Energy | . Note that categorical values are sorted according to their categorical order, not their alphabetical order: . city_eco.sort_values(by=&quot;economy&quot;, ascending=False) . population city state eco_code economy . 5 413201 | Miami | Florida | 34 | Tourism | . 6 2242193 | Houston | Texas | 20 | Energy | . 4 8363710 | New York | New-York | 17 | Finance | . 3 808976 | San Francisco | California | 17 | Finance | . What next? . As you probably noticed by now, pandas is quite a large library with many features. Although we went through the most important features, there is still a lot to discover. Probably the best way to learn more is to get your hands dirty with some real-life data. It is also a good idea to go through pandas&#39; excellent documentation, in particular the Cookbook. .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/tools_pandas.html",
            "relUrl": "/2020/03/09/tools_pandas.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Tools - NumPy . permalink: /tools_numpy | . NumPy is the fundamental library for scientific computing with Python. NumPy is centered around a powerful N-dimensional array object, and it also contains useful linear algebra, Fourier transform, and random number functions. . Creating arrays . Now let&#39;s import numpy. Most people import it as np: . import numpy as np . np.zeros . The zeros function creates an array containing any number of zeros: . np.zeros(5) . array([ 0., 0., 0., 0., 0.]) . It&#39;s just as easy to create a 2D array (ie. a matrix) by providing a tuple with the desired number of rows and columns. For example, here&#39;s a 3x4 matrix: . np.zeros((3,4)) . array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) . Some vocabulary . In NumPy, each dimension is called an axis. | The number of axes is called the rank. For example, the above 3x4 matrix is an array of rank 2 (it is 2-dimensional). | The first axis has length 3, the second has length 4. | . | An array&#39;s list of axis lengths is called the shape of the array. For example, the above matrix&#39;s shape is (3, 4). | The rank is equal to the shape&#39;s length. | . | The size of an array is the total number of elements, which is the product of all axis lengths (eg. 3*4=12) | . a = np.zeros((3,4)) a . array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) . a.shape . (3, 4) . a.ndim # equal to len(a.shape) . 2 . a.size . 12 . N-dimensional arrays . You can also create an N-dimensional array of arbitrary rank. For example, here&#39;s a 3D array (rank=3), with shape (2,3,4): . np.zeros((2,3,4)) . array([[[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]]) . Array type . NumPy arrays have the type ndarrays: . type(np.zeros((3,4))) . numpy.ndarray . np.ones . Many other NumPy functions create ndarrays. . Here&#39;s a 3x4 matrix full of ones: . np.ones((3,4)) . array([[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 1., 1., 1., 1.]]) . np.full . Creates an array of the given shape initialized with the given value. Here&#39;s a 3x4 matrix full of Ï€. . np.full((3,4), np.pi) . array([[ 3.14159265, 3.14159265, 3.14159265, 3.14159265], [ 3.14159265, 3.14159265, 3.14159265, 3.14159265], [ 3.14159265, 3.14159265, 3.14159265, 3.14159265]]) . np.empty . An uninitialized 2x3 array (its content is not predictable, as it is whatever is in memory at that point): . np.empty((2,3)) . array([[ 2.68156159e+154, -2.32035135e+077, 2.22457274e-314], [ 2.24336635e-314, 2.23817082e-314, 4.17203587e-309]]) . np.array . Of course you can initialize an ndarray using a regular python array. Just call the array function: . np.array([[1,2,3,4], [10, 20, 30, 40]]) . array([[ 1, 2, 3, 4], [10, 20, 30, 40]]) . np.arange . You can create an ndarray using NumPy&#39;s range function, which is similar to python&#39;s built-in range function: . np.arange(1, 5) . array([1, 2, 3, 4]) . It also works with floats: . np.arange(1.0, 5.0) . array([ 1., 2., 3., 4.]) . Of course you can provide a step parameter: . np.arange(1, 5, 0.5) . array([ 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5]) . However, when dealing with floats, the exact number of elements in the array is not always predictible. For example, consider this: . print(np.arange(0, 5/3, 1/3)) # depending on floating point errors, the max value is 4/3 or 5/3. print(np.arange(0, 5/3, 0.333333333)) print(np.arange(0, 5/3, 0.333333334)) . [ 0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] [ 0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] [ 0. 0.33333333 0.66666667 1. 1.33333334] . np.linspace . For this reason, it is generally preferable to use the linspace function instead of arange when working with floats. The linspace function returns an array containing a specific number of points evenly distributed between two values (note that the maximum value is included, contrary to arange): . print(np.linspace(0, 5/3, 6)) . [ 0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] . np.rand and np.randn . A number of functions are available in NumPy&#39;s random module to create ndarrays initialized with random values. For example, here is a 3x4 matrix initialized with random floats between 0 and 1 (uniform distribution): . np.random.rand(3,4) . array([[ 0.37454012, 0.95071431, 0.73199394, 0.59865848], [ 0.15601864, 0.15599452, 0.05808361, 0.86617615], [ 0.60111501, 0.70807258, 0.02058449, 0.96990985]]) . Here&#39;s a 3x4 matrix containing random floats sampled from a univariate normal distribution (Gaussian distribution) of mean 0 and variance 1: . np.random.randn(3,4) . array([[-0.46947439, 0.54256004, -0.46341769, -0.46572975], [ 0.24196227, -1.91328024, -1.72491783, -0.56228753], [-1.01283112, 0.31424733, -0.90802408, -1.4123037 ]]) . To give you a feel of what these distributions look like, let&#39;s use matplotlib (see the matplotlib tutorial for more details): . %matplotlib inline import matplotlib.pyplot as plt . plt.hist(np.random.rand(100000), normed=True, bins=100, histtype=&quot;step&quot;, color=&quot;blue&quot;, label=&quot;rand&quot;) plt.hist(np.random.randn(100000), normed=True, bins=100, histtype=&quot;step&quot;, color=&quot;red&quot;, label=&quot;randn&quot;) plt.axis([-2.5, 2.5, 0, 1.1]) plt.legend(loc = &quot;upper left&quot;) plt.title(&quot;Random distributions&quot;) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Density&quot;) plt.show() . np.fromfunction . You can also initialize an ndarray using a function: . def my_function(z, y, x): return x * y + z np.fromfunction(my_function, (3, 2, 10)) . array([[[ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]], [[ 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.]], [[ 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], [ 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.]]]) . NumPy first creates three ndarrays (one per dimension), each of shape (2, 10). Each array has values equal to the coordinate along a specific axis. For example, all elements in the z array are equal to their z-coordinate: . [[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] [[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.] [ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]] . So the terms x, y and z in the expression x * y + z above are in fact ndarrays (we will discuss arithmetic operations on arrays below). The point is that the function my_function is only called once, instead of once per element. This makes initialization very efficient. . Array data . dtype . NumPy&#39;s ndarrays are also efficient in part because all their elements must have the same type (usually numbers). You can check what the data type is by looking at the dtype attribute: . c = np.arange(1, 5) print(c.dtype, c) . int64 [1 2 3 4] . c = np.arange(1.0, 5.0) print(c.dtype, c) . float64 [ 1. 2. 3. 4.] . Instead of letting NumPy guess what data type to use, you can set it explicitly when creating an array by setting the dtype parameter: . d = np.arange(1, 5, dtype=np.complex64) print(d.dtype, d) . complex64 [ 1.+0.j 2.+0.j 3.+0.j 4.+0.j] . Available data types include int8, int16, int32, int64, uint8|16|32|64, float16|32|64 and complex64|128. Check out the documentation for the full list. . itemsize . The itemsize attribute returns the size (in bytes) of each item: . e = np.arange(1, 5, dtype=np.complex64) e.itemsize . 8 . data buffer . An array&#39;s data is actually stored in memory as a flat (one dimensional) byte buffer. It is available via the data attribute (you will rarely need it, though). . f = np.array([[1,2],[1000, 2000]], dtype=np.int32) f.data . &lt;read-write buffer for 0x10f8a18a0, size 16, offset 0 at 0x10f9dbbb0&gt; . In python 2, f.data is a buffer. In python 3, it is a memoryview. . if (hasattr(f.data, &quot;tobytes&quot;)): data_bytes = f.data.tobytes() # python 3 else: data_bytes = memoryview(f.data).tobytes() # python 2 data_bytes . &#39; x01 x00 x00 x00 x02 x00 x00 x00 xe8 x03 x00 x00 xd0 x07 x00 x00&#39; . Several ndarrays can share the same data buffer, meaning that modifying one will also modify the others. We will see an example in a minute. . Reshaping an array . In place . Changing the shape of an ndarray is as simple as setting its shape attribute. However, the array&#39;s size must remain the same. . g = np.arange(24) print(g) print(&quot;Rank:&quot;, g.ndim) . [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] Rank: 1 . g.shape = (6, 4) print(g) print(&quot;Rank:&quot;, g.ndim) . [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15] [16 17 18 19] [20 21 22 23]] Rank: 2 . g.shape = (2, 3, 4) print(g) print(&quot;Rank:&quot;, g.ndim) . [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] Rank: 3 . reshape . The reshape function returns a new ndarray object pointing at the same data. This means that modifying one array will also modify the other. . g2 = g.reshape(4,6) print(g2) print(&quot;Rank:&quot;, g2.ndim) . [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] Rank: 2 . Set item at row 1, col 2 to 999 (more about indexing below). . g2[1, 2] = 999 g2 . array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 999, 9, 10, 11], [ 12, 13, 14, 15, 16, 17], [ 18, 19, 20, 21, 22, 23]]) . The corresponding element in g has been modified. . g . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [999, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]]) . ravel . Finally, the ravel function returns a new one-dimensional ndarray that also points to the same data: . g.ravel() . array([ 0, 1, 2, 3, 4, 5, 6, 7, 999, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . Arithmetic operations . All the usual arithmetic operators (+, -, *, /, //, **, etc.) can be used with ndarrays. They apply elementwise: . a = np.array([14, 23, 32, 41]) b = np.array([5, 4, 3, 2]) print(&quot;a + b =&quot;, a + b) print(&quot;a - b =&quot;, a - b) print(&quot;a * b =&quot;, a * b) print(&quot;a / b =&quot;, a / b) print(&quot;a // b =&quot;, a // b) print(&quot;a % b =&quot;, a % b) print(&quot;a ** b =&quot;, a ** b) . a + b = [19 27 35 43] a - b = [ 9 19 29 39] a * b = [70 92 96 82] a / b = [ 2.8 5.75 10.66666667 20.5 ] a // b = [ 2 5 10 20] a % b = [4 3 2 1] a ** b = [537824 279841 32768 1681] . Note that the multiplication is not a matrix multiplication. We will discuss matrix operations below. . The arrays must have the same shape. If they do not, NumPy will apply the broadcasting rules. . Broadcasting . In general, when NumPy expects arrays of the same shape but finds that this is not the case, it applies the so-called broadcasting rules: . First rule . If the arrays do not have the same rank, then a 1 will be prepended to the smaller ranking arrays until their ranks match. . h = np.arange(5).reshape(1, 1, 5) h . array([[[0, 1, 2, 3, 4]]]) . Now let&#39;s try to add a 1D array of shape (5,) to this 3D array of shape (1,1,5). Applying the first rule of broadcasting! . h + [10, 20, 30, 40, 50] # same as: h + [[[10, 20, 30, 40, 50]]] . array([[[10, 21, 32, 43, 54]]]) . Second rule . Arrays with a 1 along a particular dimension act as if they had the size of the array with the largest shape along that dimension. The value of the array element is repeated along that dimension. . k = np.arange(6).reshape(2, 3) k . array([[0, 1, 2], [3, 4, 5]]) . Let&#39;s try to add a 2D array of shape (2,1) to this 2D ndarray of shape (2, 3). NumPy will apply the second rule of broadcasting: . k + [[100], [200]] # same as: k + [[100, 100, 100], [200, 200, 200]] . array([[100, 101, 102], [203, 204, 205]]) . Combining rules 1 &amp; 2, we can do this: . k + [100, 200, 300] # after rule 1: [[100, 200, 300]], and after rule 2: [[100, 200, 300], [100, 200, 300]] . array([[100, 201, 302], [103, 204, 305]]) . And also, very simply: . k + 1000 # same as: k + [[1000, 1000, 1000], [1000, 1000, 1000]] . array([[1000, 1001, 1002], [1003, 1004, 1005]]) . Third rule . After rules 1 &amp; 2, the sizes of all arrays must match. . try: k + [33, 44] except ValueError as e: print(e) . operands could not be broadcast together with shapes (2,3) (2,) . Broadcasting rules are used in many NumPy operations, not just arithmetic operations, as we will see below. For more details about broadcasting, check out the documentation. . Upcasting . When trying to combine arrays with different dtypes, NumPy will upcast to a type capable of handling all possible values (regardless of what the actual values are). . k1 = np.arange(0, 5, dtype=np.uint8) print(k1.dtype, k1) . uint8 [0 1 2 3 4] . k2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8) print(k2.dtype, k2) . int16 [ 5 7 9 11 13] . Note that int16 is required to represent all possible int8 and uint8 values (from -128 to 255), even though in this case a uint8 would have sufficed. . k3 = k1 + 1.5 print(k3.dtype, k3) . float64 [ 1.5 2.5 3.5 4.5 5.5] . Conditional operators . The conditional operators also apply elementwise: . m = np.array([20, -5, 30, 40]) m &lt; [15, 16, 35, 36] . array([False, True, True, False], dtype=bool) . And using broadcasting: . m &lt; 25 # equivalent to m &lt; [25, 25, 25, 25] . array([ True, True, False, False], dtype=bool) . This is most useful in conjunction with boolean indexing (discussed below). . m[m &lt; 25] . array([20, -5]) . Mathematical and statistical functions . Many mathematical and statistical functions are available for ndarrays. . ndarray methods . Some functions are simply ndarray methods, for example: . a = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) print(a) print(&quot;mean =&quot;, a.mean()) . [[ -2.5 3.1 7. ] [ 10. 11. 12. ]] mean = 6.76666666667 . Note that this computes the mean of all elements in the ndarray, regardless of its shape. . Here are a few more useful ndarray methods: . for func in (a.min, a.max, a.sum, a.prod, a.std, a.var): print(func.__name__, &quot;=&quot;, func()) . min = -2.5 max = 12.0 sum = 40.6 prod = -71610.0 std = 5.08483584352 var = 25.8555555556 . These functions accept an optional argument axis which lets you ask for the operation to be performed on elements along the given axis. For example: . c=np.arange(24).reshape(2,3,4) c . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) . c.sum(axis=0) # sum across matrices . array([[12, 14, 16, 18], [20, 22, 24, 26], [28, 30, 32, 34]]) . c.sum(axis=1) # sum across rows . array([[12, 15, 18, 21], [48, 51, 54, 57]]) . You can also sum over multiple axes: . c.sum(axis=(0,2)) # sum across matrices and columns . array([ 60, 92, 124]) . 0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23 . (60, 92, 124) . Universal functions . NumPy also provides fast elementwise functions called universal functions, or ufunc. They are vectorized wrappers of simple functions. For example square returns a new ndarray which is a copy of the original ndarray except that each element is squared: . a = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) np.square(a) . array([[ 6.25, 9.61, 49. ], [ 100. , 121. , 144. ]]) . Here are a few more useful unary ufuncs: . print(&quot;Original ndarray&quot;) print(a) for func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos): print(&quot; n&quot;, func.__name__) print(func(a)) . Original ndarray [[ -2.5 3.1 7. ] [ 10. 11. 12. ]] absolute [[ 2.5 3.1 7. ] [ 10. 11. 12. ]] sqrt [[ nan 1.76068169 2.64575131] [ 3.16227766 3.31662479 3.46410162]] exp [[ 8.20849986e-02 2.21979513e+01 1.09663316e+03] [ 2.20264658e+04 5.98741417e+04 1.62754791e+05]] log [[ nan 1.13140211 1.94591015] [ 2.30258509 2.39789527 2.48490665]] sign [[-1. 1. 1.] [ 1. 1. 1.]] ceil [[ -2. 4. 7.] [ 10. 11. 12.]] modf (array([[-0.5, 0.1, 0. ], [ 0. , 0. , 0. ]]), array([[ -2., 3., 7.], [ 10., 11., 12.]])) isnan [[False False False] [False False False]] cos [[-0.80114362 -0.99913515 0.75390225] [-0.83907153 0.0044257 0.84385396]] . -c:5: RuntimeWarning: invalid value encountered in sqrt -c:5: RuntimeWarning: invalid value encountered in log . Binary ufuncs . There are also many binary ufuncs, that apply elementwise on two ndarrays. Broadcasting rules are applied if the arrays do not have the same shape: . a = np.array([1, -2, 3, 4]) b = np.array([2, 8, -1, 7]) np.add(a, b) # equivalent to a + b . array([ 3, 6, 2, 11]) . np.greater(a, b) # equivalent to a &gt; b . array([False, False, True, False], dtype=bool) . np.maximum(a, b) . array([2, 8, 3, 7]) . np.copysign(a, b) . array([ 1., 2., -3., 4.]) . Array indexing . One-dimensional arrays . One-dimensional NumPy arrays can be accessed more or less like regular python arrays: . a = np.array([1, 5, 3, 19, 13, 7, 3]) a[3] . 19 . a[2:5] . array([ 3, 19, 13]) . a[2:-1] . array([ 3, 19, 13, 7]) . a[:2] . array([1, 5]) . a[2::2] . array([ 3, 13, 3]) . a[::-1] . array([ 3, 7, 13, 19, 3, 5, 1]) . Of course, you can modify elements: . a[3]=999 a . array([ 1, 5, 3, 999, 13, 7, 3]) . You can also modify an ndarray slice: . a[2:5] = [997, 998, 999] a . array([ 1, 5, 997, 998, 999, 7, 3]) . Differences with regular python arrays . Contrary to regular python arrays, if you assign a single value to an ndarray slice, it is copied across the whole slice, thanks to broadcasting rules discussed above. . a[2:5] = -1 a . array([ 1, 5, -1, -1, -1, 7, 3]) . Also, you cannot grow or shrink ndarrays this way: . try: a[2:5] = [1,2,3,4,5,6] # too long except ValueError as e: print(e) . cannot copy sequence with size 6 to array axis with dimension 3 . You cannot delete elements either: . try: del a[2:5] except ValueError as e: print(e) . cannot delete array elements . Last but not least, ndarray slices are actually views on the same data buffer. This means that if you create a slice and modify it, you are actually going to modify the original ndarray as well! . a_slice = a[2:6] a_slice[1] = 1000 a # the original array was modified! . array([ 1, 5, -1, 1000, -1, 7, 3]) . a[3] = 2000 a_slice # similarly, modifying the original array modifies the slice! . array([ -1, 2000, -1, 7]) . If you want a copy of the data, you need to use the copy method: . another_slice = a[2:6].copy() another_slice[1] = 3000 a # the original array is untouched . array([ 1, 5, -1, 2000, -1, 7, 3]) . a[3] = 4000 another_slice # similary, modifying the original array does not affect the slice copy . array([ -1, 3000, -1, 7]) . Multi-dimensional arrays . Multi-dimensional arrays can be accessed in a similar way by providing an index or slice for each axis, separated by commas: . b = np.arange(48).reshape(4, 12) b . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]) . b[1, 2] # row 1, col 2 . 14 . b[1, :] # row 1, all columns . array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . b[:, 1] # all rows, column 1 . array([ 1, 13, 25, 37]) . Caution: note the subtle difference between these two expressions: . b[1, :] . array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . b[1:2, :] . array([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]) . The first expression returns row 1 as a 1D array of shape (12,), while the second returns that same row as a 2D array of shape (1, 12). . Fancy indexing . You may also specify a list of indices that you are interested in. This is referred to as fancy indexing. . b[(0,2), 2:5] # rows 0 and 2, columns 2 to 4 (5-1) . array([[ 2, 3, 4], [26, 27, 28]]) . b[:, (-1, 2, -1)] # all rows, columns -1 (last), 2 and -1 (again, and in this order) . array([[11, 2, 11], [23, 14, 23], [35, 26, 35], [47, 38, 47]]) . If you provide multiple index arrays, you get a 1D ndarray containing the values of the elements at the specified coordinates. . b[(-1, 2, -1, 2), (5, 9, 1, 9)] # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again) . array([41, 33, 37, 33]) . Higher dimensions . Everything works just as well with higher dimensional arrays, but it&#39;s useful to look at a few examples: . c = b.reshape(4,2,6) c . array([[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]], [[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]], [[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]], [[36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47]]]) . c[2, 1, 4] # matrix 2, row 1, col 4 . 34 . c[2, :, 3] # matrix 2, all rows, col 3 . array([27, 33]) . If you omit coordinates for some axes, then all elements in these axes are returned: . c[2, 1] # Return matrix 2, row 1, all columns. This is equivalent to c[2, 1, :] . array([30, 31, 32, 33, 34, 35]) . Ellipsis (...) . You may also write an ellipsis (...) to ask that all non-specified axes be entirely included. . c[2, ...] # matrix 2, all rows, all columns. This is equivalent to c[2, :, :] . array([[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]]) . c[2, 1, ...] # matrix 2, row 1, all columns. This is equivalent to c[2, 1, :] . array([30, 31, 32, 33, 34, 35]) . c[2, ..., 3] # matrix 2, all rows, column 3. This is equivalent to c[2, :, 3] . array([27, 33]) . c[..., 3] # all matrices, all rows, column 3. This is equivalent to c[:, :, 3] . array([[ 3, 9], [15, 21], [27, 33], [39, 45]]) . Boolean indexing . You can also provide an ndarray of boolean values on one axis to specify the indices that you want to access. . b = np.arange(48).reshape(4, 12) b . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]) . rows_on = np.array([True, False, True, False]) b[rows_on, :] # Rows 0 and 2, all columns. Equivalent to b[(0, 2), :] . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]]) . cols_on = np.array([False, True, False] * 4) b[:, cols_on] # All rows, columns 1, 4, 7 and 10 . array([[ 1, 4, 7, 10], [13, 16, 19, 22], [25, 28, 31, 34], [37, 40, 43, 46]]) . np.ix_ . You cannot use boolean indexing this way on multiple axes, but you can work around this by using the ix_ function: . b[np.ix_(rows_on, cols_on)] . array([[ 1, 4, 7, 10], [25, 28, 31, 34]]) . np.ix_(rows_on, cols_on) . (array([[0], [2]]), array([[ 1, 4, 7, 10]])) . If you use a boolean array that has the same shape as the ndarray, then you get in return a 1D array containing all the values that have True at their coordinate. This is generally used along with conditional operators: . b[b % 3 == 1] . array([ 1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46]) . Iterating . Iterating over ndarrays is very similar to iterating over regular python arrays. Note that iterating over multidimensional arrays is done with respect to the first axis. . c = np.arange(24).reshape(2, 3, 4) # A 3D array (composed of two 3x4 matrices) c . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) . for m in c: print(&quot;Item:&quot;) print(m) . Item: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] Item: [[12 13 14 15] [16 17 18 19] [20 21 22 23]] . for i in range(len(c)): # Note that len(c) == c.shape[0] print(&quot;Item:&quot;) print(c[i]) . Item: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] Item: [[12 13 14 15] [16 17 18 19] [20 21 22 23]] . If you want to iterate on all elements in the ndarray, simply iterate over the flat attribute: . for i in c.flat: print(&quot;Item:&quot;, i) . Item: 0 Item: 1 Item: 2 Item: 3 Item: 4 Item: 5 Item: 6 Item: 7 Item: 8 Item: 9 Item: 10 Item: 11 Item: 12 Item: 13 Item: 14 Item: 15 Item: 16 Item: 17 Item: 18 Item: 19 Item: 20 Item: 21 Item: 22 Item: 23 . Stacking arrays . It is often useful to stack together different arrays. NumPy offers several functions to do just that. Let&#39;s start by creating a few arrays. . q1 = np.full((3,4), 1.0) q1 . array([[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 1., 1., 1., 1.]]) . q2 = np.full((4,4), 2.0) q2 . array([[ 2., 2., 2., 2.], [ 2., 2., 2., 2.], [ 2., 2., 2., 2.], [ 2., 2., 2., 2.]]) . q3 = np.full((3,4), 3.0) q3 . array([[ 3., 3., 3., 3.], [ 3., 3., 3., 3.], [ 3., 3., 3., 3.]]) . vstack . Now let&#39;s stack them vertically using vstack: . q4 = np.vstack((q1, q2, q3)) q4 . array([[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], [ 2., 2., 2., 2.], [ 2., 2., 2., 2.], [ 2., 2., 2., 2.], [ 3., 3., 3., 3.], [ 3., 3., 3., 3.], [ 3., 3., 3., 3.]]) . q4.shape . (10, 4) . This was possible because q1, q2 and q3 all have the same shape (except for the vertical axis, but that&#39;s ok since we are stacking on that axis). . hstack . We can also stack arrays horizontally using hstack: . q5 = np.hstack((q1, q3)) q5 . array([[ 1., 1., 1., 1., 3., 3., 3., 3.], [ 1., 1., 1., 1., 3., 3., 3., 3.], [ 1., 1., 1., 1., 3., 3., 3., 3.]]) . q5.shape . (3, 8) . This is possible because q1 and q3 both have 3 rows. But since q2 has 4 rows, it cannot be stacked horizontally with q1 and q3: . try: q5 = np.hstack((q1, q2, q3)) except ValueError as e: print(e) . all the input array dimensions except for the concatenation axis must match exactly . concatenate . The concatenate function stacks arrays along any given existing axis. . q7 = np.concatenate((q1, q2, q3), axis=0) # Equivalent to vstack q7 . array([[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], [ 2., 2., 2., 2.], [ 2., 2., 2., 2.], [ 2., 2., 2., 2.], [ 3., 3., 3., 3.], [ 3., 3., 3., 3.], [ 3., 3., 3., 3.]]) . q7.shape . (10, 4) . As you might guess, hstack is equivalent to calling concatenate with axis=1. . stack . The stack function stacks arrays along a new axis. All arrays have to have the same shape. . q8 = np.stack((q1, q3)) q8 . array([[[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 1., 1., 1., 1.]], [[ 3., 3., 3., 3.], [ 3., 3., 3., 3.], [ 3., 3., 3., 3.]]]) . q8.shape . (2, 3, 4) . Splitting arrays . Splitting is the opposite of stacking. For example, let&#39;s use the vsplit function to split a matrix vertically. . First let&#39;s create a 6x4 matrix: . r = np.arange(24).reshape(6,4) r . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]) . Now let&#39;s split it in three equal parts, vertically: . r1, r2, r3 = np.vsplit(r, 3) r1 . array([[0, 1, 2, 3], [4, 5, 6, 7]]) . r2 . array([[ 8, 9, 10, 11], [12, 13, 14, 15]]) . r3 . array([[16, 17, 18, 19], [20, 21, 22, 23]]) . There is also a split function which splits an array along any given axis. Calling vsplit is equivalent to calling split with axis=0. There is also an hsplit function, equivalent to calling split with axis=1: . r4, r5 = np.hsplit(r, 2) r4 . array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13], [16, 17], [20, 21]]) . r5 . array([[ 2, 3], [ 6, 7], [10, 11], [14, 15], [18, 19], [22, 23]]) . Transposing arrays . The transpose method creates a new view on an ndarray&#39;s data, with axes permuted in the given order. . For example, let&#39;s create a 3D array: . t = np.arange(24).reshape(4,2,3) t . array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23]]]) . Now let&#39;s create an ndarray such that the axes 0, 1, 2 (depth, height, width) are re-ordered to 1, 2, 0 (depthâ†’width, heightâ†’depth, widthâ†’height): . t1 = t.transpose((1,2,0)) t1 . array([[[ 0, 6, 12, 18], [ 1, 7, 13, 19], [ 2, 8, 14, 20]], [[ 3, 9, 15, 21], [ 4, 10, 16, 22], [ 5, 11, 17, 23]]]) . t1.shape . (2, 3, 4) . By default, transpose reverses the order of the dimensions: . t2 = t.transpose() # equivalent to t.transpose((2, 1, 0)) t2 . array([[[ 0, 6, 12, 18], [ 3, 9, 15, 21]], [[ 1, 7, 13, 19], [ 4, 10, 16, 22]], [[ 2, 8, 14, 20], [ 5, 11, 17, 23]]]) . t2.shape . (3, 2, 4) . NumPy provides a convenience function swapaxes to swap two axes. For example, let&#39;s create a new view of t with depth and height swapped: . t3 = t.swapaxes(0,1) # equivalent to t.transpose((1, 0, 2)) t3 . array([[[ 0, 1, 2], [ 6, 7, 8], [12, 13, 14], [18, 19, 20]], [[ 3, 4, 5], [ 9, 10, 11], [15, 16, 17], [21, 22, 23]]]) . t3.shape . (2, 4, 3) . Linear algebra . NumPy 2D arrays can be used to represent matrices efficiently in python. We will just quickly go through some of the main matrix operations available. For more details about Linear Algebra, vectors and matrics, go through the Linear Algebra tutorial. . Matrix transpose . The T attribute is equivalent to calling transpose() when the rank is â‰¥2: . m1 = np.arange(10).reshape(2,5) m1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . m1.T . array([[0, 5], [1, 6], [2, 7], [3, 8], [4, 9]]) . The T attribute has no effect on rank 0 (empty) or rank 1 arrays: . m2 = np.arange(5) m2 . array([0, 1, 2, 3, 4]) . m2.T . array([0, 1, 2, 3, 4]) . We can get the desired transposition by first reshaping the 1D array to a single-row matrix (2D): . m2r = m2.reshape(1,5) m2r . array([[0, 1, 2, 3, 4]]) . m2r.T . array([[0], [1], [2], [3], [4]]) . Matrix dot product . Let&#39;s create two matrices and execute a matrix dot product using the dot method. . n1 = np.arange(10).reshape(2, 5) n1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . n2 = np.arange(15).reshape(5,3) n2 . array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) . n1.dot(n2) . array([[ 90, 100, 110], [240, 275, 310]]) . Caution: as mentionned previously, n1*n2 is not a dot product, it is an elementwise product. . Matrix inverse and pseudo-inverse . Many of the linear algebra functions are available in the numpy.linalg module, in particular the inv function to compute a square matrix&#39;s inverse: . import numpy.linalg as linalg m3 = np.array([[1,2,3],[5,7,11],[21,29,31]]) m3 . array([[ 1, 2, 3], [ 5, 7, 11], [21, 29, 31]]) . linalg.inv(m3) . array([[-2.31818182, 0.56818182, 0.02272727], [ 1.72727273, -0.72727273, 0.09090909], [-0.04545455, 0.29545455, -0.06818182]]) . You can also compute the pseudoinverse using pinv: . linalg.pinv(m3) . array([[-2.31818182, 0.56818182, 0.02272727], [ 1.72727273, -0.72727273, 0.09090909], [-0.04545455, 0.29545455, -0.06818182]]) . Identity matrix . The product of a matrix by its inverse returns the identiy matrix (with small floating point errors): . m3.dot(linalg.inv(m3)) . array([[ 1.00000000e+00, -1.11022302e-16, -6.93889390e-18], [ -1.33226763e-15, 1.00000000e+00, -5.55111512e-17], [ 2.88657986e-15, 0.00000000e+00, 1.00000000e+00]]) . You can create an identity matrix of size NxN by calling eye: . np.eye(3) . array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) . QR decomposition . The qr function computes the QR decomposition of a matrix: . q, r = linalg.qr(m3) q . array([[-0.04627448, 0.98786672, 0.14824986], [-0.23137241, 0.13377362, -0.96362411], [-0.97176411, -0.07889213, 0.22237479]]) . r . array([[-21.61018278, -29.89331494, -32.80860727], [ 0. , 0.62427688, 1.9894538 ], [ 0. , 0. , -3.26149699]]) . q.dot(r) # q.r equals m3 . array([[ 1., 2., 3.], [ 5., 7., 11.], [ 21., 29., 31.]]) . Determinant . The det function computes the matrix determinant: . linalg.det(m3) # Computes the matrix determinant . 43.999999999999972 . Eigenvalues and eigenvectors . The eig function computes the eigenvalues and eigenvectors of a square matrix: . eigenvalues, eigenvectors = linalg.eig(m3) eigenvalues # Î» . array([ 42.26600592, -0.35798416, -2.90802176]) . eigenvectors # v . array([[-0.08381182, -0.76283526, -0.18913107], [-0.3075286 , 0.64133975, -0.6853186 ], [-0.94784057, -0.08225377, 0.70325518]]) . m3.dot(eigenvectors) - eigenvalues * eigenvectors # m3.v - Î»*v = 0 . array([[ 8.88178420e-15, 2.49800181e-15, -3.33066907e-16], [ 1.77635684e-14, -1.66533454e-16, -3.55271368e-15], [ 3.55271368e-14, 3.61516372e-15, -4.44089210e-16]]) . Singular Value Decomposition . The svd function takes a matrix and returns its singular value decomposition: . m4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]]) m4 . array([[1, 0, 0, 0, 2], [0, 0, 3, 0, 0], [0, 0, 0, 0, 0], [0, 2, 0, 0, 0]]) . U, S_diag, V = linalg.svd(m4) U . array([[ 0., 1., 0., 0.], [ 1., 0., 0., 0.], [ 0., 0., 0., -1.], [ 0., 0., 1., 0.]]) . S_diag . array([ 3. , 2.23606798, 2. , 0. ]) . The svd function just returns the values in the diagonal of Î£, but we want the full Î£ matrix, so let&#39;s create it: . S = np.zeros((4, 5)) S[np.diag_indices(4)] = S_diag S # Î£ . array([[ 3. , 0. , 0. , 0. , 0. ], [ 0. , 2.23606798, 0. , 0. , 0. ], [ 0. , 0. , 2. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. ]]) . V . array([[-0. , 0. , 1. , -0. , 0. ], [ 0.4472136 , 0. , 0. , 0. , 0.89442719], [-0. , 1. , 0. , -0. , 0. ], [ 0. , 0. , 0. , 1. , 0. ], [-0.89442719, 0. , 0. , 0. , 0.4472136 ]]) . U.dot(S).dot(V) # U.Î£.V == m4 . array([[ 1., 0., 0., 0., 2.], [ 0., 0., 3., 0., 0.], [ 0., 0., 0., 0., 0.], [ 0., 2., 0., 0., 0.]]) . Diagonal and trace . np.diag(m3) # the values in the diagonal of m3 (top left to bottom right) . array([ 1, 7, 31]) . np.trace(m3) # equivalent to np.diag(m3).sum() . 39 . Solving a system of linear scalar equations . The solve function solves a system of linear scalar equations, such as: . $2x + 6y = 6$ | $5x + 3y = -9$ | . coeffs = np.array([[2, 6], [5, 3]]) depvars = np.array([6, -9]) solution = linalg.solve(coeffs, depvars) solution . array([-3., 2.]) . Let&#39;s check the solution: . coeffs.dot(solution), depvars # yep, it&#39;s the same . (array([ 6., -9.]), array([ 6, -9])) . Looks good! Another way to check the solution: . np.allclose(coeffs.dot(solution), depvars) . True . Vectorization . Instead of executing operations on individual array items, one at a time, your code is much more efficient if you try to stick to array operations. This is called vectorization. This way, you can benefit from NumPy&#39;s many optimizations. . For example, let&#39;s say we want to generate a 768x1024 array based on the formula $sin(xy/40.5)$. A bad option would be to do the math in python using nested loops: . import math data = np.empty((768, 1024)) for y in range(768): for x in range(1024): data[y, x] = math.sin(x*y/40.5) # BAD! Very inefficient. . Sure, this works, but it&#39;s terribly inefficient since the loops are taking place in pure python. Let&#39;s vectorize this algorithm. First, we will use NumPy&#39;s meshgrid function which generates coordinate matrices from coordinate vectors. . x_coords = np.arange(0, 1024) # [0, 1, 2, ..., 1023] y_coords = np.arange(0, 768) # [0, 1, 2, ..., 767] X, Y = np.meshgrid(x_coords, y_coords) X . array([[ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], ..., [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023]]) . Y . array([[ 0, 0, 0, ..., 0, 0, 0], [ 1, 1, 1, ..., 1, 1, 1], [ 2, 2, 2, ..., 2, 2, 2], ..., [765, 765, 765, ..., 765, 765, 765], [766, 766, 766, ..., 766, 766, 766], [767, 767, 767, ..., 767, 767, 767]]) . As you can see, both X and Y are 768x1024 arrays, and all values in X correspond to the horizontal coordinate, while all values in Y correspond to the the vertical coordinate. . Now we can simply compute the result using array operations: . data = np.sin(X*Y/40.5) . Now we can plot this data using matplotlib&#39;s imshow function (see the matplotlib tutorial). . import matplotlib.pyplot as plt import matplotlib.cm as cm fig = plt.figure(1, figsize=(7, 6)) plt.imshow(data, cmap=cm.hot, interpolation=&quot;bicubic&quot;) plt.show() . Saving and loading . NumPy makes it easy to save and load ndarrays in binary or text format. . Binary .npy format . Let&#39;s create a random array and save it. . a = np.random.rand(2,3) a . array([[ 0.41307972, 0.20933385, 0.32025581], [ 0.19853514, 0.408001 , 0.6038287 ]]) . np.save(&quot;my_array&quot;, a) . Done! Since the file name contains no file extension was provided, NumPy automatically added .npy. Let&#39;s take a peek at the file content: . with open(&quot;my_array.npy&quot;, &quot;rb&quot;) as f: content = f.read() content . &#34; x93NUMPY x01 x00F x00{&#39;descr&#39;: &#39;&lt;f8&#39;, &#39;fortran_order&#39;: False, &#39;shape&#39;: (2, 3), } n xa8 x96 x1d xeb xe5o xda? x06W xa1s xcb xca?* xdeB&gt; x12 x7f xd4?x&lt;h x81 x99i xc9?@ xa4 x027 xb0 x1c xda?&lt;P x05 x8f x90R xe3?&#34; . To load this file into a NumPy array, simply call load: . a_loaded = np.load(&quot;my_array.npy&quot;) a_loaded . array([[ 0.41307972, 0.20933385, 0.32025581], [ 0.19853514, 0.408001 , 0.6038287 ]]) . Text format . Let&#39;s try saving the array in text format: . np.savetxt(&quot;my_array.csv&quot;, a) . Now let&#39;s look at the file content: . with open(&quot;my_array.csv&quot;, &quot;rt&quot;) as f: print(f.read()) . 4.130797191668116319e-01 2.093338525574361952e-01 3.202558143634371968e-01 1.985351449843368865e-01 4.080009972772735694e-01 6.038286965726977762e-01 . This is a CSV file with tabs as delimiters. You can set a different delimiter: . np.savetxt(&quot;my_array.csv&quot;, a, delimiter=&quot;,&quot;) . To load this file, just use loadtxt: . a_loaded = np.loadtxt(&quot;my_array.csv&quot;, delimiter=&quot;,&quot;) a_loaded . array([[ 0.41307972, 0.20933385, 0.32025581], [ 0.19853514, 0.408001 , 0.6038287 ]]) . Zipped .npz format . It is also possible to save multiple arrays in one zipped file: . b = np.arange(24, dtype=np.uint8).reshape(2, 3, 4) b . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]], dtype=uint8) . np.savez(&quot;my_arrays&quot;, my_a=a, my_b=b) . Again, let&#39;s take a peek at the file content. Note that the .npz file extension was automatically added. . with open(&quot;my_arrays.npz&quot;, &quot;rb&quot;) as f: content = f.read() repr(content)[:180] + &quot;[...]&quot; . u&#39;&#34;PK x03 x04 x14 x00 x00 x00 x00 x00x x94cH xb6 x96 xe4{h x00 x00 x00h x00 x00 x00 x08 x00 x00 x00my_b.npy x93NUMPY x01 x00F x00{ &#39;descr &#39;: &#39;|u1 &#39;, &#39;fortran_order &#39;: False, &#39;shape &#39;: (2,[...]&#39; . You then load this file like so: . my_arrays = np.load(&quot;my_arrays.npz&quot;) my_arrays . &lt;numpy.lib.npyio.NpzFile at 0x10fa4d4d0&gt; . This is a dict-like object which loads the arrays lazily: . my_arrays.keys() . [&#39;my_b&#39;, &#39;my_a&#39;] . my_arrays[&quot;my_a&quot;] . array([[ 0.41307972, 0.20933385, 0.32025581], [ 0.19853514, 0.408001 , 0.6038287 ]]) . What next? . Now you know all the fundamentals of NumPy, but there are many more options available. The best way to learn more is to experiment with NumPy, and go through the excellent reference documentation to find more functions and features you may be interested in. .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/tools_numpy.html",
            "relUrl": "/2020/03/09/tools_numpy.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "Tools - matplotlib . permalink: /tools_matplotlib | . This notebook demonstrates how to use the matplotlib library to plot beautiful graphs. . Table of Contents . 1&nbsp;&nbsp;Plotting your first graph2&nbsp;&nbsp;Line style and color3&nbsp;&nbsp;Saving a figure4&nbsp;&nbsp;Subplots5&nbsp;&nbsp;Multiple figures6&nbsp;&nbsp;Pyplot&#39;s state machine: implicit vs explicit7&nbsp;&nbsp;Pylab vs Pyplot vs Matplotlib8&nbsp;&nbsp;Drawing text9&nbsp;&nbsp;Legends10&nbsp;&nbsp;Non linear scales11&nbsp;&nbsp;Ticks and tickers12&nbsp;&nbsp;Polar projection13&nbsp;&nbsp;3D projection14&nbsp;&nbsp;Scatter plot15&nbsp;&nbsp;Lines16&nbsp;&nbsp;Histograms17&nbsp;&nbsp;Images18&nbsp;&nbsp;Animations19&nbsp;&nbsp;Saving animations to video files20&nbsp;&nbsp;What next? . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Plotting your first graph . First we need to import the matplotlib library. . import matplotlib . Matplotlib can output graphs using various backend graphics libraries, such as Tk, wxPython, etc. When running python using the command line, the graphs are typically shown in a separate window. In a Jupyter notebook, we can simply output the graphs within the notebook itself by running the %matplotlib inline magic command. . %matplotlib inline # matplotlib.use(&quot;TKAgg&quot;) # use this instead in your program if you want to use Tk as your graphics backend. . Now let&#39;s plot our first graph! :) . import matplotlib.pyplot as plt plt.plot([1, 2, 4, 9, 5, 3]) plt.show() . Yep, it&#39;s as simple as calling the plot function with some data, and then calling the show function! . If the plot function is given one array of data, it will use it as the coordinates on the vertical axis, and it will just use each data point&#39;s index in the array as the horizontal coordinate. You can also provide two arrays: one for the horizontal axis x, and the second for the vertical axis y: . plt.plot([-3, -2, 5, 0], [1, 6, 4, 3]) plt.show() . The axes automatically match the extent of the data. We would like to give the graph a bit more room, so let&#39;s call the axis function to change the extent of each axis [xmin, xmax, ymin, ymax]. . plt.plot([-3, -2, 5, 0], [1, 6, 4, 3]) plt.axis([-4, 6, 0, 7]) plt.show() . Now, let&#39;s plot a mathematical function. We use NumPy&#39;s linspace function to create an array x containing 500 floats ranging from -2 to 2, then we create a second array y computed as the square of x (to learn about NumPy, read the NumPy tutorial). . import numpy as np x = np.linspace(-2, 2, 500) y = x**2 plt.plot(x, y) plt.show() . That&#39;s a bit dry, let&#39;s add a title, and x and y labels, and draw a grid. . plt.plot(x, y) plt.title(&quot;Square function&quot;) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y = x**2&quot;) plt.grid(True) plt.show() . Line style and color . By default, matplotlib draws a line between consecutive points. . plt.plot([0, 100, 100, 0, 0, 100, 50, 0, 100], [0, 0, 100, 100, 0, 100, 130, 100, 0]) plt.axis([-10, 110, -10, 140]) plt.show() . You can pass a 3rd argument to change the line&#39;s style and color. For example &quot;g--&quot; means &quot;green dashed line&quot;. . plt.plot([0, 100, 100, 0, 0, 100, 50, 0, 100], [0, 0, 100, 100, 0, 100, 130, 100, 0], &quot;g--&quot;) plt.axis([-10, 110, -10, 140]) plt.show() . You can plot multiple lines on one graph very simply: just pass x1, y1, [style1], x2, y2, [style2], ... . For example: . plt.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], &quot;r-&quot;, [0, 100, 50, 0, 100], [0, 100, 130, 100, 0], &quot;g--&quot;) plt.axis([-10, 110, -10, 140]) plt.show() . Or simply call plot multiple times before calling show. . plt.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], &quot;r-&quot;) plt.plot([0, 100, 50, 0, 100], [0, 100, 130, 100, 0], &quot;g--&quot;) plt.axis([-10, 110, -10, 140]) plt.show() . You can also draw simple points instead of lines. Here&#39;s an example with green dashes, red dotted line and blue triangles. Check out the documentation for the full list of style &amp; color options. . x = np.linspace(-1.4, 1.4, 30) plt.plot(x, x, &#39;g--&#39;, x, x**2, &#39;r:&#39;, x, x**3, &#39;b^&#39;) plt.show() . The plot function returns a list of Line2D objects (one for each line). You can set extra attributes on these lines, such as the line width, the dash style or the alpha level. See the full list of attributes in the documentation. . x = np.linspace(-1.4, 1.4, 30) line1, line2, line3 = plt.plot(x, x, &#39;g--&#39;, x, x**2, &#39;r:&#39;, x, x**3, &#39;b^&#39;) line1.set_linewidth(3.0) line1.set_dash_capstyle(&quot;round&quot;) line3.set_alpha(0.2) plt.show() . Saving a figure . Saving a figure to disk is as simple as calling savefig with the name of the file (or a file object). The available image formats depend on the graphics backend you use. . x = np.linspace(-1.4, 1.4, 30) plt.plot(x, x**2) plt.savefig(&quot;my_square_function.png&quot;, transparent=True) . Subplots . A matplotlib figure may contain multiple subplots. These subplots are organized in a grid. To create a subplot, just call the subplot function, and specify the number of rows and columns in the figure, and the index of the subplot you want to draw on (starting from 1, then left to right, and top to bottom). Note that pyplot keeps track of the currently active subplot (which you can get a reference to by calling plt.gca()), so when you call the plot function, it draws on the active subplot. . x = np.linspace(-1.4, 1.4, 30) plt.subplot(2, 2, 1) # 2 rows, 2 columns, 1st subplot = top left plt.plot(x, x) plt.subplot(2, 2, 2) # 2 rows, 2 columns, 2nd subplot = top right plt.plot(x, x**2) plt.subplot(2, 2, 3) # 2 rows, 2 columns, 3rd subplot = bottow left plt.plot(x, x**3) plt.subplot(2, 2, 4) # 2 rows, 2 columns, 4th subplot = bottom right plt.plot(x, x**4) plt.show() . Note that subplot(223) is a shorthand for subplot(2, 2, 3). | . It is easy to create subplots that span across multiple grid cells like so: . plt.subplot(2, 2, 1) # 2 rows, 2 columns, 1st subplot = top left plt.plot(x, x) plt.subplot(2, 2, 2) # 2 rows, 2 columns, 2nd subplot = top right plt.plot(x, x**2) plt.subplot(2, 1, 2) # 2 rows, *1* column, 2nd subplot = bottom plt.plot(x, x**3) plt.show() . If you need more complex subplot positionning, you can use subplot2grid instead of subplot. You specify the number of rows and columns in the grid, then your subplot&#39;s position in that grid (top-left = (0,0)), and optionally how many rows and/or columns it spans. For example: . plt.subplot2grid((3,3), (0, 0), rowspan=2, colspan=2) plt.plot(x, x**2) plt.subplot2grid((3,3), (0, 2)) plt.plot(x, x**3) plt.subplot2grid((3,3), (1, 2), rowspan=2) plt.plot(x, x**4) plt.subplot2grid((3,3), (2, 0), colspan=2) plt.plot(x, x**5) plt.show() . If you need even more flexibility in subplot positioning, check out the GridSpec documentation . Multiple figures . It is also possible to draw multiple figures. Each figure may contain one or more subplots. By default, matplotlib creates figure(1) automatically. When you switch figure, pyplot keeps track of the currently active figure (which you can get a reference to by calling plt.gcf()), and the active subplot of that figure becomes the current subplot. . x = np.linspace(-1.4, 1.4, 30) plt.figure(1) plt.subplot(211) plt.plot(x, x**2) plt.title(&quot;Square and Cube&quot;) plt.subplot(212) plt.plot(x, x**3) plt.figure(2, figsize=(10, 5)) plt.subplot(121) plt.plot(x, x**4) plt.title(&quot;y = x**4&quot;) plt.subplot(122) plt.plot(x, x**5) plt.title(&quot;y = x**5&quot;) plt.figure(1) # back to figure 1, current subplot is 212 (bottom) plt.plot(x, -x**3, &quot;r:&quot;) plt.show() . Pyplot&#39;s state machine: implicit vs explicit . So far we have used Pyplot&#39;s state machine which keeps track of the currently active subplot. Every time you call the plot function, pyplot just draws on the currently active subplot. It also does some more magic, such as automatically creating a figure and a subplot when you call plot, if they don&#39;t exist yet. This magic is convenient in an interactive environment (such as Jupyter). . But when you are writing a program, explicit is better than implicit. Explicit code is usually easier to debug and maintain, and if you don&#39;t believe me just read the 2nd rule in the Zen of Python: . import this . The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren&#39;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you&#39;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it&#39;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let&#39;s do more of those! . Fortunately, Pyplot allows you to ignore the state machine entirely, so you can write beautifully explicit code. Simply call the subplots function and use the figure object and the list of axes objects that are returned. No more magic! For example: . x = np.linspace(-2, 2, 200) fig1, (ax_top, ax_bottom) = plt.subplots(2, 1, sharex=True) fig1.set_size_inches(10,5) line1, line2 = ax_top.plot(x, np.sin(3*x**2), &quot;r-&quot;, x, np.cos(5*x**2), &quot;b-&quot;) line3, = ax_bottom.plot(x, np.sin(3*x), &quot;r-&quot;) ax_top.grid(True) fig2, ax = plt.subplots(1, 1) ax.plot(x, x**2) plt.show() . For consistency, we will continue to use pyplot&#39;s state machine in the rest of this tutorial, but we recommend using the object-oriented interface in your programs. . Pylab vs Pyplot vs Matplotlib . There is some confusion around the relationship between pylab, pyplot and matplotlib. It&#39;s simple: matplotlib is the full library, it contains everything including pylab and pyplot. . Pyplot provides a number of tools to plot graphs, including the state-machine interface to the underlying object-oriented plotting library. . Pylab is a convenience module that imports matplotlib.pyplot and NumPy in a single name space. You will find many examples using pylab, but it is no longer recommended (because explicit imports are better than implicit ones). . Drawing text . You can call text to add text at any location in the graph. Just specify the horizontal and vertical coordinates and the text, and optionally some extra attributes. Any text in matplotlib may contain TeX equation expressions, see the documentation for more details. . x = np.linspace(-1.5, 1.5, 30) px = 0.8 py = px**2 plt.plot(x, x**2, &quot;b-&quot;, px, py, &quot;ro&quot;) plt.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;blue&#39;, horizontalalignment=&quot;center&quot;) plt.text(px - 0.08, py, &quot;Beautiful point&quot;, ha=&quot;right&quot;, weight=&quot;heavy&quot;) plt.text(px, py, &quot;x = %0.2f ny = %0.2f&quot;%(px, py), rotation=50, color=&#39;gray&#39;) plt.show() . Note: ha is an alias for horizontalalignment | . For more text properties, visit the documentation. . It is quite frequent to annotate elements of a graph, such as the beautiful point above. The annotate function makes this easy: just indicate the location of the point of interest, and the position of the text, plus optionally some extra attributes for the text and the arrow. . plt.plot(x, x**2, px, py, &quot;ro&quot;) plt.annotate(&quot;Beautiful point&quot;, xy=(px, py), xytext=(px-1.3,py+0.5), color=&quot;green&quot;, weight=&quot;heavy&quot;, fontsize=14, arrowprops={&quot;facecolor&quot;: &quot;lightgreen&quot;}) plt.show() . You can also add a bounding box around your text by using the bbox attribute: . plt.plot(x, x**2, px, py, &quot;ro&quot;) bbox_props = dict(boxstyle=&quot;rarrow,pad=0.3&quot;, ec=&quot;b&quot;, lw=2, fc=&quot;lightblue&quot;) plt.text(px-0.2, py, &quot;Beautiful point&quot;, bbox=bbox_props, ha=&quot;right&quot;) bbox_props = dict(boxstyle=&quot;round4,pad=1,rounding_size=0.2&quot;, ec=&quot;black&quot;, fc=&quot;#EEEEFF&quot;, lw=5) plt.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;black&#39;, ha=&quot;center&quot;, bbox=bbox_props) plt.show() . Just for fun, if you want an xkcd-style plot, just draw within a with plt.xkcd() section: . with plt.xkcd(): plt.plot(x, x**2, px, py, &quot;ro&quot;) bbox_props = dict(boxstyle=&quot;rarrow,pad=0.3&quot;, ec=&quot;b&quot;, lw=2, fc=&quot;lightblue&quot;) plt.text(px-0.2, py, &quot;Beautiful point&quot;, bbox=bbox_props, ha=&quot;right&quot;) bbox_props = dict(boxstyle=&quot;round4,pad=1,rounding_size=0.2&quot;, ec=&quot;black&quot;, fc=&quot;#EEEEFF&quot;, lw=5) plt.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;black&#39;, ha=&quot;center&quot;, bbox=bbox_props) plt.show() . Legends . The simplest way to add a legend is to set a label on all lines, then just call the legend function. . x = np.linspace(-1.4, 1.4, 50) plt.plot(x, x**2, &quot;r--&quot;, label=&quot;Square function&quot;) plt.plot(x, x**3, &quot;g-&quot;, label=&quot;Cube function&quot;) plt.legend(loc=&quot;best&quot;) plt.grid(True) plt.show() . Non linear scales . Matplotlib supports non linear scales, such as logarithmic or logit scales. . x = np.linspace(0.1, 15, 500) y = x**3/np.exp(2*x) plt.figure(1) plt.plot(x, y) plt.yscale(&#39;linear&#39;) plt.title(&#39;linear&#39;) plt.grid(True) plt.figure(2) plt.plot(x, y) plt.yscale(&#39;log&#39;) plt.title(&#39;log&#39;) plt.grid(True) plt.figure(3) plt.plot(x, y) plt.yscale(&#39;logit&#39;) plt.title(&#39;logit&#39;) plt.grid(True) plt.figure(4) plt.plot(x, y - y.mean()) plt.yscale(&#39;symlog&#39;, linthreshy=0.05) plt.title(&#39;symlog&#39;) plt.grid(True) plt.show() . Ticks and tickers . The axes have little marks called &quot;ticks&quot;. To be precise, &quot;ticks&quot; are the locations of the marks (eg. (-1, 0, 1)), &quot;tick lines&quot; are the small lines drawn at those locations, &quot;tick labels&quot; are the labels drawn next to the tick lines, and &quot;tickers&quot; are objects that are capable of deciding where to place ticks. The default tickers typically do a pretty good job at placing ~5 to 8 ticks at a reasonable distance from one another. . But sometimes you need more control (eg. there are too many tick labels on the logit graph above). Fortunately, matplotlib gives you full control over ticks. You can even activate minor ticks. . x = np.linspace(-2, 2, 100) plt.figure(1, figsize=(15,10)) plt.subplot(131) plt.plot(x, x**3) plt.grid(True) plt.title(&quot;Default ticks&quot;) ax = plt.subplot(132) plt.plot(x, x**3) ax.xaxis.set_ticks(np.arange(-2, 2, 1)) plt.grid(True) plt.title(&quot;Manual ticks on the x-axis&quot;) ax = plt.subplot(133) plt.plot(x, x**3) plt.minorticks_on() ax.tick_params(axis=&#39;x&#39;, which=&#39;minor&#39;, bottom=&#39;off&#39;) ax.xaxis.set_ticks([-2, 0, 1, 2]) ax.yaxis.set_ticks(np.arange(-5, 5, 1)) ax.yaxis.set_ticklabels([&quot;min&quot;, -4, -3, -2, -1, 0, 1, 2, 3, &quot;max&quot;]) plt.title(&quot;Manual ticks and tick labels n(plus minor ticks) on the y-axis&quot;) plt.grid(True) plt.show() . Polar projection . Drawing a polar graph is as easy as setting the projection attribute to &quot;polar&quot; when creating the subplot. . radius = 1 theta = np.linspace(0, 2*np.pi*radius, 1000) plt.subplot(111, projection=&#39;polar&#39;) plt.plot(theta, np.sin(5*theta), &quot;g-&quot;) plt.plot(theta, 0.5*np.cos(20*theta), &quot;b-&quot;) plt.show() . 3D projection . Plotting 3D graphs is quite straightforward. You need to import Axes3D, which registers the &quot;3d&quot; projection. Then create a subplot setting the projection to &quot;3d&quot;. This returns an Axes3DSubplot object, which you can use to call plot_surface, giving x, y, and z coordinates, plus optional attributes. . from mpl_toolkits.mplot3d import Axes3D x = np.linspace(-5, 5, 50) y = np.linspace(-5, 5, 50) X, Y = np.meshgrid(x, y) R = np.sqrt(X**2 + Y**2) Z = np.sin(R) figure = plt.figure(1, figsize = (12, 4)) subplot3d = plt.subplot(111, projection=&#39;3d&#39;) surface = subplot3d.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, linewidth=0.1) plt.show() . Another way to display this same data is via a contour plot. . plt.contourf(X, Y, Z, cmap=matplotlib.cm.coolwarm) plt.colorbar() plt.show() . Scatter plot . To draw a scatter plot, simply provide the x and y coordinates of the points. . from numpy.random import rand x, y = rand(2, 100) plt.scatter(x, y) plt.show() . You may also optionally provide the scale of each point. . x, y, scale = rand(3, 100) scale = 500 * scale ** 5 plt.scatter(x, y, s=scale) plt.show() . And as usual there are a number of other attributes you can set, such as the fill and edge colors and the alpha level. . for color in [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]: n = 100 x, y = rand(2, n) scale = 500.0 * rand(n) ** 5 plt.scatter(x, y, s=scale, c=color, alpha=0.3, edgecolors=&#39;blue&#39;) plt.grid(True) plt.show() . Lines . You can draw lines simply using the plot function, as we have done so far. However, it is often convenient to create a utility function that plots a (seemingly) infinite line across the graph, given a slope and an intercept. You can also use the hlines and vlines functions that plot horizontal and vertical line segments. For example: . from numpy.random import randn def plot_line(axis, slope, intercept, **kargs): xmin, xmax = axis.get_xlim() plt.plot([xmin, xmax], [xmin*slope+intercept, xmax*slope+intercept], **kargs) x = randn(1000) y = 0.5*x + 5 + randn(1000)*2 plt.axis([-2.5, 2.5, -5, 15]) plt.scatter(x, y, alpha=0.2) plt.plot(1, 0, &quot;ro&quot;) plt.vlines(1, -5, 0, color=&quot;red&quot;) plt.hlines(0, -2.5, 1, color=&quot;red&quot;) plot_line(axis=plt.gca(), slope=0.5, intercept=5, color=&quot;magenta&quot;) plt.grid(True) plt.show() . Histograms . data = [1, 1.1, 1.8, 2, 2.1, 3.2, 3, 3, 3, 3] plt.subplot(211) plt.hist(data, bins = 10, rwidth=0.8) plt.subplot(212) plt.hist(data, bins = [1, 1.5, 2, 2.5, 3], rwidth=0.95) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Frequency&quot;) plt.show() . data1 = np.random.randn(400) data2 = np.random.randn(500) + 3 data3 = np.random.randn(450) + 6 data4a = np.random.randn(200) + 9 data4b = np.random.randn(100) + 10 plt.hist(data1, bins=5, color=&#39;g&#39;, alpha=0.75, label=&#39;bar hist&#39;) # default histtype=&#39;bar&#39; plt.hist(data2, color=&#39;b&#39;, alpha=0.65, histtype=&#39;stepfilled&#39;, label=&#39;stepfilled hist&#39;) plt.hist(data3, color=&#39;r&#39;, histtype=&#39;step&#39;, label=&#39;step hist&#39;) plt.hist((data4a, data4b), color=(&#39;r&#39;,&#39;m&#39;), alpha=0.55, histtype=&#39;barstacked&#39;, label=(&#39;barstacked a&#39;, &#39;barstacked b&#39;)) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Frequency&quot;) plt.legend() plt.grid(True) plt.show() . Images . Reading, generating and plotting images in matplotlib is quite straightforward. . To read an image, just import the matplotlib.image module, and call its imread function, passing it the file name (or file object). This returns the image data, as a NumPy array. Let&#39;s try this with the my_square_function.png image we saved earlier. . import matplotlib.image as mpimg img = mpimg.imread(&#39;my_square_function.png&#39;) print(img.shape, img.dtype) . (288, 432, 4) float32 . We have loaded a 288x432 image. Each pixel is represented by a 4-element array: red, green, blue, and alpha levels, stored as 32-bit floats between 0 andÂ 1. Now all we need to do is to call imshow: . plt.imshow(img) plt.show() . Tadaaa! You may want to hide the axes when you are displaying an image: . plt.imshow(img) plt.axis(&#39;off&#39;) plt.show() . It&#39;s just as easy to generate your own image: . img = np.arange(100*100).reshape(100, 100) print(img) plt.imshow(img) plt.show() . [[ 0 1 2 ..., 97 98 99] [ 100 101 102 ..., 197 198 199] [ 200 201 202 ..., 297 298 299] ..., [9700 9701 9702 ..., 9797 9798 9799] [9800 9801 9802 ..., 9897 9898 9899] [9900 9901 9902 ..., 9997 9998 9999]] . As we did not provide RGB levels, the imshow function automatically maps values to a color gradient. By default, the color gradient goes from blue (for low values) to red (for high values), but you can select another color map. For example: . plt.imshow(img, cmap=&quot;hot&quot;) plt.show() . You can also generate an RGB image directly: . img = np.empty((20,30,3)) img[:, :10] = [0, 0, 0.6] img[:, 10:20] = [1, 1, 1] img[:, 20:] = [0.6, 0, 0] plt.imshow(img) plt.show() . Since the img array is just quite small (20x30), when the imshow function displays it, it grows the image to the figure&#39;s size. By default it uses bilinear interpolation to fill the added pixels. This is why the edges look blurry. You can select another interpolation algorithm, such as copying the color of the nearest pixel: . plt.imshow(img, interpolation=&quot;nearest&quot;) plt.show() . Animations . Although matplotlib is mostly used to generate images, it is also capable of displaying animations, depending on the Backend you use. In a Jupyter notebook, we need to use the nbagg backend to use interactive matplotlib features, including animations. We also need to import matplotlib.animation. . %matplotlib nbagg import matplotlib.animation as animation . In this example, we start by creating data points, then we create an empty plot, we define the update function that will be called at every iteration of the animation, and finally we add an animation to the plot by creating a FuncAnimation instance. . The FuncAnimation constructor takes a figure, an update function and optional arguments. We specify that we want a 100-frame long animation, with 20ms between each frame. At each iteration, FuncAnimation calls our update function and passes it the frame number num (from 0 to 99 in our case) followed by the extra arguments that we specified with fargs. . Our update function simply sets the line data to be the first num data points (so the data gets drawn gradually), and just for fun we also add a small random number to each data point so that the line appears to wiggle. . x = np.linspace(-1, 1, 100) y = np.sin(x**2*25) data = np.array([x, y]) fig = plt.figure() line, = plt.plot([], [], &quot;r-&quot;) # start with an empty plot plt.axis([-1.1, 1.1, -1.1, 1.1]) plt.plot([-0.5, 0.5], [0, 0], &quot;b-&quot;, [0, 0], [-0.5, 0.5], &quot;b-&quot;, 0, 0, &quot;ro&quot;) plt.grid(True) plt.title(&quot;Marvelous animation&quot;) # this function will be called at every iteration def update_line(num, data, line): line.set_data(data[..., :num] + np.random.rand(2, num) / 25) # we only plot the first `num` data points. return line, line_ani = animation.FuncAnimation(fig, update_line, frames=100, fargs=(data, line), interval=67) plt.show() . Saving animations to video files . Matplotlib relies on 3rd-party libraries to write videos such as FFMPEG or mencoder. In this example we will be using FFMPEG so be sure to install it first. . Writer = animation.writers[&#39;ffmpeg&#39;] writer = Writer(fps=15, metadata=dict(artist=&#39;Me&#39;), bitrate=1800) line_ani.save(&#39;my_wiggly_animation.mp4&#39;, writer=writer) . What next? . Now you know all the basics of matplotlib, but there are many more options available. The best way to learn more, is to visit the gallery, look at the images, choose a plot that you are interested in, then just copy the code in a Jupyter notebook and play around with it. . &lt;/div&gt; .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/tools_matplotlib.html",
            "relUrl": "/2020/03/09/tools_matplotlib.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": "Math - Linear Algebra . permalink: /math_linear_algebra | . Linear Algebra is the branch of mathematics that studies vector spaces and linear transformations between vector spaces, such as rotating a shape, scaling it up or down, translating it (ie. moving it), etc. . Machine Learning relies heavily on Linear Algebra, so it is essential to understand what vectors and matrices are, what operations you can perform with them, and how they can be useful. . Vectors . Definition . A vector is a quantity defined by a magnitude and a direction. For example, a rocket&#39;s velocity is a 3-dimensional vector: its magnitude is the speed of the rocket, and its direction is (hopefully) up. A vector can be represented by an array of numbers called scalars. Each scalar corresponds to the magnitude of the vector with regards to each dimension. . For example, say the rocket is going up at a slight angle: it has a vertical speed of 5,000Â m/s, and also a slight speed towards the East at 10 m/s, and a slight speed towards the North at 50 m/s. The rocket&#39;s velocity may be represented by the following vector: . velocity $= begin{pmatrix} 10 50 5000 end{pmatrix}$ . Note: by convention vectors are generally presented in the form of columns. Also, vector names are generally lowercase to distinguish them from matrices (which we will discuss below) and in bold (when possible) to distinguish them from simple scalar values such as ${meters _per _second} = 5026$. . A list of N numbers may also represent the coordinates of a point in an N-dimensional space, so it is quite frequent to represent vectors as simple points instead of arrows. A vector with 1 element may be represented as an arrow or a point on an axis, a vector with 2 elements is an arrow or a point on a plane, a vector with 3 elements is an arrow or point in space, and a vector with N elements is an arrow or a point in an N-dimensional spaceâ€¦ which most people find hard to imagine. . Purpose . Vectors have many purposes in Machine Learning, most notably to represent observations and predictions. For example, say we built a Machine Learning system to classify videos into 3 categories (good, spam, clickbait) based on what we know about them. For each video, we would have a vector representing what we know about it, such as: . video $= begin{pmatrix} 10.5 5.2 3.25 7.0 end{pmatrix}$ . This vector could represent a video that lasts 10.5 minutes, but only 5.2% viewers watch for more than a minute, it gets 3.25 views per day on average, and it was flagged 7 times as spam. As you can see, each axis may have a different meaning. . Based on this vector our Machine Learning system may predict that there is an 80% probability that it is a spam video, 18% that it is clickbait, and 2% that it is a good video. This could be represented as the following vector: . class_probabilities $= begin{pmatrix} 0.80 0.18 0.02 end{pmatrix}$ . Vectors in python . In python, a vector can be represented in many ways, the simplest being a regular python list of numbers: . [10.5, 5.2, 3.25, 7.0] . [10.5, 5.2, 3.25, 7.0] . Since we plan to do quite a lot of scientific calculations, it is much better to use NumPy&#39;s ndarray, which provides a lot of convenient and optimized implementations of essential mathematical operations on vectors (for more details about NumPy, check out the NumPy tutorial). For example: . import numpy as np video = np.array([10.5, 5.2, 3.25, 7.0]) video . array([ 10.5 , 5.2 , 3.25, 7. ]) . The size of a vector can be obtained using the size attribute: . video.size . 4 . The $i^{th}$ element (also called entry or item) of a vector $ textbf{v}$ is noted $ textbf{v}_i$. . Note that indices in mathematics generally start at 1, but in programming they usually start at 0. So to access $ textbf{video}_3$ programmatically, we would write: . video[2] # 3rd element . 3.25 . Plotting vectors . To plot vectors we will use matplotlib, so let&#39;s start by importing it (for details about matplotlib, check the matplotlib tutorial): . %matplotlib inline import matplotlib.pyplot as plt . 2D vectors . Let&#39;s create a couple very simple 2D vectors to plot: . u = np.array([2, 5]) v = np.array([3, 1]) . These vectors each have 2 elements, so they can easily be represented graphically on a 2D graph, for example as points: . x_coords, y_coords = zip(u, v) plt.scatter(x_coords, y_coords, color=[&quot;r&quot;,&quot;b&quot;]) plt.axis([0, 9, 0, 6]) plt.grid() plt.show() . Vectors can also be represented as arrows. Let&#39;s create a small convenience function to draw nice arrows: . def plot_vector2d(vector2d, origin=[0, 0], **options): return plt.arrow(origin[0], origin[1], vector2d[0], vector2d[1], head_width=0.2, head_length=0.3, length_includes_head=True, **options) . Now let&#39;s draw the vectors u and v as arrows: . plot_vector2d(u, color=&quot;r&quot;) plot_vector2d(v, color=&quot;b&quot;) plt.axis([0, 9, 0, 6]) plt.grid() plt.show() . 3D vectors . Plotting 3D vectors is also relatively straightforward. First let&#39;s create two 3D vectors: . a = np.array([1, 2, 8]) b = np.array([5, 6, 3]) . Now let&#39;s plot them using matplotlib&#39;s Axes3D: . from mpl_toolkits.mplot3d import Axes3D subplot3d = plt.subplot(111, projection=&#39;3d&#39;) x_coords, y_coords, z_coords = zip(a,b) subplot3d.scatter(x_coords, y_coords, z_coords) subplot3d.set_zlim3d([0, 9]) plt.show() . It is a bit hard to visualize exactly where in space these two points are, so let&#39;s add vertical lines. We&#39;ll create a small convenience function to plot a list of 3d vectors with vertical lines attached: . def plot_vectors3d(ax, vectors3d, z0, **options): for v in vectors3d: x, y, z = v ax.plot([x,x], [y,y], [z0, z], color=&quot;gray&quot;, linestyle=&#39;dotted&#39;, marker=&quot;.&quot;) x_coords, y_coords, z_coords = zip(*vectors3d) ax.scatter(x_coords, y_coords, z_coords, **options) subplot3d = plt.subplot(111, projection=&#39;3d&#39;) subplot3d.set_zlim([0, 9]) plot_vectors3d(subplot3d, [a,b], 0, color=(&quot;r&quot;,&quot;b&quot;)) plt.show() . Norm . The norm of a vector $ textbf{u}$, noted $ left Vert textbf{u} right |$, is a measure of the length (a.k.a. the magnitude) of $ textbf{u}$. There are multiple possible norms, but the most common one (and the only one we will discuss here) is the Euclidian norm, which is defined as: . $ left Vert textbf{u} right | = sqrt{ sum_{i}{ textbf{u}_i}^2}$ . We could implement this easily in pure python, recalling that $ sqrt x = x^{ frac{1}{2}}$ . def vector_norm(vector): squares = [element**2 for element in vector] return sum(squares)**0.5 print(&quot;||&quot;, u, &quot;|| =&quot;) vector_norm(u) . || [2 5] || = . 5.3851648071345037 . However, it is much more efficient to use NumPy&#39;s norm function, available in the linalg (Linear Algebra) module: . import numpy.linalg as LA LA.norm(u) . 5.3851648071345037 . Let&#39;s plot a little diagram to confirm that the length of vector $ textbf{v}$ is indeed $ approx5.4$: . radius = LA.norm(u) plt.gca().add_artist(plt.Circle((0,0), radius, color=&quot;#DDDDDD&quot;)) plot_vector2d(u, color=&quot;red&quot;) plt.axis([0, 8.7, 0, 6]) plt.grid() plt.show() . Looks about right! . Addition . Vectors of same size can be added together. Addition is performed elementwise: . print(&quot; &quot;, u) print(&quot;+&quot;, v) print(&quot;-&quot;*10) u + v . [2 5] + [3 1] - . array([5, 6]) . Let&#39;s look at what vector addition looks like graphically: . plot_vector2d(u, color=&quot;r&quot;) plot_vector2d(v, color=&quot;b&quot;) plot_vector2d(v, origin=u, color=&quot;b&quot;, linestyle=&quot;dotted&quot;) plot_vector2d(u, origin=v, color=&quot;r&quot;, linestyle=&quot;dotted&quot;) plot_vector2d(u+v, color=&quot;g&quot;) plt.axis([0, 9, 0, 7]) plt.text(0.7, 3, &quot;u&quot;, color=&quot;r&quot;, fontsize=18) plt.text(4, 3, &quot;u&quot;, color=&quot;r&quot;, fontsize=18) plt.text(1.8, 0.2, &quot;v&quot;, color=&quot;b&quot;, fontsize=18) plt.text(3.1, 5.6, &quot;v&quot;, color=&quot;b&quot;, fontsize=18) plt.text(2.4, 2.5, &quot;u+v&quot;, color=&quot;g&quot;, fontsize=18) plt.grid() plt.show() . Vector addition is commutative, meaning that $ textbf{u} + textbf{v} = textbf{v} + textbf{u}$. You can see it on the previous image: following $ textbf{u}$ then $ textbf{v}$ leads to the same point as following $ textbf{v}$ then $ textbf{u}$. . Vector addition is also associative, meaning that $ textbf{u} + ( textbf{v} + textbf{w}) = ( textbf{u} + textbf{v}) + textbf{w}$. . If you have a shape defined by a number of points (vectors), and you add a vector $ textbf{v}$ to all of these points, then the whole shape gets shifted by $ textbf{v}$. This is called a geometric translation: . t1 = np.array([2, 0.25]) t2 = np.array([2.5, 3.5]) t3 = np.array([1, 2]) x_coords, y_coords = zip(t1, t2, t3, t1) plt.plot(x_coords, y_coords, &quot;c--&quot;, x_coords, y_coords, &quot;co&quot;) plot_vector2d(v, t1, color=&quot;r&quot;, linestyle=&quot;:&quot;) plot_vector2d(v, t2, color=&quot;r&quot;, linestyle=&quot;:&quot;) plot_vector2d(v, t3, color=&quot;r&quot;, linestyle=&quot;:&quot;) t1b = t1 + v t2b = t2 + v t3b = t3 + v x_coords_b, y_coords_b = zip(t1b, t2b, t3b, t1b) plt.plot(x_coords_b, y_coords_b, &quot;b-&quot;, x_coords_b, y_coords_b, &quot;bo&quot;) plt.text(4, 4.2, &quot;v&quot;, color=&quot;r&quot;, fontsize=18) plt.text(3, 2.3, &quot;v&quot;, color=&quot;r&quot;, fontsize=18) plt.text(3.5, 0.4, &quot;v&quot;, color=&quot;r&quot;, fontsize=18) plt.axis([0, 6, 0, 5]) plt.grid() plt.show() . Finally, substracting a vector is like adding the opposite vector. . Multiplication by a scalar . Vectors can be multiplied by scalars. All elements in the vector are multiplied by that number, for example: . print(&quot;1.5 *&quot;, u, &quot;=&quot;) 1.5 * u . 1.5 * [2 5] = . array([ 3. , 7.5]) . Graphically, scalar multiplication results in changing the scale of a figure, hence the name scalar. The distance from the origin (the point at coordinates equal to zero) is also multiplied by the scalar. For example, let&#39;s scale up by a factor of k = 2.5: . k = 2.5 t1c = k * t1 t2c = k * t2 t3c = k * t3 plt.plot(x_coords, y_coords, &quot;c--&quot;, x_coords, y_coords, &quot;co&quot;) plot_vector2d(t1, color=&quot;r&quot;) plot_vector2d(t2, color=&quot;r&quot;) plot_vector2d(t3, color=&quot;r&quot;) x_coords_c, y_coords_c = zip(t1c, t2c, t3c, t1c) plt.plot(x_coords_c, y_coords_c, &quot;b-&quot;, x_coords_c, y_coords_c, &quot;bo&quot;) plot_vector2d(k * t1, color=&quot;b&quot;, linestyle=&quot;:&quot;) plot_vector2d(k * t2, color=&quot;b&quot;, linestyle=&quot;:&quot;) plot_vector2d(k * t3, color=&quot;b&quot;, linestyle=&quot;:&quot;) plt.axis([0, 9, 0, 9]) plt.grid() plt.show() . As you might guess, dividing a vector by a scalar is equivalent to multiplying by its multiplicative inverse (reciprocal): . $ dfrac{ textbf{u}}{ lambda} = dfrac{1}{ lambda} times textbf{u}$ . Scalar multiplication is commutative: $ lambda times textbf{u} = textbf{u} times lambda$. . It is also associative: $ lambda_1 times ( lambda_2 times textbf{u}) = ( lambda_1 times lambda_2) times textbf{u}$. . Finally, it is distributive over addition of vectors: $ lambda times ( textbf{u} + textbf{v}) = lambda times textbf{u} + lambda times textbf{v}$. . Zero, unit and normalized vectors . A zero-vector is a vector full of 0s. | A unit vector is a vector with a norm equal to 1. | The normalized vector of a non-null vector $ textbf{u}$, noted $ hat{ textbf{u}}$, is the unit vector that points in the same direction as $ textbf{u}$. It is equal to: $ hat{ textbf{u}} = dfrac{ textbf{u}}{ left Vert textbf{u} right |}$ | . plt.gca().add_artist(plt.Circle((0,0),1,color=&#39;c&#39;)) plt.plot(0, 0, &quot;ko&quot;) plot_vector2d(v / LA.norm(v), color=&quot;k&quot;) plot_vector2d(v, color=&quot;b&quot;, linestyle=&quot;:&quot;) plt.text(0.3, 0.3, &quot;$ hat{u}$&quot;, color=&quot;k&quot;, fontsize=18) plt.text(1.5, 0.7, &quot;$u$&quot;, color=&quot;b&quot;, fontsize=18) plt.axis([-1.5, 5.5, -1.5, 3.5]) plt.grid() plt.show() . Dot product . Definition . The dot product (also called scalar product or inner product in the context of the Euclidian space) of two vectors $ textbf{u}$ and $ textbf{v}$ is a useful operation that comes up fairly often in linear algebra. It is noted $ textbf{u} cdot textbf{v}$, or sometimes $âŸ¨ textbf{u}| textbf{v}âŸ©$ or $( textbf{u}| textbf{v})$, and it is defined as: . $ textbf{u} cdot textbf{v} = left Vert textbf{u} right | times left Vert textbf{v} right | times cos( theta)$ . where $ theta$ is the angle between $ textbf{u}$ and $ textbf{v}$. . Another way to calculate the dot product is: . $ textbf{u} cdot textbf{v} = sum_i{ textbf{u}_i times textbf{v}_i}$ . In python . The dot product is pretty simple to implement: . def dot_product(v1, v2): return sum(v1i * v2i for v1i, v2i in zip(v1, v2)) dot_product(u, v) . 11 . But a much more efficient implementation is provided by NumPy with the dot function: . np.dot(u,v) . 11 . Equivalently, you can use the dot method of ndarrays: . u.dot(v) . 11 . Caution: the * operator will perform an elementwise multiplication, NOT a dot product: . print(&quot; &quot;,u) print(&quot;* &quot;,v, &quot;(NOT a dot product)&quot;) print(&quot;-&quot;*10) u * v . [2 5] * [3 1] (NOT a dot product) - . array([6, 5]) . Main properties . The dot product is commutative: $ textbf{u} cdot textbf{v} = textbf{v} cdot textbf{u}$. | The dot product is only defined between two vectors, not between a scalar and a vector. This means that we cannot chain dot products: for example, the expression $ textbf{u} cdot textbf{v} cdot textbf{w}$ is not defined since $ textbf{u} cdot textbf{v}$ is a scalar and $ textbf{w}$ is a vector. | This also means that the dot product is NOT associative: $( textbf{u} cdot textbf{v}) cdot textbf{w} â‰  textbf{u} cdot ( textbf{v} cdot textbf{w})$ since neither are defined. | However, the dot product is associative with regards to scalar multiplication: $ lambda times ( textbf{u} cdot textbf{v}) = ( lambda times textbf{u}) cdot textbf{v} = textbf{u} cdot ( lambda times textbf{v})$ | Finally, the dot product is distributive over addition of vectors: $ textbf{u} cdot ( textbf{v} + textbf{w}) = textbf{u} cdot textbf{v} + textbf{u} cdot textbf{w}$. | . Calculating the angle between vectors . One of the many uses of the dot product is to calculate the angle between two non-zero vectors. Looking at the dot product definition, we can deduce the following formula: . $ theta = arccos{ left ( dfrac{ textbf{u} cdot textbf{v}}{ left Vert textbf{u} right | times left Vert textbf{v} right |} right ) }$ . Note that if $ textbf{u} cdot textbf{v} = 0$, it follows that $ theta = dfrac{Ï€}{2}$. In other words, if the dot product of two non-null vectors is zero, it means that they are orthogonal. . Let&#39;s use this formula to calculate the angle between $ textbf{u}$ and $ textbf{v}$ (in radians): . def vector_angle(u, v): cos_theta = u.dot(v) / LA.norm(u) / LA.norm(v) return np.arccos(np.clip(cos_theta, -1, 1)) theta = vector_angle(u, v) print(&quot;Angle =&quot;, theta, &quot;radians&quot;) print(&quot; =&quot;, theta * 180 / np.pi, &quot;degrees&quot;) . Angle = 0.868539395286 radians = 49.7636416907 degrees . Note: due to small floating point errors, cos_theta may be very slightly outside of the $[-1, 1]$ interval, which would make arccos fail. This is why we clipped the value within the range, using NumPy&#39;s clip function. . Projecting a point onto an axis . The dot product is also very useful to project points onto an axis. The projection of vector $ textbf{v}$ onto $ textbf{u}$&#39;s axis is given by this formula: . $ textbf{proj}_{ textbf{u}}{ textbf{v}} = dfrac{ textbf{u} cdot textbf{v}}{ left Vert textbf{u} right | ^2} times textbf{u}$ . Which is equivalent to: . $ textbf{proj}_{ textbf{u}}{ textbf{v}} = ( textbf{v} cdot hat{ textbf{u}}) times hat{ textbf{u}}$ . u_normalized = u / LA.norm(u) proj = v.dot(u_normalized) * u_normalized plot_vector2d(u, color=&quot;r&quot;) plot_vector2d(v, color=&quot;b&quot;) plot_vector2d(proj, color=&quot;k&quot;, linestyle=&quot;:&quot;) plt.plot(proj[0], proj[1], &quot;ko&quot;) plt.plot([proj[0], v[0]], [proj[1], v[1]], &quot;b:&quot;) plt.text(1, 2, &quot;$proj_u v$&quot;, color=&quot;k&quot;, fontsize=18) plt.text(1.8, 0.2, &quot;$v$&quot;, color=&quot;b&quot;, fontsize=18) plt.text(0.8, 3, &quot;$u$&quot;, color=&quot;r&quot;, fontsize=18) plt.axis([0, 8, 0, 5.5]) plt.grid() plt.show() . Matrices . A matrix is a rectangular array of scalars (ie. any number: integer, real or complex) arranged in rows and columns, for example: . begin{bmatrix} 10 &amp; 20 &amp; 30 40 &amp; 50 &amp; 60 end{bmatrix}You can also think of a matrix as a list of vectors: the previous matrix contains either 2 horizontal 3D vectors or 3 vertical 2D vectors. . Matrices are convenient and very efficient to run operations on many vectors at a time. We will also see that they are great at representing and performing linear transformations such rotations, translations and scaling. . Matrices in python . In python, a matrix can be represented in various ways. The simplest is just a list of python lists: . [ [10, 20, 30], [40, 50, 60] ] . [[10, 20, 30], [40, 50, 60]] . A much more efficient way is to use the NumPy library which provides optimized implementations of many matrix operations: . A = np.array([ [10,20,30], [40,50,60] ]) A . array([[10, 20, 30], [40, 50, 60]]) . By convention matrices generally have uppercase names, such as $A$. . In the rest of this tutorial, we will assume that we are using NumPy arrays (type ndarray) to represent matrices. . Size . The size of a matrix is defined by its number of rows and number of columns. It is noted $rows times columns$. For example, the matrix $A$ above is an example of a $2 times 3$ matrix: 2 rows, 3 columns. Caution: a $3 times 2$ matrix would have 3 rows and 2 columns. . To get a matrix&#39;s size in NumPy: . A.shape . (2, 3) . Caution: the size attribute represents the number of elements in the ndarray, not the matrix&#39;s size: . A.size . 6 . Element indexing . The number located in the $i^{th}$ row, and $j^{th}$ column of a matrix $X$ is sometimes noted $X_{i,j}$ or $X_{ij}$, but there is no standard notation, so people often prefer to explicitely name the elements, like this: &quot;let $X = (x_{i,j})_{1 â‰¤ i â‰¤ m, 1 â‰¤ j â‰¤ n}$&quot;. This means that $X$ is equal to: . $X = begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; x_{1,3} &amp; cdots &amp; x_{1,n} x_{2,1} &amp; x_{2,2} &amp; x_{2,3} &amp; cdots &amp; x_{2,n} x_{3,1} &amp; x_{3,2} &amp; x_{3,3} &amp; cdots &amp; x_{3,n} vdots &amp; vdots &amp; vdots &amp; ddots &amp; vdots x_{m,1} &amp; x_{m,2} &amp; x_{m,3} &amp; cdots &amp; x_{m,n} end{bmatrix}$ . However in this notebook we will use the $X_{i,j}$ notation, as it matches fairly well NumPy&#39;s notation. Note that in math indices generally start at 1, but in programming they usually start at 0. So to access $A_{2,3}$ programmatically, we need to write this: . A[1,2] # 2nd row, 3rd column . 60 . The $i^{th}$ row vector is sometimes noted $M_i$ or $M_{i,*}$, but again there is no standard notation so people often prefer to explicitely define their own names, for example: &quot;let x$_{i}$ be the $i^{th}$ row vector of matrix $X$&quot;. We will use the $M_{i,*}$, for the same reason as above. For example, to access $A_{2,*}$ (ie. $A$&#39;s 2nd row vector): . A[1, :] # 2nd row vector (as a 1D array) . array([40, 50, 60]) . Similarly, the $j^{th}$ column vector is sometimes noted $M^j$ or $M_{*,j}$, but there is no standard notation. We will use $M_{*,j}$. For example, to access $A_{*,3}$ (ie. $A$&#39;s 3rd column vector): . A[:, 2] # 3rd column vector (as a 1D array) . array([30, 60]) . Note that the result is actually a one-dimensional NumPy array: there is no such thing as a vertical or horizontal one-dimensional array. If you need to actually represent a row vector as a one-row matrix (ie. a 2D NumPy array), or a column vector as a one-column matrix, then you need to use a slice instead of an integer when accessing the row or column, for example: . A[1:2, :] # rows 2 to 3 (excluded): this returns row 2 as a one-row matrix . array([[40, 50, 60]]) . A[:, 2:3] # columns 3 to 4 (excluded): this returns column 3 as a one-column matrix . array([[30], [60]]) . Square, triangular, diagonal and identity matrices . A square matrix is a matrix that has the same number of rows and columns, for example a $3 times 3$ matrix: . begin{bmatrix} 4 &amp; 9 &amp; 2 3 &amp; 5 &amp; 7 8 &amp; 1 &amp; 6 end{bmatrix} An upper triangular matrix is a special kind of square matrix where all the elements below the main diagonal (top-left to bottom-right) are zero, for example: . begin{bmatrix} 4 &amp; 9 &amp; 2 0 &amp; 5 &amp; 7 0 &amp; 0 &amp; 6 end{bmatrix} Similarly, a lower triangular matrix is a square matrix where all elements above the main diagonal are zero, for example: . begin{bmatrix} 4 &amp; 0 &amp; 0 3 &amp; 5 &amp; 0 8 &amp; 1 &amp; 6 end{bmatrix} A triangular matrix is one that is either lower triangular or upper triangular. . A matrix that is both upper and lower triangular is called a diagonal matrix, for example: . begin{bmatrix} 4 &amp; 0 &amp; 0 0 &amp; 5 &amp; 0 0 &amp; 0 &amp; 6 end{bmatrix}You can construct a diagonal matrix using NumPy&#39;s diag function: . np.diag([4, 5, 6]) . array([[4, 0, 0], [0, 5, 0], [0, 0, 6]]) . If you pass a matrix to the diag function, it will happily extract the diagonal values: . D = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9], ]) np.diag(D) . array([1, 5, 9]) . Finally, the identity matrix of size $n$, noted $I_n$, is a diagonal matrix of size $n times n$ with $1$&#39;s in the main diagonal, for example $I_3$: . begin{bmatrix} 1 &amp; 0 &amp; 0 0 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 end{bmatrix}Numpy&#39;s eye function returns the identity matrix of the desired size: . np.eye(3) . array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) . The identity matrix is often noted simply $I$ (instead of $I_n$) when its size is clear given the context. It is called the identity matrix because multiplying a matrix with it leaves the matrix unchanged as we will see below. . Adding matrices . If two matrices $Q$ and $R$ have the same size $m times n$, they can be added together. Addition is performed elementwise: the result is also a $m times n$ matrix $S$ where each element is the sum of the elements at the corresponding position: $S_{i,j} = Q_{i,j} + R_{i,j}$ . $S = begin{bmatrix} Q_{11} + R_{11} &amp; Q_{12} + R_{12} &amp; Q_{13} + R_{13} &amp; cdots &amp; Q_{1n} + R_{1n} Q_{21} + R_{21} &amp; Q_{22} + R_{22} &amp; Q_{23} + R_{23} &amp; cdots &amp; Q_{2n} + R_{2n} Q_{31} + R_{31} &amp; Q_{32} + R_{32} &amp; Q_{33} + R_{33} &amp; cdots &amp; Q_{3n} + R_{3n} vdots &amp; vdots &amp; vdots &amp; ddots &amp; vdots Q_{m1} + R_{m1} &amp; Q_{m2} + R_{m2} &amp; Q_{m3} + R_{m3} &amp; cdots &amp; Q_{mn} + R_{mn} end{bmatrix}$ . For example, let&#39;s create a $2 times 3$ matrix $B$ and compute $A + B$: . B = np.array([[1,2,3], [4, 5, 6]]) B . array([[1, 2, 3], [4, 5, 6]]) . A . array([[10, 20, 30], [40, 50, 60]]) . A + B . array([[11, 22, 33], [44, 55, 66]]) . Addition is commutative, meaning that $A + B = B + A$: . B + A . array([[11, 22, 33], [44, 55, 66]]) . It is also associative, meaning that $A + (B + C) = (A + B) + C$: . C = np.array([[100,200,300], [400, 500, 600]]) A + (B + C) . array([[111, 222, 333], [444, 555, 666]]) . (A + B) + C . array([[111, 222, 333], [444, 555, 666]]) . Scalar multiplication . A matrix $M$ can be multiplied by a scalar $ lambda$. The result is noted $ lambda M$, and it is a matrix of the same size as $M$ with all elements multiplied by $ lambda$: . $ lambda M = begin{bmatrix} lambda times M_{11} &amp; lambda times M_{12} &amp; lambda times M_{13} &amp; cdots &amp; lambda times M_{1n} lambda times M_{21} &amp; lambda times M_{22} &amp; lambda times M_{23} &amp; cdots &amp; lambda times M_{2n} lambda times M_{31} &amp; lambda times M_{32} &amp; lambda times M_{33} &amp; cdots &amp; lambda times M_{3n} vdots &amp; vdots &amp; vdots &amp; ddots &amp; vdots lambda times M_{m1} &amp; lambda times M_{m2} &amp; lambda times M_{m3} &amp; cdots &amp; lambda times M_{mn} end{bmatrix}$ . A more concise way of writing this is: . $( lambda M)_{i,j} = lambda (M)_{i,j}$ . In NumPy, simply use the * operator to multiply a matrix by a scalar. For example: . 2 * A . array([[ 20, 40, 60], [ 80, 100, 120]]) . Scalar multiplication is also defined on the right hand side, and gives the same result: $M lambda = lambda M$. For example: . A * 2 . array([[ 20, 40, 60], [ 80, 100, 120]]) . This makes scalar multiplication commutative. . It is also associative, meaning that $ alpha ( beta M) = ( alpha times beta) M$, where $ alpha$ and $ beta$ are scalars. For example: . 2 * (3 * A) . array([[ 60, 120, 180], [240, 300, 360]]) . (2 * 3) * A . array([[ 60, 120, 180], [240, 300, 360]]) . Finally, it is distributive over addition of matrices, meaning that $ lambda (Q + R) = lambda Q + lambda R$: . 2 * (A + B) . array([[ 22, 44, 66], [ 88, 110, 132]]) . 2 * A + 2 * B . array([[ 22, 44, 66], [ 88, 110, 132]]) . Matrix multiplication . So far, matrix operations have been rather intuitive. But multiplying matrices is a bit more involved. . A matrix $Q$ of size $m times n$ can be multiplied by a matrix $R$ of size $n times q$. It is noted simply $QR$ without multiplication sign or dot. The result $P$ is an $m times q$ matrix where each element is computed as a sum of products: . $P_{i,j} = sum_{k=1}^n{Q_{i,k} times R_{k,j}}$ . The element at position $i,j$ in the resulting matrix is the sum of the products of elements in row $i$ of matrix $Q$ by the elements in column $j$ of matrix $R$. . $P = begin{bmatrix} Q_{11} R_{11} + Q_{12} R_{21} + cdots + Q_{1n} R_{n1} &amp; Q_{11} R_{12} + Q_{12} R_{22} + cdots + Q_{1n} R_{n2} &amp; cdots &amp; Q_{11} R_{1q} + Q_{12} R_{2q} + cdots + Q_{1n} R_{nq} Q_{21} R_{11} + Q_{22} R_{21} + cdots + Q_{2n} R_{n1} &amp; Q_{21} R_{12} + Q_{22} R_{22} + cdots + Q_{2n} R_{n2} &amp; cdots &amp; Q_{21} R_{1q} + Q_{22} R_{2q} + cdots + Q_{2n} R_{nq} vdots &amp; vdots &amp; ddots &amp; vdots Q_{m1} R_{11} + Q_{m2} R_{21} + cdots + Q_{mn} R_{n1} &amp; Q_{m1} R_{12} + Q_{m2} R_{22} + cdots + Q_{mn} R_{n2} &amp; cdots &amp; Q_{m1} R_{1q} + Q_{m2} R_{2q} + cdots + Q_{mn} R_{nq} end{bmatrix}$ . You may notice that each element $P_{i,j}$ is the dot product of the row vector $Q_{i,*}$ and the column vector $R_{*,j}$: . $P_{i,j} = Q_{i,*} cdot R_{*,j}$ . So we can rewrite $P$ more concisely as: . $P = begin{bmatrix} Q_{1,*} cdot R_{*,1} &amp; Q_{1,*} cdot R_{*,2} &amp; cdots &amp; Q_{1,*} cdot R_{*,q} Q_{2,*} cdot R_{*,1} &amp; Q_{2,*} cdot R_{*,2} &amp; cdots &amp; Q_{2,*} cdot R_{*,q} vdots &amp; vdots &amp; ddots &amp; vdots Q_{m,*} cdot R_{*,1} &amp; Q_{m,*} cdot R_{*,2} &amp; cdots &amp; Q_{m,*} cdot R_{*,q} end{bmatrix}$ . Let&#39;s multiply two matrices in NumPy, using ndarray&#39;s dot method: . $E = AD = begin{bmatrix} 10 &amp; 20 &amp; 30 40 &amp; 50 &amp; 60 end{bmatrix} begin{bmatrix} 2 &amp; 3 &amp; 5 &amp; 7 11 &amp; 13 &amp; 17 &amp; 19 23 &amp; 29 &amp; 31 &amp; 37 end{bmatrix} = begin{bmatrix} 930 &amp; 1160 &amp; 1320 &amp; 1560 2010 &amp; 2510 &amp; 2910 &amp; 3450 end{bmatrix}$ . D = np.array([ [ 2, 3, 5, 7], [11, 13, 17, 19], [23, 29, 31, 37] ]) E = A.dot(D) E . array([[ 930, 1160, 1320, 1560], [2010, 2510, 2910, 3450]]) . Let&#39;s check this result by looking at one element, just to be sure: looking at $E_{2,3}$ for example, we need to multiply elements in $A$&#39;s $2^{nd}$ row by elements in $D$&#39;s $3^{rd}$ column, and sum up these products: . 40*5 + 50*17 + 60*31 . 2910 . E[1,2] # row 2, column 3 . 2910 . Looks good! You can check the other elements until you get used to the algorithm. . We multiplied a $2 times 3$ matrix by a $3 times 4$ matrix, so the result is a $2 times 4$ matrix. The first matrix&#39;s number of columns has to be equal to the second matrix&#39;s number of rows. If we try to multiple $D$ by $A$, we get an error because D has 4 columns while A has 2 rows: . try: D.dot(A) except ValueError as e: print(&quot;ValueError:&quot;, e) . ValueError: shapes (3,4) and (2,3) not aligned: 4 (dim 1) != 2 (dim 0) . This illustrates the fact that matrix multiplication is NOT commutative: in general $QR â‰  RQ$ . In fact, $QR$ and $RQ$ are only both defined if $Q$ has size $m times n$ and $R$ has size $n times m$. Let&#39;s look at an example where both are defined and show that they are (in general) NOT equal: . F = np.array([ [5,2], [4,1], [9,3] ]) A.dot(F) . array([[400, 130], [940, 310]]) . F.dot(A) . array([[130, 200, 270], [ 80, 130, 180], [210, 330, 450]]) . On the other hand, matrix multiplication is associative, meaning that $Q(RS) = (QR)S$. Let&#39;s create a $4 times 5$ matrix $G$ to illustrate this: . G = np.array([ [8, 7, 4, 2, 5], [2, 5, 1, 0, 5], [9, 11, 17, 21, 0], [0, 1, 0, 1, 2]]) A.dot(D).dot(G) # (AB)G . array([[21640, 28390, 27320, 31140, 13570], [47290, 62080, 60020, 68580, 29500]]) . A.dot(D.dot(G)) # A(BG) . array([[21640, 28390, 27320, 31140, 13570], [47290, 62080, 60020, 68580, 29500]]) . It is also distributive over addition of matrices, meaning that $(Q + R)S = QS + RS$. For example: . (A + B).dot(D) . array([[1023, 1276, 1452, 1716], [2211, 2761, 3201, 3795]]) . A.dot(D) + B.dot(D) . array([[1023, 1276, 1452, 1716], [2211, 2761, 3201, 3795]]) . The product of a matrix $M$ by the identity matrix (of matching size) results in the same matrix $M$. More formally, if $M$ is an $m times n$ matrix, then: . $M I_n = I_m M = M$ . This is generally written more concisely (since the size of the identity matrices is unambiguous given the context): . $MI = IM = M$ . For example: . A.dot(np.eye(3)) . array([[ 10., 20., 30.], [ 40., 50., 60.]]) . np.eye(2).dot(A) . array([[ 10., 20., 30.], [ 40., 50., 60.]]) . Caution: NumPy&#39;s * operator performs elementwise multiplication, NOT a matrix multiplication: . A * B # NOT a matrix multiplication . array([[ 10, 40, 90], [160, 250, 360]]) . The @ infix operator . Python 3.5 introduced the @ infix operator for matrix multiplication, and NumPy 1.10 added support for it. If you are using Python 3.5+ and NumPy 1.10+, you can simply write A @ D instead of A.dot(D), making your code much more readable (but less portable). This operator also works for vector dot products. . import sys print(&quot;Python version: {}.{}.{}&quot;.format(*sys.version_info)) print(&quot;Numpy version:&quot;, np.version.version) # Uncomment the following line if your Python version is â‰¥3.5 # and your NumPy version is â‰¥1.10: #A @ D . Python version: 3.5.3 Numpy version: 1.12.1 . Note: Q @ R is actually equivalent to Q.__matmul__(R) which is implemented by NumPy as np.matmul(Q, R), not as Q.dot(R). The main difference is that matmul does not support scalar multiplication, while dot does, so you can write Q.dot(3), which is equivalent to Q * 3, but you cannot write Q @ 3 (more details). . Matrix transpose . The transpose of a matrix $M$ is a matrix noted $M^T$ such that the $i^{th}$ row in $M^T$ is equal to the $i^{th}$ column in $M$: . $ A^T = begin{bmatrix} 10 &amp; 20 &amp; 30 40 &amp; 50 &amp; 60 end{bmatrix}^T = begin{bmatrix} 10 &amp; 40 20 &amp; 50 30 &amp; 60 end{bmatrix}$ . In other words, ($A^T)_{i,j}$ = $A_{j,i}$ . Obviously, if $M$ is an $m times n$ matrix, then $M^T$ is an $n times m$ matrix. . Note: there are a few other notations, such as $M^t$, $Mâ€²$, or ${^t}M$. . In NumPy, a matrix&#39;s transpose can be obtained simply using the T attribute: . A . array([[10, 20, 30], [40, 50, 60]]) . A.T . array([[10, 40], [20, 50], [30, 60]]) . As you might expect, transposing a matrix twice returns the original matrix: . A.T.T . array([[10, 20, 30], [40, 50, 60]]) . Transposition is distributive over addition of matrices, meaning that $(Q + R)^T = Q^T + R^T$. For example: . (A + B).T . array([[11, 44], [22, 55], [33, 66]]) . A.T + B.T . array([[11, 44], [22, 55], [33, 66]]) . Moreover, $(Q cdot R)^T = R^T cdot Q^T$. Note that the order is reversed. For example: . (A.dot(D)).T . array([[ 930, 2010], [1160, 2510], [1320, 2910], [1560, 3450]]) . D.T.dot(A.T) . array([[ 930, 2010], [1160, 2510], [1320, 2910], [1560, 3450]]) . A symmetric matrix $M$ is defined as a matrix that is equal to its transpose: $M^T = M$. This definition implies that it must be a square matrix whose elements are symmetric relative to the main diagonal, for example: . begin{bmatrix} 17 &amp; 22 &amp; 27 &amp; 49 22 &amp; 29 &amp; 36 &amp; 0 27 &amp; 36 &amp; 45 &amp; 2 49 &amp; 0 &amp; 2 &amp; 99 end{bmatrix}The product of a matrix by its transpose is always a symmetric matrix, for example: . D.dot(D.T) . array([[ 87, 279, 547], [ 279, 940, 1860], [ 547, 1860, 3700]]) . Converting 1D arrays to 2D arrays in NumPy . As we mentionned earlier, in NumPy (as opposed to Matlab, for example), 1D really means 1D: there is no such thing as a vertical 1D-array or a horizontal 1D-array. So you should not be surprised to see that transposing a 1D array does not do anything: . u . array([2, 5]) . u.T . array([2, 5]) . We want to convert $ textbf{u}$ into a row vector before transposing it. There are a few ways to do this: . u_row = np.array([u]) u_row . array([[2, 5]]) . Notice the extra square brackets: this is a 2D array with just one row (ie. a 1x2 matrix). In other words it really is a row vector. . u[np.newaxis, :] . array([[2, 5]]) . This quite explicit: we are asking for a new vertical axis, keeping the existing data as the horizontal axis. . u[np.newaxis] . array([[2, 5]]) . This is equivalent, but a little less explicit. . u[None] . array([[2, 5]]) . This is the shortest version, but you probably want to avoid it because it is unclear. The reason it works is that np.newaxis is actually equal to None, so this is equivalent to the previous version. . Ok, now let&#39;s transpose our row vector: . u_row.T . array([[2], [5]]) . Great! We now have a nice column vector. . Rather than creating a row vector then transposing it, it is also possible to convert a 1D array directly into a column vector: . u[:, np.newaxis] . array([[2], [5]]) . Plotting a matrix . We have already seen that vectors can been represented as points or arrows in N-dimensional space. Is there a good graphical representation of matrices? Well you can simply see a matrix as a list of vectors, so plotting a matrix results in many points or arrows. For example, let&#39;s create a $2 times 4$ matrix P and plot it as points: . P = np.array([ [3.0, 4.0, 1.0, 4.6], [0.2, 3.5, 2.0, 0.5] ]) x_coords_P, y_coords_P = P plt.scatter(x_coords_P, y_coords_P) plt.axis([0, 5, 0, 4]) plt.show() . Of course we could also have stored the same 4 vectors as row vectors instead of column vectors, resulting in a $4 times 2$ matrix (the transpose of $P$, in fact). It is really an arbitrary choice. . Since the vectors are ordered, you can see the matrix as a path and represent it with connected dots: . plt.plot(x_coords_P, y_coords_P, &quot;bo&quot;) plt.plot(x_coords_P, y_coords_P, &quot;b--&quot;) plt.axis([0, 5, 0, 4]) plt.grid() plt.show() . Or you can represent it as a polygon: matplotlib&#39;s Polygon class expects an $n times 2$ NumPy array, not a $2 times n$ array, so we just need to give it $P^T$: . from matplotlib.patches import Polygon plt.gca().add_artist(Polygon(P.T)) plt.axis([0, 5, 0, 4]) plt.grid() plt.show() . Geometric applications of matrix operations . We saw earlier that vector addition results in a geometric translation, vector multiplication by a scalar results in rescaling (zooming in or out, centered on the origin), and vector dot product results in projecting a vector onto another vector, rescaling and measuring the resulting coordinate. . Similarly, matrix operations have very useful geometric applications. . Addition = multiple geometric translations . First, adding two matrices together is equivalent to adding all their vectors together. For example, let&#39;s create a $2 times 4$ matrix $H$ and add it to $P$, and look at the result: . H = np.array([ [ 0.5, -0.2, 0.2, -0.1], [ 0.4, 0.4, 1.5, 0.6] ]) P_moved = P + H plt.gca().add_artist(Polygon(P.T, alpha=0.2)) plt.gca().add_artist(Polygon(P_moved.T, alpha=0.3, color=&quot;r&quot;)) for vector, origin in zip(H.T, P.T): plot_vector2d(vector, origin=origin) plt.text(2.2, 1.8, &quot;$P$&quot;, color=&quot;b&quot;, fontsize=18) plt.text(2.0, 3.2, &quot;$P+H$&quot;, color=&quot;r&quot;, fontsize=18) plt.text(2.5, 0.5, &quot;$H_{*,1}$&quot;, color=&quot;k&quot;, fontsize=18) plt.text(4.1, 3.5, &quot;$H_{*,2}$&quot;, color=&quot;k&quot;, fontsize=18) plt.text(0.4, 2.6, &quot;$H_{*,3}$&quot;, color=&quot;k&quot;, fontsize=18) plt.text(4.4, 0.2, &quot;$H_{*,4}$&quot;, color=&quot;k&quot;, fontsize=18) plt.axis([0, 5, 0, 4]) plt.grid() plt.show() . If we add a matrix full of identical vectors, we get a simple geometric translation: . H2 = np.array([ [-0.5, -0.5, -0.5, -0.5], [ 0.4, 0.4, 0.4, 0.4] ]) P_translated = P + H2 plt.gca().add_artist(Polygon(P.T, alpha=0.2)) plt.gca().add_artist(Polygon(P_translated.T, alpha=0.3, color=&quot;r&quot;)) for vector, origin in zip(H2.T, P.T): plot_vector2d(vector, origin=origin) plt.axis([0, 5, 0, 4]) plt.grid() plt.show() . Although matrices can only be added together if they have the same size, NumPy allows adding a row vector or a column vector to a matrix: this is called broadcasting and is explained in further details in the NumPy tutorial. We could have obtained the same result as above with: . P + [[-0.5], [0.4]] # same as P + H2, thanks to NumPy broadcasting . array([[ 2.5, 3.5, 0.5, 4.1], [ 0.6, 3.9, 2.4, 0.9]]) . Scalar multiplication . Multiplying a matrix by a scalar results in all its vectors being multiplied by that scalar, so unsurprisingly, the geometric result is a rescaling of the entire figure. For example, let&#39;s rescale our polygon by a factor of 60% (zooming out, centered on the origin): . def plot_transformation(P_before, P_after, text_before, text_after, axis = [0, 5, 0, 4], arrows=False): if arrows: for vector_before, vector_after in zip(P_before.T, P_after.T): plot_vector2d(vector_before, color=&quot;blue&quot;, linestyle=&quot;--&quot;) plot_vector2d(vector_after, color=&quot;red&quot;, linestyle=&quot;-&quot;) plt.gca().add_artist(Polygon(P_before.T, alpha=0.2)) plt.gca().add_artist(Polygon(P_after.T, alpha=0.3, color=&quot;r&quot;)) plt.text(P_before[0].mean(), P_before[1].mean(), text_before, fontsize=18, color=&quot;blue&quot;) plt.text(P_after[0].mean(), P_after[1].mean(), text_after, fontsize=18, color=&quot;red&quot;) plt.axis(axis) plt.grid() P_rescaled = 0.60 * P plot_transformation(P, P_rescaled, &quot;$P$&quot;, &quot;$0.6 P$&quot;, arrows=True) plt.show() . Matrix multiplication &#8211; Projection onto an axis . Matrix multiplication is more complex to visualize, but it is also the most powerful tool in the box. . Let&#39;s start simple, by defining a $1 times 2$ matrix $U = begin{bmatrix} 1 &amp; 0 end{bmatrix}$. This row vector is just the horizontal unit vector. . U = np.array([[1, 0]]) . Now let&#39;s look at the dot product $U cdot P$: . U.dot(P) . array([[ 3. , 4. , 1. , 4.6]]) . These are the horizontal coordinates of the vectors in $P$. In other words, we just projected $P$ onto the horizontal axis: . def plot_projection(U, P): U_P = U.dot(P) axis_end = 100 * U plot_vector2d(axis_end[0], color=&quot;black&quot;) plt.gca().add_artist(Polygon(P.T, alpha=0.2)) for vector, proj_coordinate in zip(P.T, U_P.T): proj_point = proj_coordinate * U plt.plot(proj_point[0][0], proj_point[0][1], &quot;ro&quot;) plt.plot([vector[0], proj_point[0][0]], [vector[1], proj_point[0][1]], &quot;r--&quot;) plt.axis([0, 5, 0, 4]) plt.grid() plt.show() plot_projection(U, P) . We can actually project on any other axis by just replacing $U$ with any other unit vector. For example, let&#39;s project on the axis that is at a 30Â° angle above the horizontal axis: . angle30 = 30 * np.pi / 180 # angle in radians U_30 = np.array([[np.cos(angle30), np.sin(angle30)]]) plot_projection(U_30, P) . Good! Remember that the dot product of a unit vector and a matrix basically performs a projection on an axis and gives us the coordinates of the resulting points on that axis. . Matrix multiplication &#8211; Rotation . Now let&#39;s create a $2 times 2$ matrix $V$ containing two unit vectors that make 30Â° and 120Â° angles with the horizontal axis: . $V = begin{bmatrix} cos(30Â°) &amp; sin(30Â°) cos(120Â°) &amp; sin(120Â°) end{bmatrix}$ . angle120 = 120 * np.pi / 180 V = np.array([ [np.cos(angle30), np.sin(angle30)], [np.cos(angle120), np.sin(angle120)] ]) V . array([[ 0.8660254, 0.5 ], [-0.5 , 0.8660254]]) . Let&#39;s look at the product $VP$: . V.dot(P) . array([[ 2.69807621, 5.21410162, 1.8660254 , 4.23371686], [-1.32679492, 1.03108891, 1.23205081, -1.8669873 ]]) . The first row is equal to $V_{1,*} P$, which is the coordinates of the projection of $P$ onto the 30Â° axis, as we have seen above. The second row is $V_{2,*} P$, which is the coordinates of the projection of $P$ onto the 120Â° axis. So basically we obtained the coordinates of $P$ after rotating the horizontal and vertical axes by 30Â° (or equivalently after rotating the polygon by -30Â° around the origin)! Let&#39;s plot $VP$ to see this: . P_rotated = V.dot(P) plot_transformation(P, P_rotated, &quot;$P$&quot;, &quot;$VP$&quot;, [-2, 6, -2, 4], arrows=True) plt.show() . Matrix $V$ is called a rotation matrix. . Matrix multiplication &#8211; Other linear transformations . More generally, any linear transformation $f$ that maps n-dimensional vectors to m-dimensional vectors can be represented as an $m times n$ matrix. For example, say $ textbf{u}$ is a 3-dimensional vector: . $ textbf{u} = begin{pmatrix} x y z end{pmatrix}$ . and $f$ is defined as: . $f( textbf{u}) = begin{pmatrix} ax + by + cz dx + ey + fz end{pmatrix}$ . This transormation $f$ maps 3-dimensional vectors to 2-dimensional vectors in a linear way (ie. the resulting coordinates only involve sums of multiples of the original coordinates). We can represent this transformation as matrix $F$: . $F = begin{bmatrix} a &amp; b &amp; c d &amp; e &amp; f end{bmatrix}$ . Now, to compute $f( textbf{u})$ we can simply do a matrix multiplication: . $f( textbf{u}) = F textbf{u}$ . If we have a matric $G = begin{bmatrix} textbf{u}_1 &amp; textbf{u}_2 &amp; cdots &amp; textbf{u}_q end{bmatrix}$, where each $ textbf{u}_i$ is a 3-dimensional column vector, then $FG$ results in the linear transformation of all vectors $ textbf{u}_i$ as defined by the matrix $F$: . $FG = begin{bmatrix}f( textbf{u}_1) &amp; f( textbf{u}_2) &amp; cdots &amp; f( textbf{u}_q) end{bmatrix}$ . To summarize, the matrix on the left hand side of a dot product specifies what linear transormation to apply to the right hand side vectors. We have already shown that this can be used to perform projections and rotations, but any other linear transformation is possible. For example, here is a transformation known as a shear mapping: . F_shear = np.array([ [1, 1.5], [0, 1] ]) plot_transformation(P, F_shear.dot(P), &quot;$P$&quot;, &quot;$F_{shear} P$&quot;, axis=[0, 10, 0, 7]) plt.show() . Let&#39;s look at how this transformation affects the unit square: . Square = np.array([ [0, 0, 1, 1], [0, 1, 1, 0] ]) plot_transformation(Square, F_shear.dot(Square), &quot;$Square$&quot;, &quot;$F_{shear} Square$&quot;, axis=[0, 2.6, 0, 1.8]) plt.show() . Now let&#39;s look at a squeeze mapping: . F_squeeze = np.array([ [1.4, 0], [0, 1/1.4] ]) plot_transformation(P, F_squeeze.dot(P), &quot;$P$&quot;, &quot;$F_{squeeze} P$&quot;, axis=[0, 7, 0, 5]) plt.show() . The effect on the unit square is: . plot_transformation(Square, F_squeeze.dot(Square), &quot;$Square$&quot;, &quot;$F_{squeeze} Square$&quot;, axis=[0, 1.8, 0, 1.2]) plt.show() . Let&#39;s show a last one: reflection through the horizontal axis: . F_reflect = np.array([ [1, 0], [0, -1] ]) plot_transformation(P, F_reflect.dot(P), &quot;$P$&quot;, &quot;$F_{reflect} P$&quot;, axis=[-2, 9, -4.5, 4.5]) plt.show() . Matrix inverse . Now that we understand that a matrix can represent any linear transformation, a natural question is: can we find a transformation matrix that reverses the effect of a given transformation matrix $F$? The answer is yesâ€¦ sometimes! When it exists, such a matrix is called the inverse of $F$, and it is noted $F^{-1}$. . For example, the rotation, the shear mapping and the squeeze mapping above all have inverse transformations. Let&#39;s demonstrate this on the shear mapping: . F_inv_shear = np.array([ [1, -1.5], [0, 1] ]) P_sheared = F_shear.dot(P) P_unsheared = F_inv_shear.dot(P_sheared) plot_transformation(P_sheared, P_unsheared, &quot;$P_{sheared}$&quot;, &quot;$P_{unsheared}$&quot;, axis=[0, 10, 0, 7]) plt.plot(P[0], P[1], &quot;b--&quot;) plt.show() . We applied a shear mapping on $P$, just like we did before, but then we applied a second transformation to the result, and lo and behold this had the effect of coming back to the original $P$ (we plotted the original $P$&#39;s outline to double check). The second transformation is the inverse of the first one. . We defined the inverse matrix $F_{shear}^{-1}$ manually this time, but NumPy provides an inv function to compute a matrix&#39;s inverse, so we could have written instead: . F_inv_shear = LA.inv(F_shear) F_inv_shear . array([[ 1. , -1.5], [ 0. , 1. ]]) . Only square matrices can be inversed. This makes sense when you think about it: if you have a transformation that reduces the number of dimensions, then some information is lost and there is no way that you can get it back. For example say you use a $2 times 3$ matrix to project a 3D object onto a plane. The result may look like this: . plt.plot([0, 0, 1, 1, 0, 0.1, 0.1, 0, 0.1, 1.1, 1.0, 1.1, 1.1, 1.0, 1.1, 0.1], [0, 1, 1, 0, 0, 0.1, 1.1, 1.0, 1.1, 1.1, 1.0, 1.1, 0.1, 0, 0.1, 0.1], &quot;r-&quot;) plt.axis([-0.5, 2.1, -0.5, 1.5]) plt.show() . Looking at this image, it is impossible to tell whether this is the projection of a cube or the projection of a narrow rectangular object. Some information has been lost in the projection. . Even square transformation matrices can lose information. For example, consider this transformation matrix: . F_project = np.array([ [1, 0], [0, 0] ]) plot_transformation(P, F_project.dot(P), &quot;$P$&quot;, &quot;$F_{project} cdot P$&quot;, axis=[0, 6, -1, 4]) plt.show() . This transformation matrix performs a projection onto the horizontal axis. Our polygon gets entirely flattened out so some information is entirely lost and it is impossible to go back to the original polygon using a linear transformation. In other words, $F_{project}$ has no inverse. Such a square matrix that cannot be inversed is called a singular matrix (aka degenerate matrix). If we ask NumPy to calculate its inverse, it raises an exception: . try: LA.inv(F_project) except LA.LinAlgError as e: print(&quot;LinAlgError:&quot;, e) . LinAlgError: Singular matrix . Here is another example of a singular matrix. This one performs a projection onto the axis at a 30Â° angle above the horizontal axis: . angle30 = 30 * np.pi / 180 F_project_30 = np.array([ [np.cos(angle30)**2, np.sin(2*angle30)/2], [np.sin(2*angle30)/2, np.sin(angle30)**2] ]) plot_transformation(P, F_project_30.dot(P), &quot;$P$&quot;, &quot;$F_{project _30} cdot P$&quot;, axis=[0, 6, -1, 4]) plt.show() . But this time, due to floating point rounding errors, NumPy manages to calculate an inverse (notice how large the elements are, though): . LA.inv(F_project_30) . array([[ 1.20095990e+16, -2.08012357e+16], [ -2.08012357e+16, 3.60287970e+16]]) . As you might expect, the dot product of a matrix by its inverse results in the identity matrix: . $M cdot M^{-1} = M^{-1} cdot M = I$ . This makes sense since doing a linear transformation followed by the inverse transformation results in no change at all. . F_shear.dot(LA.inv(F_shear)) . array([[ 1., 0.], [ 0., 1.]]) . Another way to express this is that the inverse of the inverse of a matrix $M$ is $M$ itself: . $((M)^{-1})^{-1} = M$ . LA.inv(LA.inv(F_shear)) . array([[ 1. , 1.5], [ 0. , 1. ]]) . Also, the inverse of scaling by a factor of $ lambda$ is of course scaling by a factor or $ frac{1}{ lambda}$: . $ ( lambda times M)^{-1} = frac{1}{ lambda} times M^{-1}$ . Once you understand the geometric interpretation of matrices as linear transformations, most of these properties seem fairly intuitive. . A matrix that is its own inverse is called an involution. The simplest examples are reflection matrices, or a rotation by 180Â°, but there are also more complex involutions, for example imagine a transformation that squeezes horizontally, then reflects over the vertical axis and finally rotates by 90Â° clockwise. Pick up a napkin and try doing that twice: you will end up in the original position. Here is the corresponding involutory matrix: . F_involution = np.array([ [0, -2], [-1/2, 0] ]) plot_transformation(P, F_involution.dot(P), &quot;$P$&quot;, &quot;$F_{involution} cdot P$&quot;, axis=[-8, 5, -4, 4]) plt.show() . Finally, a square matrix $H$ whose inverse is its own transpose is an orthogonal matrix: . $H^{-1} = H^T$ . Therefore: . $H cdot H^T = H^T cdot H = I$ . It corresponds to a transformation that preserves distances, such as rotations and reflections, and combinations of these, but not rescaling, shearing or squeezing. Let&#39;s check that $F_{reflect}$ is indeed orthogonal: . F_reflect.dot(F_reflect.T) . array([[1, 0], [0, 1]]) . Determinant . The determinant of a square matrix $M$, noted $ det(M)$ or $ det M$ or $|M|$ is a value that can be calculated from its elements $(M_{i,j})$ using various equivalent methods. One of the simplest methods is this recursive approach: . $|M| = M_{1,1} times|M^{(1,1)}| - M_{2,1} times|M^{(2,1)}| + M_{3,1} times|M^{(3,1)}| - M_{4,1} times|M^{(4,1)}| + cdots Â± M_{n,1} times|M^{(n,1)}|$ . Where $M^{(i,j)}$ is the matrix $M$ without row $i$ and column $j$. | . For example, let&#39;s calculate the determinant of the following $3 times 3$ matrix: . $M = begin{bmatrix} 1 &amp; 2 &amp; 3 4 &amp; 5 &amp; 6 7 &amp; 8 &amp; 0 end{bmatrix}$ . Using the method above, we get: . $|M| = 1 times left | begin{bmatrix} 5 &amp; 6 8 &amp; 0 end{bmatrix} right | . - 2 times left | begin{bmatrix} 4 &amp; 6 7 &amp; 0 end{bmatrix} right | + 3 times left | begin{bmatrix} 4 &amp; 5 7 &amp; 8 end{bmatrix} right |$ . Now we need to compute the determinant of each of these $2 times 2$ matrices (these determinants are called minors): . $ left | begin{bmatrix} 5 &amp; 6 8 &amp; 0 end{bmatrix} right | = 5 times 0 - 6 times 8 = -48$ . $ left | begin{bmatrix} 4 &amp; 6 7 &amp; 0 end{bmatrix} right | = 4 times 0 - 6 times 7 = -42$ . $ left | begin{bmatrix} 4 &amp; 5 7 &amp; 8 end{bmatrix} right | = 4 times 8 - 5 times 7 = -3$ . Now we can calculate the final result: . $|M| = 1 times (-48) - 2 times (-42) + 3 times (-3) = 27$ . To get the determinant of a matrix, you can call NumPy&#39;s det function in the numpy.linalg module: . M = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 0] ]) LA.det(M) . 27.0 . One of the main uses of the determinant is to determine whether a square matrix can be inversed or not: if the determinant is equal to 0, then the matrix cannot be inversed (it is a singular matrix), and if the determinant is not 0, then it can be inversed. . For example, let&#39;s compute the determinant for the $F_{project}$, $F_{project _30}$ and $F_{shear}$ matrices that we defined earlier: . LA.det(F_project) . 0.0 . That&#39;s right, $F_{project}$ is singular, as we saw earlier. . LA.det(F_project_30) . 2.0816681711721642e-17 . This determinant is suspiciously close to 0: it really should be 0, but it&#39;s not due to tiny floating point errors. The matrix is actually singular. . LA.det(F_shear) . 1.0 . Perfect! This matrix can be inversed as we saw earlier. Wow, math really works! . The determinant can also be used to measure how much a linear transformation affects surface areas: for example, the projection matrices $F_{project}$ and $F_{project _30}$ completely flatten the polygon $P$, until its area is zero. This is why the determinant of these matrices is 0. The shear mapping modified the shape of the polygon, but it did not affect its surface area, which is why the determinant is 1. You can try computing the determinant of a rotation matrix, and you should also find 1. What about a scaling matrix? Let&#39;s see: . F_scale = np.array([ [0.5, 0], [0, 0.5] ]) plot_transformation(P, F_scale.dot(P), &quot;$P$&quot;, &quot;$F_{scale} cdot P$&quot;, axis=[0, 6, -1, 4]) plt.show() . We rescaled the polygon by a factor of 1/2 on both vertical and horizontal axes so the surface area of the resulting polygon is 1/4$^{th}$ of the original polygon. Let&#39;s compute the determinant and check that: . LA.det(F_scale) . 0.25 . Correct! . The determinant can actually be negative, when the transformation results in a &quot;flipped over&quot; version of the original polygon (eg. a left hand glove becomes a right hand glove). For example, the determinant of the F_reflect matrix is -1 because the surface area is preserved but the polygon gets flipped over: . LA.det(F_reflect) . -1.0 . Composing linear transformations . Several linear transformations can be chained simply by performing multiple dot products in a row. For example, to perform a squeeze mapping followed by a shear mapping, just write: . P_squeezed_then_sheared = F_shear.dot(F_squeeze.dot(P)) . Since the dot product is associative, the following code is equivalent: . P_squeezed_then_sheared = (F_shear.dot(F_squeeze)).dot(P) . Note that the order of the transformations is the reverse of the dot product order. . If we are going to perform this composition of linear transformations more than once, we might as well save the composition matrix like this: . F_squeeze_then_shear = F_shear.dot(F_squeeze) P_squeezed_then_sheared = F_squeeze_then_shear.dot(P) . From now on we can perform both transformations in just one dot product, which can lead to a very significant performance boost. . What if you want to perform the inverse of this double transformation? Well, if you squeezed and then you sheared, and you want to undo what you have done, it should be obvious that you should unshear first and then unsqueeze. In more mathematical terms, given two invertible (aka nonsingular) matrices $Q$ and $R$: . $(Q cdot R)^{-1} = R^{-1} cdot Q^{-1}$ . And in NumPy: . LA.inv(F_shear.dot(F_squeeze)) == LA.inv(F_squeeze).dot(LA.inv(F_shear)) . array([[ True, True], [ True, True]], dtype=bool) . Singular Value Decomposition . It turns out that any $m times n$ matrix $M$ can be decomposed into the dot product of three simple matrices: . a rotation matrix $U$ (an $m times m$ orthogonal matrix) | a scaling &amp; projecting matrix $ Sigma$ (an $m times n$ diagonal matrix) | and another rotation matrix $V^T$ (an $n times n$ orthogonal matrix) | . $M = U cdot Sigma cdot V^{T}$ . For example, let&#39;s decompose the shear transformation: . U, S_diag, V_T = LA.svd(F_shear) # note: in python 3 you can rename S_diag to Î£_diag U . array([[ 0.89442719, -0.4472136 ], [ 0.4472136 , 0.89442719]]) . S_diag . array([ 2. , 0.5]) . Note that this is just a 1D array containing the diagonal values of Î£. To get the actual matrix Î£, we can use NumPy&#39;s diag function: . S = np.diag(S_diag) S . array([[ 2. , 0. ], [ 0. , 0.5]]) . Now let&#39;s check that $U cdot Sigma cdot V^T$ is indeed equal to F_shear: . U.dot(np.diag(S_diag)).dot(V_T) . array([[ 1. , 1.5], [ 0. , 1. ]]) . F_shear . array([[ 1. , 1.5], [ 0. , 1. ]]) . It worked like a charm. Let&#39;s apply these transformations one by one (in reverse order) on the unit square to understand what&#39;s going on. First, let&#39;s apply the first rotation $V^T$: . plot_transformation(Square, V_T.dot(Square), &quot;$Square$&quot;, &quot;$V^T cdot Square$&quot;, axis=[-0.5, 3.5 , -1.5, 1.5]) plt.show() . Now let&#39;s rescale along the vertical and horizontal axes using $ Sigma$: . plot_transformation(V_T.dot(Square), S.dot(V_T).dot(Square), &quot;$V^T cdot Square$&quot;, &quot;$ Sigma cdot V^T cdot Square$&quot;, axis=[-0.5, 3.5 , -1.5, 1.5]) plt.show() . Finally, we apply the second rotation $U$: . plot_transformation(S.dot(V_T).dot(Square), U.dot(S).dot(V_T).dot(Square),&quot;$ Sigma cdot V^T cdot Square$&quot;, &quot;$U cdot Sigma cdot V^T cdot Square$&quot;, axis=[-0.5, 3.5 , -1.5, 1.5]) plt.show() . And we can see that the result is indeed a shear mapping of the original unit square. . Eigenvectors and eigenvalues . An eigenvector of a square matrix $M$ (also called a characteristic vector) is a non-zero vector that remains on the same line after transformation by the linear transformation associated with $M$. A more formal definition is any vector $v$ such that: . $M cdot v = lambda times v$ . Where $ lambda$ is a scalar value called the eigenvalue associated to the vector $v$. . For example, any horizontal vector remains horizontal after applying the shear mapping (as you can see on the image above), so it is an eigenvector of $M$. A vertical vector ends up tilted to the right, so vertical vectors are NOT eigenvectors of $M$. . If we look at the squeeze mapping, we find that any horizontal or vertical vector keeps its direction (although its length changes), so all horizontal and vertical vectors are eigenvectors of $F_{squeeze}$. . However, rotation matrices have no eigenvectors at all (except if the rotation angle is 0Â° or 180Â°, in which case all non-zero vectors are eigenvectors). . NumPy&#39;s eig function returns the list of unit eigenvectors and their corresponding eigenvalues for any square matrix. Let&#39;s look at the eigenvectors and eigenvalues of the squeeze mapping matrix $F_{squeeze}$: . eigenvalues, eigenvectors = LA.eig(F_squeeze) eigenvalues # [Î»0, Î»1, â€¦] . array([ 1.4 , 0.71428571]) . eigenvectors # [v0, v1, â€¦] . array([[ 1., 0.], [ 0., 1.]]) . Indeed the horizontal vectors are stretched by a factor of 1.4, and the vertical vectors are shrunk by a factor of 1/1.4=0.714â€¦, so far so good. Let&#39;s look at the shear mapping matrix $F_{shear}$: . eigenvalues2, eigenvectors2 = LA.eig(F_shear) eigenvalues2 # [Î»0, Î»1, â€¦] . array([ 1., 1.]) . eigenvectors2 # [v0, v1, â€¦] . array([[ 1.00000000e+00, -1.00000000e+00], [ 0.00000000e+00, 1.48029737e-16]]) . Wait, what!? We expected just one unit eigenvector, not two. The second vector is almost equal to $ begin{pmatrix}-1 0 end{pmatrix}$, which is on the same line as the first vector $ begin{pmatrix}1 0 end{pmatrix}$. This is due to floating point errors. We can safely ignore vectors that are (almost) colinear (ie. on the same line). . Trace . The trace of a square matrix $M$, noted $tr(M)$ is the sum of the values on its main diagonal. For example: . D = np.array([ [100, 200, 300], [ 10, 20, 30], [ 1, 2, 3], ]) np.trace(D) . 123 . The trace does not have a simple geometric interpretation (in general), but it has a number of properties that make it useful in many areas: . $tr(A + B) = tr(A) + tr(B)$ | $tr(A cdot B) = tr(B cdot A)$ | $tr(A cdot B cdot cdots cdot Y cdot Z) = tr(Z cdot A cdot B cdot cdots cdot Y)$ | $tr(A^T cdot B) = tr(A cdot B^T) = tr(B^T cdot A) = tr(B cdot A^T) = sum_{i,j}X_{i,j} times Y_{i,j}$ | â€¦ | . It does, however, have a useful geometric interpretation in the case of projection matrices (such as $F_{project}$ that we discussed earlier): it corresponds to the number of dimensions after projection. For example: . np.trace(F_project) . 1 . What next? . This concludes this introduction to Linear Algebra. Although these basics cover most of what you will need to know for Machine Learning, if you wish to go deeper into this topic there are many options available: Linear Algebra books, Khan Academy lessons, or just Wikipedia pages. .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/math_linear_algebra.html",
            "relUrl": "/2020/03/09/math_linear_algebra.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Title",
            "content": "Machine Learning Notebooks . Welcome to the Machine Learning Notebooks! . Prerequisites (see below) . Notebooks . The Machine Learning landscape | End-to-end Machine Learning project | Classification | Training Models | Support Vector Machines | Decision Trees | Ensemble Learning and Random Forests | Dimensionality Reduction | Unsupervised Learning Techniques | Artificial Neural Nets with Keras | Training Deep Neural Networks | Custom Models and Training with TensorFlow | Loading and Preprocessing Data | Deep Computer Vision Using Convolutional Neural Networks | Processing Sequences Using RNNs and CNNs | Natural Language Processing with RNNs and Attention | Representation Learning Using Autoencoders | Reinforcement Learning | Training and Deploying TensorFlow Models at Scale | Scientific Python tutorials . NumPy | Matplotlib | Pandas | . Math Tutorials . Linear Algebra | . Extra Material . Work in progress . Misc. . Equations (list of equations in the book) | . Prerequisites . To understand . Python â€“ you don&#39;t need to be an expert python programmer, but you do need to know the basics. If you don&#39;t, the official Python tutorial is a good place to start. | Scientific Python â€“ We will be using a few popular python libraries, in particular NumPy, matplotlib and pandas. If you are not familiar with these libraries, you should probably start by going through the tutorials in the Tools section (especially NumPy). | Math â€“ We will also use some notions of Linear Algebra, Calculus, Statistics and Probability theory. You should be able to follow along if you learned these in the past as it won&#39;t be very advanced, but if you don&#39;t know about these topics or you need a refresher then go through the appropriate introduction in the Math section. | . To run the examples . Jupyter â€“ These notebooks are based on Jupyter. You can run these notebooks in just one click using a hosted platform such as Binder, Deepnote or Colaboratory (no installation required), or you can just view them using Jupyter.org&#39;s viewer, or you can install everything on your machine, as you prefer. Check out the home page for more details. | .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/index.html",
            "relUrl": "/2020/03/09/index.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Title",
            "content": "Comparison of Batch, Mini-Batch and Stochastic Gradient Descent . permalink: /extra_gradient_descent_comparison | . This notebook displays an animation comparing Batch, Mini-Batch and Stochastic Gradient Descent (introduced in Chapter 4). Thanks to Daniel Ingram who contributed this notebook. . import numpy as np %matplotlib nbagg import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation . m = 100 X = 2*np.random.rand(m, 1) X_b = np.c_[np.ones((m, 1)), X] y = 4 + 3*X + np.random.rand(m, 1) . def batch_gradient_descent(): n_iterations = 1000 learning_rate = 0.05 thetas = np.random.randn(2, 1) thetas_path = [thetas] for i in range(n_iterations): gradients = 2*X_b.T.dot(X_b.dot(thetas) - y)/m thetas = thetas - learning_rate*gradients thetas_path.append(thetas) return thetas_path . def stochastic_gradient_descent(): n_epochs = 50 t0, t1 = 5, 50 thetas = np.random.randn(2, 1) thetas_path = [thetas] for epoch in range(n_epochs): for i in range(m): random_index = np.random.randint(m) xi = X_b[random_index:random_index+1] yi = y[random_index:random_index+1] gradients = 2*xi.T.dot(xi.dot(thetas) - yi) eta = learning_schedule(epoch*m + i, t0, t1) thetas = thetas - eta*gradients thetas_path.append(thetas) return thetas_path . def mini_batch_gradient_descent(): n_iterations = 50 minibatch_size = 20 t0, t1 = 200, 1000 thetas = np.random.randn(2, 1) thetas_path = [thetas] t = 0 for epoch in range(n_iterations): shuffled_indices = np.random.permutation(m) X_b_shuffled = X_b[shuffled_indices] y_shuffled = y[shuffled_indices] for i in range(0, m, minibatch_size): t += 1 xi = X_b_shuffled[i:i+minibatch_size] yi = y_shuffled[i:i+minibatch_size] gradients = 2*xi.T.dot(xi.dot(thetas) - yi)/minibatch_size eta = learning_schedule(t, t0, t1) thetas = thetas - eta*gradients thetas_path.append(thetas) return thetas_path . def compute_mse(theta): return np.sum((np.dot(X_b, theta) - y)**2)/m . def learning_schedule(t, t0, t1): return t0/(t+t1) . theta0, theta1 = np.meshgrid(np.arange(0, 5, 0.1), np.arange(0, 5, 0.1)) r, c = theta0.shape cost_map = np.array([[0 for _ in range(c)] for _ in range(r)]) for i in range(r): for j in range(c): theta = np.array([theta0[i,j], theta1[i,j]]) cost_map[i,j] = compute_mse(theta) . exact_solution = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) bgd_thetas = np.array(batch_gradient_descent()) sgd_thetas = np.array(stochastic_gradient_descent()) mbgd_thetas = np.array(mini_batch_gradient_descent()) . bgd_len = len(bgd_thetas) sgd_len = len(sgd_thetas) mbgd_len = len(mbgd_thetas) n_iter = min(bgd_len, sgd_len, mbgd_len) . fig = plt.figure(figsize=(10, 5)) data_ax = fig.add_subplot(121) cost_ax = fig.add_subplot(122) cost_ax.plot(exact_solution[0,0], exact_solution[1,0], &#39;y*&#39;) cost_img = cost_ax.pcolor(theta0, theta1, cost_map) fig.colorbar(cost_img) . &lt;matplotlib.colorbar.Colorbar at 0x107d27f28&gt; . def animate(i): data_ax.cla() cost_ax.cla() data_ax.plot(X, y, &#39;k.&#39;) cost_ax.plot(exact_solution[0,0], exact_solution[1,0], &#39;y*&#39;) cost_ax.pcolor(theta0, theta1, cost_map) data_ax.plot(X, X_b.dot(bgd_thetas[i,:]), &#39;r-&#39;) cost_ax.plot(bgd_thetas[:i,0], bgd_thetas[:i,1], &#39;r--&#39;) data_ax.plot(X, X_b.dot(sgd_thetas[i,:]), &#39;g-&#39;) cost_ax.plot(sgd_thetas[:i,0], sgd_thetas[:i,1], &#39;g--&#39;) data_ax.plot(X, X_b.dot(mbgd_thetas[i,:]), &#39;b-&#39;) cost_ax.plot(mbgd_thetas[:i,0], mbgd_thetas[:i,1], &#39;b--&#39;) data_ax.set_xlim([0, 2]) data_ax.set_ylim([0, 15]) cost_ax.set_xlim([0, 5]) cost_ax.set_ylim([0, 5]) data_ax.set_xlabel(r&#39;$x_1$&#39;) data_ax.set_ylabel(r&#39;$y$&#39;, rotation=0) cost_ax.set_xlabel(r&#39;$ theta_0$&#39;) cost_ax.set_ylabel(r&#39;$ theta_1$&#39;) data_ax.legend((&#39;Data&#39;, &#39;BGD&#39;, &#39;SGD&#39;, &#39;MBGD&#39;), loc=&quot;upper left&quot;) cost_ax.legend((&#39;Normal Equation&#39;, &#39;BGD&#39;, &#39;SGD&#39;, &#39;MBGD&#39;), loc=&quot;upper left&quot;) . animation = FuncAnimation(fig, animate, frames=n_iter) plt.show() .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/extra_gradient_descent_comparison.html",
            "relUrl": "/2020/03/09/extra_gradient_descent_comparison.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Title",
            "content": "Equations . permalink: /book_equations | . This notebook lists all the equations in the book. If you decide to print them on a T-Shirt, I definitely want a copy! ;-) . Warning: GitHub&#39;s notebook viewer does not render equations properly. You should either view this notebook within Jupyter itself or use Jupyter&#39;s online viewer. . . Chapter 1 . Equation 1-1: A simple linear model . $ text{life_satisfaction} = theta_0 + theta_1 times text{GDP_per_capita} $ . Chapter 2 . Equation 2-1: Root Mean Square Error (RMSE) . $ text{RMSE}( mathbf{X}, h) = sqrt{ frac{1}{m} sum limits_{i=1}^{m} left(h( mathbf{x}^{(i)}) - y^{(i)} right)^2} $ . Notations (page 38): . $ mathbf{x}^{(1)} = begin{pmatrix} -118.29 33.91 1,416 38,372 end{pmatrix} $ . $ y^{(1)}=156,400 $ . $ mathbf{X} = begin{pmatrix} ( mathbf{x}^{(1)})^T ( mathbf{x}^{(2)})^T vdots ( mathbf{x}^{(1999)})^T ( mathbf{x}^{(2000)})^T end{pmatrix} = begin{pmatrix} -118.29 &amp; 33.91 &amp; 1,416 &amp; 38,372 vdots &amp; vdots &amp; vdots &amp; vdots end{pmatrix} $ . Equation 2-2: Mean Absolute Error . $ text{MAE}( mathbf{X}, h) = frac{1}{m} sum limits_{i=1}^{m} left| h( mathbf{x}^{(i)}) - y^{(i)} right| $ . $ ell_k$ norms (page 39): . $ left | mathbf{v} right | _k = ( left| v_0 right|^k + left| v_1 right|^k + dots + left| v_n right|^k)^{ frac{1}{k}} $ . Chapter 3 . Equation 3-1: Precision . $ text{precision} = cfrac{TP}{TP + FP} $ . Equation 3-2: Recall . $ text{recall} = cfrac{TP}{TP + FN} $ . Equation 3-3: $F_1$ score . $ F_1 = cfrac{2}{ cfrac{1}{ text{precision}} + cfrac{1}{ text{recall}}} = 2 times cfrac{ text{precision} , times , text{recall}}{ text{precision} , + , text{recall}} = cfrac{TP}{TP + cfrac{FN + FP}{2}} $ . Chapter 4 . Equation 4-1: Linear Regression model prediction . $ hat{y} = theta_0 + theta_1 x_1 + theta_2 x_2 + dots + theta_n x_n $ . Equation 4-2: Linear Regression model prediction (vectorized form) . $ hat{y} = h_{ boldsymbol{ theta}}( mathbf{x}) = boldsymbol{ theta} cdot mathbf{x} $ . Equation 4-3: MSE cost function for a Linear Regression model . $ text{MSE}( mathbf{X}, h_{ boldsymbol{ theta}}) = dfrac{1}{m} sum limits_{i=1}^{m}{( boldsymbol{ theta}^T mathbf{x}^{(i)} - y^{(i)})^2} $ . Equation 4-4: Normal Equation . $ hat{ boldsymbol{ theta}} = ( mathbf{X}^T mathbf{X})^{-1} mathbf{X}^T mathbf{y} $ . Partial derivatives notation (page 114): . $ frac{ partial}{ partial theta_j} text{MSE}( boldsymbol{ theta})$ . Equation 4-5: Partial derivatives of the cost function . $ dfrac{ partial}{ partial theta_j} text{MSE}( boldsymbol{ theta}) = dfrac{2}{m} sum limits_{i=1}^{m}( boldsymbol{ theta}^T mathbf{x}^{(i)} - y^{(i)}) , x_j^{(i)} $ . Equation 4-6: Gradient vector of the cost function . $ nabla_{ boldsymbol{ theta}} , text{MSE}( boldsymbol{ theta}) = begin{pmatrix} frac{ partial}{ partial theta_0} text{MSE}( boldsymbol{ theta}) frac{ partial}{ partial theta_1} text{MSE}( boldsymbol{ theta}) vdots frac{ partial}{ partial theta_n} text{MSE}( boldsymbol{ theta}) end{pmatrix} = dfrac{2}{m} mathbf{X}^T ( mathbf{X} boldsymbol{ theta} - mathbf{y}) $ . Equation 4-7: Gradient Descent step . $ boldsymbol{ theta}^{( text{next step})} = boldsymbol{ theta} - eta nabla_{ boldsymbol{ theta}} , text{MSE}( boldsymbol{ theta}) $ . $ O( frac{1}{ text{iterations}}) $ . $ hat{y} = 0.56 x_1^2 + 0.93 x_1 + 1.78 $ . $ y = 0.5 x_1^2 + 1.0 x_1 + 2.0 + text{Gaussian noise} $ . $ dfrac{(n+d)!}{d! ,n!} $ . $ alpha sum_{i=1}^{n}{{ theta_i}^2}$ . Equation 4-8: Ridge Regression cost function . $ J( boldsymbol{ theta}) = text{MSE}( boldsymbol{ theta}) + alpha dfrac{1}{2} sum limits_{i=1}^{n}{ theta_i}^2 $ . Equation 4-9: Ridge Regression closed-form solution . $ hat{ boldsymbol{ theta}} = ( mathbf{X}^T mathbf{X} + alpha mathbf{A})^{-1} mathbf{X}^T mathbf{y} $ . Equation 4-10: Lasso Regression cost function . $ J( boldsymbol{ theta}) = text{MSE}( boldsymbol{ theta}) + alpha sum limits_{i=1}^{n} left| theta_i right| $ . Equation 4-11: Lasso Regression subgradient vector . $ g( boldsymbol{ theta}, J) = nabla_{ boldsymbol{ theta}} , text{MSE}( boldsymbol{ theta}) + alpha begin{pmatrix} operatorname{sign}( theta_1) operatorname{sign}( theta_2) vdots operatorname{sign}( theta_n) end{pmatrix} quad text{where } operatorname{sign}( theta_i) = begin{cases} -1 &amp; text{if } theta_i &lt; 0 0 &amp; text{if } theta_i = 0 +1 &amp; text{if } theta_i &gt; 0 end{cases} $ . Equation 4-12: Elastic Net cost function . $ J( boldsymbol{ theta}) = text{MSE}( boldsymbol{ theta}) + r alpha sum limits_{i=1}^{n} left| theta_i right| + dfrac{1 - r}{2} alpha sum limits_{i=1}^{n}{{ theta_i}^2} $ . Equation 4-13: Logistic Regression model estimated probability (vectorized form) . $ hat{p} = h_{ boldsymbol{ theta}}( mathbf{x}) = sigma( boldsymbol{ theta}^T mathbf{x}) $ . Equation 4-14: Logistic function . $ sigma(t) = dfrac{1}{1 + exp(-t)} $ . Equation 4-15: Logistic Regression model prediction . $ hat{y} = begin{cases} 0 &amp; text{if } hat{p} &lt; 0.5, 1 &amp; text{if } hat{p} geq 0.5. end{cases} $ . Equation 4-16: Cost function of a single training instance . $ c( boldsymbol{ theta}) = begin{cases} - log( hat{p}) &amp; text{if } y = 1, - log(1 - hat{p}) &amp; text{if } y = 0. end{cases} $ . Equation 4-17: Logistic Regression cost function (log loss) . $ J( boldsymbol{ theta}) = - dfrac{1}{m} sum limits_{i=1}^{m}{ left[ y^{(i)} log left( hat{p}^{(i)} right) + (1 - y^{(i)}) log left(1 - hat{p}^{(i)} right) right]} $ . Equation 4-18: Logistic cost function partial derivatives . $ dfrac{ partial}{ partial theta_j} text{J}( boldsymbol{ theta}) = dfrac{1}{m} sum limits_{i=1}^{m} left( mathbf{ sigma( boldsymbol{ theta}}^T mathbf{x}^{(i)}) - y^{(i)} right) , x_j^{(i)} $ . Equation 4-19: Softmax score for class k . $ s_k( mathbf{x}) = ({ boldsymbol{ theta}^{(k)}})^T mathbf{x} $ . Equation 4-20: Softmax function . $ hat{p}_k = sigma left( mathbf{s}( mathbf{x}) right)_k = dfrac{ exp left(s_k( mathbf{x}) right)}{ sum limits_{j=1}^{K}{ exp left(s_j( mathbf{x}) right)}} $ . Equation 4-21: Softmax Regression classifier prediction . $ hat{y} = underset{k}{ operatorname{argmax}} , sigma left( mathbf{s}( mathbf{x}) right)_k = underset{k}{ operatorname{argmax}} , s_k( mathbf{x}) = underset{k}{ operatorname{argmax}} , left( ({ boldsymbol{ theta}^{(k)}})^T mathbf{x} right) $ . Equation 4-22: Cross entropy cost function . $ J( boldsymbol{ Theta}) = - dfrac{1}{m} sum limits_{i=1}^{m} sum limits_{k=1}^{K}{y_k^{(i)} log left( hat{p}_k^{(i)} right)} $ . Cross entropy between two discrete probability distributions $p$ and $q$ (page 141): $ H(p, q) = - sum limits_{x}p(x) log q(x) $ . Equation 4-23: Cross entropy gradient vector for class k . $ nabla_{ boldsymbol{ theta}^{(k)}} , J( boldsymbol{ Theta}) = dfrac{1}{m} sum limits_{i=1}^{m}{ left ( hat{p}^{(i)}_k - y_k^{(i)} right ) mathbf{x}^{(i)}} $ . Chapter 5 . Equation 5-1: Gaussian RBF . $ { displaystyle phi_{ gamma}( mathbf{x}, boldsymbol{ ell})} = { displaystyle exp({ displaystyle - gamma left | mathbf{x} - boldsymbol{ ell} right |^2})} $ . Equation 5-2: Linear SVM classifier prediction . $ hat{y} = begin{cases} 0 &amp; text{if } mathbf{w}^T mathbf{x} + b &lt; 0, 1 &amp; text{if } mathbf{w}^T mathbf{x} + b geq 0 end{cases} $ . Equation 5-3: Hard margin linear SVM classifier objective . $ begin{split} &amp; underset{ mathbf{w}, b}{ operatorname{minimize}} quad{ frac{1}{2} mathbf{w}^T mathbf{w}} &amp; text{subject to} quad t^{(i)}( mathbf{w}^T mathbf{x}^{(i)} + b) ge 1 quad text{for } i = 1, 2, dots, m end{split} $ . Equation 5-4: Soft margin linear SVM classifier objective . $ begin{split} &amp; underset{ mathbf{w}, b, mathbf{ zeta}}{ operatorname{minimize}} quad{ dfrac{1}{2} mathbf{w}^T mathbf{w} + C sum limits_{i=1}^m{ zeta^{(i)}}} &amp; text{subject to} quad t^{(i)}( mathbf{w}^T mathbf{x}^{(i)} + b) ge 1 - zeta^{(i)} quad text{and} quad zeta^{(i)} ge 0 quad text{for } i = 1, 2, dots, m end{split} $ . Equation 5-5: Quadratic Programming problem . $ begin{split} underset{ mathbf{p}}{ text{Minimize}} quad &amp; dfrac{1}{2} mathbf{p}^T mathbf{H} mathbf{p} quad + quad mathbf{f}^T mathbf{p} text{subject to} quad &amp; mathbf{A} mathbf{p} le mathbf{b} text{where } &amp; begin{cases} mathbf{p} &amp; text{ is an }n_p text{-dimensional vector (} n_p = text{number of parameters),} mathbf{H} &amp; text{ is an }n_p times n_p text{ matrix,} mathbf{f} &amp; text{ is an }n_p text{-dimensional vector,} mathbf{A} &amp; text{ is an } n_c times n_p text{ matrix (}n_c = text{number of constraints),} mathbf{b} &amp; text{ is an }n_c text{-dimensional vector.} end{cases} end{split} $ . Equation 5-6: Dual form of the linear SVM objective . $ begin{split} underset{ mathbf{ alpha}}{ operatorname{minimize}} dfrac{1}{2} sum limits_{i=1}^{m}{ sum limits_{j=1}^{m}{ alpha^{(i)} alpha^{(j)} t^{(i)} t^{(j)} { mathbf{x}^{(i)}}^T mathbf{x}^{(j)} } } quad - quad sum limits_{i=1}^{m}{ alpha^{(i)}} text{subject to} quad alpha^{(i)} ge 0 quad text{for }i = 1, 2, dots, m end{split} $ . Equation 5-7: From the dual solution to the primal solution . $ begin{split} &amp; hat{ mathbf{w}} = sum_{i=1}^{m}{ hat{ alpha}}^{(i)}t^{(i)} mathbf{x}^{(i)} &amp; hat{b} = dfrac{1}{n_s} sum limits_{ scriptstyle i=1 atop { scriptstyle { hat{ alpha}}^{(i)} &gt; 0}}^{m}{ left(t^{(i)} - ({ hat{ mathbf{w}}}^T mathbf{x}^{(i)}) right)} end{split} $ . Equation 5-8: Second-degree polynomial mapping . $ phi left( mathbf{x} right) = phi left( begin{pmatrix} x_1 x_2 end{pmatrix} right) = begin{pmatrix} {x_1}^2 sqrt{2} , x_1 x_2 {x_2}^2 end{pmatrix} $ . Equation 5-9: Kernel trick for a 2^nd^-degree polynomial mapping . $ begin{split} phi( mathbf{a})^T phi( mathbf{b}) &amp; quad = begin{pmatrix} {a_1}^2 sqrt{2} , a_1 a_2 {a_2}^2 end{pmatrix}^T begin{pmatrix} {b_1}^2 sqrt{2} , b_1 b_2 {b_2}^2 end{pmatrix} = {a_1}^2 {b_1}^2 + 2 a_1 b_1 a_2 b_2 + {a_2}^2 {b_2}^2 &amp; quad = left( a_1 b_1 + a_2 b_2 right)^2 = left( begin{pmatrix} a_1 a_2 end{pmatrix}^T begin{pmatrix} b_1 b_2 end{pmatrix} right)^2 = ( mathbf{a}^T mathbf{b})^2 end{split} $ . In the text about the kernel trick (page 162): [...], then you can replace this dot product of transformed vectors simply by $ ({ mathbf{x}^{(i)}}^T mathbf{x}^{(j)})^2 $ . Equation 5-10: Common kernels . $ begin{split} text{Linear:} &amp; quad K( mathbf{a}, mathbf{b}) = mathbf{a}^T mathbf{b} text{Polynomial:} &amp; quad K( mathbf{a}, mathbf{b}) = left( gamma mathbf{a}^T mathbf{b} + r right)^d text{Gaussian RBF:} &amp; quad K( mathbf{a}, mathbf{b}) = exp({ displaystyle - gamma left | mathbf{a} - mathbf{b} right |^2}) text{Sigmoid:} &amp; quad K( mathbf{a}, mathbf{b}) = tanh left( gamma mathbf{a}^T mathbf{b} + r right) end{split} $ . Equation 5-11: Making predictions with a kernelized SVM . $ begin{split} h_{ hat{ mathbf{w}}, hat{b}} left( phi( mathbf{x}^{(n)}) right) &amp; = , hat{ mathbf{w}}^T phi( mathbf{x}^{(n)}) + hat{b} = left( sum_{i=1}^{m}{ hat{ alpha}}^{(i)}t^{(i)} phi( mathbf{x}^{(i)}) right)^T phi( mathbf{x}^{(n)}) + hat{b} &amp; = , sum_{i=1}^{m}{ hat{ alpha}}^{(i)}t^{(i)} left( phi( mathbf{x}^{(i)})^T phi( mathbf{x}^{(n)}) right) + hat{b} &amp; = sum limits_{ scriptstyle i=1 atop { scriptstyle { hat{ alpha}}^{(i)} &gt; 0}}^{m}{ hat{ alpha}}^{(i)}t^{(i)} K( mathbf{x}^{(i)}, mathbf{x}^{(n)}) + hat{b} end{split} $ . Equation 5-12: Computing the bias term using the kernel trick . $ begin{split} hat{b} &amp; = dfrac{1}{n_s} sum limits_{ scriptstyle i=1 atop { scriptstyle { hat{ alpha}}^{(i)} &gt; 0}}^{m}{ left(t^{(i)} - { hat{ mathbf{w}}}^T phi( mathbf{x}^{(i)}) right)} = dfrac{1}{n_s} sum limits_{ scriptstyle i=1 atop { scriptstyle { hat{ alpha}}^{(i)} &gt; 0}}^{m}{ left(t^{(i)} - { left( sum_{j=1}^{m}{ hat{ alpha}}^{(j)}t^{(j)} phi( mathbf{x}^{(j)}) right) }^T phi( mathbf{x}^{(i)}) right)} &amp; = dfrac{1}{n_s} sum limits_{ scriptstyle i=1 atop { scriptstyle { hat{ alpha}}^{(i)} &gt; 0}}^{m}{ left(t^{(i)} - sum limits_{ scriptstyle j=1 atop { scriptstyle { hat{ alpha}}^{(j)} &gt; 0}}^{m}{ { hat{ alpha}}^{(j)} t^{(j)} K( mathbf{x}^{(i)}, mathbf{x}^{(j)}) } right)} end{split} $ . Equation 5-13: Linear SVM classifier cost function . $ J( mathbf{w}, b) = dfrac{1}{2} mathbf{w}^T mathbf{w} quad + quad C { displaystyle sum limits_{i=1}^{m}max left(0, t^{(i)} - ( mathbf{w}^T mathbf{x}^{(i)} + b) right)} $ . Chapter 6 . Equation 6-1: Gini impurity . $ G_i = 1 - sum limits_{k=1}^{n}{{p_{i,k}}^2} $ . Equation 6-2: CART cost function for classification . $ begin{split} &amp;J(k, t_k) = dfrac{m_{ text{left}}}{m}G_ text{left} + dfrac{m_{ text{right}}}{m}G_{ text{right}} &amp; text{where } begin{cases} G_ text{left/right} text{ measures the impurity of the left/right subset,} m_ text{left/right} text{ is the number of instances in the left/right subset.} end{cases} end{split} $ . Entropy computation example (page 173): . $ - frac{49}{54} log_2( frac{49}{54}) - frac{5}{54} log_2( frac{5}{54}) $ . Equation 6-3: Entropy . $ H_i = - sum limits_{k=1 atop p_{i,k} ne 0}^{n}{{p_{i,k}} log_2(p_{i,k})} $ . Equation 6-4: CART cost function for regression . $ J(k, t_k) = dfrac{m_{ text{left}}}{m} text{MSE}_ text{left} + dfrac{m_{ text{right}}}{m} text{MSE}_{ text{right}} quad text{where } begin{cases} text{MSE}_{ text{node}} = sum limits_{ scriptstyle i in text{node}}( hat{y}_{ text{node}} - y^{(i)})^2 hat{y}_ text{node} = dfrac{1}{m_{ text{node}}} sum limits_{ scriptstyle i in text{node}}y^{(i)} end{cases} $ . Chapter 7 . Equation 7-1: Weighted error rate of the $j^ text{th}$ predictor . $ r_j = dfrac{ displaystyle sum limits_{ textstyle {i=1 atop hat{y}_j^{(i)} ne y^{(i)}}}^{m}{w^{(i)}}}{ displaystyle sum limits_{i=1}^{m}{w^{(i)}}} quad text{where } hat{y}_j^{(i)} text{ is the }j^{ text{th}} text{ predictor&#39;s prediction for the }i^{ text{th}} text{ instance.} $ . Equation 7-2: Predictor weight . $ begin{split} alpha_j = eta log{ dfrac{1 - r_j}{r_j}} end{split} $ . Equation 7-3: Weight update rule . $ begin{split} &amp; text{ for } i = 1, 2, dots, m &amp; w^{(i)} leftarrow begin{cases} w^{(i)} &amp; text{if } hat{y_j}^{(i)} = y^{(i)} w^{(i)} exp( alpha_j) &amp; text{if } hat{y_j}^{(i)} ne y^{(i)} end{cases} end{split} $ . In the text page 194: . Then all the instance weights are normalized (i.e., divided by $ sum_{i=1}^{m}{w^{(i)}} $). . Equation 7-4: AdaBoost predictions . $ hat{y}( mathbf{x}) = underset{k}{ operatorname{argmax}}{ sum limits_{ scriptstyle j=1 atop scriptstyle hat{y}_j( mathbf{x}) = k}^{N}{ alpha_j}} quad text{where }N text{ is the number of predictors.} $ . Chapter 8 . Equation 8-1: Principal components matrix . $ mathbf{V}^T = begin{pmatrix} mid &amp; mid &amp; &amp; mid mathbf{c_1} &amp; mathbf{c_2} &amp; cdots &amp; mathbf{c_n} mid &amp; mid &amp; &amp; mid end{pmatrix} $ . Equation 8-2: Projecting the training set down to d dimensions . $ mathbf{X}_{d text{-proj}} = mathbf{X} mathbf{W}_d $ . Equation 8-3: PCA inverse transformation, back to the original number of dimensions . $ mathbf{X}_{ text{recovered}} = mathbf{X}_{d text{-proj}} { mathbf{W}_d}^T $ . $ sum_{j=1}^{m}{w_{i,j} mathbf{x}^{(j)}} $ . Equation 8-4: LLE step 1: linearly modeling local relationships . $ begin{split} &amp; hat{ mathbf{W}} = underset{ mathbf{W}}{ operatorname{argmin}}{ displaystyle sum limits_{i=1}^{m}} left | mathbf{x}^{(i)} - sum limits_{j=1}^{m}{w_{i,j}} mathbf{x}^{(j)} right |^2 &amp; text{subject to } begin{cases} w_{i,j}=0 &amp; text{if } mathbf{x}^{(j)} text{ is not one of the }k text{ c.n. of } mathbf{x}^{(i)} sum limits_{j=1}^{m}w_{i,j} = 1 &amp; text{for }i=1, 2, dots, m end{cases} end{split} $ . In the text page 223: . [...] then we want the squared distance between $ mathbf{z}^{(i)}$ and $ sum_{j=1}^{m}{ hat{w}_{i,j} mathbf{z}^{(j)}} $ to be as small as possible. . Equation 8-5: LLE step 2: reducing dimensionality while preserving relationships . $ hat{ mathbf{Z}} = underset{ mathbf{Z}}{ operatorname{argmin}}{ displaystyle sum limits_{i=1}^{m}} left | mathbf{z}^{(i)} - sum limits_{j=1}^{m}{ hat{w}_{i,j}} mathbf{z}^{(j)} right |^2 $ . Chapter 9 . Equation 9-1: Rectified linear unit . $ h_{ mathbf{w}, b}( mathbf{X}) = max( mathbf{X} mathbf{w} + b, 0) $ . Chapter 10 . Equation 10-1: Common step functions used in Perceptrons . $ begin{split} operatorname{heaviside}(z) = begin{cases} 0 &amp; text{if }z &lt; 0 1 &amp; text{if }z ge 0 end{cases} &amp; quad quad operatorname{sgn}(z) = begin{cases} -1 &amp; text{if }z &lt; 0 0 &amp; text{if }z = 0 +1 &amp; text{if }z &gt; 0 end{cases} end{split} $ . Equation 10-2: Perceptron learning rule (weight update) . $ {w_{i,j}}^{( text{next step})} = w_{i,j} + eta (y_j - hat{y}_j) x_i $ . In the text page 266: . It will be initialized randomly, using a truncated normal (Gaussian) distribution with a standard deviation of $ 2 / sqrt{ text{n}_ text{inputs}} $. . Chapter 11 . Equation 11-1: Xavier initialization (when using the logistic activation function) . $ begin{split} &amp; text{Normal distribution with mean 0 and standard deviation } sigma = sqrt{ dfrac{2}{n_ text{inputs} + n_ text{outputs}}} &amp; text{Or a uniform distribution between -r and +r, with } r = sqrt{ dfrac{6}{n_ text{inputs} + n_ text{outputs}}} end{split} $ . In the text page 278: . When the number of input connections is roughly equal to the number of output connections, you get simpler equations (e.g., $ sigma = 1 / sqrt{n_ text{inputs}} $ or $ r = sqrt{3} / sqrt{n_ text{inputs}} $). . Table 11-1: Initialization parameters for each type of activation function . Logistic uniform: $ r = sqrt{ dfrac{6}{n_ text{inputs} + n_ text{outputs}}} $ | Logistic normal: $ sigma = sqrt{ dfrac{2}{n_ text{inputs} + n_ text{outputs}}} $ | Hyperbolic tangent uniform: $ r = 4 sqrt{ dfrac{6}{n_ text{inputs} + n_ text{outputs}}} $ | Hyperbolic tangent normal: $ sigma = 4 sqrt{ dfrac{2}{n_ text{inputs} + n_ text{outputs}}} $ | ReLU (and its variants) uniform: $ r = sqrt{2} sqrt{ dfrac{6}{n_ text{inputs} + n_ text{outputs}}} $ | ReLU (and its variants) normal: $ sigma = sqrt{2} sqrt{ dfrac{2}{n_ text{inputs} + n_ text{outputs}}} $ | . Equation 11-2: ELU activation function . $ operatorname{ELU}_ alpha(z) = begin{cases} alpha( exp(z) - 1) &amp; text{if } z &lt; 0 z &amp; if z ge 0 end{cases} $ . Equation 11-3: Batch Normalization algorithm . $ begin{split} 1. quad &amp; mathbf{ mu}_B = dfrac{1}{m_B} sum limits_{i=1}^{m_B}{ mathbf{x}^{(i)}} 2. quad &amp; { mathbf{ sigma}_B}^2 = dfrac{1}{m_B} sum limits_{i=1}^{m_B}{( mathbf{x}^{(i)} - mathbf{ mu}_B)^2} 3. quad &amp; hat{ mathbf{x}}^{(i)} = dfrac{ mathbf{x}^{(i)} - mathbf{ mu}_B}{ sqrt{{ mathbf{ sigma}_B}^2 + epsilon}} 4. quad &amp; mathbf{z}^{(i)} = gamma hat{ mathbf{x}}^{(i)} + beta end{split} $ . In the text page 285: . [...] given a new value $v$, the running average $v$ is updated through the equation: . $ hat{v} gets hat{v} times text{momentum} + v times (1 - text{momentum}) $ . Equation 11-4: Momentum algorithm . $ mathbf{m} gets beta mathbf{m} - eta nabla_ boldsymbol{ theta}J( boldsymbol{ theta})$ | $ boldsymbol{ theta} gets boldsymbol{ theta} + mathbf{m}$ | In the text page 296: . You can easily verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate Î· multiplied by $ frac{1}{1 - beta} $. . Equation 11-5: Nesterov Accelerated Gradient algorithm . $ mathbf{m} gets beta mathbf{m} - eta nabla_ boldsymbol{ theta}J( boldsymbol{ theta} + beta mathbf{m})$ | $ boldsymbol{ theta} gets boldsymbol{ theta} + mathbf{m}$ | Equation 11-6: AdaGrad algorithm . $ mathbf{s} gets mathbf{s} + nabla_ boldsymbol{ theta}J( boldsymbol{ theta}) otimes nabla_ boldsymbol{ theta}J( boldsymbol{ theta})$ | $ boldsymbol{ theta} gets boldsymbol{ theta} - eta , nabla_ boldsymbol{ theta}J( boldsymbol{ theta}) oslash { sqrt{ mathbf{s} + epsilon}}$ | In the text page 298-299: . This vectorized form is equivalent to computing $s_i gets s_i + left( dfrac{ partial J( boldsymbol{ theta})}{ partial theta_i} right)^2$ for each element $s_i$ of the vector $ mathbf{s}$. . In the text page 299: . This vectorized form is equivalent to computing $ theta_i gets theta_i - eta , dfrac{ partial J( boldsymbol{ theta})}{ partial theta_i} dfrac{1}{ sqrt{s_i + epsilon}} $ for all parameters $ theta_i$ (simultaneously). . Equation 11-7: RMSProp algorithm . $ mathbf{s} gets beta mathbf{s} + (1 - beta ) nabla_ boldsymbol{ theta}J( boldsymbol{ theta}) otimes nabla_ boldsymbol{ theta}J( boldsymbol{ theta})$ | $ boldsymbol{ theta} gets boldsymbol{ theta} - eta , nabla_ boldsymbol{ theta}J( boldsymbol{ theta}) oslash { sqrt{ mathbf{s} + epsilon}}$ | Equation 11-8: Adam algorithm . $ mathbf{m} gets beta_1 mathbf{m} - (1 - beta_1) nabla_ boldsymbol{ theta}J( boldsymbol{ theta})$ | $ mathbf{s} gets beta_2 mathbf{s} + (1 - beta_2) nabla_ boldsymbol{ theta}J( boldsymbol{ theta}) otimes nabla_ boldsymbol{ theta}J( boldsymbol{ theta})$ | $ hat{ mathbf{m}} gets left( dfrac{ mathbf{m}}{1 - { beta_1}^T} right)$ | $ hat{ mathbf{s}} gets left( dfrac{ mathbf{s}}{1 - { beta_2}^T} right)$ | $ boldsymbol{ theta} gets boldsymbol{ theta} + eta , hat{ mathbf{m}} oslash { sqrt{ hat{ mathbf{s}} + epsilon}}$ | In the text page 309: . We typically implement this constraint by computing $ left | mathbf{w} right |_2$ after each training step and clipping $ mathbf{w}$ if needed $ left( mathbf{w} gets mathbf{w} dfrac{r}{ left | mathbf{w} right |_2} right) $. . Chapter 13 . Equation 13-1: Computing the output of a neuron in a convolutional layer . $ z_{i,j,k} = b_k + sum limits_{u = 0}^{f_h - 1} , , sum limits_{v = 0}^{f_w - 1} , , sum limits_{k&#39; = 0}^{f_{n&#39;} - 1} , , x_{i&#39;, j&#39;, k&#39;} times w_{u, v, k&#39;, k} quad text{with } begin{cases} i&#39; = i times s_h + u j&#39; = j times s_w + v end{cases} $ . Equation 13-2: Local response normalization . $ b_i = a_i left(k + alpha sum limits_{j=j_ text{low}}^{j_ text{high}}{{a_j}^2} right)^{- beta} quad text{with } begin{cases} j_ text{high} = min left(i + dfrac{r}{2}, f_n-1 right) j_ text{low} = max left(0, i - dfrac{r}{2} right) end{cases} $ . Chapter 14 . Equation 14-1: Output of a recurrent layer for a single instance . $ mathbf{y}_{(t)} = phi left({ mathbf{W}_x}^T{ mathbf{x}_{(t)}} + {{ mathbf{W}_y}^T mathbf{y}_{(t-1)}} + mathbf{b} right) $ . Equation 14-2: Outputs of a layer of recurrent neurons for all instances in a mini-batch . $ begin{split} mathbf{Y}_{(t)} &amp; = phi left( mathbf{X}_{(t)} mathbf{W}_{x} + mathbf{Y}_{(t-1)} mathbf{W}_{y} + mathbf{b} right) &amp; = phi left( left[ mathbf{X}_{(t)} quad mathbf{Y}_{(t-1)} right] mathbf{W} + mathbf{b} right) text{ with } mathbf{W}= left[ begin{matrix} mathbf{W}_x mathbf{W}_y end{matrix} right] end{split} $ . In the text page 391: . Just like in regular backpropagation, there is a first forward pass through the unrolled network (represented by the dashed arrows); then the output sequence is evaluated using a cost function $ C( mathbf{Y}_{(t_ text{min})}, mathbf{Y}_{(t_ text{min}+1)}, dots, mathbf{Y}_{(t_ text{max})}) $ (where $t_ text{min}$ and $t_ text{max}$ are the first and last output time steps, not counting the ignored outputs)[...] . Equation 14-3: LSTM computations . $ begin{split} mathbf{i}_{(t)}&amp;= sigma({ mathbf{W}_{xi}}^T mathbf{x}_{(t)} + { mathbf{W}_{hi}}^T mathbf{h}_{(t-1)} + mathbf{b}_i) mathbf{f}_{(t)}&amp;= sigma({ mathbf{W}_{xf}}^T mathbf{x}_{(t)} + { mathbf{W}_{hf}}^T mathbf{h}_{(t-1)} + mathbf{b}_f) mathbf{o}_{(t)}&amp;= sigma({ mathbf{W}_{xo}}^T mathbf{x}_{(t)} + { mathbf{W}_{ho}}^T mathbf{h}_{(t-1)} + mathbf{b}_o) mathbf{g}_{(t)}&amp;= operatorname{tanh}({ mathbf{W}_{xg}}^T mathbf{x}_{(t)} + { mathbf{W}_{hg}}^T mathbf{h}_{(t-1)} + mathbf{b}_g) mathbf{c}_{(t)}&amp;= mathbf{f}_{(t)} otimes mathbf{c}_{(t-1)} , + , mathbf{i}_{(t)} otimes mathbf{g}_{(t)} mathbf{y}_{(t)}&amp;= mathbf{h}_{(t)} = mathbf{o}_{(t)} otimes operatorname{tanh}( mathbf{c}_{(t)}) end{split} $ . Equation 14-4: GRU computations . $ begin{split} mathbf{z}_{(t)}&amp;= sigma({ mathbf{W}_{xz}}^T mathbf{x}_{(t)} + { mathbf{W}_{hz}}^T mathbf{h}_{(t-1)}) mathbf{r}_{(t)}&amp;= sigma({ mathbf{W}_{xr}}^T mathbf{x}_{(t)} + { mathbf{W}_{hr}}^T mathbf{h}_{(t-1)}) mathbf{g}_{(t)}&amp;= operatorname{tanh} left({ mathbf{W}_{xg}}^T mathbf{x}_{(t)} + { mathbf{W}_{hg}}^T ( mathbf{r}_{(t)} otimes mathbf{h}_{(t-1)}) right) mathbf{h}_{(t)}&amp;=(1- mathbf{z}_{(t)}) otimes mathbf{h}_{(t-1)} + mathbf{z}_{(t)} otimes mathbf{g}_{(t)} end{split} $ . Chapter 15 . Equation 15-1: Kullbackâ€“Leibler divergence . $ D_{ mathrm{KL}}(P |Q) = sum limits_{i} P(i) log dfrac{P(i)}{Q(i)} $ . Equation: KL divergence between the target sparsity p and the actual sparsity q . $ D_{ mathrm{KL}}(p |q) = p , log dfrac{p}{q} + (1-p) log dfrac{1-p}{1-q} $ . In the text page 433: . One common variant is to train the encoder to output $ gamma = log left( sigma^2 right)$ rather than $ sigma$. Wherever we need $ sigma$ we can just compute $ sigma = exp left( dfrac{ gamma}{2} right) $. . Chapter 16 . Equation 16-1: Bellman Optimality Equation . $ V^*(s) = underset{a}{ max} sum limits_{s&#39;}{T(s, a, s&#39;) [R(s, a, s&#39;) + gamma . V^*(s&#39;)]} quad text{for all }s $ . Equation 16-2: Value Iteration algorithm . $ V_{k+1}(s) gets underset{a}{ max} sum limits_{s&#39;}{T(s, a, s&#39;) [R(s, a, s&#39;) + gamma . V_k(s&#39;)]} quad text{for all }s $ . Equation 16-3: Q-Value Iteration algorithm . $ Q_{k+1}(s, a) gets sum limits_{s&#39;}{T(s, a, s&#39;) [R(s, a, s&#39;) + gamma . underset{a&#39;}{ max} ,{Q_k(s&#39;,a&#39;)}]} quad text{for all } (s,a) $ . In the text page 458: . Once you have the optimal Q-Values, defining the optimal policy, noted $ pi^{*}(s)$, is trivial: when the agent is in state $s$, it should choose the action with the highest Q-Value for that state: $ pi^{*}(s) = underset{a}{ operatorname{argmax}} , Q^*(s, a) $. . Equation 16-4: TD Learning algorithm . $ V_{k+1}(s) gets (1- alpha)V_k(s) + alpha left(r + gamma . V_k(s&#39;) right) $ . Equation 16-5: Q-Learning algorithm . $ Q_{k+1}(s, a) gets (1- alpha)Q_k(s,a) + alpha left(r + gamma . underset{a&#39;}{ max} , Q_k(s&#39;, a&#39;) right) $ . Equation 16-6: Q-Learning using an exploration function . $ Q(s, a) gets (1- alpha)Q(s,a) + alpha left(r + gamma , underset{a&#39;}{ max}f(Q(s&#39;, a&#39;), N(s&#39;, a&#39;)) right) $ . Equation 16-7: Target Q-Value . $ y(s,a)=r+ gamma , max_{a&#39;} ,Q_ boldsymbol theta(s&#39;,a&#39;) $ . Appendix A . Equations that appear in the text: . $ mathbf{H} = begin{pmatrix} mathbf{H&#39;} &amp; 0 &amp; cdots 0 &amp; 0 &amp; vdots &amp; &amp; ddots end{pmatrix} $ . $ mathbf{A} = begin{pmatrix} mathbf{A&#39;} &amp; mathbf{I}_m mathbf{0} &amp; - mathbf{I}_m end{pmatrix} $ . $ 1 - frac{1}{5}^2 - frac{4}{5}^2 $ . $ 1 - frac{1}{2}^2 - frac{1}{2}^2 $ . $ frac{2}{5} times $ . $ frac{3}{5} times 0 $ . Appendix C . Equations that appear in the text: . $ ( hat{x}, hat{y}) $ . $ hat{ alpha} $ . $ ( hat{x}, hat{y}, hat{ alpha}) $ . $ begin{cases} frac{ partial}{ partial x}g(x, y, alpha) = 2x - 3 alpha frac{ partial}{ partial y}g(x, y, alpha) = 2 - 2 alpha frac{ partial}{ partial alpha}g(x, y, alpha) = -3x - 2y - 1 end{cases} $ . $ 2 hat{x} - 3 hat{ alpha} = 2 - 2 hat{ alpha} = -3 hat{x} - 2 hat{y} - 1 = 0 $ . $ hat{x} = frac{3}{2} $ . $ hat{y} = - frac{11}{4} $ . $ hat{ alpha} = 1 $ . Equation C-1: Generalized Lagrangian for the hard margin problem . $ begin{split} mathcal{L}( mathbf{w}, b, mathbf{ alpha}) = frac{1}{2} mathbf{w}^T mathbf{w} - sum limits_{i=1}^{m}{ alpha^{(i)} left(t^{(i)}( mathbf{w}^T mathbf{x}^{(i)} + b) - 1 right)} text{with} quad alpha^{(i)} ge 0 quad text{for }i = 1, 2, dots, m end{split} $ . More equations in the text: . $ ( hat{ mathbf{w}}, hat{b}, hat{ mathbf{ alpha}}) $ . $ t^{(i)}( hat{ mathbf{w}}^T mathbf{x}^{(i)} + hat{b}) ge 1 quad text{for } i = 1, 2, dots, m $ . $ { hat{ alpha}}^{(i)} ge 0 quad text{for } i = 1, 2, dots, m $ . $ { hat{ alpha}}^{(i)} = 0 $ . $ t^{(i)}(( hat{ mathbf{w}})^T mathbf{x}^{(i)} + hat{b}) = 1 $ . $ { hat{ alpha}}^{(i)} = 0 $ . Equation C-2: Partial derivatives of the generalized Lagrangian . $ begin{split} nabla_{ mathbf{w}} mathcal{L}( mathbf{w}, b, mathbf{ alpha}) = mathbf{w} - sum limits_{i=1}^{m} alpha^{(i)}t^{(i)} mathbf{x}^{(i)} dfrac{ partial}{ partial b} mathcal{L}( mathbf{w}, b, mathbf{ alpha}) = - sum limits_{i=1}^{m} alpha^{(i)}t^{(i)} end{split} $ . Equation C-3: Properties of the stationary points . $ begin{split} hat{ mathbf{w}} = sum_{i=1}^{m}{ hat{ alpha}}^{(i)}t^{(i)} mathbf{x}^{(i)} sum_{i=1}^{m}{ hat{ alpha}}^{(i)}t^{(i)} = 0 end{split} $ . Equation C-4: Dual form of the SVM problem . $ begin{split} mathcal{L}( hat{ mathbf{w}}, hat{b}, mathbf{ alpha}) = dfrac{1}{2} sum limits_{i=1}^{m}{ sum limits_{j=1}^{m}{ alpha^{(i)} alpha^{(j)} t^{(i)} t^{(j)} { mathbf{x}^{(i)}}^T mathbf{x}^{(j)} } } quad - quad sum limits_{i=1}^{m}{ alpha^{(i)}} text{with} quad alpha^{(i)} ge 0 quad text{for }i = 1, 2, dots, m end{split} $ . Some more equations in the text: . $ hat{ mathbf{ alpha}} $ . $ { hat{ alpha}}^{(i)} ge 0 $ . $ hat{ mathbf{ alpha}} $ . $ hat{ mathbf{w}} $ . $ hat{b} $ . $ hat{b} = t^{(k)} - { hat{ mathbf{w}}}^T mathbf{x}^{(k)} $ . Equation C-5: Bias term estimation using the dual form . $ hat{b} = dfrac{1}{n_s} sum limits_{ scriptstyle i=1 atop { scriptstyle { hat{ alpha}}^{(i)} &gt; 0}}^{m}{ left[t^{(i)} - { hat{ mathbf{w}}}^T mathbf{x}^{(i)} right]} $ . Appendix D . Equation D-1: Partial derivatives of $f(x,y)$ . $ begin{split} dfrac{ partial f}{ partial x} &amp; = dfrac{ partial(x^2y)}{ partial x} + dfrac{ partial y}{ partial x} + dfrac{ partial 2}{ partial x} = y dfrac{ partial(x^2)}{ partial x} + 0 + 0 = 2xy dfrac{ partial f}{ partial y} &amp; = dfrac{ partial(x^2y)}{ partial y} + dfrac{ partial y}{ partial y} + dfrac{ partial 2}{ partial y} = x^2 + 1 + 0 = x^2 + 1 end{split} $ . In the text: . $ frac{ partial g}{ partial x} = 0 + (0 times x + y times 1) = y $ . $ frac{ partial x}{ partial x} = 1 $ . $ frac{ partial y}{ partial x} = 0 $ . $ frac{ partial (u times v)}{ partial x} = frac{ partial v}{ partial x} times u + frac{ partial u}{ partial x} times u $ . $ frac{ partial g}{ partial x} = 0 + (0 times x + y times 1) $ . $ frac{ partial g}{ partial x} = y $ . Equation D-2: Derivative of a function h(x) at point x~0~ . $ begin{split} h&#39;(x) &amp; = underset{ textstyle x to x_0}{ lim} dfrac{h(x) - h(x_0)}{x - x_0} &amp; = underset{ textstyle epsilon to 0}{ lim} dfrac{h(x_0 + epsilon) - h(x_0)}{ epsilon} end{split} $ . Equation D-3: A few operations with dual numbers . $ begin{split} &amp; lambda(a + b epsilon) = lambda a + lambda b epsilon &amp;(a + b epsilon) + (c + d epsilon) = (a + c) + (b + d) epsilon &amp;(a + b epsilon) times (c + d epsilon) = ac + (ad + bc) epsilon + (bd) epsilon^2 = ac + (ad + bc) epsilon end{split} $ . In the text: . $ frac{ partial f}{ partial x}(3, 4) $ . $ frac{ partial f}{ partial y}(3, 4) $ . Equation D-4: Chain rule . $ dfrac{ partial f}{ partial x} = dfrac{ partial f}{ partial n_i} times dfrac{ partial n_i}{ partial x} $ . In the text: . $ frac{ partial f}{ partial n_7} = 1 $ . $ frac{ partial f}{ partial n_5} = frac{ partial f}{ partial n_7} times frac{ partial n_7}{ partial n_5} $ . $ frac{ partial f}{ partial n_7} = 1 $ . $ frac{ partial n_7}{ partial n_5} $ . $ frac{ partial n_7}{ partial n_5} = 1 $ . $ frac{ partial f}{ partial n_5} = 1 times 1 = 1 $ . $ frac{ partial f}{ partial n_4} = frac{ partial f}{ partial n_5} times frac{ partial n_5}{ partial n_4} $ . $ frac{ partial n_5}{ partial n_4} = n_2 $ . $ frac{ partial f}{ partial n_4} = 1 times n_2 = 4 $ . $ frac{ partial f}{ partial x} = 24 $ . $ frac{ partial f}{ partial y} = 10 $ . Appendix E . Equation E-1: Probability that the i^th^ neuron will output 1 . $ p left(s_i^{( text{next step})} = 1 right) , = , sigma left( frac{ textstyle sum limits_{j = 1}^N{w_{i,j}s_j + b_i}}{ textstyle T} right) $ . In the text: . $ dot{ mathbf{x}} $ . $ dot{ mathbf{h}} $ . Equation E-2: Contrastive divergence weight update . $ w_{i,j}^{( text{next step})} = w_{i,j} + eta( mathbf{x} mathbf{h}^T - dot{ mathbf{x}} dot { mathbf{h}}^T) $ . Glossary . In the text: . $ ell _1$ . $ ell _2$ . $ ell _k$ . $ chi^2 $ . Just in case your eyes hurt after all these equations, let&#39;s finish with the single most beautiful equation in the world. No, it&#39;s not $E = mcÂ²$, it&#39;s obviously Euler&#39;s identity: . $e^{i pi}+1=0$ . .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/book_equations.html",
            "relUrl": "/2020/03/09/book_equations.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Unsupervised Learning",
            "content": "Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;unsupervised_learning&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) # Ignore useless warnings (see SciPy issue #5998) import warnings warnings.filterwarnings(action=&quot;ignore&quot;, message=&quot;^internal gelsd&quot;) . . Clustering . Introduction &#8211; Classification vs Clustering . from sklearn.datasets import load_iris . data = load_iris() X = data.data y = data.target data.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . #collapse-show plt.figure(figsize=(9, 3.5)) plt.subplot(121) plt.plot(X[y==0, 2], X[y==0, 3], &quot;yo&quot;, label=&quot;Iris setosa&quot;) plt.plot(X[y==1, 2], X[y==1, 3], &quot;bs&quot;, label=&quot;Iris versicolor&quot;) plt.plot(X[y==2, 2], X[y==2, 3], &quot;g^&quot;, label=&quot;Iris virginica&quot;) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.legend(fontsize=12) plt.subplot(122) plt.scatter(X[:, 2], X[:, 3], c=&quot;k&quot;, marker=&quot;.&quot;) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.tick_params(labelleft=False) save_fig(&quot;classification_vs_clustering_plot&quot;) plt.show() . . Saving figure classification_vs_clustering_plot . A Gaussian mixture model (explained below) can actually separate these clusters pretty well (using all 4 features: petal length &amp; width, and sepal length &amp; width). . from sklearn.mixture import GaussianMixture . y_pred = GaussianMixture(n_components=3, random_state=42).fit(X).predict(X) mapping = np.array([2, 0, 1]) y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred]) . plt.plot(X[y_pred==0, 2], X[y_pred==0, 3], &quot;yo&quot;, label=&quot;Cluster 1&quot;) plt.plot(X[y_pred==1, 2], X[y_pred==1, 3], &quot;bs&quot;, label=&quot;Cluster 2&quot;) plt.plot(X[y_pred==2, 2], X[y_pred==2, 3], &quot;g^&quot;, label=&quot;Cluster 3&quot;) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.legend(loc=&quot;upper left&quot;, fontsize=12) plt.show() . np.sum(y_pred==y) . 145 . np.sum(y_pred==y) / len(y_pred) . 0.9666666666666667 . K-Means . Let&#39;s start by generating some blobs: . from sklearn.datasets import make_blobs . blob_centers = np.array( [[ 0.2, 2.3], [-1.5 , 2.3], [-2.8, 1.8], [-2.8, 2.8], [-2.8, 1.3]]) blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1]) . X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std, random_state=7) . Now let&#39;s plot them: . def plot_clusters(X, y=None): plt.scatter(X[:, 0], X[:, 1], c=y, s=1) plt.xlabel(&quot;$x_1$&quot;, fontsize=14) plt.ylabel(&quot;$x_2$&quot;, fontsize=14, rotation=0) . plt.figure(figsize=(8, 4)) plot_clusters(X) save_fig(&quot;blobs_plot&quot;) plt.show() . Saving figure blobs_plot . Fit and Predict . Let&#39;s train a K-Means clusterer on this dataset. It will try to find each blob&#39;s center and assign each instance to the closest blob: . from sklearn.cluster import KMeans . k = 5 kmeans = KMeans(n_clusters=k, random_state=42) y_pred = kmeans.fit_predict(X) . Each instance was assigned to one of the 5 clusters: . y_pred . array([4, 0, 1, ..., 2, 1, 0], dtype=int32) . y_pred is kmeans.labels_ . True . And the following 5 centroids (i.e., cluster centers) were estimated: . kmeans.cluster_centers_ . array([[-2.80389616, 1.80117999], [ 0.20876306, 2.25551336], [-2.79290307, 2.79641063], [-1.46679593, 2.28585348], [-2.80037642, 1.30082566]]) . Note that the KMeans instance preserves the labels of the instances it was trained on. Somewhat confusingly, in this context, the label of an instance is the index of the cluster that instance gets assigned to: . kmeans.labels_ . array([4, 0, 1, ..., 2, 1, 0], dtype=int32) . Of course, we can predict the labels of new instances: . X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]]) kmeans.predict(X_new) . array([1, 1, 2, 2], dtype=int32) . Decision Boundaries . Let&#39;s plot the model&#39;s decision boundaries. This gives us a Voronoi diagram: . #collapse-show def plot_data(X): plt.plot(X[:, 0], X[:, 1], &#39;k.&#39;, markersize=2) def plot_centroids(centroids, weights=None, circle_color=&#39;w&#39;, cross_color=&#39;k&#39;): if weights is not None: centroids = centroids[weights &gt; weights.max() / 10] plt.scatter(centroids[:, 0], centroids[:, 1], marker=&#39;o&#39;, s=30, linewidths=8, color=circle_color, zorder=10, alpha=0.9) plt.scatter(centroids[:, 0], centroids[:, 1], marker=&#39;x&#39;, s=50, linewidths=50, color=cross_color, zorder=11, alpha=1) def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True, show_xlabels=True, show_ylabels=True): mins = X.min(axis=0) - 0.1 maxs = X.max(axis=0) + 0.1 xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution), np.linspace(mins[1], maxs[1], resolution)) Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), cmap=&quot;Pastel2&quot;) plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), linewidths=1, colors=&#39;k&#39;) plot_data(X) if show_centroids: plot_centroids(clusterer.cluster_centers_) if show_xlabels: plt.xlabel(&quot;$x_1$&quot;, fontsize=14) else: plt.tick_params(labelbottom=False) if show_ylabels: plt.ylabel(&quot;$x_2$&quot;, fontsize=14, rotation=0) else: plt.tick_params(labelleft=False) . . plt.figure(figsize=(8, 4)) plot_decision_boundaries(kmeans, X) save_fig(&quot;voronoi_plot&quot;) plt.show() . Saving figure voronoi_plot . Not bad! Some of the instances near the edges were probably assigned to the wrong cluster, but overall it looks pretty good. . Hard Clustering vs Soft Clustering . Rather than arbitrarily choosing the closest cluster for each instance, which is called hard clustering, it might be better measure the distance of each instance to all 5 centroids. This is what the transform() method does: . kmeans.transform(X_new) . array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901], [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351], [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031], [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]]) . You can verify that this is indeed the Euclidian distance between each instance and each centroid: . np.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2) - kmeans.cluster_centers_, axis=2) . array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901], [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351], [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031], [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]]) . K-Means Algorithm . The K-Means algorithm is one of the fastest clustering algorithms, but also one of the simplest: . First initialize $k$ centroids randomly: $k$ distinct instances are chosen randomly from the dataset and the centroids are placed at their locations. | Repeat until convergence (i.e., until the centroids stop moving): Assign each instance to the closest centroid. | Update the centroids to be the mean of the instances that are assigned to them. | . | . The KMeans class applies an optimized algorithm by default. To get the original K-Means algorithm (for educational purposes only), you must set init=&quot;random&quot;, n_init=1and algorithm=&quot;full&quot;. These hyperparameters will be explained below. . Let&#39;s run the K-Means algorithm for 1, 2 and 3 iterations, to see how the centroids move around: . kmeans_iter1 = KMeans(n_clusters=5, init=&quot;random&quot;, n_init=1, algorithm=&quot;full&quot;, max_iter=1, random_state=1) kmeans_iter2 = KMeans(n_clusters=5, init=&quot;random&quot;, n_init=1, algorithm=&quot;full&quot;, max_iter=2, random_state=1) kmeans_iter3 = KMeans(n_clusters=5, init=&quot;random&quot;, n_init=1, algorithm=&quot;full&quot;, max_iter=3, random_state=1) kmeans_iter1.fit(X) kmeans_iter2.fit(X) kmeans_iter3.fit(X) . KMeans(algorithm=&#39;full&#39;, copy_x=True, init=&#39;random&#39;, max_iter=3, n_clusters=5, n_init=1, n_jobs=None, precompute_distances=&#39;auto&#39;, random_state=1, tol=0.0001, verbose=0) . And let&#39;s plot this: . plt.figure(figsize=(10, 8)) plt.subplot(321) plot_data(X) plot_centroids(kmeans_iter1.cluster_centers_, circle_color=&#39;r&#39;, cross_color=&#39;w&#39;) plt.ylabel(&quot;$x_2$&quot;, fontsize=14, rotation=0) plt.tick_params(labelbottom=False) plt.title(&quot;Update the centroids (initially randomly)&quot;, fontsize=14) plt.subplot(322) plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False) plt.title(&quot;Label the instances&quot;, fontsize=14) plt.subplot(323) plot_decision_boundaries(kmeans_iter1, X, show_centroids=False, show_xlabels=False) plot_centroids(kmeans_iter2.cluster_centers_) plt.subplot(324) plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False) plt.subplot(325) plot_decision_boundaries(kmeans_iter2, X, show_centroids=False) plot_centroids(kmeans_iter3.cluster_centers_) plt.subplot(326) plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False) save_fig(&quot;kmeans_algorithm_plot&quot;) plt.show() . Saving figure kmeans_algorithm_plot . K-Means Variability . In the original K-Means algorithm, the centroids are just initialized randomly, and the algorithm simply runs a single iteration to gradually improve the centroids, as we saw above. . However, one major problem with this approach is that if you run K-Means multiple times (or with different random seeds), it can converge to very different solutions, as you can see below: . def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None): clusterer1.fit(X) clusterer2.fit(X) plt.figure(figsize=(10, 3.2)) plt.subplot(121) plot_decision_boundaries(clusterer1, X) if title1: plt.title(title1, fontsize=14) plt.subplot(122) plot_decision_boundaries(clusterer2, X, show_ylabels=False) if title2: plt.title(title2, fontsize=14) . kmeans_rnd_init1 = KMeans(n_clusters=5, init=&quot;random&quot;, n_init=1, algorithm=&quot;full&quot;, random_state=11) kmeans_rnd_init2 = KMeans(n_clusters=5, init=&quot;random&quot;, n_init=1, algorithm=&quot;full&quot;, random_state=19) plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X, &quot;Solution 1&quot;, &quot;Solution 2 (with a different random init)&quot;) save_fig(&quot;kmeans_variability_plot&quot;) plt.show() . Saving figure kmeans_variability_plot . Inertia . To select the best model, we will need a way to evaluate a K-Mean model&#39;s performance. Unfortunately, clustering is an unsupervised task, so we do not have the targets. But at least we can measure the distance between each instance and its centroid. This is the idea behind the inertia metric: . kmeans.inertia_ . 211.5985372581684 . As you can easily verify, inertia is the sum of the squared distances between each training instance and its closest centroid: . X_dist = kmeans.transform(X) np.sum(X_dist[np.arange(len(X_dist)), kmeans.labels_]**2) . 211.59853725816856 . The score() method returns the negative inertia. Why negative? Well, it is because a predictor&#39;s score() method must always respect the &quot;great is better&quot; rule. . kmeans.score(X) . -211.59853725816856 . Multiple Initializations . So one approach to solve the variability issue is to simply run the K-Means algorithm multiple times with different random initializations, and select the solution that minimizes the inertia. For example, here are the inertias of the two &quot;bad&quot; models shown in the previous figure: . kmeans_rnd_init1.inertia_ . 223.29108572819035 . kmeans_rnd_init2.inertia_ . 237.46249169442845 . As you can see, they have a higher inertia than the first &quot;good&quot; model we trained, which means they are probably worse. . When you set the n_init hyperparameter, Scikit-Learn runs the original algorithm n_init times, and selects the solution that minimizes the inertia. By default, Scikit-Learn sets n_init=10. . kmeans_rnd_10_inits = KMeans(n_clusters=5, init=&quot;random&quot;, n_init=10, algorithm=&quot;full&quot;, random_state=11) kmeans_rnd_10_inits.fit(X) . KMeans(algorithm=&#39;full&#39;, copy_x=True, init=&#39;random&#39;, max_iter=300, n_clusters=5, n_init=10, n_jobs=None, precompute_distances=&#39;auto&#39;, random_state=11, tol=0.0001, verbose=0) . As you can see, we end up with the initial model, which is certainly the optimal K-Means solution (at least in terms of inertia, and assuming $k=5$). . plt.figure(figsize=(8, 4)) plot_decision_boundaries(kmeans_rnd_10_inits, X) plt.show() . K-Means++ . Instead of initializing the centroids entirely randomly, it is preferable to initialize them using the following algorithm, proposed in a 2006 paper by David Arthur and Sergei Vassilvitskii: . Take one centroid $c_1$, chosen uniformly at random from the dataset. | Take a new center $c_i$, choosing an instance $ mathbf{x}_i$ with probability: $D( mathbf{x}_i)^2$ / $ sum limits_{j=1}^{m}{D( mathbf{x}_j)}^2$ where $D( mathbf{x}_i)$ is the distance between the instance $ mathbf{x}_i$ and the closest centroid that was already chosen. This probability distribution ensures that instances that are further away from already chosen centroids are much more likely be selected as centroids. | Repeat the previous step until all $k$ centroids have been chosen. | . The rest of the K-Means++ algorithm is just regular K-Means. With this initialization, the K-Means algorithm is much less likely to converge to a suboptimal solution, so it is possible to reduce n_init considerably. Most of the time, this largely compensates for the additional complexity of the initialization process. . To set the initialization to K-Means++, simply set init=&quot;k-means++&quot; (this is actually the default): . KMeans() . KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=8, n_init=10, n_jobs=None, precompute_distances=&#39;auto&#39;, random_state=None, tol=0.0001, verbose=0) . good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]]) kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42) kmeans.fit(X) kmeans.inertia_ . 211.5985372581684 . Accelerated K-Means . The K-Means algorithm can be significantly accelerated by avoiding many unnecessary distance calculations: this is achieved by exploiting the triangle inequality (given three points A, B and C, the distance AC is always such that AC â‰¤ AB + BC) and by keeping track of lower and upper bounds for distances between instances and centroids (see this 2003 paper by Charles Elkan for more details). . To use Elkan&#39;s variant of K-Means, just set algorithm=&quot;elkan&quot;. Note that it does not support sparse data, so by default, Scikit-Learn uses &quot;elkan&quot; for dense data, and &quot;full&quot; (the regular K-Means algorithm) for sparse data. . %timeit -n 50 KMeans(algorithm=&quot;elkan&quot;).fit(X) . 83.7 ms Â± 2.07 ms per loop (mean Â± std. dev. of 7 runs, 50 loops each) . %timeit -n 50 KMeans(algorithm=&quot;full&quot;).fit(X) . 106 ms Â± 2.34 ms per loop (mean Â± std. dev. of 7 runs, 50 loops each) . Mini-Batch K-Means . Scikit-Learn also implements a variant of the K-Means algorithm that supports mini-batches (see this paper): . from sklearn.cluster import MiniBatchKMeans . minibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42) minibatch_kmeans.fit(X) . MiniBatchKMeans(batch_size=100, compute_labels=True, init=&#39;k-means++&#39;, init_size=None, max_iter=100, max_no_improvement=10, n_clusters=5, n_init=3, random_state=42, reassignment_ratio=0.01, tol=0.0, verbose=0) . minibatch_kmeans.inertia_ . 211.93186531476775 . If the dataset does not fit in memory, the simplest option is to use the memmap class, just like we did for incremental PCA in the previous chapter. First let&#39;s load MNIST: . import urllib from sklearn.datasets import fetch_openml mnist = fetch_openml(&#39;mnist_784&#39;, version=1) mnist.target = mnist.target.astype(np.int64) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( mnist[&quot;data&quot;], mnist[&quot;target&quot;], random_state=42) . Next, let&#39;s write it to a memmap: . filename = &quot;my_mnist.data&quot; X_mm = np.memmap(filename, dtype=&#39;float32&#39;, mode=&#39;write&#39;, shape=X_train.shape) X_mm[:] = X_train . minibatch_kmeans = MiniBatchKMeans(n_clusters=10, batch_size=10, random_state=42) minibatch_kmeans.fit(X_mm) . MiniBatchKMeans(batch_size=10, compute_labels=True, init=&#39;k-means++&#39;, init_size=None, max_iter=100, max_no_improvement=10, n_clusters=10, n_init=3, random_state=42, reassignment_ratio=0.01, tol=0.0, verbose=0) . If your data is so large that you cannot use memmap, things get more complicated. Let&#39;s start by writing a function to load the next batch (in real life, you would load the data from disk): . def load_next_batch(batch_size): return X[np.random.choice(len(X), batch_size, replace=False)] . Now we can train the model by feeding it one batch at a time. We also need to implement multiple initializations and keep the model with the lowest inertia: . np.random.seed(42) . k = 5 n_init = 10 n_iterations = 100 batch_size = 100 init_size = 500 # more data for K-Means++ initialization evaluate_on_last_n_iters = 10 best_kmeans = None for init in range(n_init): minibatch_kmeans = MiniBatchKMeans(n_clusters=k, init_size=init_size) X_init = load_next_batch(init_size) minibatch_kmeans.partial_fit(X_init) minibatch_kmeans.sum_inertia_ = 0 for iteration in range(n_iterations): X_batch = load_next_batch(batch_size) minibatch_kmeans.partial_fit(X_batch) if iteration &gt;= n_iterations - evaluate_on_last_n_iters: minibatch_kmeans.sum_inertia_ += minibatch_kmeans.inertia_ if (best_kmeans is None or minibatch_kmeans.sum_inertia_ &lt; best_kmeans.sum_inertia_): best_kmeans = minibatch_kmeans . best_kmeans.score(X) . -211.70999744411483 . Mini-batch K-Means is much faster than regular K-Means: . %timeit KMeans(n_clusters=5).fit(X) . 47 ms Â± 1.44 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each) . %timeit MiniBatchKMeans(n_clusters=5).fit(X) . 26.2 ms Â± 2.41 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each) . That&#39;s much faster! However, its performance is often lower (higher inertia), and it keeps degrading as k increases. Let&#39;s plot the inertia ratio and the training time ratio between Mini-batch K-Means and regular K-Means: . from timeit import timeit . times = np.empty((100, 2)) inertias = np.empty((100, 2)) for k in range(1, 101): kmeans_ = KMeans(n_clusters=k, random_state=42) minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42) print(&quot; r{}/{}&quot;.format(k, 100), end=&quot;&quot;) times[k-1, 0] = timeit(&quot;kmeans_.fit(X)&quot;, number=10, globals=globals()) times[k-1, 1] = timeit(&quot;minibatch_kmeans.fit(X)&quot;, number=10, globals=globals()) inertias[k-1, 0] = kmeans_.inertia_ inertias[k-1, 1] = minibatch_kmeans.inertia_ . 100/100 . plt.figure(figsize=(10,4)) plt.subplot(121) plt.plot(range(1, 101), inertias[:, 0], &quot;r--&quot;, label=&quot;K-Means&quot;) plt.plot(range(1, 101), inertias[:, 1], &quot;b.-&quot;, label=&quot;Mini-batch K-Means&quot;) plt.xlabel(&quot;$k$&quot;, fontsize=16) plt.title(&quot;Inertia&quot;, fontsize=14) plt.legend(fontsize=14) plt.axis([1, 100, 0, 100]) plt.subplot(122) plt.plot(range(1, 101), times[:, 0], &quot;r--&quot;, label=&quot;K-Means&quot;) plt.plot(range(1, 101), times[:, 1], &quot;b.-&quot;, label=&quot;Mini-batch K-Means&quot;) plt.xlabel(&quot;$k$&quot;, fontsize=16) plt.title(&quot;Training time (seconds)&quot;, fontsize=14) plt.axis([1, 100, 0, 6]) save_fig(&quot;minibatch_kmeans_vs_kmeans&quot;) plt.show() . Saving figure minibatch_kmeans_vs_kmeans . Finding the optimal number of clusters . What if the number of clusters was set to a lower or greater value than 5? . kmeans_k3 = KMeans(n_clusters=3, random_state=42) kmeans_k8 = KMeans(n_clusters=8, random_state=42) plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, &quot;$k=3$&quot;, &quot;$k=8$&quot;) save_fig(&quot;bad_n_clusters_plot&quot;) plt.show() . Saving figure bad_n_clusters_plot . Ouch, these two models don&#39;t look great. What about their inertias? . kmeans_k3.inertia_ . 653.2167190021553 . kmeans_k8.inertia_ . 119.11983416102879 . No, we cannot simply take the value of $k$ that minimizes the inertia, since it keeps getting lower as we increase $k$. Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be. However, we can plot the inertia as a function of $k$ and analyze the resulting curve: . kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X) for k in range(1, 10)] inertias = [model.inertia_ for model in kmeans_per_k] . plt.figure(figsize=(8, 3.5)) plt.plot(range(1, 10), inertias, &quot;bo-&quot;) plt.xlabel(&quot;$k$&quot;, fontsize=14) plt.ylabel(&quot;Inertia&quot;, fontsize=14) plt.annotate(&#39;Elbow&#39;, xy=(4, inertias[3]), xytext=(0.55, 0.55), textcoords=&#39;figure fraction&#39;, fontsize=16, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.1) ) plt.axis([1, 8.5, 0, 1300]) save_fig(&quot;inertia_vs_k_plot&quot;) plt.show() . Saving figure inertia_vs_k_diagram . As you can see, there is an elbow at $k=4$, which means that less clusters than that would be bad, and more clusters would not help much and might cut clusters in half. So $k=4$ is a pretty good choice. Of course in this example it is not perfect since it means that the two blobs in the lower left will be considered as just a single cluster, but it&#39;s a pretty good clustering nonetheless. . plot_decision_boundaries(kmeans_per_k[4-1], X) plt.show() . Another approach is to look at the silhouette score, which is the mean silhouette coefficient over all the instances. An instance&#39;s silhouette coefficient is equal to $(b - a)/ max(a, b)$ where $a$ is the mean distance to the other instances in the same cluster (it is the mean intra-cluster distance), and $b$ is the mean nearest-cluster distance, that is the mean distance to the instances of the next closest cluster (defined as the one that minimizes $b$, excluding the instance&#39;s own cluster). The silhouette coefficient can vary between -1 and +1: a coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster. . Let&#39;s plot the silhouette score as a function of $k$: . from sklearn.metrics import silhouette_score . silhouette_score(X, kmeans.labels_) . 0.655517642572828 . silhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[1:]] . plt.figure(figsize=(8, 3)) plt.plot(range(2, 10), silhouette_scores, &quot;bo-&quot;) plt.xlabel(&quot;$k$&quot;, fontsize=14) plt.ylabel(&quot;Silhouette score&quot;, fontsize=14) plt.axis([1.8, 8.5, 0.55, 0.7]) save_fig(&quot;silhouette_score_vs_k_plot&quot;) plt.show() . Saving figure silhouette_score_vs_k_plot . As you can see, this visualization is much richer than the previous one: in particular, although it confirms that $k=4$ is a very good choice, but it also underlines the fact that $k=5$ is quite good as well. . An even more informative visualization is given when you plot every instance&#39;s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a silhouette diagram: . #collapse-show from sklearn.metrics import silhouette_samples from matplotlib.ticker import FixedLocator, FixedFormatter plt.figure(figsize=(11, 9)) for k in (3, 4, 5, 6): plt.subplot(2, 2, k - 2) y_pred = kmeans_per_k[k - 1].labels_ silhouette_coefficients = silhouette_samples(X, y_pred) padding = len(X) // 30 pos = padding ticks = [] for i in range(k): coeffs = silhouette_coefficients[y_pred == i] coeffs.sort() color = mpl.cm.Spectral(i / k) plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs, facecolor=color, edgecolor=color, alpha=0.7) ticks.append(pos + len(coeffs) // 2) pos += len(coeffs) + padding plt.gca().yaxis.set_major_locator(FixedLocator(ticks)) plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k))) if k in (3, 5): plt.ylabel(&quot;Cluster&quot;) if k in (5, 6): plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]) plt.xlabel(&quot;Silhouette Coefficient&quot;) else: plt.tick_params(labelbottom=False) plt.axvline(x=silhouette_scores[k - 2], color=&quot;red&quot;, linestyle=&quot;--&quot;) plt.title(&quot;$k={}$&quot;.format(k), fontsize=16) save_fig(&quot;silhouette_analysis_plot&quot;) plt.show() . . Saving figure silhouette_analysis_diagram . Limits of K-Means . X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42) X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]])) X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42) X2 = X2 + [6, -8] X = np.r_[X1, X2] y = np.r_[y1, y2] . plot_clusters(X) . kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=42) kmeans_bad = KMeans(n_clusters=3, random_state=42) kmeans_good.fit(X) kmeans_bad.fit(X) . KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=3, n_init=10, n_jobs=None, precompute_distances=&#39;auto&#39;, random_state=42, tol=0.0001, verbose=0) . plt.figure(figsize=(10, 3.2)) plt.subplot(121) plot_decision_boundaries(kmeans_good, X) plt.title(&quot;Inertia = {:.1f}&quot;.format(kmeans_good.inertia_), fontsize=14) plt.subplot(122) plot_decision_boundaries(kmeans_bad, X, show_ylabels=False) plt.title(&quot;Inertia = {:.1f}&quot;.format(kmeans_bad.inertia_), fontsize=14) save_fig(&quot;bad_kmeans_plot&quot;) plt.show() . Saving figure bad_kmeans_diagram . Using clustering for image segmentation . # Download the ladybug image images_path = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, &quot;unsupervised_learning&quot;) os.makedirs(images_path, exist_ok=True) DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; filename = &quot;ladybug.png&quot; print(&quot;Downloading&quot;, filename) url = DOWNLOAD_ROOT + &quot;images/unsupervised_learning/&quot; + filename urllib.request.urlretrieve(url, os.path.join(images_path, filename)) . from matplotlib.image import imread image = imread(os.path.join(images_path, filename)) image.shape . (533, 800, 3) . X = image.reshape(-1, 3) kmeans = KMeans(n_clusters=8, random_state=42).fit(X) segmented_img = kmeans.cluster_centers_[kmeans.labels_] segmented_img = segmented_img.reshape(image.shape) . segmented_imgs = [] n_colors = (10, 8, 6, 4, 2) for n_clusters in n_colors: kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X) segmented_img = kmeans.cluster_centers_[kmeans.labels_] segmented_imgs.append(segmented_img.reshape(image.shape)) . #collapse-show plt.figure(figsize=(10,5)) plt.subplots_adjust(wspace=0.05, hspace=0.1) plt.subplot(231) plt.imshow(image) plt.title(&quot;Original image&quot;) plt.axis(&#39;off&#39;) for idx, n_clusters in enumerate(n_colors): plt.subplot(232 + idx) plt.imshow(segmented_imgs[idx]) plt.title(&quot;{} colors&quot;.format(n_clusters)) plt.axis(&#39;off&#39;) save_fig(&#39;image_segmentation_diagram&#39;, tight_layout=False) plt.show() . . Saving figure image_segmentation_diagram . Using Clustering for Preprocessing . Let&#39;s tackle the digits dataset which is a simple MNIST-like dataset containing 1,797 grayscale 8Ã—8 images representing digits 0 to 9. . from sklearn.datasets import load_digits . X_digits, y_digits = load_digits(return_X_y=True) . Let&#39;s split it into a training set and a test set: . from sklearn.model_selection import train_test_split . X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42) . Now let&#39;s fit a Logistic Regression model and evaluate it on the test set: . from sklearn.linear_model import LogisticRegression . log_reg = LogisticRegression(multi_class=&quot;ovr&quot;, solver=&quot;lbfgs&quot;, max_iter=5000, random_state=42) log_reg.fit(X_train, y_train) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=5000, multi_class=&#39;ovr&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . log_reg.score(X_test, y_test) . 0.9688888888888889 . Okay, that&#39;s our baseline: 96.89% accuracy. Let&#39;s see if we can do better by using K-Means as a preprocessing step. We will create a pipeline that will first cluster the training set into 50 clusters and replace the images with their distances to the 50 clusters, then apply a logistic regression model: . from sklearn.pipeline import Pipeline . pipeline = Pipeline([ (&quot;kmeans&quot;, KMeans(n_clusters=50, random_state=42)), (&quot;log_reg&quot;, LogisticRegression(multi_class=&quot;ovr&quot;, solver=&quot;lbfgs&quot;, max_iter=5000, random_state=42)), ]) pipeline.fit(X_train, y_train) . Pipeline(memory=None, steps=[(&#39;kmeans&#39;, KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=50, n_init=10, n_jobs=None, precompute_distances=&#39;auto&#39;, random_state=42, tol=0.0001, verbose=0)), (&#39;log_reg&#39;, LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=5000, multi_class=&#39;ovr&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False))]) . pipeline.score(X_test, y_test) . 0.9777777777777777 . 1 - (1 - 0.977777) / (1 - 0.968888) . 0.28570969400874346 . How about that? We reduced the error rate by over 28%! But we chose the number of clusters $k$ completely arbitrarily, we can surely do better. Since K-Means is just a preprocessing step in a classification pipeline, finding a good value for $k$ is much simpler than earlier: there&#39;s no need to perform silhouette analysis or minimize the inertia, the best value of $k$ is simply the one that results in the best classification performance. . from sklearn.model_selection import GridSearchCV . param_grid = dict(kmeans__n_clusters=range(2, 100)) grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2) grid_clf.fit(X_train, y_train) . Fitting 3 folds for each of 98 candidates, totalling 294 fits [CV] kmeans__n_clusters=2 ............................................ [CV] ............................. kmeans__n_clusters=2, total= 0.1s [CV] kmeans__n_clusters=2 ............................................ . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 0.1s remaining: 0.0s . [CV] ............................. kmeans__n_clusters=2, total= 0.1s [CV] kmeans__n_clusters=2 ............................................ [CV] ............................. kmeans__n_clusters=2, total= 0.2s [CV] kmeans__n_clusters=3 ............................................ [CV] ............................. kmeans__n_clusters=3, total= 0.2s [CV] kmeans__n_clusters=3 ............................................ [CV] ............................. kmeans__n_clusters=3, total= 0.2s [CV] kmeans__n_clusters=3 ............................................ [CV] ............................. kmeans__n_clusters=3, total= 0.2s [CV] kmeans__n_clusters=4 ............................................ [CV] ............................. kmeans__n_clusters=4, total= 0.2s [CV] kmeans__n_clusters=4 ............................................ [CV] ............................. kmeans__n_clusters=4, total= 0.2s [CV] kmeans__n_clusters=4 ............................................ [CV] ............................. kmeans__n_clusters=4, total= 0.2s [CV] kmeans__n_clusters=5 ............................................ [CV] ............................. kmeans__n_clusters=5, total= 0.2s [CV] kmeans__n_clusters=5 ............................................ [CV] ............................. kmeans__n_clusters=5, total= 0.2s [CV] kmeans__n_clusters=5 ............................................ [CV] ............................. kmeans__n_clusters=5, total= 0.3s [CV] kmeans__n_clusters=6 ............................................ [CV] ............................. kmeans__n_clusters=6, total= 0.3s [CV] kmeans__n_clusters=6 ............................................ [CV] ............................. kmeans__n_clusters=6, total= 0.3s [CV] kmeans__n_clusters=6 ............................................ [CV] ............................. kmeans__n_clusters=6, total= 0.3s [CV] kmeans__n_clusters=7 ............................................ [CV] ............................. kmeans__n_clusters=7, total= 0.3s &lt;&lt;522 more lines&gt;&gt; [CV] kmeans__n_clusters=94 ........................................... [CV] ............................ kmeans__n_clusters=94, total= 3.4s [CV] kmeans__n_clusters=94 ........................................... [CV] ............................ kmeans__n_clusters=94, total= 3.0s [CV] kmeans__n_clusters=95 ........................................... [CV] ............................ kmeans__n_clusters=95, total= 3.4s [CV] kmeans__n_clusters=95 ........................................... [CV] ............................ kmeans__n_clusters=95, total= 3.7s [CV] kmeans__n_clusters=95 ........................................... [CV] ............................ kmeans__n_clusters=95, total= 3.4s [CV] kmeans__n_clusters=96 ........................................... [CV] ............................ kmeans__n_clusters=96, total= 3.5s [CV] kmeans__n_clusters=96 ........................................... [CV] ............................ kmeans__n_clusters=96, total= 3.3s [CV] kmeans__n_clusters=96 ........................................... [CV] ............................ kmeans__n_clusters=96, total= 3.7s [CV] kmeans__n_clusters=97 ........................................... [CV] ............................ kmeans__n_clusters=97, total= 3.7s [CV] kmeans__n_clusters=97 ........................................... [CV] ............................ kmeans__n_clusters=97, total= 3.9s [CV] kmeans__n_clusters=97 ........................................... [CV] ............................ kmeans__n_clusters=97, total= 3.9s [CV] kmeans__n_clusters=98 ........................................... [CV] ............................ kmeans__n_clusters=98, total= 3.3s [CV] kmeans__n_clusters=98 ........................................... [CV] ............................ kmeans__n_clusters=98, total= 3.5s [CV] kmeans__n_clusters=98 ........................................... [CV] ............................ kmeans__n_clusters=98, total= 3.3s [CV] kmeans__n_clusters=99 ........................................... [CV] ............................ kmeans__n_clusters=99, total= 3.4s [CV] kmeans__n_clusters=99 ........................................... [CV] ............................ kmeans__n_clusters=99, total= 3.9s [CV] kmeans__n_clusters=99 ........................................... [CV] ............................ kmeans__n_clusters=99, total= 3.5s . [Parallel(n_jobs=1)]: Done 294 out of 294 | elapsed: 13.9min finished . GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=Pipeline(memory=None, steps=[(&#39;kmeans&#39;, KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=50, n_init=10, n_jobs=None, precompute_distances=&#39;auto&#39;, random_state=42, tol=0.0001, verbose=0)), (&#39;log_reg&#39;, LogisticRegression(C=1.0, class_weight=None, ...penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False))]), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid={&#39;kmeans__n_clusters&#39;: range(2, 100)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=2) . Let&#39;s see what the best number of clusters is: . grid_clf.best_params_ . {&#39;kmeans__n_clusters&#39;: 99} . grid_clf.score(X_test, y_test) . 0.9822222222222222 . Clustering for Semi-supervised Learning . Another use case for clustering is in semi-supervised learning, when we have plenty of unlabeled instances and very few labeled instances. . Let&#39;s look at the performance of a logistic regression model when we only have 50 labeled instances: . n_labeled = 50 . log_reg = LogisticRegression(multi_class=&quot;ovr&quot;, solver=&quot;lbfgs&quot;, random_state=42) log_reg.fit(X_train[:n_labeled], y_train[:n_labeled]) log_reg.score(X_test, y_test) . 0.8333333333333334 . It&#39;s much less than earlier of course. Let&#39;s see how we can do better. First, let&#39;s cluster the training set into 50 clusters, then for each cluster let&#39;s find the image closest to the centroid. We will call these images the representative images: . k = 50 . kmeans = KMeans(n_clusters=k, random_state=42) X_digits_dist = kmeans.fit_transform(X_train) representative_digit_idx = np.argmin(X_digits_dist, axis=0) X_representative_digits = X_train[representative_digit_idx] . Now let&#39;s plot these representative images and label them manually: . plt.figure(figsize=(8, 2)) for index, X_representative_digit in enumerate(X_representative_digits): plt.subplot(k // 10, 10, index + 1) plt.imshow(X_representative_digit.reshape(8, 8), cmap=&quot;binary&quot;, interpolation=&quot;bilinear&quot;) plt.axis(&#39;off&#39;) save_fig(&quot;representative_images_diagram&quot;, tight_layout=False) plt.show() . Saving figure representative_images_diagram . y_representative_digits = np.array([ 4, 8, 0, 6, 8, 3, 7, 7, 9, 2, 5, 5, 8, 5, 2, 1, 2, 9, 6, 1, 1, 6, 9, 0, 8, 3, 0, 7, 4, 1, 6, 5, 2, 4, 1, 8, 6, 3, 9, 2, 4, 2, 9, 4, 7, 6, 2, 3, 1, 1]) . Now we have a dataset with just 50 labeled instances, but instead of being completely random instances, each of them is a representative image of its cluster. Let&#39;s see if the performance is any better: . log_reg = LogisticRegression(multi_class=&quot;ovr&quot;, solver=&quot;lbfgs&quot;, max_iter=5000, random_state=42) log_reg.fit(X_representative_digits, y_representative_digits) log_reg.score(X_test, y_test) . 0.9222222222222223 . Wow! We jumped from 83.3% accuracy to 92.2%, although we are still only training the model on 50 instances. Since it&#39;s often costly and painful to label instances, especially when it has to be done manually by experts, it&#39;s a good idea to make them label representative instances rather than just random instances. . But perhaps we can go one step further: what if we propagated the labels to all the other instances in the same cluster? . y_train_propagated = np.empty(len(X_train), dtype=np.int32) for i in range(k): y_train_propagated[kmeans.labels_==i] = y_representative_digits[i] . log_reg = LogisticRegression(multi_class=&quot;ovr&quot;, solver=&quot;lbfgs&quot;, max_iter=5000, random_state=42) log_reg.fit(X_train, y_train_propagated) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=5000, multi_class=&#39;ovr&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . log_reg.score(X_test, y_test) . 0.9333333333333333 . We got a tiny little accuracy boost. Better than nothing, but we should probably have propagated the labels only to the instances closest to the centroid, because by propagating to the full cluster, we have certainly included some outliers. Let&#39;s only propagate the labels to the 20th percentile closest to the centroid: . percentile_closest = 20 X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_] for i in range(k): in_cluster = (kmeans.labels_ == i) cluster_dist = X_cluster_dist[in_cluster] cutoff_distance = np.percentile(cluster_dist, percentile_closest) above_cutoff = (X_cluster_dist &gt; cutoff_distance) X_cluster_dist[in_cluster &amp; above_cutoff] = -1 . partially_propagated = (X_cluster_dist != -1) X_train_partially_propagated = X_train[partially_propagated] y_train_partially_propagated = y_train_propagated[partially_propagated] . log_reg = LogisticRegression(multi_class=&quot;ovr&quot;, solver=&quot;lbfgs&quot;, max_iter=5000, random_state=42) log_reg.fit(X_train_partially_propagated, y_train_partially_propagated) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=5000, multi_class=&#39;ovr&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . log_reg.score(X_test, y_test) . 0.94 . Nice! With just 50 labeled instances (just 5 examples per class on average!), we got 94% performance, which is pretty close to the performance of logistic regression on the fully labeled digits dataset (which was 96.9%). . This is because the propagated labels are actually pretty good: their accuracy is very close to 99%: . np.mean(y_train_partially_propagated == y_train[partially_propagated]) . 0.9896907216494846 . You could now do a few iterations of active learning: . Manually label the instances that the classifier is least sure about, if possible by picking them in distinct clusters. | Train a new model with these additional labels. | DBSCAN . from sklearn.datasets import make_moons . X, y = make_moons(n_samples=1000, noise=0.05, random_state=42) . from sklearn.cluster import DBSCAN . dbscan = DBSCAN(eps=0.05, min_samples=5) dbscan.fit(X) . DBSCAN(algorithm=&#39;auto&#39;, eps=0.05, leaf_size=30, metric=&#39;euclidean&#39;, metric_params=None, min_samples=5, n_jobs=None, p=None) . dbscan.labels_[:10] . array([ 0, 2, -1, -1, 1, 0, 0, 0, 2, 5]) . len(dbscan.core_sample_indices_) . 808 . dbscan.core_sample_indices_[:10] . array([ 0, 4, 5, 6, 7, 8, 10, 11, 12, 13]) . dbscan.components_[:3] . array([[-0.02137124, 0.40618608], [-0.84192557, 0.53058695], [ 0.58930337, -0.32137599]]) . np.unique(dbscan.labels_) . array([-1, 0, 1, 2, 3, 4, 5, 6]) . dbscan2 = DBSCAN(eps=0.2) dbscan2.fit(X) . DBSCAN(algorithm=&#39;auto&#39;, eps=0.2, leaf_size=30, metric=&#39;euclidean&#39;, metric_params=None, min_samples=5, n_jobs=None, p=None) . def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True): core_mask = np.zeros_like(dbscan.labels_, dtype=bool) core_mask[dbscan.core_sample_indices_] = True anomalies_mask = dbscan.labels_ == -1 non_core_mask = ~(core_mask | anomalies_mask) cores = dbscan.components_ anomalies = X[anomalies_mask] non_cores = X[non_core_mask] plt.scatter(cores[:, 0], cores[:, 1], c=dbscan.labels_[core_mask], marker=&#39;o&#39;, s=size, cmap=&quot;Paired&quot;) plt.scatter(cores[:, 0], cores[:, 1], marker=&#39;*&#39;, s=20, c=dbscan.labels_[core_mask]) plt.scatter(anomalies[:, 0], anomalies[:, 1], c=&quot;r&quot;, marker=&quot;x&quot;, s=100) plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=&quot;.&quot;) if show_xlabels: plt.xlabel(&quot;$x_1$&quot;, fontsize=14) else: plt.tick_params(labelbottom=False) if show_ylabels: plt.ylabel(&quot;$x_2$&quot;, fontsize=14, rotation=0) else: plt.tick_params(labelleft=False) plt.title(&quot;eps={:.2f}, min_samples={}&quot;.format(dbscan.eps, dbscan.min_samples), fontsize=14) . plt.figure(figsize=(9, 3.2)) plt.subplot(121) plot_dbscan(dbscan, X, size=100) plt.subplot(122) plot_dbscan(dbscan2, X, size=600, show_ylabels=False) save_fig(&quot;dbscan_plot&quot;) plt.show() . Saving figure dbscan_diagram . dbscan = dbscan2 . from sklearn.neighbors import KNeighborsClassifier . knn = KNeighborsClassifier(n_neighbors=50) knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_]) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=50, p=2, weights=&#39;uniform&#39;) . X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]]) knn.predict(X_new) . array([1, 0, 1, 0]) . knn.predict_proba(X_new) . array([[0.18, 0.82], [1. , 0. ], [0.12, 0.88], [1. , 0. ]]) . plt.figure(figsize=(6, 3)) plot_decision_boundaries(knn, X, show_centroids=False) plt.scatter(X_new[:, 0], X_new[:, 1], c=&quot;b&quot;, marker=&quot;+&quot;, s=200, zorder=10) save_fig(&quot;cluster_classification_plot&quot;) plt.show() . Saving figure cluster_classification_diagram . y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1) y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx] y_pred[y_dist &gt; 0.2] = -1 y_pred.ravel() . array([-1, 0, 1, -1]) . Other Clustering Algorithms . Spectral Clustering . from sklearn.cluster import SpectralClustering . sc1 = SpectralClustering(n_clusters=2, gamma=100, random_state=42) sc1.fit(X) . SpectralClustering(affinity=&#39;rbf&#39;, assign_labels=&#39;kmeans&#39;, coef0=1, degree=3, eigen_solver=None, eigen_tol=0.0, gamma=100, kernel_params=None, n_clusters=2, n_init=10, n_jobs=None, n_neighbors=10, random_state=42) . sc2 = SpectralClustering(n_clusters=2, gamma=1, random_state=42) sc2.fit(X) . SpectralClustering(affinity=&#39;rbf&#39;, assign_labels=&#39;kmeans&#39;, coef0=1, degree=3, eigen_solver=None, eigen_tol=0.0, gamma=1, kernel_params=None, n_clusters=2, n_init=10, n_jobs=None, n_neighbors=10, random_state=42) . np.percentile(sc1.affinity_matrix_, 95) . 0.04251990648936265 . def plot_spectral_clustering(sc, X, size, alpha, show_xlabels=True, show_ylabels=True): plt.scatter(X[:, 0], X[:, 1], marker=&#39;o&#39;, s=size, c=&#39;gray&#39;, cmap=&quot;Paired&quot;, alpha=alpha) plt.scatter(X[:, 0], X[:, 1], marker=&#39;o&#39;, s=30, c=&#39;w&#39;) plt.scatter(X[:, 0], X[:, 1], marker=&#39;.&#39;, s=10, c=sc.labels_, cmap=&quot;Paired&quot;) if show_xlabels: plt.xlabel(&quot;$x_1$&quot;, fontsize=14) else: plt.tick_params(labelbottom=False) if show_ylabels: plt.ylabel(&quot;$x_2$&quot;, fontsize=14, rotation=0) else: plt.tick_params(labelleft=False) plt.title(&quot;RBF gamma={}&quot;.format(sc.gamma), fontsize=14) . plt.figure(figsize=(9, 3.2)) plt.subplot(121) plot_spectral_clustering(sc1, X, size=500, alpha=0.1) plt.subplot(122) plot_spectral_clustering(sc2, X, size=4000, alpha=0.01, show_ylabels=False) plt.show() . Agglomerative Clustering . from sklearn.cluster import AgglomerativeClustering . X = np.array([0, 2, 5, 8.5]).reshape(-1, 1) agg = AgglomerativeClustering(linkage=&quot;complete&quot;).fit(X) . def learned_parameters(estimator): return [attrib for attrib in dir(estimator) if attrib.endswith(&quot;_&quot;) and not attrib.startswith(&quot;_&quot;)] . learned_parameters(agg) . [&#39;children_&#39;, &#39;labels_&#39;, &#39;n_components_&#39;, &#39;n_leaves_&#39;] . agg.children_ . array([[0, 1], [2, 3], [4, 5]]) . Gaussian Mixtures . X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42) X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]])) X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42) X2 = X2 + [6, -8] X = np.r_[X1, X2] y = np.r_[y1, y2] . Let&#39;s train a Gaussian mixture model on the previous dataset: . from sklearn.mixture import GaussianMixture . gm = GaussianMixture(n_components=3, n_init=10, random_state=42) gm.fit(X) . GaussianMixture(covariance_type=&#39;full&#39;, init_params=&#39;kmeans&#39;, max_iter=100, means_init=None, n_components=3, n_init=10, precisions_init=None, random_state=42, reg_covar=1e-06, tol=0.001, verbose=0, verbose_interval=10, warm_start=False, weights_init=None) . Let&#39;s look at the parameters that the EM algorithm estimated: . gm.weights_ . array([0.20965228, 0.4000662 , 0.39028152]) . gm.means_ . array([[ 3.39909717, 1.05933727], [-1.40763984, 1.42710194], [ 0.05135313, 0.07524095]]) . gm.covariances_ . array([[[ 1.14807234, -0.03270354], [-0.03270354, 0.95496237]], [[ 0.63478101, 0.72969804], [ 0.72969804, 1.1609872 ]], [[ 0.68809572, 0.79608475], [ 0.79608475, 1.21234145]]]) . Did the algorithm actually converge? . gm.converged_ . True . Yes, good. How many iterations did it take? . gm.n_iter_ . 4 . You can now use the model to predict which cluster each instance belongs to (hard clustering) or the probabilities that it came from each cluster. For this, just use predict() method or the predict_proba() method: . gm.predict(X) . array([2, 2, 1, ..., 0, 0, 0]) . gm.predict_proba(X) . array([[2.32389467e-02, 6.77397850e-07, 9.76760376e-01], [1.64685609e-02, 6.75361303e-04, 9.82856078e-01], [2.01535333e-06, 9.99923053e-01, 7.49319577e-05], ..., [9.99999571e-01, 2.13946075e-26, 4.28788333e-07], [1.00000000e+00, 1.46454409e-41, 5.12459171e-16], [1.00000000e+00, 8.02006365e-41, 2.27626238e-15]]) . This is a generative model, so you can sample new instances from it (and get their labels): . X_new, y_new = gm.sample(6) X_new . array([[ 2.95400315, 2.63680992], [-1.16654575, 1.62792705], [-1.39477712, -1.48511338], [ 0.27221525, 0.690366 ], [ 0.54095936, 0.48591934], [ 0.38064009, -0.56240465]]) . y_new . array([0, 1, 2, 2, 2, 2]) . Notice that they are sampled sequentially from each cluster. . You can also estimate the log of the probability density function (PDF) at any location using the score_samples() method: . gm.score_samples(X) . array([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783, -4.39802535, -3.80743859]) . Let&#39;s check that the PDF integrates to 1 over the whole space. We just take a large square around the clusters, and chop it into a grid of tiny squares, then we compute the approximate probability that the instances will be generated in each tiny square (by multiplying the PDF at one corner of the tiny square by the area of the square), and finally summing all these probabilities). The result is very close to 1: . resolution = 100 grid = np.arange(-10, 10, 1 / resolution) xx, yy = np.meshgrid(grid, grid) X_full = np.vstack([xx.ravel(), yy.ravel()]).T pdf = np.exp(gm.score_samples(X_full)) pdf_probas = pdf * (1 / resolution) ** 2 pdf_probas.sum() . 0.9999999999217851 . Now let&#39;s plot the resulting decision boundaries (dashed lines) and density contours: . #collapse-show from matplotlib.colors import LogNorm def plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True): mins = X.min(axis=0) - 0.1 maxs = X.max(axis=0) + 0.1 xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution), np.linspace(mins[1], maxs[1], resolution)) Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, norm=LogNorm(vmin=1.0, vmax=30.0), levels=np.logspace(0, 2, 12)) plt.contour(xx, yy, Z, norm=LogNorm(vmin=1.0, vmax=30.0), levels=np.logspace(0, 2, 12), linewidths=1, colors=&#39;k&#39;) Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contour(xx, yy, Z, linewidths=2, colors=&#39;r&#39;, linestyles=&#39;dashed&#39;) plt.plot(X[:, 0], X[:, 1], &#39;k.&#39;, markersize=2) plot_centroids(clusterer.means_, clusterer.weights_) plt.xlabel(&quot;$x_1$&quot;, fontsize=14) if show_ylabels: plt.ylabel(&quot;$x_2$&quot;, fontsize=14, rotation=0) else: plt.tick_params(labelleft=False) . . plt.figure(figsize=(8, 4)) plot_gaussian_mixture(gm, X) save_fig(&quot;gaussian_mixtures_plot&quot;) plt.show() . Saving figure gaussian_mixtures_diagram . You can impose constraints on the covariance matrices that the algorithm looks for by setting the covariance_type hyperparameter: . &quot;full&quot; (default): no constraint, all clusters can take on any ellipsoidal shape of any size. | &quot;tied&quot;: all clusters must have the same shape, which can be any ellipsoid (i.e., they all share the same covariance matrix). | &quot;spherical&quot;: all clusters must be spherical, but they can have different diameters (i.e., different variances). | &quot;diag&quot;: clusters can take on any ellipsoidal shape of any size, but the ellipsoid&#39;s axes must be parallel to the axes (i.e., the covariance matrices must be diagonal). | . gm_full = GaussianMixture(n_components=3, n_init=10, covariance_type=&quot;full&quot;, random_state=42) gm_tied = GaussianMixture(n_components=3, n_init=10, covariance_type=&quot;tied&quot;, random_state=42) gm_spherical = GaussianMixture(n_components=3, n_init=10, covariance_type=&quot;spherical&quot;, random_state=42) gm_diag = GaussianMixture(n_components=3, n_init=10, covariance_type=&quot;diag&quot;, random_state=42) gm_full.fit(X) gm_tied.fit(X) gm_spherical.fit(X) gm_diag.fit(X) . GaussianMixture(covariance_type=&#39;diag&#39;, init_params=&#39;kmeans&#39;, max_iter=100, means_init=None, n_components=3, n_init=10, precisions_init=None, random_state=42, reg_covar=1e-06, tol=0.001, verbose=0, verbose_interval=10, warm_start=False, weights_init=None) . def compare_gaussian_mixtures(gm1, gm2, X): plt.figure(figsize=(9, 4)) plt.subplot(121) plot_gaussian_mixture(gm1, X) plt.title(&#39;covariance_type=&quot;{}&quot;&#39;.format(gm1.covariance_type), fontsize=14) plt.subplot(122) plot_gaussian_mixture(gm2, X, show_ylabels=False) plt.title(&#39;covariance_type=&quot;{}&quot;&#39;.format(gm2.covariance_type), fontsize=14) . compare_gaussian_mixtures(gm_tied, gm_spherical, X) save_fig(&quot;covariance_type_plot&quot;) plt.show() . Saving figure covariance_type_diagram . compare_gaussian_mixtures(gm_full, gm_diag, X) plt.tight_layout() plt.show() . Anomaly Detection using Gaussian Mixtures . Gaussian Mixtures can be used for anomaly detection: instances located in low-density regions can be considered anomalies. You must define what density threshold you want to use. For example, in a manufacturing company that tries to detect defective products, the ratio of defective products is usually well-known. Say it is equal to 4%, then you can set the density threshold to be the value that results in having 4% of the instances located in areas below that threshold density: . densities = gm.score_samples(X) density_threshold = np.percentile(densities, 4) anomalies = X[densities &lt; density_threshold] . plt.figure(figsize=(8, 4)) plot_gaussian_mixture(gm, X) plt.scatter(anomalies[:, 0], anomalies[:, 1], color=&#39;r&#39;, marker=&#39;*&#39;) plt.ylim(top=5.1) save_fig(&quot;mixture_anomaly_detection_plot&quot;) plt.show() . Saving figure mixture_anomaly_detection_diagram . Model selection . We cannot use the inertia or the silhouette score because they both assume that the clusters are spherical. Instead, we can try to find the model that minimizes a theoretical information criterion such as the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC): . ${BIC} = { log(m)p - 2 log({ hat L})}$ . ${AIC} = 2p - 2 log( hat L)$ . $m$ is the number of instances. | $p$ is the number of parameters learned by the model. | $ hat L$ is the maximized value of the likelihood function of the model. This is the conditional probability of the observed data $ mathbf{X}$, given the model and its optimized parameters. | . Both BIC and AIC penalize models that have more parameters to learn (e.g., more clusters), and reward models that fit the data well (i.e., models that give a high likelihood to the observed data). . gm.bic(X) . 8189.74345832983 . gm.aic(X) . 8102.518178214792 . We could compute the BIC manually like this: . n_clusters = 3 n_dims = 2 n_params_for_weights = n_clusters - 1 n_params_for_means = n_clusters * n_dims n_params_for_covariance = n_clusters * n_dims * (n_dims + 1) // 2 n_params = n_params_for_weights + n_params_for_means + n_params_for_covariance max_log_likelihood = gm.score(X) * len(X) # log(L^) bic = np.log(len(X)) * n_params - 2 * max_log_likelihood aic = 2 * n_params - 2 * max_log_likelihood . bic, aic . (8189.74345832983, 8102.518178214792) . n_params . 17 . There&#39;s one weight per cluster, but the sum must be equal to 1, so we have one degree of freedom less, hence the -1. Similarly, the degrees of freedom for an $n times n$ covariance matrix is not $n^2$, but $1 + 2 + dots + n = dfrac{n (n+1)}{2}$. . Let&#39;s train Gaussian Mixture models with various values of $k$ and measure their BIC: . gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X) for k in range(1, 11)] . bics = [model.bic(X) for model in gms_per_k] aics = [model.aic(X) for model in gms_per_k] . plt.figure(figsize=(8, 3)) plt.plot(range(1, 11), bics, &quot;bo-&quot;, label=&quot;BIC&quot;) plt.plot(range(1, 11), aics, &quot;go--&quot;, label=&quot;AIC&quot;) plt.xlabel(&quot;$k$&quot;, fontsize=14) plt.ylabel(&quot;Information Criterion&quot;, fontsize=14) plt.axis([1, 9.5, np.min(aics) - 50, np.max(aics) + 50]) plt.annotate(&#39;Minimum&#39;, xy=(3, bics[2]), xytext=(0.35, 0.6), textcoords=&#39;figure fraction&#39;, fontsize=14, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.1) ) plt.legend() save_fig(&quot;aic_bic_vs_k_plot&quot;) plt.show() . Saving figure aic_bic_vs_k_diagram . Let&#39;s search for best combination of values for both the number of clusters and the covariance_type hyperparameter: . min_bic = np.infty for k in range(1, 11): for covariance_type in (&quot;full&quot;, &quot;tied&quot;, &quot;spherical&quot;, &quot;diag&quot;): bic = GaussianMixture(n_components=k, n_init=10, covariance_type=covariance_type, random_state=42).fit(X).bic(X) if bic &lt; min_bic: min_bic = bic best_k = k best_covariance_type = covariance_type . best_k . 3 . best_covariance_type . &#39;full&#39; . Variational Bayesian Gaussian Mixtures . Rather than manually searching for the optimal number of clusters, it is possible to use instead the BayesianGaussianMixture class which is capable of giving weights equal (or close) to zero to unnecessary clusters. Just set the number of components to a value that you believe is greater than the optimal number of clusters, and the algorithm will eliminate the unnecessary clusters automatically. . from sklearn.mixture import BayesianGaussianMixture . bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42) bgm.fit(X) . BayesianGaussianMixture(covariance_prior=None, covariance_type=&#39;full&#39;, degrees_of_freedom_prior=None, init_params=&#39;kmeans&#39;, max_iter=100, mean_precision_prior=None, mean_prior=None, n_components=10, n_init=10, random_state=42, reg_covar=1e-06, tol=0.001, verbose=0, verbose_interval=10, warm_start=False, weight_concentration_prior=None, weight_concentration_prior_type=&#39;dirichlet_process&#39;) . The algorithm automatically detected that only 3 components are needed: . np.round(bgm.weights_, 2) . array([0.4 , 0.21, 0.4 , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]) . plt.figure(figsize=(8, 5)) plot_gaussian_mixture(bgm, X) plt.show() . bgm_low = BayesianGaussianMixture(n_components=10, max_iter=1000, n_init=1, weight_concentration_prior=0.01, random_state=42) bgm_high = BayesianGaussianMixture(n_components=10, max_iter=1000, n_init=1, weight_concentration_prior=10000, random_state=42) nn = 73 bgm_low.fit(X[:nn]) bgm_high.fit(X[:nn]) . BayesianGaussianMixture(covariance_prior=None, covariance_type=&#39;full&#39;, degrees_of_freedom_prior=None, init_params=&#39;kmeans&#39;, max_iter=1000, mean_precision_prior=None, mean_prior=None, n_components=10, n_init=1, random_state=42, reg_covar=1e-06, tol=0.001, verbose=0, verbose_interval=10, warm_start=False, weight_concentration_prior=10000, weight_concentration_prior_type=&#39;dirichlet_process&#39;) . np.round(bgm_low.weights_, 2) . array([0.52, 0.48, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]) . np.round(bgm_high.weights_, 2) . array([0.01, 0.18, 0.27, 0.11, 0.01, 0.01, 0.01, 0.01, 0.37, 0.01]) . plt.figure(figsize=(9, 4)) plt.subplot(121) plot_gaussian_mixture(bgm_low, X[:nn]) plt.title(&quot;weight_concentration_prior = 0.01&quot;, fontsize=14) plt.subplot(122) plot_gaussian_mixture(bgm_high, X[:nn], show_ylabels=False) plt.title(&quot;weight_concentration_prior = 10000&quot;, fontsize=14) save_fig(&quot;mixture_concentration_prior_plot&quot;) plt.show() . Saving figure mixture_concentration_prior_diagram . Note: the fact that you see only 3 regions in the right plot although there are 4 centroids is not a bug. The weight of the top-right cluster is much larger than the weight of the lower-right cluster, so the probability that any given point in this region belongs to the top right cluster is greater than the probability that it belongs to the lower-right cluster. . X_moons, y_moons = make_moons(n_samples=1000, noise=0.05, random_state=42) . bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42) bgm.fit(X_moons) . BayesianGaussianMixture(covariance_prior=None, covariance_type=&#39;full&#39;, degrees_of_freedom_prior=None, init_params=&#39;kmeans&#39;, max_iter=100, mean_precision_prior=None, mean_prior=None, n_components=10, n_init=10, random_state=42, reg_covar=1e-06, tol=0.001, verbose=0, verbose_interval=10, warm_start=False, weight_concentration_prior=None, weight_concentration_prior_type=&#39;dirichlet_process&#39;) . plt.figure(figsize=(9, 3.2)) plt.subplot(121) plot_data(X_moons) plt.xlabel(&quot;$x_1$&quot;, fontsize=14) plt.ylabel(&quot;$x_2$&quot;, fontsize=14, rotation=0) plt.subplot(122) plot_gaussian_mixture(bgm, X_moons, show_ylabels=False) save_fig(&quot;moons_vs_bgm_plot&quot;) plt.show() . Saving figure moons_vs_bgm_diagram . Oops, not great... instead of detecting 2 moon-shaped clusters, the algorithm detected 8 ellipsoidal clusters. However, the density plot does not look too bad, so it might be usable for anomaly detection. . Likelihood Function . from scipy.stats import norm . xx = np.linspace(-6, 4, 101) ss = np.linspace(1, 2, 101) XX, SS = np.meshgrid(xx, ss) ZZ = 2 * norm.pdf(XX - 1.0, 0, SS) + norm.pdf(XX + 4.0, 0, SS) ZZ = ZZ / ZZ.sum(axis=1)[:,np.newaxis] / (xx[1] - xx[0]) . #collapse-show from matplotlib.patches import Polygon plt.figure(figsize=(8, 4.5)) x_idx = 85 s_idx = 30 plt.subplot(221) plt.contourf(XX, SS, ZZ, cmap=&quot;GnBu&quot;) plt.plot([-6, 4], [ss[s_idx], ss[s_idx]], &quot;k-&quot;, linewidth=2) plt.plot([xx[x_idx], xx[x_idx]], [1, 2], &quot;b-&quot;, linewidth=2) plt.xlabel(r&quot;$x$&quot;) plt.ylabel(r&quot;$ theta$&quot;, fontsize=14, rotation=0) plt.title(r&quot;Model $f(x; theta)$&quot;, fontsize=14) plt.subplot(222) plt.plot(ss, ZZ[:, x_idx], &quot;b-&quot;) max_idx = np.argmax(ZZ[:, x_idx]) max_val = np.max(ZZ[:, x_idx]) plt.plot(ss[max_idx], max_val, &quot;r.&quot;) plt.plot([ss[max_idx], ss[max_idx]], [0, max_val], &quot;r:&quot;) plt.plot([0, ss[max_idx]], [max_val, max_val], &quot;r:&quot;) plt.text(1.01, max_val + 0.005, r&quot;$ hat{L}$&quot;, fontsize=14) plt.text(ss[max_idx]+ 0.01, 0.055, r&quot;$ hat{ theta}$&quot;, fontsize=14) plt.text(ss[max_idx]+ 0.01, max_val - 0.012, r&quot;$Max$&quot;, fontsize=12) plt.axis([1, 2, 0.05, 0.15]) plt.xlabel(r&quot;$ theta$&quot;, fontsize=14) plt.grid(True) plt.text(1.99, 0.135, r&quot;$=f(x=2.5; theta)$&quot;, fontsize=14, ha=&quot;right&quot;) plt.title(r&quot;Likelihood function $ mathcal{L}( theta|x=2.5)$&quot;, fontsize=14) plt.subplot(223) plt.plot(xx, ZZ[s_idx], &quot;k-&quot;) plt.axis([-6, 4, 0, 0.25]) plt.xlabel(r&quot;$x$&quot;, fontsize=14) plt.grid(True) plt.title(r&quot;PDF $f(x; theta=1.3)$&quot;, fontsize=14) verts = [(xx[41], 0)] + list(zip(xx[41:81], ZZ[s_idx, 41:81])) + [(xx[80], 0)] poly = Polygon(verts, facecolor=&#39;0.9&#39;, edgecolor=&#39;0.5&#39;) plt.gca().add_patch(poly) plt.subplot(224) plt.plot(ss, np.log(ZZ[:, x_idx]), &quot;b-&quot;) max_idx = np.argmax(np.log(ZZ[:, x_idx])) max_val = np.max(np.log(ZZ[:, x_idx])) plt.plot(ss[max_idx], max_val, &quot;r.&quot;) plt.plot([ss[max_idx], ss[max_idx]], [-5, max_val], &quot;r:&quot;) plt.plot([0, ss[max_idx]], [max_val, max_val], &quot;r:&quot;) plt.axis([1, 2, -2.4, -2]) plt.xlabel(r&quot;$ theta$&quot;, fontsize=14) plt.text(ss[max_idx]+ 0.01, max_val - 0.05, r&quot;$Max$&quot;, fontsize=12) plt.text(ss[max_idx]+ 0.01, -2.39, r&quot;$ hat{ theta}$&quot;, fontsize=14) plt.text(1.01, max_val + 0.02, r&quot;$ log , hat{L}$&quot;, fontsize=14) plt.grid(True) plt.title(r&quot;$ log , mathcal{L}( theta|x=2.5)$&quot;, fontsize=14) save_fig(&quot;likelihood_function_plot&quot;) plt.show() . . Saving figure likelihood_function_diagram . Exercise solutions . 1. to 9. . See Appendix A. . 10. Cluster the Olivetti Faces Dataset . Exercise: The classic Olivetti faces dataset contains 400 grayscale 64 Ã— 64â€“pixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. Load the dataset using the sklearn.datasets.fetch_olivetti_faces() function. . from sklearn.datasets import fetch_olivetti_faces olivetti = fetch_olivetti_faces() . print(olivetti.DESCR) . .. _olivetti_faces_dataset: The Olivetti faces dataset -- `This dataset contains a set of face images`_ taken between April 1992 and April 1994 at AT&amp;T Laboratories Cambridge. The :func:`sklearn.datasets.fetch_olivetti_faces` function is the data fetching / caching function that downloads the data archive from AT&amp;T. .. _This dataset contains a set of face images: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html As described on the original website: There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). **Data Set Characteristics:** ================= ===================== Classes 40 Samples total 400 Dimensionality 4096 Features real, between 0 and 1 ================= ===================== The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms. The &#34;target&#34; for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective. The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images. When using these images, please give credit to AT&amp;T Laboratories Cambridge. . olivetti.target . array([ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]) . Exercise: Then split it into a training set, a validation set, and a test set (note that the dataset is already scaled between 0 and 1). Since the dataset is quite small, you probably want to use stratified sampling to ensure that there are the same number of images per person in each set. . from sklearn.model_selection import StratifiedShuffleSplit strat_split = StratifiedShuffleSplit(n_splits=1, test_size=40, random_state=42) train_valid_idx, test_idx = next(strat_split.split(olivetti.data, olivetti.target)) X_train_valid = olivetti.data[train_valid_idx] y_train_valid = olivetti.target[train_valid_idx] X_test = olivetti.data[test_idx] y_test = olivetti.target[test_idx] strat_split = StratifiedShuffleSplit(n_splits=1, test_size=80, random_state=43) train_idx, valid_idx = next(strat_split.split(X_train_valid, y_train_valid)) X_train = X_train_valid[train_idx] y_train = y_train_valid[train_idx] X_valid = X_train_valid[valid_idx] y_valid = y_train_valid[valid_idx] . print(X_train.shape, y_train.shape) print(X_valid.shape, y_valid.shape) print(X_test.shape, y_test.shape) . (280, 4096) (280,) (80, 4096) (80,) (40, 4096) (40,) . To speed things up, we&#39;ll reduce the data&#39;s dimensionality using PCA: . from sklearn.decomposition import PCA pca = PCA(0.99) X_train_pca = pca.fit_transform(X_train) X_valid_pca = pca.transform(X_valid) X_test_pca = pca.transform(X_test) pca.n_components_ . 199 . Exercise: Next, cluster the images using K-Means, and ensure that you have a good number of clusters (using one of the techniques discussed in this chapter). . from sklearn.cluster import KMeans k_range = range(5, 150, 5) kmeans_per_k = [] for k in k_range: print(&quot;k={}&quot;.format(k)) kmeans = KMeans(n_clusters=k, random_state=42).fit(X_train_pca) kmeans_per_k.append(kmeans) . k=5 k=10 k=15 k=20 k=25 k=30 k=35 k=40 k=45 k=50 k=55 k=60 k=65 k=70 k=75 k=80 k=85 k=90 k=95 k=100 k=105 k=110 k=115 k=120 k=125 k=130 k=135 k=140 k=145 . from sklearn.metrics import silhouette_score silhouette_scores = [silhouette_score(X_train_pca, model.labels_) for model in kmeans_per_k] best_index = np.argmax(silhouette_scores) best_k = k_range[best_index] best_score = silhouette_scores[best_index] plt.figure(figsize=(8, 3)) plt.plot(k_range, silhouette_scores, &quot;bo-&quot;) plt.xlabel(&quot;$k$&quot;, fontsize=14) plt.ylabel(&quot;Silhouette score&quot;, fontsize=14) plt.plot(best_k, best_score, &quot;rs&quot;) plt.show() . best_k . 120 . It looks like the best number of clusters is quite high, at 120. You might have expected it to be 40, since there are 40 different people on the pictures. However, the same person may look quite different on different pictures (e.g., with or without glasses, or simply shifted left or right). . inertias = [model.inertia_ for model in kmeans_per_k] best_inertia = inertias[best_index] plt.figure(figsize=(8, 3.5)) plt.plot(k_range, inertias, &quot;bo-&quot;) plt.xlabel(&quot;$k$&quot;, fontsize=14) plt.ylabel(&quot;Inertia&quot;, fontsize=14) plt.plot(best_k, best_inertia, &quot;rs&quot;) plt.show() . The optimal number of clusters is not clear on this inertia diagram, as there is no obvious elbow, so let&#39;s stick with k=120. . best_model = kmeans_per_k[best_index] . Exercise: Visualize the clusters: do you see similar faces in each cluster? . def plot_faces(faces, labels, n_cols=5): n_rows = (len(faces) - 1) // n_cols + 1 plt.figure(figsize=(n_cols, n_rows * 1.1)) for index, (face, label) in enumerate(zip(faces, labels)): plt.subplot(n_rows, n_cols, index + 1) plt.imshow(face.reshape(64, 64), cmap=&quot;gray&quot;) plt.axis(&quot;off&quot;) plt.title(label) plt.show() for cluster_id in np.unique(best_model.labels_): print(&quot;Cluster&quot;, cluster_id) in_cluster = best_model.labels_==cluster_id faces = X_train[in_cluster].reshape(-1, 64, 64) labels = y_train[in_cluster] plot_faces(faces, labels) . Cluster 0 . Cluster 1 . Cluster 2 . Cluster 3 . Cluster 4 . Cluster 5 . Cluster 6 . Cluster 7 . Cluster 8 . Cluster 9 . Cluster 10 . Cluster 11 . Cluster 12 . Cluster 13 . Cluster 14 . Cluster 15 . Cluster 16 . Cluster 17 . Cluster 18 . Cluster 19 . Cluster 20 . Cluster 21 . Cluster 22 . Cluster 23 . Cluster 24 . Cluster 25 . Cluster 26 . Cluster 27 . Cluster 28 . Cluster 29 . Cluster 30 . Cluster 31 . Cluster 32 . Cluster 33 . Cluster 34 . Cluster 35 . Cluster 36 . Cluster 37 . Cluster 38 . Cluster 39 . Cluster 40 . Cluster 41 . Cluster 42 . Cluster 43 . Cluster 44 . Cluster 45 . Cluster 46 . Cluster 47 . Cluster 48 . Cluster 49 . Cluster 50 . Cluster 51 . Cluster 52 . Cluster 53 . Cluster 54 . Cluster 55 . Cluster 56 . Cluster 57 . Cluster 58 . Cluster 59 . Cluster 60 . Cluster 61 . Cluster 62 . Cluster 63 . Cluster 64 . Cluster 65 . Cluster 66 . Cluster 67 . Cluster 68 . Cluster 69 . Cluster 70 . Cluster 71 . Cluster 72 . Cluster 73 . Cluster 74 . Cluster 75 . Cluster 76 . Cluster 77 . Cluster 78 . Cluster 79 . Cluster 80 . Cluster 81 . Cluster 82 . Cluster 83 . Cluster 84 . Cluster 85 . Cluster 86 . Cluster 87 . Cluster 88 . Cluster 89 . Cluster 90 . Cluster 91 . Cluster 92 . Cluster 93 . Cluster 94 . Cluster 95 . Cluster 96 . Cluster 97 . Cluster 98 . Cluster 99 . Cluster 100 . Cluster 101 . Cluster 102 . Cluster 103 . Cluster 104 . Cluster 105 . Cluster 106 . Cluster 107 . Cluster 108 . Cluster 109 . Cluster 110 . Cluster 111 . Cluster 112 . Cluster 113 . Cluster 114 . Cluster 115 . Cluster 116 . Cluster 117 . Cluster 118 . Cluster 119 . About 2 out of 3 clusters are useful: that is, they contain at least 2 pictures, all of the same person. However, the rest of the clusters have either one or more intruders, or they have just a single picture. . Clustering images this way may be too imprecise to be directly useful when training a model (as we will see below), but it can be tremendously useful when labeling images in a new dataset: it will usually make labelling much faster. . 11. Using Clustering as Preprocessing for Classification . Exercise: Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set. . from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier(n_estimators=150, random_state=42) clf.fit(X_train_pca, y_train) clf.score(X_valid_pca, y_valid) . 0.925 . Exercise: Next, use K-Means as a dimensionality reduction tool, and train a classifier on the reduced set. . X_train_reduced = best_model.transform(X_train_pca) X_valid_reduced = best_model.transform(X_valid_pca) X_test_reduced = best_model.transform(X_test_pca) clf = RandomForestClassifier(n_estimators=150, random_state=42) clf.fit(X_train_reduced, y_train) clf.score(X_valid_reduced, y_valid) . 0.7 . Yikes! That&#39;s not better at all! Let&#39;s see if tuning the number of clusters helps. . Exercise: Search for the number of clusters that allows the classifier to get the best performance: what performance can you reach? . We could use a GridSearchCV like we did earlier in this notebook, but since we already have a validation set, we don&#39;t need K-fold cross-validation, and we&#39;re only exploring a single hyperparameter, so it&#39;s simpler to just run a loop manually: . from sklearn.pipeline import Pipeline for n_clusters in k_range: pipeline = Pipeline([ (&quot;kmeans&quot;, KMeans(n_clusters=n_clusters, random_state=n_clusters)), (&quot;forest_clf&quot;, RandomForestClassifier(n_estimators=150, random_state=42)) ]) pipeline.fit(X_train_pca, y_train) print(n_clusters, pipeline.score(X_valid_pca, y_valid)) . 5 0.3625 10 0.55 15 0.6125 20 0.6625 25 0.6625 30 0.7 35 0.6875 40 0.7125 45 0.7 50 0.7375 55 0.7375 60 0.75 65 0.725 70 0.7375 75 0.7875 80 0.7125 85 0.725 90 0.775 95 0.7625 100 0.65 105 0.7125 110 0.725 115 0.7625 120 0.7625 125 0.725 130 0.775 135 0.7375 140 0.7375 145 0.725 . Oh well, even by tuning the number of clusters, we never get beyond 80% accuracy. Looks like the distances to the cluster centroids are not as informative as the original images. . Exercise: What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)? . X_train_extended = np.c_[X_train_pca, X_train_reduced] X_valid_extended = np.c_[X_valid_pca, X_valid_reduced] X_test_extended = np.c_[X_test_pca, X_test_reduced] . clf = RandomForestClassifier(n_estimators=150, random_state=42) clf.fit(X_train_extended, y_train) clf.score(X_valid_extended, y_valid) . 0.8125 . That&#39;s a bit better, but still worse than without the cluster features. The clusters are not useful to directly train a classifier in this case (but they can still help when labelling new training instances). . 12. A Gaussian Mixture Model for the Olivetti Faces Dataset . Exercise: Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the algorithm, you should probably reduce the dataset&#39;s dimensionality (e.g., use PCA, preserving 99% of the variance). . from sklearn.mixture import GaussianMixture gm = GaussianMixture(n_components=40, random_state=42) y_pred = gm.fit_predict(X_train_pca) . Exercise: Use the model to generate some new faces (using the sample() method), and visualize them (if you used PCA, you will need to use its inverse_transform() method). . n_gen_faces = 20 gen_faces_reduced, y_gen_faces = gm.sample(n_samples=n_gen_faces) gen_faces = pca.inverse_transform(gen_faces_reduced) . plot_faces(gen_faces, y_gen_faces) . Exercise: Try to modify some images (e.g., rotate, flip, darken) and see if the model can detect the anomalies (i.e., compare the output of the score_samples() method for normal images and for anomalies). . n_rotated = 4 rotated = np.transpose(X_train[:n_rotated].reshape(-1, 64, 64), axes=[0, 2, 1]) rotated = rotated.reshape(-1, 64*64) y_rotated = y_train[:n_rotated] n_flipped = 3 flipped = X_train[:n_flipped].reshape(-1, 64, 64)[:, ::-1] flipped = flipped.reshape(-1, 64*64) y_flipped = y_train[:n_flipped] n_darkened = 3 darkened = X_train[:n_darkened].copy() darkened[:, 1:-1] *= 0.3 darkened = darkened.reshape(-1, 64*64) y_darkened = y_train[:n_darkened] X_bad_faces = np.r_[rotated, flipped, darkened] y_bad = np.concatenate([y_rotated, y_flipped, y_darkened]) plot_faces(X_bad_faces, y_bad) . X_bad_faces_pca = pca.transform(X_bad_faces) . gm.score_samples(X_bad_faces_pca) . array([-2.43643150e+07, -1.89784991e+07, -3.78112161e+07, -4.98187638e+07, -3.20478804e+07, -1.37531031e+07, -2.92373904e+07, -1.05489264e+08, -1.19575656e+08, -6.74257782e+07]) . The bad faces are all considered highly unlikely by the Gaussian Mixture model. Compare this to the scores of some training instances: . gm.score_samples(X_train_pca[:10]) . array([1163.0202095 , 1134.03637995, 1156.32132802, 1170.67602773, 1141.45404783, 1154.35205193, 1091.32894533, 1111.41149431, 1096.43048989, 1132.98982741]) . 13. Using Dimensionality Reduction Techniques for Anomaly Detection . Exercise: Some dimensionality reduction techniques can also be used for anomaly detection. For example, take the Olivetti faces dataset and reduce it with PCA, preserving 99% of the variance. Then compute the reconstruction error for each image. Next, take some of the modified images you built in the previous exercise, and look at their reconstruction error: notice how much larger the reconstruction error is. If you plot a reconstructed image, you will see why: it tries to reconstruct a normal face. . We already reduced the dataset using PCA earlier: . X_train_pca . array([[ 3.78082848e+00, -1.85478747e+00, -5.14403582e+00, ..., -1.35635510e-01, -2.14079559e-01, 6.11926578e-02], [ 1.01488495e+01, -1.52754533e+00, -7.66978443e-01, ..., 1.23931095e-01, -1.35269478e-01, -2.32696794e-02], [-1.00152884e+01, 2.87728882e+00, -9.19886231e-01, ..., 7.26117715e-02, -2.96737882e-03, 1.24885648e-01], ..., [ 2.47586989e+00, 2.95597243e+00, 1.29985559e+00, ..., -2.09138989e-02, 3.48485485e-02, -1.54334918e-01], [-3.22031879e+00, 5.34897900e+00, 1.39427102e+00, ..., 5.75523153e-02, -2.28309885e-01, 1.55576378e-01], [-9.22876537e-01, -3.64703059e+00, 2.26088119e+00, ..., 1.36851206e-01, -6.91276789e-02, 6.26953915e-02]], dtype=float32) . def reconstruction_errors(pca, X): X_pca = pca.transform(X) X_reconstructed = pca.inverse_transform(X_pca) mse = np.square(X_reconstructed - X).mean(axis=-1) return mse . reconstruction_errors(pca, X_train).mean() . 0.00019205351 . reconstruction_errors(pca, X_bad_faces).mean() . 0.0047073546 . plot_faces(X_bad_faces, y_gen_faces) . X_bad_faces_reconstructed = pca.inverse_transform(X_bad_faces_pca) plot_faces(X_bad_faces_reconstructed, y_gen_faces) .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/_unsupervised_learning.html",
            "relUrl": "/2020/03/09/_unsupervised_learning.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Training Linear Models",
            "content": "This notebook contains all the sample code and solutions to the exercises in chapter 4. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;training_linear_models&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) # Ignore useless warnings (see SciPy issue #5998) import warnings warnings.filterwarnings(action=&quot;ignore&quot;, message=&quot;^internal gelsd&quot;) . . Linear regression using the Normal Equation . import numpy as np X = 2 * np.random.rand(100, 1) y = 4 + 3 * X + np.random.randn(100, 1) . plt.plot(X, y, &quot;b.&quot;) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18) plt.axis([0, 2, 0, 15]) save_fig(&quot;generated_data_plot&quot;) plt.show() . Saving figure generated_data_plot . X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) . theta_best . array([[4.21509616], [2.77011339]]) . X_new = np.array([[0], [2]]) X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance y_predict = X_new_b.dot(theta_best) y_predict . array([[4.21509616], [9.75532293]]) . plt.plot(X_new, y_predict, &quot;r-&quot;) plt.plot(X, y, &quot;b.&quot;) plt.axis([0, 2, 0, 15]) plt.show() . The figure in the book actually corresponds to the following code, with a legend and axis labels: . plt.plot(X_new, y_predict, &quot;r-&quot;, linewidth=2, label=&quot;Predictions&quot;) plt.plot(X, y, &quot;b.&quot;) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18) plt.legend(loc=&quot;upper left&quot;, fontsize=14) plt.axis([0, 2, 0, 15]) save_fig(&quot;linear_model_predictions_plot&quot;) plt.show() . Saving figure linear_model_predictions_plot . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X, y) lin_reg.intercept_, lin_reg.coef_ . (array([4.21509616]), array([[2.77011339]])) . lin_reg.predict(X_new) . array([[4.21509616], [9.75532293]]) . The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for &quot;least squares&quot;), which you could call directly: . theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6) theta_best_svd . array([[4.21509616], [2.77011339]]) . This function computes $ mathbf{X}^+ mathbf{y}$, where $ mathbf{X}^{+}$ is the pseudoinverse of $ mathbf{X}$ (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly: . np.linalg.pinv(X_b).dot(y) . array([[4.21509616], [2.77011339]]) . Linear regression using batch gradient descent . eta = 0.1 # learning rate n_iterations = 1000 m = 100 theta = np.random.randn(2,1) # random initialization for iteration in range(n_iterations): gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradients . theta . array([[4.21509616], [2.77011339]]) . X_new_b.dot(theta) . array([[4.21509616], [9.75532293]]) . #collapse-show theta_path_bgd = [] def plot_gradient_descent(theta, eta, theta_path=None): m = len(X_b) plt.plot(X, y, &quot;b.&quot;) n_iterations = 1000 for iteration in range(n_iterations): if iteration &lt; 10: y_predict = X_new_b.dot(theta) style = &quot;b-&quot; if iteration &gt; 0 else &quot;r--&quot; plt.plot(X_new, y_predict, style) gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradients if theta_path is not None: theta_path.append(theta) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.axis([0, 2, 0, 15]) plt.title(r&quot;$ eta = {}$&quot;.format(eta), fontsize=16) . . np.random.seed(42) theta = np.random.randn(2,1) # random initialization plt.figure(figsize=(10,4)) plt.subplot(131); plot_gradient_descent(theta, eta=0.02) plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18) plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd) plt.subplot(133); plot_gradient_descent(theta, eta=0.5) save_fig(&quot;gradient_descent_plot&quot;) plt.show() . Saving figure gradient_descent_plot . Stochastic Gradient Descent . theta_path_sgd = [] m = len(X_b) np.random.seed(42) . #collapse-show n_epochs = 50 t0, t1 = 5, 50 # learning schedule hyperparameters def learning_schedule(t): return t0 / (t + t1) theta = np.random.randn(2,1) # random initialization for epoch in range(n_epochs): for i in range(m): if epoch == 0 and i &lt; 20: # not shown in the book y_predict = X_new_b.dot(theta) # not shown style = &quot;b-&quot; if i &gt; 0 else &quot;r--&quot; # not shown plt.plot(X_new, y_predict, style) # not shown random_index = np.random.randint(m) xi = X_b[random_index:random_index+1] yi = y[random_index:random_index+1] gradients = 2 * xi.T.dot(xi.dot(theta) - yi) eta = learning_schedule(epoch * m + i) theta = theta - eta * gradients theta_path_sgd.append(theta) # not shown plt.plot(X, y, &quot;b.&quot;) # not shown plt.xlabel(&quot;$x_1$&quot;, fontsize=18) # not shown plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18) # not shown plt.axis([0, 2, 0, 15]) # not shown save_fig(&quot;sgd_plot&quot;) # not shown plt.show() # not shown . . Saving figure sgd_plot . theta . array([[4.21076011], [2.74856079]]) . from sklearn.linear_model import SGDRegressor sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42) sgd_reg.fit(X, y.ravel()) . SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;invscaling&#39;, loss=&#39;squared_loss&#39;, max_iter=1000, n_iter_no_change=5, penalty=None, power_t=0.25, random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) . sgd_reg.intercept_, sgd_reg.coef_ . (array([4.24365286]), array([2.8250878])) . Mini-batch gradient descent . #collapse-show theta_path_mgd = [] n_iterations = 50 minibatch_size = 20 np.random.seed(42) theta = np.random.randn(2,1) # random initialization t0, t1 = 200, 1000 def learning_schedule(t): return t0 / (t + t1) t = 0 for epoch in range(n_iterations): shuffled_indices = np.random.permutation(m) X_b_shuffled = X_b[shuffled_indices] y_shuffled = y[shuffled_indices] for i in range(0, m, minibatch_size): t += 1 xi = X_b_shuffled[i:i+minibatch_size] yi = y_shuffled[i:i+minibatch_size] gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi) eta = learning_schedule(t) theta = theta - eta * gradients theta_path_mgd.append(theta) . . theta . array([[4.25214635], [2.7896408 ]]) . theta_path_bgd = np.array(theta_path_bgd) theta_path_sgd = np.array(theta_path_sgd) theta_path_mgd = np.array(theta_path_mgd) . plt.figure(figsize=(7,4)) plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], &quot;r-s&quot;, linewidth=1, label=&quot;Stochastic&quot;) plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], &quot;g-+&quot;, linewidth=2, label=&quot;Mini-batch&quot;) plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], &quot;b-o&quot;, linewidth=3, label=&quot;Batch&quot;) plt.legend(loc=&quot;upper left&quot;, fontsize=16) plt.xlabel(r&quot;$ theta_0$&quot;, fontsize=20) plt.ylabel(r&quot;$ theta_1$ &quot;, fontsize=20, rotation=0) plt.axis([2.5, 4.5, 2.3, 3.9]) save_fig(&quot;gradient_descent_paths_plot&quot;) plt.show() . Saving figure gradient_descent_paths_plot . Polynomial regression . import numpy as np import numpy.random as rnd np.random.seed(42) . m = 100 X = 6 * np.random.rand(m, 1) - 3 y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1) . plt.plot(X, y, &quot;b.&quot;) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18) plt.axis([-3, 3, 0, 10]) save_fig(&quot;quadratic_data_plot&quot;) plt.show() . Saving figure quadratic_data_plot . from sklearn.preprocessing import PolynomialFeatures poly_features = PolynomialFeatures(degree=2, include_bias=False) X_poly = poly_features.fit_transform(X) X[0] . array([-0.75275929]) . X_poly[0] . array([-0.75275929, 0.56664654]) . lin_reg = LinearRegression() lin_reg.fit(X_poly, y) lin_reg.intercept_, lin_reg.coef_ . (array([1.78134581]), array([[0.93366893, 0.56456263]])) . X_new=np.linspace(-3, 3, 100).reshape(100, 1) X_new_poly = poly_features.transform(X_new) y_new = lin_reg.predict(X_new_poly) plt.plot(X, y, &quot;b.&quot;) plt.plot(X_new, y_new, &quot;r-&quot;, linewidth=2, label=&quot;Predictions&quot;) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18) plt.legend(loc=&quot;upper left&quot;, fontsize=14) plt.axis([-3, 3, 0, 10]) save_fig(&quot;quadratic_predictions_plot&quot;) plt.show() . Saving figure quadratic_predictions_plot . #collapse-show from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline for style, width, degree in ((&quot;g-&quot;, 1, 300), (&quot;b--&quot;, 2, 2), (&quot;r-+&quot;, 2, 1)): polybig_features = PolynomialFeatures(degree=degree, include_bias=False) std_scaler = StandardScaler() lin_reg = LinearRegression() polynomial_regression = Pipeline([ (&quot;poly_features&quot;, polybig_features), (&quot;std_scaler&quot;, std_scaler), (&quot;lin_reg&quot;, lin_reg), ]) polynomial_regression.fit(X, y) y_newbig = polynomial_regression.predict(X_new) plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width) plt.plot(X, y, &quot;b.&quot;, linewidth=3) plt.legend(loc=&quot;upper left&quot;) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18) plt.axis([-3, 3, 0, 10]) save_fig(&quot;high_degree_polynomials_plot&quot;) plt.show() . . Saving figure high_degree_polynomials_plot . #collapse-show from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split def plot_learning_curves(model, X, y): X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10) train_errors, val_errors = [], [] for m in range(1, len(X_train)): model.fit(X_train[:m], y_train[:m]) y_train_predict = model.predict(X_train[:m]) y_val_predict = model.predict(X_val) train_errors.append(mean_squared_error(y_train[:m], y_train_predict)) val_errors.append(mean_squared_error(y_val, y_val_predict)) plt.plot(np.sqrt(train_errors), &quot;r-+&quot;, linewidth=2, label=&quot;train&quot;) plt.plot(np.sqrt(val_errors), &quot;b-&quot;, linewidth=3, label=&quot;val&quot;) plt.legend(loc=&quot;upper right&quot;, fontsize=14) # not shown in the book plt.xlabel(&quot;Training set size&quot;, fontsize=14) # not shown plt.ylabel(&quot;RMSE&quot;, fontsize=14) # not shown . . lin_reg = LinearRegression() plot_learning_curves(lin_reg, X, y) plt.axis([0, 80, 0, 3]) # not shown in the book save_fig(&quot;underfitting_learning_curves_plot&quot;) # not shown plt.show() # not shown . Saving figure underfitting_learning_curves_plot . #collapse-show from sklearn.pipeline import Pipeline polynomial_regression = Pipeline([ (&quot;poly_features&quot;, PolynomialFeatures(degree=10, include_bias=False)), (&quot;lin_reg&quot;, LinearRegression()), ]) plot_learning_curves(polynomial_regression, X, y) plt.axis([0, 80, 0, 3]) # not shown save_fig(&quot;learning_curves_plot&quot;) # not shown plt.show() # not shown . . Saving figure learning_curves_plot . Regularized models . np.random.seed(42) m = 20 X = 3 * np.random.rand(m, 1) y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5 X_new = np.linspace(0, 3, 100).reshape(100, 1) . from sklearn.linear_model import Ridge ridge_reg = Ridge(alpha=1, solver=&quot;cholesky&quot;, random_state=42) ridge_reg.fit(X, y) ridge_reg.predict([[1.5]]) . array([[1.55071465]]) . ridge_reg = Ridge(alpha=1, solver=&quot;sag&quot;, random_state=42) ridge_reg.fit(X, y) ridge_reg.predict([[1.5]]) . array([[1.5507201]]) . #collapse-show from sklearn.linear_model import Ridge def plot_model(model_class, polynomial, alphas, **model_kargs): for alpha, style in zip(alphas, (&quot;b-&quot;, &quot;g--&quot;, &quot;r:&quot;)): model = model_class(alpha, **model_kargs) if alpha &gt; 0 else LinearRegression() if polynomial: model = Pipeline([ (&quot;poly_features&quot;, PolynomialFeatures(degree=10, include_bias=False)), (&quot;std_scaler&quot;, StandardScaler()), (&quot;regul_reg&quot;, model), ]) model.fit(X, y) y_new_regul = model.predict(X_new) lw = 2 if alpha &gt; 0 else 1 plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r&quot;$ alpha = {}$&quot;.format(alpha)) plt.plot(X, y, &quot;b.&quot;, linewidth=3) plt.legend(loc=&quot;upper left&quot;, fontsize=15) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.axis([0, 3, 0, 4]) plt.figure(figsize=(8,4)) plt.subplot(121) plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42) plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18) plt.subplot(122) plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42) save_fig(&quot;ridge_regression_plot&quot;) plt.show() . . Saving figure ridge_regression_plot . Note: to be future-proof, we set max_iter=1000 and tol=1e-3 because these will be the default values in Scikit-Learn 0.21. . sgd_reg = SGDRegressor(penalty=&quot;l2&quot;, max_iter=1000, tol=1e-3, random_state=42) sgd_reg.fit(X, y.ravel()) sgd_reg.predict([[1.5]]) . array([1.47012588]) . from sklearn.linear_model import Lasso plt.figure(figsize=(8,4)) plt.subplot(121) plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42) plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18) plt.subplot(122) plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42) save_fig(&quot;lasso_regression_plot&quot;) plt.show() . /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.802867703827461, tolerance: 0.0009294783355207351 positive) . Saving figure lasso_regression_plot . from sklearn.linear_model import Lasso lasso_reg = Lasso(alpha=0.1) lasso_reg.fit(X, y) lasso_reg.predict([[1.5]]) . array([1.53788174]) . from sklearn.linear_model import ElasticNet elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42) elastic_net.fit(X, y) elastic_net.predict([[1.5]]) . array([1.54333232]) . np.random.seed(42) m = 100 X = 6 * np.random.rand(m, 1) - 3 y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1) X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10) . Early stopping example: . #collapse-show from sklearn.base import clone poly_scaler = Pipeline([ (&quot;poly_features&quot;, PolynomialFeatures(degree=90, include_bias=False)), (&quot;std_scaler&quot;, StandardScaler()) ]) X_train_poly_scaled = poly_scaler.fit_transform(X_train) X_val_poly_scaled = poly_scaler.transform(X_val) sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None, learning_rate=&quot;constant&quot;, eta0=0.0005, random_state=42) minimum_val_error = float(&quot;inf&quot;) best_epoch = None best_model = None for epoch in range(1000): sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off y_val_predict = sgd_reg.predict(X_val_poly_scaled) val_error = mean_squared_error(y_val, y_val_predict) if val_error &lt; minimum_val_error: minimum_val_error = val_error best_epoch = epoch best_model = clone(sgd_reg) . . Create the graph: . #collapse-show sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None, learning_rate=&quot;constant&quot;, eta0=0.0005, random_state=42) n_epochs = 500 train_errors, val_errors = [], [] for epoch in range(n_epochs): sgd_reg.fit(X_train_poly_scaled, y_train) y_train_predict = sgd_reg.predict(X_train_poly_scaled) y_val_predict = sgd_reg.predict(X_val_poly_scaled) train_errors.append(mean_squared_error(y_train, y_train_predict)) val_errors.append(mean_squared_error(y_val, y_val_predict)) best_epoch = np.argmin(val_errors) best_val_rmse = np.sqrt(val_errors[best_epoch]) plt.annotate(&#39;Best model&#39;, xy=(best_epoch, best_val_rmse), xytext=(best_epoch, best_val_rmse + 1), ha=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05), fontsize=16, ) best_val_rmse -= 0.03 # just to make the graph look better plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], &quot;k:&quot;, linewidth=2) plt.plot(np.sqrt(val_errors), &quot;b-&quot;, linewidth=3, label=&quot;Validation set&quot;) plt.plot(np.sqrt(train_errors), &quot;r--&quot;, linewidth=2, label=&quot;Training set&quot;) plt.legend(loc=&quot;upper right&quot;, fontsize=14) plt.xlabel(&quot;Epoch&quot;, fontsize=14) plt.ylabel(&quot;RMSE&quot;, fontsize=14) save_fig(&quot;early_stopping_plot&quot;) plt.show() . . Saving figure early_stopping_plot . best_epoch, best_model . (239, SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1, eta0=0.0005, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;constant&#39;, loss=&#39;squared_loss&#39;, max_iter=1, n_iter_no_change=5, penalty=None, power_t=0.25, random_state=42, shuffle=True, tol=-inf, validation_fraction=0.1, verbose=0, warm_start=True)) . %matplotlib inline import matplotlib.pyplot as plt import numpy as np . t1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5 t1s = np.linspace(t1a, t1b, 500) t2s = np.linspace(t2a, t2b, 500) t1, t2 = np.meshgrid(t1s, t2s) T = np.c_[t1.ravel(), t2.ravel()] Xr = np.array([[1, 1], [1, -1], [1, 0.5]]) yr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:] J = (1/len(Xr) * np.sum((T.dot(Xr.T) - yr.T)**2, axis=1)).reshape(t1.shape) N1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape) N2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape) t_min_idx = np.unravel_index(np.argmin(J), J.shape) t1_min, t2_min = t1[t_min_idx], t2[t_min_idx] t_init = np.array([[0.25], [-1]]) . #collapse-show def bgd_path(theta, X, y, l1, l2, core = 1, eta = 0.05, n_iterations = 200): path = [theta] for iteration in range(n_iterations): gradients = core * 2/len(X) * X.T.dot(X.dot(theta) - y) + l1 * np.sign(theta) + l2 * theta theta = theta - eta * gradients path.append(theta) return np.array(path) fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8)) for i, N, l1, l2, title in ((0, N1, 2., 0, &quot;Lasso&quot;), (1, N2, 0, 2., &quot;Ridge&quot;)): JR = J + l1 * N1 + l2 * 0.5 * N2**2 tr_min_idx = np.unravel_index(np.argmin(JR), JR.shape) t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx] levelsJ=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(J) - np.min(J)) + np.min(J) levelsJR=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(JR) - np.min(JR)) + np.min(JR) levelsN=np.linspace(0, np.max(N), 10) path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0) path_JR = bgd_path(t_init, Xr, yr, l1, l2) path_N = bgd_path(np.array([[2.0], [0.5]]), Xr, yr, np.sign(l1)/3, np.sign(l2), core=0) ax = axes[i, 0] ax.grid(True) ax.axhline(y=0, color=&#39;k&#39;) ax.axvline(x=0, color=&#39;k&#39;) ax.contourf(t1, t2, N / 2., levels=levelsN) ax.plot(path_N[:, 0], path_N[:, 1], &quot;y--&quot;) ax.plot(0, 0, &quot;ys&quot;) ax.plot(t1_min, t2_min, &quot;ys&quot;) ax.set_title(r&quot;$ ell_{}$ penalty&quot;.format(i + 1), fontsize=16) ax.axis([t1a, t1b, t2a, t2b]) if i == 1: ax.set_xlabel(r&quot;$ theta_1$&quot;, fontsize=16) ax.set_ylabel(r&quot;$ theta_2$&quot;, fontsize=16, rotation=0) ax = axes[i, 1] ax.grid(True) ax.axhline(y=0, color=&#39;k&#39;) ax.axvline(x=0, color=&#39;k&#39;) ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9) ax.plot(path_JR[:, 0], path_JR[:, 1], &quot;w-o&quot;) ax.plot(path_N[:, 0], path_N[:, 1], &quot;y--&quot;) ax.plot(0, 0, &quot;ys&quot;) ax.plot(t1_min, t2_min, &quot;ys&quot;) ax.plot(t1r_min, t2r_min, &quot;rs&quot;) ax.set_title(title, fontsize=16) ax.axis([t1a, t1b, t2a, t2b]) if i == 1: ax.set_xlabel(r&quot;$ theta_1$&quot;, fontsize=16) save_fig(&quot;lasso_vs_ridge_plot&quot;) plt.show() . . Saving figure lasso_vs_ridge_plot . Logistic regression . t = np.linspace(-10, 10, 100) sig = 1 / (1 + np.exp(-t)) plt.figure(figsize=(9, 3)) plt.plot([-10, 10], [0, 0], &quot;k-&quot;) plt.plot([-10, 10], [0.5, 0.5], &quot;k:&quot;) plt.plot([-10, 10], [1, 1], &quot;k:&quot;) plt.plot([0, 0], [-1.1, 1.1], &quot;k-&quot;) plt.plot(t, sig, &quot;b-&quot;, linewidth=2, label=r&quot;$ sigma(t) = frac{1}{1 + e^{-t}}$&quot;) plt.xlabel(&quot;t&quot;) plt.legend(loc=&quot;upper left&quot;, fontsize=20) plt.axis([-10, 10, -0.1, 1.1]) save_fig(&quot;logistic_function_plot&quot;) plt.show() . Saving figure logistic_function_plot . from sklearn import datasets iris = datasets.load_iris() list(iris.keys()) . [&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;] . print(iris.DESCR) . .. _iris_dataset: Iris plants dataset -- **Data Set Characteristics:** :Number of Instances: 150 (50 in each of three classes) :Number of Attributes: 4 numeric, predictive attributes and the class :Attribute Information: - sepal length in cm - sepal width in cm - petal length in cm - petal width in cm - class: - Iris-Setosa - Iris-Versicolour - Iris-Virginica :Summary Statistics: ============== ==== ==== ======= ===== ==================== Min Max Mean SD Class Correlation ============== ==== ==== ======= ===== ==================== sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) ============== ==== ==== ======= ===== ==================== :Missing Attribute Values: None :Class Distribution: 33.3% for each of 3 classes. :Creator: R.A. Fisher :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) :Date: July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&#39;s paper. Note that it&#39;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fisher&#39;s paper is a classic in the field and is referenced frequently to this day. (See Duda &amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. .. topic:: References - Fisher, R.A. &#34;The use of multiple measurements in taxonomic problems&#34; Annual Eugenics, 7, Part II, 179-188 (1936); also in &#34;Contributions to Mathematical Statistics&#34; (John Wiley, NY, 1950). - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley &amp; Sons. ISBN 0-471-22361-1. See page 218. - Dasarathy, B.V. (1980) &#34;Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments&#34;. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. - Gates, G.W. (1972) &#34;The Reduced Nearest Neighbor Rule&#34;. IEEE Transactions on Information Theory, May 1972, 431-433. - See also: 1988 MLC Proceedings, 54-64. Cheeseman et al&#34;s AUTOCLASS II conceptual clustering system finds 3 classes in the data. - Many, many more ... . X = iris[&quot;data&quot;][:, 3:] # petal width y = (iris[&quot;target&quot;] == 2).astype(np.int) # 1 if Iris virginica, else 0 . Note: To be future-proof we set solver=&quot;lbfgs&quot; since this will be the default value in Scikit-Learn 0.22. . from sklearn.linear_model import LogisticRegression log_reg = LogisticRegression(solver=&quot;lbfgs&quot;, random_state=42) log_reg.fit(X, y) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . X_new = np.linspace(0, 3, 1000).reshape(-1, 1) y_proba = log_reg.predict_proba(X_new) plt.plot(X_new, y_proba[:, 1], &quot;g-&quot;, linewidth=2, label=&quot;Iris virginica&quot;) plt.plot(X_new, y_proba[:, 0], &quot;b--&quot;, linewidth=2, label=&quot;Not Iris virginica&quot;) . [&lt;matplotlib.lines.Line2D at 0x12718b5c0&gt;] . The figure in the book actually is actually a bit fancier: . X_new = np.linspace(0, 3, 1000).reshape(-1, 1) y_proba = log_reg.predict_proba(X_new) decision_boundary = X_new[y_proba[:, 1] &gt;= 0.5][0] plt.figure(figsize=(8, 3)) plt.plot(X[y==0], y[y==0], &quot;bs&quot;) plt.plot(X[y==1], y[y==1], &quot;g^&quot;) plt.plot([decision_boundary, decision_boundary], [-1, 2], &quot;k:&quot;, linewidth=2) plt.plot(X_new, y_proba[:, 1], &quot;g-&quot;, linewidth=2, label=&quot;Iris virginica&quot;) plt.plot(X_new, y_proba[:, 0], &quot;b--&quot;, linewidth=2, label=&quot;Not Iris virginica&quot;) plt.text(decision_boundary+0.02, 0.15, &quot;Decision boundary&quot;, fontsize=14, color=&quot;k&quot;, ha=&quot;center&quot;) plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc=&#39;b&#39;, ec=&#39;b&#39;) plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc=&#39;g&#39;, ec=&#39;g&#39;) plt.xlabel(&quot;Petal width (cm)&quot;, fontsize=14) plt.ylabel(&quot;Probability&quot;, fontsize=14) plt.legend(loc=&quot;center left&quot;, fontsize=14) plt.axis([0, 3, -0.02, 1.02]) save_fig(&quot;logistic_regression_plot&quot;) plt.show() . Saving figure logistic_regression_plot . decision_boundary . array([1.66066066]) . log_reg.predict([[1.7], [1.5]]) . array([1, 0]) . from sklearn.linear_model import LogisticRegression X = iris[&quot;data&quot;][:, (2, 3)] # petal length, petal width y = (iris[&quot;target&quot;] == 2).astype(np.int) log_reg = LogisticRegression(solver=&quot;lbfgs&quot;, C=10**10, random_state=42) log_reg.fit(X, y) x0, x1 = np.meshgrid( np.linspace(2.9, 7, 500).reshape(-1, 1), np.linspace(0.8, 2.7, 200).reshape(-1, 1), ) X_new = np.c_[x0.ravel(), x1.ravel()] y_proba = log_reg.predict_proba(X_new) plt.figure(figsize=(10, 4)) plt.plot(X[y==0, 0], X[y==0, 1], &quot;bs&quot;) plt.plot(X[y==1, 0], X[y==1, 1], &quot;g^&quot;) zz = y_proba[:, 1].reshape(x0.shape) contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg) left_right = np.array([2.9, 7]) boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1] plt.clabel(contour, inline=1, fontsize=12) plt.plot(left_right, boundary, &quot;k--&quot;, linewidth=3) plt.text(3.5, 1.5, &quot;Not Iris virginica&quot;, fontsize=14, color=&quot;b&quot;, ha=&quot;center&quot;) plt.text(6.5, 2.3, &quot;Iris virginica&quot;, fontsize=14, color=&quot;g&quot;, ha=&quot;center&quot;) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.axis([2.9, 7, 0.8, 2.7]) save_fig(&quot;logistic_regression_contour_plot&quot;) plt.show() . Saving figure logistic_regression_contour_plot . X = iris[&quot;data&quot;][:, (2, 3)] # petal length, petal width y = iris[&quot;target&quot;] softmax_reg = LogisticRegression(multi_class=&quot;multinomial&quot;,solver=&quot;lbfgs&quot;, C=10, random_state=42) softmax_reg.fit(X, y) . LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;multinomial&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . x0, x1 = np.meshgrid( np.linspace(0, 8, 500).reshape(-1, 1), np.linspace(0, 3.5, 200).reshape(-1, 1), ) X_new = np.c_[x0.ravel(), x1.ravel()] y_proba = softmax_reg.predict_proba(X_new) y_predict = softmax_reg.predict(X_new) zz1 = y_proba[:, 1].reshape(x0.shape) zz = y_predict.reshape(x0.shape) plt.figure(figsize=(10, 4)) plt.plot(X[y==2, 0], X[y==2, 1], &quot;g^&quot;, label=&quot;Iris virginica&quot;) plt.plot(X[y==1, 0], X[y==1, 1], &quot;bs&quot;, label=&quot;Iris versicolor&quot;) plt.plot(X[y==0, 0], X[y==0, 1], &quot;yo&quot;, label=&quot;Iris setosa&quot;) from matplotlib.colors import ListedColormap custom_cmap = ListedColormap([&#39;#fafab0&#39;,&#39;#9898ff&#39;,&#39;#a0faa0&#39;]) plt.contourf(x0, x1, zz, cmap=custom_cmap) contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg) plt.clabel(contour, inline=1, fontsize=12) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.legend(loc=&quot;center left&quot;, fontsize=14) plt.axis([0, 7, 0, 3.5]) save_fig(&quot;softmax_regression_contour_plot&quot;) plt.show() . Saving figure softmax_regression_contour_plot . softmax_reg.predict([[5, 2]]) . array([2]) . softmax_reg.predict_proba([[5, 2]]) . array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]]) . Exercise solutions . 1. to 11. . See appendix A. . 12. Batch Gradient Descent with early stopping for Softmax Regression . (without using Scikit-Learn) . Let&#39;s start by loading the data. We will just reuse the Iris dataset we loaded earlier. . X = iris[&quot;data&quot;][:, (2, 3)] # petal length, petal width y = iris[&quot;target&quot;] . We need to add the bias term for every instance ($x_0 = 1$): . X_with_bias = np.c_[np.ones([len(X), 1]), X] . And let&#39;s set the random seed so the output of this exercise solution is reproducible: . np.random.seed(2042) . The easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn&#39;s train_test_split() function, but the point of this exercise is to try understand the algorithms by implementing them manually. So here is one possible implementation: . test_ratio = 0.2 validation_ratio = 0.2 total_size = len(X_with_bias) test_size = int(total_size * test_ratio) validation_size = int(total_size * validation_ratio) train_size = total_size - test_size - validation_size rnd_indices = np.random.permutation(total_size) X_train = X_with_bias[rnd_indices[:train_size]] y_train = y[rnd_indices[:train_size]] X_valid = X_with_bias[rnd_indices[train_size:-test_size]] y_valid = y[rnd_indices[train_size:-test_size]] X_test = X_with_bias[rnd_indices[-test_size:]] y_test = y[rnd_indices[-test_size:]] . The targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for ay given instance is a one-hot vector). Let&#39;s write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance: . def to_one_hot(y): n_classes = y.max() + 1 m = len(y) Y_one_hot = np.zeros((m, n_classes)) Y_one_hot[np.arange(m), y] = 1 return Y_one_hot . Let&#39;s test this function on the first 10 instances: . y_train[:10] . array([0, 1, 2, 1, 1, 0, 1, 1, 1, 0]) . to_one_hot(y_train[:10]) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [0., 1., 0.], [1., 0., 0.], [0., 1., 0.], [0., 1., 0.], [0., 1., 0.], [1., 0., 0.]]) . Looks good, so let&#39;s create the target class probabilities matrix for the training set and the test set: . Y_train_one_hot = to_one_hot(y_train) Y_valid_one_hot = to_one_hot(y_valid) Y_test_one_hot = to_one_hot(y_test) . Now let&#39;s implement the Softmax function. Recall that it is defined by the following equation: . $ sigma left( mathbf{s}( mathbf{x}) right)_k = dfrac{ exp left(s_k( mathbf{x}) right)}{ sum limits_{j=1}^{K}{ exp left(s_j( mathbf{x}) right)}}$ . def softmax(logits): exps = np.exp(logits) exp_sums = np.sum(exps, axis=1, keepdims=True) return exps / exp_sums . We are almost ready to start training. Let&#39;s define the number of inputs and outputs: . n_inputs = X_train.shape[1] # == 3 (2 features plus the bias term) n_outputs = len(np.unique(y_train)) # == 3 (3 iris classes) . Now here comes the hardest part: training! Theoretically, it&#39;s simple: it&#39;s just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it&#39;s easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it&#39;s working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won&#39;t have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what&#39;s going on under the hood. . So the equations we will need are the cost function: . $J( mathbf{ Theta}) = . dfrac{1}{m} sum limits{i=1}^{m} sum limits{k=1}^{K}{y_k^{(i)} log left( hat{p}_k^{(i)} right)}$ | . And the equation for the gradients: . $ nabla_{ mathbf{ theta}^{(k)}} , J( mathbf{ Theta}) = dfrac{1}{m} sum limits_{i=1}^{m}{ left ( hat{p}^{(i)}_k - y_k^{(i)} right ) mathbf{x}^{(i)}}$ . Note that $ log left( hat{p}_k^{(i)} right)$ may not be computable if $ hat{p}_k^{(i)} = 0$. So we will add a tiny value $ epsilon$ to $ log left( hat{p}_k^{(i)} right)$ to avoid getting nan values. . eta = 0.01 n_iterations = 5001 m = len(X_train) epsilon = 1e-7 Theta = np.random.randn(n_inputs, n_outputs) for iteration in range(n_iterations): logits = X_train.dot(Theta) Y_proba = softmax(logits) loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1)) error = Y_proba - Y_train_one_hot if iteration % 500 == 0: print(iteration, loss) gradients = 1/m * X_train.T.dot(error) Theta = Theta - eta * gradients . 0 5.446205811872683 500 0.8350062641405651 1000 0.6878801447192402 1500 0.6012379137693313 2000 0.5444496861981873 2500 0.5038530181431525 3000 0.4729228972192248 3500 0.4482424418895776 4000 0.4278651093928793 4500 0.41060071429187134 5000 0.3956780375390374 . And that&#39;s it! The Softmax model is trained. Let&#39;s look at the model parameters: . Theta . array([[ 3.32094157, -0.6501102 , -2.99979416], [-1.1718465 , 0.11706172, 0.10507543], [-0.70224261, -0.09527802, 1.4786383 ]]) . Let&#39;s make predictions for the validation set and check the accuracy score: . logits = X_valid.dot(Theta) Y_proba = softmax(logits) y_predict = np.argmax(Y_proba, axis=1) accuracy_score = np.mean(y_predict == y_valid) accuracy_score . 0.9666666666666667 . Well, this model looks pretty good. For the sake of the exercise, let&#39;s add a bit of $ ell_2$ regularization. The following training code is similar to the one above, but the loss now has an additional $ ell_2$ penalty, and the gradients have the proper additional term (note that we don&#39;t regularize the first element of Theta since this corresponds to the bias term). Also, let&#39;s try increasing the learning rate eta. . eta = 0.1 n_iterations = 5001 m = len(X_train) epsilon = 1e-7 alpha = 0.1 # regularization hyperparameter Theta = np.random.randn(n_inputs, n_outputs) for iteration in range(n_iterations): logits = X_train.dot(Theta) Y_proba = softmax(logits) xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1)) l2_loss = 1/2 * np.sum(np.square(Theta[1:])) loss = xentropy_loss + alpha * l2_loss error = Y_proba - Y_train_one_hot if iteration % 500 == 0: print(iteration, loss) gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]] Theta = Theta - eta * gradients . 0 6.629842469083912 500 0.5339667976629505 1000 0.503640075014894 1500 0.49468910594603216 2000 0.4912968418075477 2500 0.489899247009333 3000 0.48929905984511984 3500 0.48903512443978603 4000 0.4889173621830818 4500 0.4888643337449303 5000 0.4888403120738818 . Because of the additional $ ell_2$ penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let&#39;s find out: . logits = X_valid.dot(Theta) Y_proba = softmax(logits) y_predict = np.argmax(Y_proba, axis=1) accuracy_score = np.mean(y_predict == y_valid) accuracy_score . 1.0 . Cool, perfect accuracy! We probably just got lucky with this validation set, but still, it&#39;s pleasant. . Now let&#39;s add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing. . eta = 0.1 n_iterations = 5001 m = len(X_train) epsilon = 1e-7 alpha = 0.1 # regularization hyperparameter best_loss = np.infty Theta = np.random.randn(n_inputs, n_outputs) for iteration in range(n_iterations): logits = X_train.dot(Theta) Y_proba = softmax(logits) xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1)) l2_loss = 1/2 * np.sum(np.square(Theta[1:])) loss = xentropy_loss + alpha * l2_loss error = Y_proba - Y_train_one_hot gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]] Theta = Theta - eta * gradients logits = X_valid.dot(Theta) Y_proba = softmax(logits) xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1)) l2_loss = 1/2 * np.sum(np.square(Theta[1:])) loss = xentropy_loss + alpha * l2_loss if iteration % 500 == 0: print(iteration, loss) if loss &lt; best_loss: best_loss = loss else: print(iteration - 1, best_loss) print(iteration, loss, &quot;early stopping!&quot;) break . 0 4.7096017363419875 500 0.5739711987633519 1000 0.5435638529109127 1500 0.5355752782580262 2000 0.5331959249285544 2500 0.5325946767399383 2765 0.5325460966791898 2766 0.5325460971327977 early stopping! . logits = X_valid.dot(Theta) Y_proba = softmax(logits) y_predict = np.argmax(Y_proba, axis=1) accuracy_score = np.mean(y_predict == y_valid) accuracy_score . 1.0 . Still perfect, but faster. . Now let&#39;s plot the model&#39;s predictions on the whole dataset: . x0, x1 = np.meshgrid( np.linspace(0, 8, 500).reshape(-1, 1), np.linspace(0, 3.5, 200).reshape(-1, 1), ) X_new = np.c_[x0.ravel(), x1.ravel()] X_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new] logits = X_new_with_bias.dot(Theta) Y_proba = softmax(logits) y_predict = np.argmax(Y_proba, axis=1) zz1 = Y_proba[:, 1].reshape(x0.shape) zz = y_predict.reshape(x0.shape) plt.figure(figsize=(10, 4)) plt.plot(X[y==2, 0], X[y==2, 1], &quot;g^&quot;, label=&quot;Iris virginica&quot;) plt.plot(X[y==1, 0], X[y==1, 1], &quot;bs&quot;, label=&quot;Iris versicolor&quot;) plt.plot(X[y==0, 0], X[y==0, 1], &quot;yo&quot;, label=&quot;Iris setosa&quot;) from matplotlib.colors import ListedColormap custom_cmap = ListedColormap([&#39;#fafab0&#39;,&#39;#9898ff&#39;,&#39;#a0faa0&#39;]) plt.contourf(x0, x1, zz, cmap=custom_cmap) contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg) plt.clabel(contour, inline=1, fontsize=12) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.legend(loc=&quot;upper left&quot;, fontsize=14) plt.axis([0, 7, 0, 3.5]) plt.show() . And now let&#39;s measure the final model&#39;s accuracy on the test set: . logits = X_test.dot(Theta) Y_proba = softmax(logits) y_predict = np.argmax(Y_proba, axis=1) accuracy_score = np.mean(y_predict == y_test) accuracy_score . 0.9333333333333333 . Our perfect model turns out to have slight imperfections. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary. .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/04_training_linear_models",
            "relUrl": "/04_training_linear_models",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Training Deep Neural Networks",
            "content": "This notebook contains all the sample code and solutions to the exercises in chapter 11. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20 and TensorFlow â‰¥2.0. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass # TensorFlow â‰¥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;deep&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . . Vanishing/Exploding Gradients Problem . def logit(z): return 1 / (1 + np.exp(-z)) . z = np.linspace(-5, 5, 200) plt.plot([-5, 5], [0, 0], &#39;k-&#39;) plt.plot([-5, 5], [1, 1], &#39;k--&#39;) plt.plot([0, 0], [-0.2, 1.2], &#39;k-&#39;) plt.plot([-5, 5], [-3/4, 7/4], &#39;g--&#39;) plt.plot(z, logit(z), &quot;b-&quot;, linewidth=2) props = dict(facecolor=&#39;black&#39;, shrink=0.1) plt.annotate(&#39;Saturating&#39;, xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=&quot;center&quot;) plt.annotate(&#39;Saturating&#39;, xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=&quot;center&quot;) plt.annotate(&#39;Linear&#39;, xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=&quot;center&quot;) plt.grid(True) plt.title(&quot;Sigmoid activation function&quot;, fontsize=14) plt.axis([-5, 5, -0.2, 1.2]) save_fig(&quot;sigmoid_saturation_plot&quot;) plt.show() . Saving figure sigmoid_saturation_plot . Xavier and He Initialization . [name for name in dir(keras.initializers) if not name.startswith(&quot;_&quot;)] . [&#39;Constant&#39;, &#39;GlorotNormal&#39;, &#39;GlorotUniform&#39;, &#39;Identity&#39;, &#39;Initializer&#39;, &#39;Ones&#39;, &#39;Orthogonal&#39;, &#39;RandomNormal&#39;, &#39;RandomUniform&#39;, &#39;TruncatedNormal&#39;, &#39;VarianceScaling&#39;, &#39;Zeros&#39;, &#39;constant&#39;, &#39;deserialize&#39;, &#39;get&#39;, &#39;glorot_normal&#39;, &#39;glorot_uniform&#39;, &#39;he_normal&#39;, &#39;he_uniform&#39;, &#39;identity&#39;, &#39;lecun_normal&#39;, &#39;lecun_uniform&#39;, &#39;ones&#39;, &#39;orthogonal&#39;, &#39;serialize&#39;, &#39;zeros&#39;] . keras.layers.Dense(10, activation=&quot;relu&quot;, kernel_initializer=&quot;he_normal&quot;) . &lt;tensorflow.python.keras.layers.core.Dense at 0x1110c82e8&gt; . init = keras.initializers.VarianceScaling(scale=2., mode=&#39;fan_avg&#39;, distribution=&#39;uniform&#39;) keras.layers.Dense(10, activation=&quot;relu&quot;, kernel_initializer=init) . &lt;tensorflow.python.keras.layers.core.Dense at 0x1110adeb8&gt; . Nonsaturating Activation Functions . Leaky ReLU . def leaky_relu(z, alpha=0.01): return np.maximum(alpha*z, z) . plt.plot(z, leaky_relu(z, 0.05), &quot;b-&quot;, linewidth=2) plt.plot([-5, 5], [0, 0], &#39;k-&#39;) plt.plot([0, 0], [-0.5, 4.2], &#39;k-&#39;) plt.grid(True) props = dict(facecolor=&#39;black&#39;, shrink=0.1) plt.annotate(&#39;Leak&#39;, xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=&quot;center&quot;) plt.title(&quot;Leaky ReLU activation function&quot;, fontsize=14) plt.axis([-5, 5, -0.5, 4.2]) save_fig(&quot;leaky_relu_plot&quot;) plt.show() . Saving figure leaky_relu_plot . [m for m in dir(keras.activations) if not m.startswith(&quot;_&quot;)] . [&#39;deserialize&#39;, &#39;elu&#39;, &#39;exponential&#39;, &#39;get&#39;, &#39;hard_sigmoid&#39;, &#39;linear&#39;, &#39;relu&#39;, &#39;selu&#39;, &#39;serialize&#39;, &#39;sigmoid&#39;, &#39;softmax&#39;, &#39;softplus&#39;, &#39;softsign&#39;, &#39;tanh&#39;] . [m for m in dir(keras.layers) if &quot;relu&quot; in m.lower()] . [&#39;LeakyReLU&#39;, &#39;PReLU&#39;, &#39;ReLU&#39;, &#39;ThresholdedReLU&#39;] . Let&#39;s train a neural network on Fashion MNIST using the Leaky ReLU: . (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() X_train_full = X_train_full / 255.0 X_test = X_test / 255.0 X_valid, X_train = X_train_full[:5000], X_train_full[5000:] y_valid, y_train = y_train_full[:5000], y_train_full[5000:] . tf.random.set_seed(42) np.random.seed(42) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, kernel_initializer=&quot;he_normal&quot;), keras.layers.LeakyReLU(), keras.layers.Dense(100, kernel_initializer=&quot;he_normal&quot;), keras.layers.LeakyReLU(), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) . history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 3s 50us/sample - loss: 1.2806 - accuracy: 0.6250 - val_loss: 0.8883 - val_accuracy: 0.7152 Epoch 2/10 55000/55000 [==============================] - 2s 40us/sample - loss: 0.7954 - accuracy: 0.7373 - val_loss: 0.7135 - val_accuracy: 0.7648 Epoch 3/10 55000/55000 [==============================] - 2s 42us/sample - loss: 0.6816 - accuracy: 0.7727 - val_loss: 0.6356 - val_accuracy: 0.7882 Epoch 4/10 55000/55000 [==============================] - 2s 42us/sample - loss: 0.6215 - accuracy: 0.7935 - val_loss: 0.5922 - val_accuracy: 0.8012 Epoch 5/10 55000/55000 [==============================] - 2s 42us/sample - loss: 0.5830 - accuracy: 0.8081 - val_loss: 0.5596 - val_accuracy: 0.8172 Epoch 6/10 55000/55000 [==============================] - 2s 42us/sample - loss: 0.5553 - accuracy: 0.8155 - val_loss: 0.5338 - val_accuracy: 0.8240 Epoch 7/10 55000/55000 [==============================] - 2s 40us/sample - loss: 0.5340 - accuracy: 0.8221 - val_loss: 0.5157 - val_accuracy: 0.8310 Epoch 8/10 55000/55000 [==============================] - 2s 41us/sample - loss: 0.5172 - accuracy: 0.8265 - val_loss: 0.5035 - val_accuracy: 0.8336 Epoch 9/10 55000/55000 [==============================] - 2s 42us/sample - loss: 0.5036 - accuracy: 0.8299 - val_loss: 0.4950 - val_accuracy: 0.8354 Epoch 10/10 55000/55000 [==============================] - 2s 42us/sample - loss: 0.4922 - accuracy: 0.8324 - val_loss: 0.4797 - val_accuracy: 0.8430 . Now let&#39;s try PReLU: . tf.random.set_seed(42) np.random.seed(42) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, kernel_initializer=&quot;he_normal&quot;), keras.layers.PReLU(), keras.layers.Dense(100, kernel_initializer=&quot;he_normal&quot;), keras.layers.PReLU(), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) . history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 3s 61us/sample - loss: 1.3460 - accuracy: 0.6233 - val_loss: 0.9251 - val_accuracy: 0.7208 Epoch 2/10 55000/55000 [==============================] - 3s 56us/sample - loss: 0.8208 - accuracy: 0.7359 - val_loss: 0.7318 - val_accuracy: 0.7626 Epoch 3/10 55000/55000 [==============================] - 3s 55us/sample - loss: 0.6974 - accuracy: 0.7695 - val_loss: 0.6500 - val_accuracy: 0.7886 Epoch 4/10 55000/55000 [==============================] - 3s 55us/sample - loss: 0.6338 - accuracy: 0.7904 - val_loss: 0.6000 - val_accuracy: 0.8070 Epoch 5/10 55000/55000 [==============================] - 3s 57us/sample - loss: 0.5920 - accuracy: 0.8045 - val_loss: 0.5662 - val_accuracy: 0.8172 Epoch 6/10 55000/55000 [==============================] - 3s 55us/sample - loss: 0.5620 - accuracy: 0.8138 - val_loss: 0.5416 - val_accuracy: 0.8230 Epoch 7/10 55000/55000 [==============================] - 3s 55us/sample - loss: 0.5393 - accuracy: 0.8203 - val_loss: 0.5218 - val_accuracy: 0.8302 Epoch 8/10 55000/55000 [==============================] - 3s 57us/sample - loss: 0.5216 - accuracy: 0.8248 - val_loss: 0.5051 - val_accuracy: 0.8340 Epoch 9/10 55000/55000 [==============================] - 3s 59us/sample - loss: 0.5069 - accuracy: 0.8289 - val_loss: 0.4923 - val_accuracy: 0.8384 Epoch 10/10 55000/55000 [==============================] - 3s 62us/sample - loss: 0.4948 - accuracy: 0.8322 - val_loss: 0.4847 - val_accuracy: 0.8372 . ELU . def elu(z, alpha=1): return np.where(z &lt; 0, alpha * (np.exp(z) - 1), z) . plt.plot(z, elu(z), &quot;b-&quot;, linewidth=2) plt.plot([-5, 5], [0, 0], &#39;k-&#39;) plt.plot([-5, 5], [-1, -1], &#39;k--&#39;) plt.plot([0, 0], [-2.2, 3.2], &#39;k-&#39;) plt.grid(True) plt.title(r&quot;ELU activation function ($ alpha=1$)&quot;, fontsize=14) plt.axis([-5, 5, -2.2, 3.2]) save_fig(&quot;elu_plot&quot;) plt.show() . Saving figure elu_plot . Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer: . keras.layers.Dense(10, activation=&quot;elu&quot;) . &lt;tensorflow.python.keras.layers.core.Dense at 0x10dca50f0&gt; . SELU . This activation function was proposed in this great paper by GÃ¼nter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use â„“1 or â„“2 regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won&#39;t self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions. . from scipy.special import erfc # alpha and scale to self normalize with mean 0 and standard deviation 1 # (see equation 14 in the paper): alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1) scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2) . def selu(z, scale=scale_0_1, alpha=alpha_0_1): return scale * elu(z, alpha) . plt.plot(z, selu(z), &quot;b-&quot;, linewidth=2) plt.plot([-5, 5], [0, 0], &#39;k-&#39;) plt.plot([-5, 5], [-1.758, -1.758], &#39;k--&#39;) plt.plot([0, 0], [-2.2, 3.2], &#39;k-&#39;) plt.grid(True) plt.title(&quot;SELU activation function&quot;, fontsize=14) plt.axis([-5, 5, -2.2, 3.2]) save_fig(&quot;selu_plot&quot;) plt.show() . Saving figure selu_plot . By default, the SELU hyperparameters (scale and alpha) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem: . np.random.seed(42) Z = np.random.normal(size=(500, 100)) # standardized inputs for layer in range(1000): W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization Z = selu(np.dot(Z, W)) means = np.mean(Z, axis=0).mean() stds = np.std(Z, axis=0).mean() if layer % 100 == 0: print(&quot;Layer {}: mean {:.2f}, std deviation {:.2f}&quot;.format(layer, means, stds)) . Layer 0: mean -0.00, std deviation 1.00 Layer 100: mean 0.02, std deviation 0.96 Layer 200: mean 0.01, std deviation 0.90 Layer 300: mean -0.02, std deviation 0.92 Layer 400: mean 0.05, std deviation 0.89 Layer 500: mean 0.01, std deviation 0.93 Layer 600: mean 0.02, std deviation 0.92 Layer 700: mean -0.02, std deviation 0.90 Layer 800: mean 0.05, std deviation 0.83 Layer 900: mean 0.02, std deviation 1.00 . Using SELU is easy: . keras.layers.Dense(10, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;) . &lt;tensorflow.python.keras.layers.core.Dense at 0x158a45630&gt; . Let&#39;s create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function: . np.random.seed(42) tf.random.set_seed(42) . model = keras.models.Sequential() model.add(keras.layers.Flatten(input_shape=[28, 28])) model.add(keras.layers.Dense(300, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;)) for layer in range(99): model.add(keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;)) model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;)) . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) . Now let&#39;s train it. Do not forget to scale the inputs to mean 0 and standard deviation 1: . pixel_means = X_train.mean(axis=0, keepdims=True) pixel_stds = X_train.std(axis=0, keepdims=True) X_train_scaled = (X_train - pixel_means) / pixel_stds X_valid_scaled = (X_valid - pixel_means) / pixel_stds X_test_scaled = (X_test - pixel_means) / pixel_stds . history = model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/5 55000/55000 [==============================] - 35s 644us/sample - loss: 1.0197 - accuracy: 0.6154 - val_loss: 0.7386 - val_accuracy: 0.7348 Epoch 2/5 55000/55000 [==============================] - 33s 607us/sample - loss: 0.7149 - accuracy: 0.7401 - val_loss: 0.6187 - val_accuracy: 0.7774 Epoch 3/5 55000/55000 [==============================] - 32s 583us/sample - loss: 0.6193 - accuracy: 0.7803 - val_loss: 0.5926 - val_accuracy: 0.8036 Epoch 4/5 55000/55000 [==============================] - 32s 586us/sample - loss: 0.5555 - accuracy: 0.8043 - val_loss: 0.5208 - val_accuracy: 0.8262 Epoch 5/5 55000/55000 [==============================] - 32s 573us/sample - loss: 0.5159 - accuracy: 0.8238 - val_loss: 0.4790 - val_accuracy: 0.8358 . Now look at what happens if we try to use the ReLU activation function instead: . np.random.seed(42) tf.random.set_seed(42) . model = keras.models.Sequential() model.add(keras.layers.Flatten(input_shape=[28, 28])) model.add(keras.layers.Dense(300, activation=&quot;relu&quot;, kernel_initializer=&quot;he_normal&quot;)) for layer in range(99): model.add(keras.layers.Dense(100, activation=&quot;relu&quot;, kernel_initializer=&quot;he_normal&quot;)) model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;)) . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) . history = model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/5 55000/55000 [==============================] - 18s 319us/sample - loss: 1.9174 - accuracy: 0.2242 - val_loss: 1.3856 - val_accuracy: 0.3846 Epoch 2/5 55000/55000 [==============================] - 15s 279us/sample - loss: 1.2147 - accuracy: 0.4750 - val_loss: 1.0691 - val_accuracy: 0.5510 Epoch 3/5 55000/55000 [==============================] - 15s 281us/sample - loss: 0.9576 - accuracy: 0.6025 - val_loss: 0.7688 - val_accuracy: 0.7036 Epoch 4/5 55000/55000 [==============================] - 15s 281us/sample - loss: 0.8116 - accuracy: 0.6762 - val_loss: 0.7276 - val_accuracy: 0.7288 Epoch 5/5 55000/55000 [==============================] - 15s 278us/sample - loss: 0.8167 - accuracy: 0.6862 - val_loss: 0.7697 - val_accuracy: 0.7032 . Not great at all, we suffered from the vanishing/exploding gradients problem. . Batch Normalization . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.BatchNormalization(), keras.layers.Dense(300, activation=&quot;relu&quot;), keras.layers.BatchNormalization(), keras.layers.Dense(100, activation=&quot;relu&quot;), keras.layers.BatchNormalization(), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) . model.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_3 (Flatten) (None, 784) 0 _________________________________________________________________ batch_normalization_v2 (Batc (None, 784) 3136 _________________________________________________________________ dense_210 (Dense) (None, 300) 235500 _________________________________________________________________ batch_normalization_v2_1 (Ba (None, 300) 1200 _________________________________________________________________ dense_211 (Dense) (None, 100) 30100 _________________________________________________________________ batch_normalization_v2_2 (Ba (None, 100) 400 _________________________________________________________________ dense_212 (Dense) (None, 10) 1010 ================================================================= Total params: 271,346 Trainable params: 268,978 Non-trainable params: 2,368 _________________________________________________________________ . bn1 = model.layers[1] [(var.name, var.trainable) for var in bn1.variables] . [(&#39;batch_normalization_v2/gamma:0&#39;, True), (&#39;batch_normalization_v2/beta:0&#39;, True), (&#39;batch_normalization_v2/moving_mean:0&#39;, False), (&#39;batch_normalization_v2/moving_variance:0&#39;, False)] . bn1.updates . ListWrapper([&lt;tf.Operation &#39;batch_normalization_v2/cond_2/Identity&#39; type=Identity&gt;, &lt;tf.Operation &#39;batch_normalization_v2/cond_3/Identity&#39; type=Identity&gt;]) . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) . history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 5s 85us/sample - loss: 0.8756 - accuracy: 0.7140 - val_loss: 0.5514 - val_accuracy: 0.8212 Epoch 2/10 55000/55000 [==============================] - 4s 74us/sample - loss: 0.5765 - accuracy: 0.8033 - val_loss: 0.4742 - val_accuracy: 0.8436 Epoch 3/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.5146 - accuracy: 0.8216 - val_loss: 0.4382 - val_accuracy: 0.8530 Epoch 4/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.4821 - accuracy: 0.8322 - val_loss: 0.4170 - val_accuracy: 0.8604 Epoch 5/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.4589 - accuracy: 0.8402 - val_loss: 0.4003 - val_accuracy: 0.8658 Epoch 6/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.4428 - accuracy: 0.8459 - val_loss: 0.3883 - val_accuracy: 0.8698 Epoch 7/10 55000/55000 [==============================] - 4s 78us/sample - loss: 0.4220 - accuracy: 0.8521 - val_loss: 0.3792 - val_accuracy: 0.8720 Epoch 8/10 55000/55000 [==============================] - 4s 77us/sample - loss: 0.4150 - accuracy: 0.8546 - val_loss: 0.3696 - val_accuracy: 0.8754 Epoch 9/10 55000/55000 [==============================] - 4s 77us/sample - loss: 0.4013 - accuracy: 0.8589 - val_loss: 0.3629 - val_accuracy: 0.8746 Epoch 10/10 55000/55000 [==============================] - 4s 74us/sample - loss: 0.3931 - accuracy: 0.8615 - val_loss: 0.3581 - val_accuracy: 0.8766 . Sometimes applying BN before the activation function works better (there&#39;s a debate on this topic). Moreover, the layer before a BatchNormalization layer does not need to have bias terms, since the BatchNormalization layer some as well, it would be a waste of parameters, so you can set use_bias=False when creating those layers: . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.BatchNormalization(), keras.layers.Dense(300, use_bias=False), keras.layers.BatchNormalization(), keras.layers.Activation(&quot;relu&quot;), keras.layers.Dense(100, use_bias=False), keras.layers.BatchNormalization(), keras.layers.Activation(&quot;relu&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) . history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 5s 89us/sample - loss: 0.8617 - accuracy: 0.7095 - val_loss: 0.5649 - val_accuracy: 0.8102 Epoch 2/10 55000/55000 [==============================] - 4s 76us/sample - loss: 0.5803 - accuracy: 0.8015 - val_loss: 0.4833 - val_accuracy: 0.8344 Epoch 3/10 55000/55000 [==============================] - 4s 79us/sample - loss: 0.5153 - accuracy: 0.8208 - val_loss: 0.4463 - val_accuracy: 0.8462 Epoch 4/10 55000/55000 [==============================] - 4s 76us/sample - loss: 0.4846 - accuracy: 0.8307 - val_loss: 0.4256 - val_accuracy: 0.8530 Epoch 5/10 55000/55000 [==============================] - 4s 79us/sample - loss: 0.4576 - accuracy: 0.8402 - val_loss: 0.4106 - val_accuracy: 0.8590 Epoch 6/10 55000/55000 [==============================] - 4s 77us/sample - loss: 0.4401 - accuracy: 0.8467 - val_loss: 0.3973 - val_accuracy: 0.8610 Epoch 7/10 55000/55000 [==============================] - 4s 78us/sample - loss: 0.4296 - accuracy: 0.8482 - val_loss: 0.3899 - val_accuracy: 0.8650 Epoch 8/10 55000/55000 [==============================] - 4s 76us/sample - loss: 0.4127 - accuracy: 0.8559 - val_loss: 0.3818 - val_accuracy: 0.8658 Epoch 9/10 55000/55000 [==============================] - 4s 78us/sample - loss: 0.4007 - accuracy: 0.8588 - val_loss: 0.3741 - val_accuracy: 0.8682 Epoch 10/10 55000/55000 [==============================] - 4s 79us/sample - loss: 0.3929 - accuracy: 0.8621 - val_loss: 0.3694 - val_accuracy: 0.8734 . Gradient Clipping . All Keras optimizers accept clipnorm or clipvalue arguments: . optimizer = keras.optimizers.SGD(clipvalue=1.0) . optimizer = keras.optimizers.SGD(clipnorm=1.0) . Reusing Pretrained Layers . Reusing a Keras model . Let&#39;s split the fashion MNIST training set in two: . X_train_A: all images of all items except for sandals and shirts (classes 5 and 6). | X_train_B: a much smaller training set of just the first 200 images of sandals or shirts. | . The validation set and the test set are also split this way, but without restricting the number of images. . We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using Dense layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter). . def split_dataset(X, y): y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts y_A = y[~y_5_or_6] y_A[y_A &gt; 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7 y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)? return ((X[~y_5_or_6], y_A), (X[y_5_or_6], y_B)) (X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train) (X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid) (X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test) X_train_B = X_train_B[:200] y_train_B = y_train_B[:200] . X_train_A.shape . (43986, 28, 28) . X_train_B.shape . (200, 28, 28) . y_train_A[:30] . array([4, 0, 5, 7, 7, 7, 4, 4, 3, 4, 0, 1, 6, 3, 4, 3, 2, 6, 5, 3, 4, 5, 1, 3, 4, 2, 0, 6, 7, 1], dtype=uint8) . y_train_B[:30] . array([1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=float32) . tf.random.set_seed(42) np.random.seed(42) . model_A = keras.models.Sequential() model_A.add(keras.layers.Flatten(input_shape=[28, 28])) for n_hidden in (300, 100, 50, 50, 50): model_A.add(keras.layers.Dense(n_hidden, activation=&quot;selu&quot;)) model_A.add(keras.layers.Dense(8, activation=&quot;softmax&quot;)) . model_A.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) . history = model_A.fit(X_train_A, y_train_A, epochs=20, validation_data=(X_valid_A, y_valid_A)) . Train on 43986 samples, validate on 4014 samples Epoch 1/20 43986/43986 [==============================] - 3s 78us/sample - loss: 0.5887 - accuracy: 0.8123 - val_loss: 0.3749 - val_accuracy: 0.8734 Epoch 2/20 43986/43986 [==============================] - 3s 69us/sample - loss: 0.3516 - accuracy: 0.8793 - val_loss: 0.3223 - val_accuracy: 0.8874 Epoch 3/20 43986/43986 [==============================] - 3s 68us/sample - loss: 0.3160 - accuracy: 0.8894 - val_loss: 0.3009 - val_accuracy: 0.8956 Epoch 4/20 43986/43986 [==============================] - 3s 70us/sample - loss: 0.2963 - accuracy: 0.8979 - val_loss: 0.2850 - val_accuracy: 0.9036 Epoch 5/20 43986/43986 [==============================] - 3s 68us/sample - loss: 0.2825 - accuracy: 0.9035 - val_loss: 0.2767 - val_accuracy: 0.9076 Epoch 6/20 43986/43986 [==============================] - 3s 69us/sample - loss: 0.2720 - accuracy: 0.9068 - val_loss: 0.2672 - val_accuracy: 0.9093 Epoch 7/20 43986/43986 [==============================] - 3s 72us/sample - loss: 0.2638 - accuracy: 0.9093 - val_loss: 0.2658 - val_accuracy: 0.9103 Epoch 8/20 43986/43986 [==============================] - 3s 70us/sample - loss: 0.2570 - accuracy: 0.9120 - val_loss: 0.2592 - val_accuracy: 0.9106 Epoch 9/20 43986/43986 [==============================] - 3s 71us/sample - loss: 0.2514 - accuracy: 0.9139 - val_loss: 0.2570 - val_accuracy: 0.9128 Epoch 10/20 43986/43986 [==============================] - 3s 72us/sample - loss: 0.2465 - accuracy: 0.9166 - val_loss: 0.2557 - val_accuracy: 0.9108 Epoch 11/20 43986/43986 [==============================] - 3s 69us/sample - loss: 0.2418 - accuracy: 0.9178 - val_loss: 0.2484 - val_accuracy: 0.9178 Epoch 12/20 43986/43986 [==============================] - 3s 70us/sample - loss: 0.2379 - accuracy: 0.9192 - val_loss: 0.2461 - val_accuracy: 0.9178 Epoch 13/20 43986/43986 [==============================] - 3s 71us/sample - loss: 0.2342 - accuracy: 0.9199 - val_loss: 0.2425 - val_accuracy: 0.9188 Epoch 14/20 43986/43986 [==============================] - 3s 68us/sample - loss: 0.2313 - accuracy: 0.9215 - val_loss: 0.2412 - val_accuracy: 0.9185 Epoch 15/20 43986/43986 [==============================] - 3s 68us/sample - loss: 0.2280 - accuracy: 0.9222 - val_loss: 0.2382 - val_accuracy: 0.9173 Epoch 16/20 43986/43986 [==============================] - 3s 71us/sample - loss: 0.2252 - accuracy: 0.9224 - val_loss: 0.2360 - val_accuracy: 0.9205 Epoch 17/20 43986/43986 [==============================] - 3s 71us/sample - loss: 0.2229 - accuracy: 0.9232 - val_loss: 0.2419 - val_accuracy: 0.9158 Epoch 18/20 43986/43986 [==============================] - 3s 71us/sample - loss: 0.2195 - accuracy: 0.9249 - val_loss: 0.2357 - val_accuracy: 0.9170 Epoch 19/20 43986/43986 [==============================] - 3s 68us/sample - loss: 0.2177 - accuracy: 0.9254 - val_loss: 0.2331 - val_accuracy: 0.9200 Epoch 20/20 43986/43986 [==============================] - 3s 70us/sample - loss: 0.2154 - accuracy: 0.9260 - val_loss: 0.2372 - val_accuracy: 0.9158 . model_A.save(&quot;my_model_A.h5&quot;) . model_B = keras.models.Sequential() model_B.add(keras.layers.Flatten(input_shape=[28, 28])) for n_hidden in (300, 100, 50, 50, 50): model_B.add(keras.layers.Dense(n_hidden, activation=&quot;selu&quot;)) model_B.add(keras.layers.Dense(1, activation=&quot;sigmoid&quot;)) . model_B.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) . history = model_B.fit(X_train_B, y_train_B, epochs=20, validation_data=(X_valid_B, y_valid_B)) . Train on 200 samples, validate on 986 samples Epoch 1/20 200/200 [==============================] - 0s 2ms/sample - loss: 0.9537 - accuracy: 0.4800 - val_loss: 0.6472 - val_accuracy: 0.5710 Epoch 2/20 200/200 [==============================] - 0s 318us/sample - loss: 0.5805 - accuracy: 0.6850 - val_loss: 0.4863 - val_accuracy: 0.8428 Epoch 3/20 200/200 [==============================] - 0s 318us/sample - loss: 0.4561 - accuracy: 0.8750 - val_loss: 0.4116 - val_accuracy: 0.8905 Epoch 4/20 200/200 [==============================] - 0s 308us/sample - loss: 0.3885 - accuracy: 0.9100 - val_loss: 0.3650 - val_accuracy: 0.9148 Epoch 5/20 200/200 [==============================] - 0s 311us/sample - loss: 0.3426 - accuracy: 0.9250 - val_loss: 0.3308 - val_accuracy: 0.9270 Epoch 6/20 200/200 [==============================] - 0s 317us/sample - loss: 0.3084 - accuracy: 0.9300 - val_loss: 0.3044 - val_accuracy: 0.9371 Epoch 7/20 200/200 [==============================] - 0s 309us/sample - loss: 0.2810 - accuracy: 0.9400 - val_loss: 0.2806 - val_accuracy: 0.9432 Epoch 8/20 200/200 [==============================] - 0s 313us/sample - loss: 0.2572 - accuracy: 0.9500 - val_loss: 0.2607 - val_accuracy: 0.9462 Epoch 9/20 200/200 [==============================] - 0s 312us/sample - loss: 0.2372 - accuracy: 0.9600 - val_loss: 0.2439 - val_accuracy: 0.9513 Epoch 10/20 200/200 [==============================] - 0s 319us/sample - loss: 0.2202 - accuracy: 0.9600 - val_loss: 0.2290 - val_accuracy: 0.9523 Epoch 11/20 200/200 [==============================] - 0s 315us/sample - loss: 0.2047 - accuracy: 0.9650 - val_loss: 0.2161 - val_accuracy: 0.9564 Epoch 12/20 200/200 [==============================] - 0s 325us/sample - loss: 0.1917 - accuracy: 0.9700 - val_loss: 0.2046 - val_accuracy: 0.9584 Epoch 13/20 200/200 [==============================] - 0s 335us/sample - loss: 0.1798 - accuracy: 0.9750 - val_loss: 0.1944 - val_accuracy: 0.9604 Epoch 14/20 200/200 [==============================] - 0s 319us/sample - loss: 0.1690 - accuracy: 0.9750 - val_loss: 0.1860 - val_accuracy: 0.9604 Epoch 15/20 200/200 [==============================] - 0s 319us/sample - loss: 0.1594 - accuracy: 0.9850 - val_loss: 0.1774 - val_accuracy: 0.9635 Epoch 16/20 200/200 [==============================] - 0s 343us/sample - loss: 0.1508 - accuracy: 0.9850 - val_loss: 0.1691 - val_accuracy: 0.9675 Epoch 17/20 200/200 [==============================] - 0s 328us/sample - loss: 0.1426 - accuracy: 0.9900 - val_loss: 0.1621 - val_accuracy: 0.9686 Epoch 18/20 200/200 [==============================] - 0s 340us/sample - loss: 0.1355 - accuracy: 0.9900 - val_loss: 0.1558 - val_accuracy: 0.9706 Epoch 19/20 200/200 [==============================] - 0s 306us/sample - loss: 0.1288 - accuracy: 0.9900 - val_loss: 0.1505 - val_accuracy: 0.9706 Epoch 20/20 200/200 [==============================] - 0s 312us/sample - loss: 0.1230 - accuracy: 0.9900 - val_loss: 0.1454 - val_accuracy: 0.9716 . model.summary() . Model: &#34;sequential_4&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_4 (Flatten) (None, 784) 0 _________________________________________________________________ batch_normalization_v2_3 (Ba (None, 784) 3136 _________________________________________________________________ dense_213 (Dense) (None, 300) 235500 _________________________________________________________________ batch_normalization_v2_4 (Ba (None, 300) 1200 _________________________________________________________________ activation (Activation) (None, 300) 0 _________________________________________________________________ dense_214 (Dense) (None, 100) 30100 _________________________________________________________________ activation_1 (Activation) (None, 100) 0 _________________________________________________________________ batch_normalization_v2_5 (Ba (None, 100) 400 _________________________________________________________________ dense_215 (Dense) (None, 10) 1010 ================================================================= Total params: 271,346 Trainable params: 268,978 Non-trainable params: 2,368 _________________________________________________________________ . model_A = keras.models.load_model(&quot;my_model_A.h5&quot;) model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) model_B_on_A.add(keras.layers.Dense(1, activation=&quot;sigmoid&quot;)) . model_A_clone = keras.models.clone_model(model_A) model_A_clone.set_weights(model_A.get_weights()) . for layer in model_B_on_A.layers[:-1]: layer.trainable = False model_B_on_A.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) . history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B)) for layer in model_B_on_A.layers[:-1]: layer.trainable = True model_B_on_A.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B)) . Train on 200 samples, validate on 986 samples Epoch 1/4 200/200 [==============================] - 0s 2ms/sample - loss: 0.5851 - accuracy: 0.6600 - val_loss: 0.5855 - val_accuracy: 0.6318 Epoch 2/4 200/200 [==============================] - 0s 303us/sample - loss: 0.5484 - accuracy: 0.6850 - val_loss: 0.5484 - val_accuracy: 0.6775 Epoch 3/4 200/200 [==============================] - 0s 294us/sample - loss: 0.5116 - accuracy: 0.7250 - val_loss: 0.5141 - val_accuracy: 0.7160 Epoch 4/4 200/200 [==============================] - 0s 316us/sample - loss: 0.4779 - accuracy: 0.7450 - val_loss: 0.4859 - val_accuracy: 0.7363 Train on 200 samples, validate on 986 samples Epoch 1/16 200/200 [==============================] - 0s 2ms/sample - loss: 0.3989 - accuracy: 0.8050 - val_loss: 0.3419 - val_accuracy: 0.8702 Epoch 2/16 200/200 [==============================] - 0s 328us/sample - loss: 0.2795 - accuracy: 0.9300 - val_loss: 0.2624 - val_accuracy: 0.9280 Epoch 3/16 200/200 [==============================] - 0s 319us/sample - loss: 0.2128 - accuracy: 0.9650 - val_loss: 0.2150 - val_accuracy: 0.9544 Epoch 4/16 200/200 [==============================] - 0s 318us/sample - loss: 0.1720 - accuracy: 0.9800 - val_loss: 0.1826 - val_accuracy: 0.9635 Epoch 5/16 200/200 [==============================] - 0s 317us/sample - loss: 0.1436 - accuracy: 0.9800 - val_loss: 0.1586 - val_accuracy: 0.9736 Epoch 6/16 200/200 [==============================] - 0s 317us/sample - loss: 0.1231 - accuracy: 0.9850 - val_loss: 0.1407 - val_accuracy: 0.9807 Epoch 7/16 200/200 [==============================] - 0s 325us/sample - loss: 0.1074 - accuracy: 0.9900 - val_loss: 0.1270 - val_accuracy: 0.9828 Epoch 8/16 200/200 [==============================] - 0s 326us/sample - loss: 0.0953 - accuracy: 0.9950 - val_loss: 0.1158 - val_accuracy: 0.9848 Epoch 9/16 200/200 [==============================] - 0s 319us/sample - loss: 0.0854 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9878 Epoch 10/16 200/200 [==============================] - 0s 322us/sample - loss: 0.0781 - accuracy: 1.0000 - val_loss: 0.1007 - val_accuracy: 0.9888 Epoch 11/16 200/200 [==============================] - 0s 316us/sample - loss: 0.0718 - accuracy: 1.0000 - val_loss: 0.0944 - val_accuracy: 0.9888 Epoch 12/16 200/200 [==============================] - 0s 319us/sample - loss: 0.0662 - accuracy: 1.0000 - val_loss: 0.0891 - val_accuracy: 0.9899 Epoch 13/16 200/200 [==============================] - 0s 318us/sample - loss: 0.0613 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 0.9899 Epoch 14/16 200/200 [==============================] - 0s 332us/sample - loss: 0.0574 - accuracy: 1.0000 - val_loss: 0.0806 - val_accuracy: 0.9899 Epoch 15/16 200/200 [==============================] - 0s 320us/sample - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9899 Epoch 16/16 200/200 [==============================] - 0s 320us/sample - loss: 0.0505 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9899 . So, what&#39;s the final verdict? . model_B.evaluate(X_test_B, y_test_B) . 2000/2000 [==============================] - 0s 41us/sample - loss: 0.1431 - accuracy: 0.9705 . [0.1430660070180893, 0.9705] . model_B_on_A.evaluate(X_test_B, y_test_B) . 2000/2000 [==============================] - 0s 38us/sample - loss: 0.0689 - accuracy: 0.9925 . [0.06887910133600235, 0.9925] . Great! We got quite a bit of transfer: the error rate dropped by a factor of almost 4! . (100 - 97.05) / (100 - 99.25) . 3.933333333333337 . Faster Optimizers . Momentum optimization . optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9) . Nesterov Accelerated Gradient . optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True) . AdaGrad . optimizer = keras.optimizers.Adagrad(lr=0.001) . RMSProp . optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9) . Adam Optimization . optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999) . Adamax Optimization . optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999) . Nadam Optimization . optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999) . Learning Rate Scheduling . Power Scheduling . lr = lr0 / (1 + steps / s)**c . Keras uses c=1 and s = 1 / decay | . optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4) . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=optimizer, metrics=[&quot;accuracy&quot;]) . n_epochs = 25 history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/25 55000/55000 [==============================] - 4s 66us/sample - loss: 0.4840 - accuracy: 0.8296 - val_loss: 0.4038 - val_accuracy: 0.8630 Epoch 2/25 55000/55000 [==============================] - 3s 63us/sample - loss: 0.3787 - accuracy: 0.8653 - val_loss: 0.3846 - val_accuracy: 0.8706 Epoch 3/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.3461 - accuracy: 0.8770 - val_loss: 0.3606 - val_accuracy: 0.8776 Epoch 4/25 55000/55000 [==============================] - 3s 63us/sample - loss: 0.3248 - accuracy: 0.8844 - val_loss: 0.3661 - val_accuracy: 0.8738 Epoch 5/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.3092 - accuracy: 0.8902 - val_loss: 0.3516 - val_accuracy: 0.8792 Epoch 6/25 55000/55000 [==============================] - 3s 63us/sample - loss: 0.2967 - accuracy: 0.8938 - val_loss: 0.3467 - val_accuracy: 0.8810 Epoch 7/25 55000/55000 [==============================] - 3s 63us/sample - loss: 0.2862 - accuracy: 0.8967 - val_loss: 0.3398 - val_accuracy: 0.8844 Epoch 8/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.2771 - accuracy: 0.8997 - val_loss: 0.3384 - val_accuracy: 0.8832 Epoch 9/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.2696 - accuracy: 0.9035 - val_loss: 0.3345 - val_accuracy: 0.8860 Epoch 10/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.2628 - accuracy: 0.9057 - val_loss: 0.3343 - val_accuracy: 0.8830 Epoch 11/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.2568 - accuracy: 0.9083 - val_loss: 0.3290 - val_accuracy: 0.8882 Epoch 12/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.2510 - accuracy: 0.9099 - val_loss: 0.3243 - val_accuracy: 0.8904 Epoch 13/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.2459 - accuracy: 0.9118 - val_loss: 0.3271 - val_accuracy: 0.8874 Epoch 14/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.2415 - accuracy: 0.9130 - val_loss: 0.3259 - val_accuracy: 0.8886 Epoch 15/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.2370 - accuracy: 0.9157 - val_loss: 0.3249 - val_accuracy: 0.8896 Epoch 16/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.2332 - accuracy: 0.9177 - val_loss: 0.3267 - val_accuracy: 0.8892 Epoch 17/25 55000/55000 [==============================] - 3s 63us/sample - loss: 0.2296 - accuracy: 0.9177 - val_loss: 0.3251 - val_accuracy: 0.8880 Epoch 18/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.2257 - accuracy: 0.9194 - val_loss: 0.3221 - val_accuracy: 0.8900 Epoch 19/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.2228 - accuracy: 0.9212 - val_loss: 0.3237 - val_accuracy: 0.8910 Epoch 20/25 55000/55000 [==============================] - 3s 60us/sample - loss: 0.2198 - accuracy: 0.9223 - val_loss: 0.3217 - val_accuracy: 0.8904 Epoch 21/25 55000/55000 [==============================] - 3s 63us/sample - loss: 0.2166 - accuracy: 0.9238 - val_loss: 0.3185 - val_accuracy: 0.8938 Epoch 22/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.2140 - accuracy: 0.9252 - val_loss: 0.3212 - val_accuracy: 0.8902 Epoch 23/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.2113 - accuracy: 0.9256 - val_loss: 0.3235 - val_accuracy: 0.8898 Epoch 24/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.2088 - accuracy: 0.9262 - val_loss: 0.3216 - val_accuracy: 0.8930 Epoch 25/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.2061 - accuracy: 0.9273 - val_loss: 0.3199 - val_accuracy: 0.8922 . learning_rate = 0.01 decay = 1e-4 batch_size = 32 n_steps_per_epoch = len(X_train) // batch_size epochs = np.arange(n_epochs) lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch) plt.plot(epochs, lrs, &quot;o-&quot;) plt.axis([0, n_epochs - 1, 0, 0.01]) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Learning Rate&quot;) plt.title(&quot;Power Scheduling&quot;, fontsize=14) plt.grid(True) plt.show() . Exponential Scheduling . lr = lr0 * 0.1**(epoch / s) . def exponential_decay_fn(epoch): return 0.01 * 0.1**(epoch / 20) . def exponential_decay(lr0, s): def exponential_decay_fn(epoch): return lr0 * 0.1**(epoch / s) return exponential_decay_fn exponential_decay_fn = exponential_decay(lr0=0.01, s=20) . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;, metrics=[&quot;accuracy&quot;]) n_epochs = 25 . lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn) history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid), callbacks=[lr_scheduler]) . Train on 55000 samples, validate on 5000 samples Epoch 1/25 55000/55000 [==============================] - 6s 107us/sample - loss: 0.8245 - accuracy: 0.7595 - val_loss: 1.0870 - val_accuracy: 0.7106 Epoch 2/25 55000/55000 [==============================] - 6s 101us/sample - loss: 0.6391 - accuracy: 0.8064 - val_loss: 0.6125 - val_accuracy: 0.8160 Epoch 3/25 55000/55000 [==============================] - 6s 101us/sample - loss: 0.5962 - accuracy: 0.8174 - val_loss: 0.6526 - val_accuracy: 0.8086 Epoch 4/25 55000/55000 [==============================] - 5s 99us/sample - loss: 0.5420 - accuracy: 0.8306 - val_loss: 0.7521 - val_accuracy: 0.7766 Epoch 5/25 55000/55000 [==============================] - 5s 100us/sample - loss: 0.4853 - accuracy: 0.8460 - val_loss: 0.5616 - val_accuracy: 0.8314 Epoch 6/25 55000/55000 [==============================] - 5s 98us/sample - loss: 0.4443 - accuracy: 0.8571 - val_loss: 0.5430 - val_accuracy: 0.8664 Epoch 7/25 55000/55000 [==============================] - 5s 99us/sample - loss: 0.4128 - accuracy: 0.8687 - val_loss: 0.4954 - val_accuracy: 0.8610 Epoch 8/25 55000/55000 [==============================] - 6s 100us/sample - loss: 0.3763 - accuracy: 0.8773 - val_loss: 0.5770 - val_accuracy: 0.8578 Epoch 9/25 55000/55000 [==============================] - 6s 102us/sample - loss: 0.3459 - accuracy: 0.8847 - val_loss: 0.5267 - val_accuracy: 0.8688 Epoch 10/25 55000/55000 [==============================] - 5s 99us/sample - loss: 0.3250 - accuracy: 0.8931 - val_loss: 0.4606 - val_accuracy: 0.8644 Epoch 11/25 55000/55000 [==============================] - 5s 97us/sample - loss: 0.2984 - accuracy: 0.9010 - val_loss: 0.5083 - val_accuracy: 0.8610 Epoch 12/25 55000/55000 [==============================] - 5s 99us/sample - loss: 0.2736 - accuracy: 0.9080 - val_loss: 0.4497 - val_accuracy: 0.8826 Epoch 13/25 55000/55000 [==============================] - 5s 99us/sample - loss: 0.2603 - accuracy: 0.9128 - val_loss: 0.4366 - val_accuracy: 0.8808 Epoch 14/25 55000/55000 [==============================] - 5s 100us/sample - loss: 0.2382 - accuracy: 0.9197 - val_loss: 0.4692 - val_accuracy: 0.8828 Epoch 15/25 55000/55000 [==============================] - 6s 102us/sample - loss: 0.2240 - accuracy: 0.9252 - val_loss: 0.4609 - val_accuracy: 0.8774 Epoch 16/25 55000/55000 [==============================] - 5s 99us/sample - loss: 0.2020 - accuracy: 0.9306 - val_loss: 0.4950 - val_accuracy: 0.8808 Epoch 17/25 55000/55000 [==============================] - 5s 100us/sample - loss: 0.1950 - accuracy: 0.9340 - val_loss: 0.4985 - val_accuracy: 0.8856 Epoch 18/25 55000/55000 [==============================] - 6s 102us/sample - loss: 0.1785 - accuracy: 0.9388 - val_loss: 0.5071 - val_accuracy: 0.8854 Epoch 19/25 55000/55000 [==============================] - 5s 100us/sample - loss: 0.1649 - accuracy: 0.9447 - val_loss: 0.4798 - val_accuracy: 0.8890 Epoch 20/25 55000/55000 [==============================] - 5s 100us/sample - loss: 0.1561 - accuracy: 0.9471 - val_loss: 0.5023 - val_accuracy: 0.8896 Epoch 21/25 55000/55000 [==============================] - 5s 98us/sample - loss: 0.1442 - accuracy: 0.9520 - val_loss: 0.5253 - val_accuracy: 0.8952 Epoch 22/25 55000/55000 [==============================] - 5s 99us/sample - loss: 0.1369 - accuracy: 0.9540 - val_loss: 0.5558 - val_accuracy: 0.8922 Epoch 23/25 55000/55000 [==============================] - 5s 98us/sample - loss: 0.1277 - accuracy: 0.9576 - val_loss: 0.5786 - val_accuracy: 0.8908 Epoch 24/25 55000/55000 [==============================] - 5s 99us/sample - loss: 0.1204 - accuracy: 0.9611 - val_loss: 0.5991 - val_accuracy: 0.8902 Epoch 25/25 55000/55000 [==============================] - 6s 102us/sample - loss: 0.1130 - accuracy: 0.9638 - val_loss: 0.5984 - val_accuracy: 0.8894 . plt.plot(history.epoch, history.history[&quot;lr&quot;], &quot;o-&quot;) plt.axis([0, n_epochs - 1, 0, 0.011]) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Learning Rate&quot;) plt.title(&quot;Exponential Scheduling&quot;, fontsize=14) plt.grid(True) plt.show() . The schedule function can take the current learning rate as a second argument: . def exponential_decay_fn(epoch, lr): return lr * 0.1**(1 / 20) . If you want to update the learning rate at each iteration rather than at each epoch, you must write your own callback class: . K = keras.backend class ExponentialDecay(keras.callbacks.Callback): def __init__(self, s=40000): super().__init__() self.s = s def on_batch_begin(self, batch, logs=None): # Note: the `batch` argument is reset at each epoch lr = K.get_value(self.model.optimizer.lr) K.set_value(self.model.optimizer.lr, lr * 0.1**(1 / s)) def on_epoch_end(self, epoch, logs=None): logs = logs or {} logs[&#39;lr&#39;] = K.get_value(self.model.optimizer.lr) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) lr0 = 0.01 optimizer = keras.optimizers.Nadam(lr=lr0) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=optimizer, metrics=[&quot;accuracy&quot;]) n_epochs = 25 s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32) exp_decay = ExponentialDecay(s) history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid), callbacks=[exp_decay]) . Train on 55000 samples, validate on 5000 samples Epoch 1/25 55000/55000 [==============================] - 7s 132us/sample - loss: 0.8067 - accuracy: 0.7678 - val_loss: 0.7942 - val_accuracy: 0.7780 Epoch 2/25 55000/55000 [==============================] - 7s 122us/sample - loss: 0.6784 - accuracy: 0.7937 - val_loss: 0.8375 - val_accuracy: 0.8120 Epoch 3/25 55000/55000 [==============================] - 6s 114us/sample - loss: 0.6060 - accuracy: 0.8148 - val_loss: 0.6303 - val_accuracy: 0.8304 Epoch 4/25 55000/55000 [==============================] - 6s 114us/sample - loss: 0.5279 - accuracy: 0.8341 - val_loss: 0.5724 - val_accuracy: 0.8196 Epoch 5/25 55000/55000 [==============================] - 6s 112us/sample - loss: 0.4803 - accuracy: 0.8486 - val_loss: 0.5488 - val_accuracy: 0.8486 Epoch 6/25 55000/55000 [==============================] - 6s 113us/sample - loss: 0.4305 - accuracy: 0.8611 - val_loss: 0.4778 - val_accuracy: 0.8470 Epoch 7/25 55000/55000 [==============================] - 6s 112us/sample - loss: 0.3969 - accuracy: 0.8699 - val_loss: 0.4922 - val_accuracy: 0.8584 Epoch 8/25 55000/55000 [==============================] - 6s 111us/sample - loss: 0.3799 - accuracy: 0.8777 - val_loss: 0.5417 - val_accuracy: 0.8614 Epoch 9/25 55000/55000 [==============================] - 6s 111us/sample - loss: 0.3475 - accuracy: 0.8851 - val_loss: 0.5032 - val_accuracy: 0.8734 Epoch 10/25 55000/55000 [==============================] - 6s 110us/sample - loss: 0.3256 - accuracy: 0.8937 - val_loss: 0.4433 - val_accuracy: 0.8802 Epoch 11/25 55000/55000 [==============================] - 6s 110us/sample - loss: 0.2944 - accuracy: 0.9017 - val_loss: 0.4888 - val_accuracy: 0.8742 Epoch 12/25 55000/55000 [==============================] - 6s 110us/sample - loss: 0.2767 - accuracy: 0.9077 - val_loss: 0.4626 - val_accuracy: 0.8706 Epoch 13/25 55000/55000 [==============================] - 6s 111us/sample - loss: 0.2572 - accuracy: 0.9134 - val_loss: 0.4750 - val_accuracy: 0.8770 Epoch 14/25 55000/55000 [==============================] - 6s 111us/sample - loss: 0.2391 - accuracy: 0.9185 - val_loss: 0.4633 - val_accuracy: 0.8900 Epoch 15/25 55000/55000 [==============================] - 6s 112us/sample - loss: 0.2180 - accuracy: 0.9251 - val_loss: 0.4573 - val_accuracy: 0.8768 Epoch 16/25 55000/55000 [==============================] - 6s 110us/sample - loss: 0.2029 - accuracy: 0.9311 - val_loss: 0.4748 - val_accuracy: 0.8840 Epoch 17/25 55000/55000 [==============================] - 6s 112us/sample - loss: 0.1884 - accuracy: 0.9357 - val_loss: 0.5171 - val_accuracy: 0.8840 Epoch 18/25 55000/55000 [==============================] - 6s 111us/sample - loss: 0.1813 - accuracy: 0.9382 - val_loss: 0.5293 - val_accuracy: 0.8822 Epoch 19/25 55000/55000 [==============================] - 6s 112us/sample - loss: 0.1618 - accuracy: 0.9445 - val_loss: 0.5328 - val_accuracy: 0.8872 Epoch 20/25 55000/55000 [==============================] - 6s 111us/sample - loss: 0.1570 - accuracy: 0.9483 - val_loss: 0.5453 - val_accuracy: 0.8870 Epoch 21/25 55000/55000 [==============================] - 6s 112us/sample - loss: 0.1422 - accuracy: 0.9523 - val_loss: 0.5596 - val_accuracy: 0.8892 Epoch 22/25 55000/55000 [==============================] - 6s 111us/sample - loss: 0.1329 - accuracy: 0.9563 - val_loss: 0.5717 - val_accuracy: 0.8894 Epoch 23/25 55000/55000 [==============================] - 6s 110us/sample - loss: 0.1248 - accuracy: 0.9592 - val_loss: 0.5959 - val_accuracy: 0.8930 Epoch 24/25 55000/55000 [==============================] - 6s 112us/sample - loss: 0.1178 - accuracy: 0.9606 - val_loss: 0.5875 - val_accuracy: 0.8896 Epoch 25/25 55000/55000 [==============================] - 6s 111us/sample - loss: 0.1103 - accuracy: 0.9646 - val_loss: 0.6103 - val_accuracy: 0.8904 . n_steps = n_epochs * len(X_train) // 32 steps = np.arange(n_steps) lrs = lr0 * 0.1**(steps / s) . plt.plot(steps, lrs, &quot;-&quot;, linewidth=2) plt.axis([0, n_steps - 1, 0, lr0 * 1.1]) plt.xlabel(&quot;Batch&quot;) plt.ylabel(&quot;Learning Rate&quot;) plt.title(&quot;Exponential Scheduling (per batch)&quot;, fontsize=14) plt.grid(True) plt.show() . Piecewise Constant Scheduling . def piecewise_constant_fn(epoch): if epoch &lt; 5: return 0.01 elif epoch &lt; 15: return 0.005 else: return 0.001 . def piecewise_constant(boundaries, values): boundaries = np.array([0] + boundaries) values = np.array(values) def piecewise_constant_fn(epoch): return values[np.argmax(boundaries &gt; epoch) - 1] return piecewise_constant_fn piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001]) . lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;, metrics=[&quot;accuracy&quot;]) n_epochs = 25 history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid), callbacks=[lr_scheduler]) . Train on 55000 samples, validate on 5000 samples Epoch 1/25 55000/55000 [==============================] - 6s 111us/sample - loss: 0.8151 - accuracy: 0.7655 - val_loss: 0.6868 - val_accuracy: 0.7780 Epoch 2/25 55000/55000 [==============================] - 6s 102us/sample - loss: 0.8153 - accuracy: 0.7659 - val_loss: 1.0604 - val_accuracy: 0.7148 Epoch 3/25 55000/55000 [==============================] - 6s 104us/sample - loss: 0.9138 - accuracy: 0.7218 - val_loss: 1.3223 - val_accuracy: 0.6660 Epoch 4/25 55000/55000 [==============================] - 6s 103us/sample - loss: 0.8506 - accuracy: 0.7627 - val_loss: 0.6807 - val_accuracy: 0.8174 Epoch 5/25 55000/55000 [==============================] - 6s 101us/sample - loss: 0.7213 - accuracy: 0.8068 - val_loss: 1.0441 - val_accuracy: 0.8030 Epoch 6/25 55000/55000 [==============================] - 6s 101us/sample - loss: 0.4882 - accuracy: 0.8548 - val_loss: 0.5411 - val_accuracy: 0.8494 Epoch 7/25 55000/55000 [==============================] - 6s 101us/sample - loss: 0.4721 - accuracy: 0.8568 - val_loss: 0.5808 - val_accuracy: 0.8448 Epoch 8/25 55000/55000 [==============================] - 6s 101us/sample - loss: 0.4412 - accuracy: 0.8659 - val_loss: 0.5466 - val_accuracy: 0.8526 Epoch 9/25 55000/55000 [==============================] - 6s 100us/sample - loss: 0.4234 - accuracy: 0.8718 - val_loss: 0.5611 - val_accuracy: 0.8528 Epoch 10/25 55000/55000 [==============================] - 5s 99us/sample - loss: 0.4300 - accuracy: 0.8721 - val_loss: 0.5049 - val_accuracy: 0.8650 Epoch 11/25 55000/55000 [==============================] - 5s 100us/sample - loss: 0.4162 - accuracy: 0.8768 - val_loss: 0.5957 - val_accuracy: 0.8534 Epoch 12/25 55000/55000 [==============================] - 6s 101us/sample - loss: 0.4122 - accuracy: 0.8780 - val_loss: 0.5707 - val_accuracy: 0.8640 Epoch 13/25 55000/55000 [==============================] - 6s 101us/sample - loss: 0.3951 - accuracy: 0.8833 - val_loss: 0.5523 - val_accuracy: 0.8690 Epoch 14/25 55000/55000 [==============================] - 5s 100us/sample - loss: 0.3961 - accuracy: 0.8834 - val_loss: 0.7371 - val_accuracy: 0.8452 Epoch 15/25 55000/55000 [==============================] - 5s 100us/sample - loss: 0.4201 - accuracy: 0.8839 - val_loss: 0.6546 - val_accuracy: 0.8558 Epoch 16/25 55000/55000 [==============================] - 6s 100us/sample - loss: 0.2645 - accuracy: 0.9162 - val_loss: 0.4655 - val_accuracy: 0.8844 Epoch 17/25 55000/55000 [==============================] - 6s 100us/sample - loss: 0.2440 - accuracy: 0.9222 - val_loss: 0.4758 - val_accuracy: 0.8830 Epoch 18/25 55000/55000 [==============================] - 6s 100us/sample - loss: 0.2320 - accuracy: 0.9256 - val_loss: 0.4917 - val_accuracy: 0.8880 Epoch 19/25 55000/55000 [==============================] - 6s 100us/sample - loss: 0.2248 - accuracy: 0.9279 - val_loss: 0.4644 - val_accuracy: 0.8878 Epoch 20/25 55000/55000 [==============================] - 6s 100us/sample - loss: 0.2172 - accuracy: 0.9302 - val_loss: 0.5036 - val_accuracy: 0.8848 Epoch 21/25 55000/55000 [==============================] - 6s 100us/sample - loss: 0.2139 - accuracy: 0.9327 - val_loss: 0.4921 - val_accuracy: 0.8914 Epoch 22/25 55000/55000 [==============================] - 6s 101us/sample - loss: 0.2030 - accuracy: 0.9360 - val_loss: 0.5197 - val_accuracy: 0.8860 Epoch 23/25 55000/55000 [==============================] - 5s 100us/sample - loss: 0.2014 - accuracy: 0.9360 - val_loss: 0.5231 - val_accuracy: 0.8892 Epoch 24/25 55000/55000 [==============================] - 5s 100us/sample - loss: 0.1912 - accuracy: 0.9391 - val_loss: 0.5223 - val_accuracy: 0.8876 Epoch 25/25 55000/55000 [==============================] - 5s 99us/sample - loss: 0.1872 - accuracy: 0.9418 - val_loss: 0.5068 - val_accuracy: 0.8886 . plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], &quot;o-&quot;) plt.axis([0, n_epochs - 1, 0, 0.011]) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Learning Rate&quot;) plt.title(&quot;Piecewise Constant Scheduling&quot;, fontsize=14) plt.grid(True) plt.show() . Performance Scheduling . tf.random.set_seed(42) np.random.seed(42) . lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=optimizer, metrics=[&quot;accuracy&quot;]) n_epochs = 25 history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid), callbacks=[lr_scheduler]) . Train on 55000 samples, validate on 5000 samples Epoch 1/25 55000/55000 [==============================] - 4s 79us/sample - loss: 0.5954 - accuracy: 0.8055 - val_loss: 0.5432 - val_accuracy: 0.8154 Epoch 2/25 55000/55000 [==============================] - 4s 74us/sample - loss: 0.5194 - accuracy: 0.8345 - val_loss: 0.5184 - val_accuracy: 0.8468 Epoch 3/25 55000/55000 [==============================] - 4s 73us/sample - loss: 0.5080 - accuracy: 0.8453 - val_loss: 0.5780 - val_accuracy: 0.8384 Epoch 4/25 55000/55000 [==============================] - 4s 73us/sample - loss: 0.5360 - accuracy: 0.8452 - val_loss: 0.7195 - val_accuracy: 0.8350 Epoch 5/25 55000/55000 [==============================] - 4s 74us/sample - loss: 0.5239 - accuracy: 0.8504 - val_loss: 0.5219 - val_accuracy: 0.8562 Epoch 6/25 55000/55000 [==============================] - 4s 74us/sample - loss: 0.5163 - accuracy: 0.8528 - val_loss: 0.5669 - val_accuracy: 0.8382 Epoch 7/25 55000/55000 [==============================] - 4s 74us/sample - loss: 0.5088 - accuracy: 0.8561 - val_loss: 0.6591 - val_accuracy: 0.8268 Epoch 8/25 55000/55000 [==============================] - 4s 77us/sample - loss: 0.3022 - accuracy: 0.8938 - val_loss: 0.3955 - val_accuracy: 0.8834 Epoch 9/25 55000/55000 [==============================] - 4s 76us/sample - loss: 0.2501 - accuracy: 0.9087 - val_loss: 0.4060 - val_accuracy: 0.8792 Epoch 10/25 55000/55000 [==============================] - 4s 75us/sample - loss: 0.2304 - accuracy: 0.9158 - val_loss: 0.3998 - val_accuracy: 0.8846 Epoch 11/25 55000/55000 [==============================] - 4s 75us/sample - loss: 0.2155 - accuracy: 0.9206 - val_loss: 0.3880 - val_accuracy: 0.8898 Epoch 12/25 55000/55000 [==============================] - 4s 75us/sample - loss: 0.2034 - accuracy: 0.9253 - val_loss: 0.4049 - val_accuracy: 0.8838 Epoch 13/25 55000/55000 [==============================] - 4s 77us/sample - loss: 0.1878 - accuracy: 0.9285 - val_loss: 0.4440 - val_accuracy: 0.8838 Epoch 14/25 55000/55000 [==============================] - 4s 80us/sample - loss: 0.1839 - accuracy: 0.9325 - val_loss: 0.4478 - val_accuracy: 0.8838 Epoch 15/25 55000/55000 [==============================] - 4s 76us/sample - loss: 0.1747 - accuracy: 0.9348 - val_loss: 0.5072 - val_accuracy: 0.8806 Epoch 16/25 55000/55000 [==============================] - 4s 75us/sample - loss: 0.1689 - accuracy: 0.9367 - val_loss: 0.4897 - val_accuracy: 0.8790 Epoch 17/25 55000/55000 [==============================] - 4s 78us/sample - loss: 0.1090 - accuracy: 0.9576 - val_loss: 0.4571 - val_accuracy: 0.8900 Epoch 18/25 55000/55000 [==============================] - 4s 74us/sample - loss: 0.0926 - accuracy: 0.9639 - val_loss: 0.4563 - val_accuracy: 0.8934 Epoch 19/25 55000/55000 [==============================] - 4s 75us/sample - loss: 0.0861 - accuracy: 0.9671 - val_loss: 0.5103 - val_accuracy: 0.8898 Epoch 20/25 55000/55000 [==============================] - 4s 75us/sample - loss: 0.0794 - accuracy: 0.9692 - val_loss: 0.5065 - val_accuracy: 0.8936 Epoch 21/25 55000/55000 [==============================] - 4s 75us/sample - loss: 0.0737 - accuracy: 0.9721 - val_loss: 0.5516 - val_accuracy: 0.8928 Epoch 22/25 55000/55000 [==============================] - 4s 76us/sample - loss: 0.0547 - accuracy: 0.9803 - val_loss: 0.5315 - val_accuracy: 0.8944 Epoch 23/25 55000/55000 [==============================] - 4s 78us/sample - loss: 0.0487 - accuracy: 0.9827 - val_loss: 0.5429 - val_accuracy: 0.8928 Epoch 24/25 55000/55000 [==============================] - 4s 80us/sample - loss: 0.0455 - accuracy: 0.9844 - val_loss: 0.5554 - val_accuracy: 0.8918 Epoch 25/25 55000/55000 [==============================] - 4s 79us/sample - loss: 0.0427 - accuracy: 0.9850 - val_loss: 0.5730 - val_accuracy: 0.8920 . plt.plot(history.epoch, history.history[&quot;lr&quot;], &quot;bo-&quot;) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Learning Rate&quot;, color=&#39;b&#39;) plt.tick_params(&#39;y&#39;, colors=&#39;b&#39;) plt.gca().set_xlim(0, n_epochs - 1) plt.grid(True) ax2 = plt.gca().twinx() ax2.plot(history.epoch, history.history[&quot;val_loss&quot;], &quot;r^-&quot;) ax2.set_ylabel(&#39;Validation Loss&#39;, color=&#39;r&#39;) ax2.tick_params(&#39;y&#39;, colors=&#39;r&#39;) plt.title(&quot;Reduce LR on Plateau&quot;, fontsize=14) plt.show() . tf.keras schedulers . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32) learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1) optimizer = keras.optimizers.SGD(learning_rate) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=optimizer, metrics=[&quot;accuracy&quot;]) n_epochs = 25 history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/25 55000/55000 [==============================] - 4s 77us/sample - loss: 0.4887 - accuracy: 0.8282 - val_loss: 0.4245 - val_accuracy: 0.8526 Epoch 2/25 55000/55000 [==============================] - 4s 71us/sample - loss: 0.3830 - accuracy: 0.8641 - val_loss: 0.3798 - val_accuracy: 0.8688 Epoch 3/25 55000/55000 [==============================] - 4s 71us/sample - loss: 0.3491 - accuracy: 0.8758 - val_loss: 0.3650 - val_accuracy: 0.8730 Epoch 4/25 55000/55000 [==============================] - 4s 78us/sample - loss: 0.3267 - accuracy: 0.8839 - val_loss: 0.3564 - val_accuracy: 0.8746 Epoch 5/25 55000/55000 [==============================] - 4s 72us/sample - loss: 0.3102 - accuracy: 0.8893 - val_loss: 0.3493 - val_accuracy: 0.8770 Epoch 6/25 55000/55000 [==============================] - 4s 73us/sample - loss: 0.2969 - accuracy: 0.8939 - val_loss: 0.3400 - val_accuracy: 0.8818 Epoch 7/25 55000/55000 [==============================] - 4s 77us/sample - loss: 0.2855 - accuracy: 0.8983 - val_loss: 0.3385 - val_accuracy: 0.8830 Epoch 8/25 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2764 - accuracy: 0.9025 - val_loss: 0.3372 - val_accuracy: 0.8824 Epoch 9/25 55000/55000 [==============================] - 4s 67us/sample - loss: 0.2684 - accuracy: 0.9039 - val_loss: 0.3337 - val_accuracy: 0.8848 Epoch 10/25 55000/55000 [==============================] - 4s 73us/sample - loss: 0.2613 - accuracy: 0.9072 - val_loss: 0.3277 - val_accuracy: 0.8862 Epoch 11/25 55000/55000 [==============================] - 4s 71us/sample - loss: 0.2555 - accuracy: 0.9086 - val_loss: 0.3273 - val_accuracy: 0.8860 Epoch 12/25 55000/55000 [==============================] - 4s 73us/sample - loss: 0.2500 - accuracy: 0.9111 - val_loss: 0.3244 - val_accuracy: 0.8840 Epoch 13/25 55000/55000 [==============================] - 4s 73us/sample - loss: 0.2454 - accuracy: 0.9124 - val_loss: 0.3194 - val_accuracy: 0.8904 Epoch 14/25 55000/55000 [==============================] - 4s 71us/sample - loss: 0.2414 - accuracy: 0.9141 - val_loss: 0.3226 - val_accuracy: 0.8884 Epoch 15/25 55000/55000 [==============================] - 4s 73us/sample - loss: 0.2378 - accuracy: 0.9160 - val_loss: 0.3233 - val_accuracy: 0.8860 Epoch 16/25 55000/55000 [==============================] - 4s 69us/sample - loss: 0.2347 - accuracy: 0.9174 - val_loss: 0.3207 - val_accuracy: 0.8904 Epoch 17/25 55000/55000 [==============================] - 4s 71us/sample - loss: 0.2318 - accuracy: 0.9179 - val_loss: 0.3195 - val_accuracy: 0.8892 Epoch 18/25 55000/55000 [==============================] - 4s 69us/sample - loss: 0.2293 - accuracy: 0.9193 - val_loss: 0.3184 - val_accuracy: 0.8916 Epoch 19/25 55000/55000 [==============================] - 4s 67us/sample - loss: 0.2272 - accuracy: 0.9201 - val_loss: 0.3196 - val_accuracy: 0.8886 Epoch 20/25 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2253 - accuracy: 0.9206 - val_loss: 0.3190 - val_accuracy: 0.8918 Epoch 21/25 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2235 - accuracy: 0.9214 - val_loss: 0.3176 - val_accuracy: 0.8912 Epoch 22/25 55000/55000 [==============================] - 4s 69us/sample - loss: 0.2220 - accuracy: 0.9220 - val_loss: 0.3181 - val_accuracy: 0.8900 Epoch 23/25 55000/55000 [==============================] - 4s 71us/sample - loss: 0.2206 - accuracy: 0.9226 - val_loss: 0.3187 - val_accuracy: 0.8894 Epoch 24/25 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2193 - accuracy: 0.9231 - val_loss: 0.3168 - val_accuracy: 0.8908 Epoch 25/25 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2181 - accuracy: 0.9234 - val_loss: 0.3171 - val_accuracy: 0.8898 . For piecewise constant scheduling, try this: . learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay( boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch], values=[0.01, 0.005, 0.001]) . 1Cycle scheduling . #collapse-show K = keras.backend class ExponentialLearningRate(keras.callbacks.Callback): def __init__(self, factor): self.factor = factor self.rates = [] self.losses = [] def on_batch_end(self, batch, logs): self.rates.append(K.get_value(self.model.optimizer.lr)) self.losses.append(logs[&quot;loss&quot;]) K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor) def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10): init_weights = model.get_weights() iterations = len(X) // batch_size * epochs factor = np.exp(np.log(max_rate / min_rate) / iterations) init_lr = K.get_value(model.optimizer.lr) K.set_value(model.optimizer.lr, min_rate) exp_lr = ExponentialLearningRate(factor) history = model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[exp_lr]) K.set_value(model.optimizer.lr, init_lr) model.set_weights(init_weights) return exp_lr.rates, exp_lr.losses def plot_lr_vs_loss(rates, losses): plt.plot(rates, losses) plt.gca().set_xscale(&#39;log&#39;) plt.hlines(min(losses), min(rates), max(rates)) plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2]) plt.xlabel(&quot;Learning rate&quot;) plt.ylabel(&quot;Loss&quot;) . . tf.random.set_seed(42) np.random.seed(42) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) . batch_size = 128 rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size) plot_lr_vs_loss(rates, losses) . Train on 55000 samples 55000/55000 [==============================] - 2s 28us/sample - loss: nan - accuracy: 0.3888 . #collapse-show class OneCycleScheduler(keras.callbacks.Callback): def __init__(self, iterations, max_rate, start_rate=None, last_iterations=None, last_rate=None): self.iterations = iterations self.max_rate = max_rate self.start_rate = start_rate or max_rate / 10 self.last_iterations = last_iterations or iterations // 10 + 1 self.half_iteration = (iterations - self.last_iterations) // 2 self.last_rate = last_rate or self.start_rate / 1000 self.iteration = 0 def _interpolate(self, iter1, iter2, rate1, rate2): return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1) def on_batch_begin(self, batch, logs): if self.iteration &lt; self.half_iteration: rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate) elif self.iteration &lt; 2 * self.half_iteration: rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_rate, self.start_rate) else: rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_rate, self.last_rate) rate = max(rate, self.last_rate) self.iteration += 1 K.set_value(self.model.optimizer.lr, rate) . . n_epochs = 25 onecycle = OneCycleScheduler(len(X_train) // batch_size * n_epochs, max_rate=0.05) history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size, validation_data=(X_valid_scaled, y_valid), callbacks=[onecycle]) . Train on 55000 samples, validate on 5000 samples Epoch 1/25 55000/55000 [==============================] - 1s 23us/sample - loss: 0.6569 - accuracy: 0.7750 - val_loss: 0.4875 - val_accuracy: 0.8300 Epoch 2/25 55000/55000 [==============================] - 1s 22us/sample - loss: 0.4584 - accuracy: 0.8391 - val_loss: 0.4390 - val_accuracy: 0.8476 Epoch 3/25 55000/55000 [==============================] - 1s 21us/sample - loss: 0.4124 - accuracy: 0.8541 - val_loss: 0.4102 - val_accuracy: 0.8570 Epoch 4/25 55000/55000 [==============================] - 1s 22us/sample - loss: 0.3842 - accuracy: 0.8643 - val_loss: 0.3893 - val_accuracy: 0.8652 Epoch 5/25 55000/55000 [==============================] - 1s 21us/sample - loss: 0.3641 - accuracy: 0.8707 - val_loss: 0.3736 - val_accuracy: 0.8678 Epoch 6/25 55000/55000 [==============================] - 1s 22us/sample - loss: 0.3456 - accuracy: 0.8781 - val_loss: 0.3652 - val_accuracy: 0.8726 Epoch 7/25 55000/55000 [==============================] - 1s 23us/sample - loss: 0.3318 - accuracy: 0.8818 - val_loss: 0.3596 - val_accuracy: 0.8768 Epoch 8/25 55000/55000 [==============================] - 1s 24us/sample - loss: 0.3180 - accuracy: 0.8862 - val_loss: 0.3845 - val_accuracy: 0.8602 Epoch 9/25 55000/55000 [==============================] - 1s 23us/sample - loss: 0.3062 - accuracy: 0.8893 - val_loss: 0.3824 - val_accuracy: 0.8660 Epoch 10/25 55000/55000 [==============================] - 1s 23us/sample - loss: 0.2938 - accuracy: 0.8934 - val_loss: 0.3516 - val_accuracy: 0.8742 Epoch 11/25 55000/55000 [==============================] - 1s 23us/sample - loss: 0.2838 - accuracy: 0.8975 - val_loss: 0.3609 - val_accuracy: 0.8740 Epoch 12/25 55000/55000 [==============================] - 1s 23us/sample - loss: 0.2716 - accuracy: 0.9025 - val_loss: 0.3843 - val_accuracy: 0.8666 Epoch 13/25 55000/55000 [==============================] - 1s 22us/sample - loss: 0.2541 - accuracy: 0.9091 - val_loss: 0.3282 - val_accuracy: 0.8844 Epoch 14/25 55000/55000 [==============================] - 1s 22us/sample - loss: 0.2390 - accuracy: 0.9139 - val_loss: 0.3336 - val_accuracy: 0.8838 Epoch 15/25 55000/55000 [==============================] - 1s 23us/sample - loss: 0.2273 - accuracy: 0.9177 - val_loss: 0.3283 - val_accuracy: 0.8884 Epoch 16/25 55000/55000 [==============================] - 1s 22us/sample - loss: 0.2156 - accuracy: 0.9234 - val_loss: 0.3288 - val_accuracy: 0.8862 Epoch 17/25 55000/55000 [==============================] - 1s 26us/sample - loss: 0.2062 - accuracy: 0.9265 - val_loss: 0.3215 - val_accuracy: 0.8896 Epoch 18/25 55000/55000 [==============================] - 1s 24us/sample - loss: 0.1973 - accuracy: 0.9299 - val_loss: 0.3284 - val_accuracy: 0.8912 Epoch 19/25 55000/55000 [==============================] - 1s 22us/sample - loss: 0.1892 - accuracy: 0.9344 - val_loss: 0.3229 - val_accuracy: 0.8904 Epoch 20/25 55000/55000 [==============================] - 1s 22us/sample - loss: 0.1822 - accuracy: 0.9366 - val_loss: 0.3196 - val_accuracy: 0.8902 Epoch 21/25 55000/55000 [==============================] - 1s 24us/sample - loss: 0.1758 - accuracy: 0.9388 - val_loss: 0.3184 - val_accuracy: 0.8940 Epoch 22/25 55000/55000 [==============================] - 1s 27us/sample - loss: 0.1699 - accuracy: 0.9422 - val_loss: 0.3221 - val_accuracy: 0.8912 Epoch 23/25 55000/55000 [==============================] - 1s 26us/sample - loss: 0.1657 - accuracy: 0.9444 - val_loss: 0.3173 - val_accuracy: 0.8944 Epoch 24/25 55000/55000 [==============================] - 1s 23us/sample - loss: 0.1630 - accuracy: 0.9457 - val_loss: 0.3162 - val_accuracy: 0.8946 Epoch 25/25 55000/55000 [==============================] - 1s 26us/sample - loss: 0.1610 - accuracy: 0.9464 - val_loss: 0.3169 - val_accuracy: 0.8942 . Avoiding Overfitting Through Regularization . $ ell_1$ and $ ell_2$ regularization . layer = keras.layers.Dense(100, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;, kernel_regularizer=keras.regularizers.l2(0.01)) # or l1(0.1) for â„“1 regularization with a factor or 0.1 # or l1_l2(0.1, 0.01) for both â„“1 and â„“2 regularization, with factors 0.1 and 0.01 respectively . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;, kernel_regularizer=keras.regularizers.l2(0.01)), keras.layers.Dense(100, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;, kernel_regularizer=keras.regularizers.l2(0.01)), keras.layers.Dense(10, activation=&quot;softmax&quot;, kernel_regularizer=keras.regularizers.l2(0.01)) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;, metrics=[&quot;accuracy&quot;]) n_epochs = 2 history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/2 55000/55000 [==============================] - 7s 128us/sample - loss: 1.6073 - accuracy: 0.8112 - val_loss: 0.7314 - val_accuracy: 0.8242 Epoch 2/2 55000/55000 [==============================] - 6s 117us/sample - loss: 0.7193 - accuracy: 0.8256 - val_loss: 0.7029 - val_accuracy: 0.8304 . from functools import partial RegularizedDense = partial(keras.layers.Dense, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;, kernel_regularizer=keras.regularizers.l2(0.01)) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), RegularizedDense(300), RegularizedDense(100), RegularizedDense(10, activation=&quot;softmax&quot;) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;, metrics=[&quot;accuracy&quot;]) n_epochs = 2 history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/2 55000/55000 [==============================] - 7s 129us/sample - loss: 1.6597 - accuracy: 0.8128 - val_loss: 0.7630 - val_accuracy: 0.8080 Epoch 2/2 55000/55000 [==============================] - 7s 124us/sample - loss: 0.7176 - accuracy: 0.8271 - val_loss: 0.6848 - val_accuracy: 0.8360 . Dropout . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dropout(rate=0.2), keras.layers.Dense(300, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;), keras.layers.Dropout(rate=0.2), keras.layers.Dense(100, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;), keras.layers.Dropout(rate=0.2), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;, metrics=[&quot;accuracy&quot;]) n_epochs = 2 history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/2 55000/55000 [==============================] - 8s 145us/sample - loss: 0.5741 - accuracy: 0.8030 - val_loss: 0.3841 - val_accuracy: 0.8572 Epoch 2/2 55000/55000 [==============================] - 7s 134us/sample - loss: 0.4218 - accuracy: 0.8469 - val_loss: 0.3534 - val_accuracy: 0.8728 . Alpha Dropout . tf.random.set_seed(42) np.random.seed(42) . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.AlphaDropout(rate=0.2), keras.layers.Dense(300, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.AlphaDropout(rate=0.2), keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;), keras.layers.AlphaDropout(rate=0.2), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=optimizer, metrics=[&quot;accuracy&quot;]) n_epochs = 20 history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/20 55000/55000 [==============================] - 6s 111us/sample - loss: 0.6639 - accuracy: 0.7582 - val_loss: 0.5840 - val_accuracy: 0.8410 Epoch 2/20 55000/55000 [==============================] - 5s 97us/sample - loss: 0.5517 - accuracy: 0.7968 - val_loss: 0.5747 - val_accuracy: 0.8430 Epoch 3/20 55000/55000 [==============================] - 5s 94us/sample - loss: 0.5260 - accuracy: 0.8062 - val_loss: 0.5233 - val_accuracy: 0.8486 Epoch 4/20 55000/55000 [==============================] - 5s 94us/sample - loss: 0.5055 - accuracy: 0.8136 - val_loss: 0.4687 - val_accuracy: 0.8606 Epoch 5/20 55000/55000 [==============================] - 5s 96us/sample - loss: 0.4897 - accuracy: 0.8187 - val_loss: 0.5188 - val_accuracy: 0.8588 Epoch 6/20 55000/55000 [==============================] - 5s 93us/sample - loss: 0.4812 - accuracy: 0.8217 - val_loss: 0.4929 - val_accuracy: 0.8508 Epoch 7/20 55000/55000 [==============================] - 5s 90us/sample - loss: 0.4687 - accuracy: 0.8251 - val_loss: 0.4840 - val_accuracy: 0.8572 Epoch 8/20 55000/55000 [==============================] - 5s 90us/sample - loss: 0.4709 - accuracy: 0.8249 - val_loss: 0.4227 - val_accuracy: 0.8660 Epoch 9/20 55000/55000 [==============================] - 5s 92us/sample - loss: 0.4515 - accuracy: 0.8313 - val_loss: 0.4796 - val_accuracy: 0.8670 Epoch 10/20 55000/55000 [==============================] - 5s 93us/sample - loss: 0.4508 - accuracy: 0.8329 - val_loss: 0.4901 - val_accuracy: 0.8588 Epoch 11/20 55000/55000 [==============================] - 5s 93us/sample - loss: 0.4484 - accuracy: 0.8338 - val_loss: 0.4678 - val_accuracy: 0.8640 Epoch 12/20 55000/55000 [==============================] - 5s 95us/sample - loss: 0.4417 - accuracy: 0.8366 - val_loss: 0.4684 - val_accuracy: 0.8610 Epoch 13/20 55000/55000 [==============================] - 5s 93us/sample - loss: 0.4421 - accuracy: 0.8370 - val_loss: 0.4347 - val_accuracy: 0.8640 Epoch 14/20 55000/55000 [==============================] - 5s 98us/sample - loss: 0.4377 - accuracy: 0.8369 - val_loss: 0.4204 - val_accuracy: 0.8734 Epoch 15/20 55000/55000 [==============================] - 5s 95us/sample - loss: 0.4329 - accuracy: 0.8384 - val_loss: 0.4820 - val_accuracy: 0.8718 Epoch 16/20 55000/55000 [==============================] - 6s 100us/sample - loss: 0.4328 - accuracy: 0.8388 - val_loss: 0.4447 - val_accuracy: 0.8754 Epoch 17/20 55000/55000 [==============================] - 5s 96us/sample - loss: 0.4243 - accuracy: 0.8413 - val_loss: 0.4502 - val_accuracy: 0.8776 Epoch 18/20 55000/55000 [==============================] - 5s 95us/sample - loss: 0.4242 - accuracy: 0.8432 - val_loss: 0.4070 - val_accuracy: 0.8720 Epoch 19/20 55000/55000 [==============================] - 5s 94us/sample - loss: 0.4195 - accuracy: 0.8437 - val_loss: 0.4738 - val_accuracy: 0.8670 Epoch 20/20 55000/55000 [==============================] - 5s 96us/sample - loss: 0.4191 - accuracy: 0.8439 - val_loss: 0.4163 - val_accuracy: 0.8790 . model.evaluate(X_test_scaled, y_test) . 10000/10000 [==============================] - 0s 39us/sample - loss: 0.4535 - accuracy: 0.8680 . [0.45350628316402436, 0.868] . model.evaluate(X_train_scaled, y_train) . 55000/55000 [==============================] - 2s 41us/sample - loss: 0.3357 - accuracy: 0.8887 . [0.335701530437036, 0.88872725] . history = model.fit(X_train_scaled, y_train) . MC Dropout . tf.random.set_seed(42) np.random.seed(42) . y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)]) y_proba = y_probas.mean(axis=0) y_std = y_probas.std(axis=0) . np.round(model.predict(X_test_scaled[:1]), 2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], dtype=float32) . np.round(y_probas[:, :1], 2) . array([[[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], [[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]]], dtype=float32) . np.round(y_proba[:1], 2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], dtype=float32) . y_std = y_probas.std(axis=0) np.round(y_std[:1], 2) . array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32) . y_pred = np.argmax(y_proba, axis=1) . accuracy = np.sum(y_pred == y_test) / len(y_test) accuracy . 0.868 . class MCDropout(keras.layers.Dropout): def call(self, inputs): return super().call(inputs, training=True) class MCAlphaDropout(keras.layers.AlphaDropout): def call(self, inputs): return super().call(inputs, training=True) . tf.random.set_seed(42) np.random.seed(42) . mc_model = keras.models.Sequential([ MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer for layer in model.layers ]) . mc_model.summary() . Model: &#34;sequential_36&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_33 (Flatten) (None, 784) 0 _________________________________________________________________ mc_alpha_dropout_3 (MCAlphaD (None, 784) 0 _________________________________________________________________ dense_311 (Dense) (None, 300) 235500 _________________________________________________________________ mc_alpha_dropout_4 (MCAlphaD (None, 300) 0 _________________________________________________________________ dense_312 (Dense) (None, 100) 30100 _________________________________________________________________ mc_alpha_dropout_5 (MCAlphaD (None, 100) 0 _________________________________________________________________ dense_313 (Dense) (None, 10) 1010 ================================================================= Total params: 266,610 Trainable params: 266,610 Non-trainable params: 0 _________________________________________________________________ . optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True) mc_model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=optimizer, metrics=[&quot;accuracy&quot;]) . mc_model.set_weights(model.get_weights()) . Now we can use the model with MC Dropout: . np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2) . array([[0. , 0. , 0. , 0. , 0. , 0.17, 0. , 0.19, 0. , 0.64]], dtype=float32) . Max norm . layer = keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;, kernel_constraint=keras.constraints.max_norm(1.)) . MaxNormDense = partial(keras.layers.Dense, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;, kernel_constraint=keras.constraints.max_norm(1.)) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), MaxNormDense(300), MaxNormDense(100), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;, metrics=[&quot;accuracy&quot;]) n_epochs = 2 history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/2 55000/55000 [==============================] - 8s 147us/sample - loss: 0.4745 - accuracy: 0.8329 - val_loss: 0.3988 - val_accuracy: 0.8584 Epoch 2/2 55000/55000 [==============================] - 7s 135us/sample - loss: 0.3554 - accuracy: 0.8688 - val_loss: 0.3681 - val_accuracy: 0.8726 . Exercises . 1. to 7. . See appendix A. . 8. Deep Learning . 8.1. . Exercise: Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function. . 8.2. . Exercise: Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later. . 8.3. . Exercise: Tune the hyperparameters using cross-validation and see what precision you can achieve. . 8.4. . Exercise: Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model? . 8.5. . Exercise: is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help? . 9. Transfer learning . 9.1. . Exercise: create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one. . 9.2. . Exercise: train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision? . 9.3. . Exercise: try caching the frozen layers, and train the model again: how much faster is it now? . 9.4. . Exercise: try again reusing just four hidden layers instead of five. Can you achieve a higher precision? . 9.5. . Exercise: now unfreeze the top two hidden layers and continue training: can you get the model to perform even better? . 10. Pretraining on an auxiliary task . In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. . 10.1. . Exercise: Start by building two DNNs (let&#39;s call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on top of both DNNs. You should use the keras.layers.concatenate() function to concatenate the outputs of both DNNs, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation function. . 10.2. . Exercise: split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes. . 10.3. . Exercise: train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not. . 10.4. . Exercise: now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class. .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/11_training_deep_neural_networks",
            "relUrl": "/11_training_deep_neural_networks",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Training and Deploying TensorFlow Models at Scale",
            "content": "This notebook contains all the sample code in chapter 19. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20 and TensorFlow â‰¥2.0. . # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x !echo &quot;deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal&quot; &gt; /etc/apt/sources.list.d/tensorflow-serving.list !curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add - !apt update &amp;&amp; apt-get install -y tensorflow-model-server !pip install -q -U tensorflow-serving-api IS_COLAB = True except Exception: IS_COLAB = False # TensorFlow â‰¥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; if not tf.test.is_gpu_available(): print(&quot;No GPU was detected. CNNs can be very slow without a GPU.&quot;) if IS_COLAB: print(&quot;Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.&quot;) # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) tf.random.set_seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;deploy&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . Deploying TensorFlow models to TensorFlow Serving (TFS) . We will use the REST API or the gRPC API. . Save/Load a SavedModel . (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data() X_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255. X_test = X_test[..., np.newaxis].astype(np.float32) / 255. X_valid, X_train = X_train_full[:5000], X_train_full[5000:] y_valid, y_train = y_train_full[:5000], y_train_full[5000:] X_new = X_test[:3] . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28, 1]), keras.layers.Dense(100, activation=&quot;relu&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-2), metrics=[&quot;accuracy&quot;]) model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 2s 40us/sample - loss: 0.7018 - accuracy: 0.8223 - val_loss: 0.3722 - val_accuracy: 0.9022 Epoch 2/10 55000/55000 [==============================] - 2s 36us/sample - loss: 0.3528 - accuracy: 0.9021 - val_loss: 0.3000 - val_accuracy: 0.9170 Epoch 3/10 55000/55000 [==============================] - 2s 36us/sample - loss: 0.3032 - accuracy: 0.9150 - val_loss: 0.2659 - val_accuracy: 0.9280 Epoch 4/10 55000/55000 [==============================] - 2s 37us/sample - loss: 0.2730 - accuracy: 0.9233 - val_loss: 0.2442 - val_accuracy: 0.9342 Epoch 5/10 55000/55000 [==============================] - 2s 37us/sample - loss: 0.2504 - accuracy: 0.9305 - val_loss: 0.2272 - val_accuracy: 0.9346 Epoch 6/10 55000/55000 [==============================] - 2s 37us/sample - loss: 0.2319 - accuracy: 0.9353 - val_loss: 0.2104 - val_accuracy: 0.9418 Epoch 7/10 55000/55000 [==============================] - 2s 37us/sample - loss: 0.2156 - accuracy: 0.9395 - val_loss: 0.1987 - val_accuracy: 0.9484 Epoch 8/10 55000/55000 [==============================] - 2s 36us/sample - loss: 0.2019 - accuracy: 0.9434 - val_loss: 0.1893 - val_accuracy: 0.9496 Epoch 9/10 55000/55000 [==============================] - 2s 41us/sample - loss: 0.1898 - accuracy: 0.9471 - val_loss: 0.1765 - val_accuracy: 0.9526 Epoch 10/10 55000/55000 [==============================] - 2s 39us/sample - loss: 0.1791 - accuracy: 0.9495 - val_loss: 0.1691 - val_accuracy: 0.9550 . &lt;tensorflow.python.keras.callbacks.History at 0x13d74aba8&gt; . np.round(model.predict(X_new), 2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0.99, 0.01, 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0.96, 0.01, 0. , 0. , 0. , 0. , 0.01, 0.01, 0. ]], dtype=float32) . model_version = &quot;0001&quot; model_name = &quot;my_mnist_model&quot; model_path = os.path.join(model_name, model_version) model_path . &#39;my_mnist_model/0001&#39; . !rm -rf {model_name} . tf.saved_model.save(model, model_path) . for root, dirs, files in os.walk(model_name): indent = &#39; &#39; * root.count(os.sep) print(&#39;{}{}/&#39;.format(indent, os.path.basename(root))) for filename in files: print(&#39;{}{}&#39;.format(indent + &#39; &#39;, filename)) . my_mnist_model/ 0001/ saved_model.pb variables/ variables.data-00000-of-00001 variables.index assets/ . !saved_model_cli show --dir {model_path} . The given SavedModel contains the following tag-sets: serve . !saved_model_cli show --dir {model_path} --tag_set serve . The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: SignatureDef key: &#34;__saved_model_init_op&#34; SignatureDef key: &#34;serving_default&#34; . !saved_model_cli show --dir {model_path} --tag_set serve --signature_def serving_default . The given SavedModel SignatureDef contains the following input(s): inputs[&#39;flatten_2_input&#39;] tensor_info: dtype: DT_FLOAT shape: (-1, 28, 28, 1) name: serving_default_flatten_2_input:0 The given SavedModel SignatureDef contains the following output(s): outputs[&#39;dense_5&#39;] tensor_info: dtype: DT_FLOAT shape: (-1, 10) name: StatefulPartitionedCall:0 Method name is: tensorflow/serving/predict . !saved_model_cli show --dir {model_path} --all . MetaGraphDef with tag-set: &#39;serve&#39; contains the following SignatureDefs: signature_def[&#39;__saved_model_init_op&#39;]: The given SavedModel SignatureDef contains the following input(s): The given SavedModel SignatureDef contains the following output(s): outputs[&#39;__saved_model_init_op&#39;] tensor_info: dtype: DT_INVALID shape: unknown_rank name: NoOp Method name is: signature_def[&#39;serving_default&#39;]: The given SavedModel SignatureDef contains the following input(s): inputs[&#39;flatten_2_input&#39;] tensor_info: dtype: DT_FLOAT shape: (-1, 28, 28, 1) name: serving_default_flatten_2_input:0 The given SavedModel SignatureDef contains the following output(s): outputs[&#39;dense_5&#39;] tensor_info: dtype: DT_FLOAT shape: (-1, 10) name: StatefulPartitionedCall:0 Method name is: tensorflow/serving/predict . Let&#39;s write the new instances to a npy file so we can pass them easily to our model: . np.save(&quot;my_mnist_tests.npy&quot;, X_new) . input_name = model.input_names[0] input_name . &#39;flatten_2_input&#39; . And now let&#39;s use saved_model_cli to make predictions for the instances we just saved: . !saved_model_cli run --dir {model_path} --tag_set serve --signature_def serving_default --inputs {input_name}=my_mnist_tests.npy . 2019-06-10 10:56:43.396851: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA WARNING: Logging before flag parsing goes to stderr. W0610 10:56:43.397369 140735810999168 deprecation.py:323] From /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py:339: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version. Instructions for updating: This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0. W0610 10:56:43.421489 140735810999168 deprecation.py:323] From /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating: Use standard file APIs to check for files with this prefix. Result for output key dense_5: [[1.17575204e-04 1.13160660e-07 5.96997386e-04 2.08104262e-03 2.57820852e-06 6.44166794e-05 2.77263990e-08 9.96703804e-01 3.96052455e-05 3.93810158e-04] [1.22226949e-03 2.92685600e-05 9.86054957e-01 9.63000767e-03 8.81790996e-08 2.88744748e-04 1.58111588e-03 1.12290488e-09 1.19344448e-03 1.09315742e-07] [6.40679718e-05 9.63618696e-01 9.04400647e-03 2.98595289e-03 5.95759891e-04 3.74212675e-03 2.50709383e-03 1.14931818e-02 5.52693009e-03 4.22279176e-04]] . np.round([[1.1739199e-04, 1.1239604e-07, 6.0210604e-04, 2.0804715e-03, 2.5779348e-06, 6.4079795e-05, 2.7411186e-08, 9.9669880e-01, 3.9654213e-05, 3.9471846e-04], [1.2294615e-03, 2.9207937e-05, 9.8599273e-01, 9.6755642e-03, 8.8930705e-08, 2.9156188e-04, 1.5831805e-03, 1.1311053e-09, 1.1980456e-03, 1.1113169e-07], [6.4066830e-05, 9.6359509e-01, 9.0598064e-03, 2.9872139e-03, 5.9552520e-04, 3.7478798e-03, 2.5074568e-03, 1.1462728e-02, 5.5553433e-03, 4.2495009e-04]], 2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0.99, 0.01, 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0.96, 0.01, 0. , 0. , 0. , 0. , 0.01, 0.01, 0. ]]) . TensorFlow Serving . Install Docker if you don&#39;t have it already. Then run: . docker pull tensorflow/serving export ML_PATH=$HOME/ml # or wherever this project is docker run -it --rm -p 8500:8500 -p 8501:8501 -v &quot;$ML_PATH/my_mnist_model:/models/my_mnist_model&quot; -e MODEL_NAME=my_mnist_model tensorflow/serving . Once you are finished using it, press Ctrl-C to shut down the server. . Alternatively, if tensorflow_model_server is installed (e.g., if you are running this notebook in Colab), then the following 3 cells will start the server: . os.environ[&quot;MODEL_DIR&quot;] = os.path.split(os.path.abspath(model_path))[0] . %%bash --bg nohup tensorflow_model_server --rest_api_port=8501 --model_name=my_mnist_model --model_base_path=&quot;${MODEL_DIR}&quot; &gt;server.log 2&gt;&amp;1 . !tail server.log . 2019-11-06 13:04:12.267136: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2019-11-06 13:04:12.283035: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle. 2019-11-06 13:04:12.300096: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /content/my_mnist_model/0002 2019-11-06 13:04:12.304438: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 39993 microseconds. 2019-11-06 13:04:12.304900: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /content/my_mnist_model/0002/assets.extra/tf_serving_warmup_requests 2019-11-06 13:04:12.305057: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: my_mnist_model version: 2} 2019-11-06 13:04:12.306462: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:8500 ... [warn] getaddrinfo: address family for nodename not supported 2019-11-06 13:04:12.307179: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ... [evhttp_server.cc : 238] NET_LOG: Entering the event loop ... . import json input_data_json = json.dumps({ &quot;signature_name&quot;: &quot;serving_default&quot;, &quot;instances&quot;: X_new.tolist(), }) . repr(input_data_json)[:1500] + &quot;...&quot; . &#39; &#39;{&#34;signature_name&#34;: &#34;serving_default&#34;, &#34;instances&#34;: [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3294117748737335, 0.7254902124404907, 0.6235294342041016, 0.5921568870544434, 0.23529411852359772, 0.1411764770746231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8705882430076599, 0.9960784316062927, 0.9960784316062927, 0.9960784316062927, 0.9960784316062927, 0.9450980424880981, 0.7764706015586853, 0.7764706015586853, 0.7764706015586853, 0.776470...&#39; . Now let&#39;s use TensorFlow Serving&#39;s REST API to make predictions: . import requests SERVER_URL = &#39;http://localhost:8501/v1/models/my_mnist_model:predict&#39; response = requests.post(SERVER_URL, data=input_data_json) response.raise_for_status() # raise an exception in case of error response = response.json() . response.keys() . dict_keys([&#39;predictions&#39;]) . y_proba = np.array(response[&quot;predictions&quot;]) y_proba.round(2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0.99, 0.01, 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0.96, 0.01, 0. , 0. , 0. , 0. , 0.01, 0.01, 0. ]]) . Using the gRPC API . from tensorflow_serving.apis.predict_pb2 import PredictRequest request = PredictRequest() request.model_spec.name = model_name request.model_spec.signature_name = &quot;serving_default&quot; input_name = model.input_names[0] request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new)) . import grpc from tensorflow_serving.apis import prediction_service_pb2_grpc channel = grpc.insecure_channel(&#39;localhost:8500&#39;) predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel) response = predict_service.Predict(request, timeout=10.0) . response . outputs { key: &#34;dense_4&#34; value { dtype: DT_FLOAT tensor_shape { dim { size: 3 } dim { size: 10 } } float_val: 2.0824443708988838e-05 float_val: 1.4913139168015732e-08 float_val: 0.0004813199338968843 float_val: 0.001888890634290874 float_val: 2.682592992186983e-07 float_val: 8.666840585647151e-06 float_val: 1.6853943241024183e-10 float_val: 0.9975269436836243 float_val: 3.833709342870861e-05 float_val: 3.4738284739432856e-05 float_val: 0.00017358684272039682 float_val: 0.0002858016814570874 float_val: 0.9816810488700867 float_val: 0.0157401692122221 float_val: 1.1949770339914068e-10 float_val: 0.00023017563216853887 float_val: 3.078056761296466e-05 float_val: 5.393230750883049e-09 float_val: 0.0018584482604637742 float_val: 1.8884094288296183e-09 float_val: 3.397366526769474e-05 float_val: 0.9835277795791626 float_val: 0.001533020636998117 float_val: 0.0014515116345137358 float_val: 0.00018795969663187861 float_val: 0.0011680654715746641 float_val: 0.0014667459763586521 float_val: 0.006120447069406509 float_val: 0.004315734840929508 float_val: 0.00019466254161670804 } } model_spec { name: &#34;my_mnist_model&#34; version { value: 2 } signature_name: &#34;serving_default&#34; } . Convert the response to a tensor: . output_name = model.output_names[0] outputs_proto = response.outputs[output_name] y_proba = tf.make_ndarray(outputs_proto) y_proba.round(2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0.98, 0.02, 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0.98, 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0. ]], dtype=float32) . Or to a NumPy array if your client does not include the TensorFlow library: . output_name = model.output_names[0] outputs_proto = response.outputs[output_name] shape = [dim.size for dim in outputs_proto.tensor_shape.dim] y_proba = np.array(outputs_proto.float_val).reshape(shape) y_proba.round(2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0.98, 0.02, 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0.98, 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0. ]]) . Deploying a new model version . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28, 1]), keras.layers.Dense(50, activation=&quot;relu&quot;), keras.layers.Dense(50, activation=&quot;relu&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-2), metrics=[&quot;accuracy&quot;]) history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 2s 39us/sample - loss: 0.7035 - accuracy: 0.8060 - val_loss: 0.3445 - val_accuracy: 0.9032 Epoch 2/10 55000/55000 [==============================] - 2s 35us/sample - loss: 0.3213 - accuracy: 0.9084 - val_loss: 0.2660 - val_accuracy: 0.9252 Epoch 3/10 55000/55000 [==============================] - 2s 37us/sample - loss: 0.2663 - accuracy: 0.9236 - val_loss: 0.2304 - val_accuracy: 0.9392 Epoch 4/10 55000/55000 [==============================] - 2s 35us/sample - loss: 0.2331 - accuracy: 0.9331 - val_loss: 0.2069 - val_accuracy: 0.9430 Epoch 5/10 55000/55000 [==============================] - 2s 35us/sample - loss: 0.2105 - accuracy: 0.9390 - val_loss: 0.1910 - val_accuracy: 0.9446 Epoch 6/10 55000/55000 [==============================] - 2s 35us/sample - loss: 0.1924 - accuracy: 0.9442 - val_loss: 0.1732 - val_accuracy: 0.9518 Epoch 7/10 55000/55000 [==============================] - 2s 37us/sample - loss: 0.1771 - accuracy: 0.9489 - val_loss: 0.1679 - val_accuracy: 0.9526 Epoch 8/10 55000/55000 [==============================] - 2s 37us/sample - loss: 0.1650 - accuracy: 0.9527 - val_loss: 0.1574 - val_accuracy: 0.9546 Epoch 9/10 55000/55000 [==============================] - 2s 35us/sample - loss: 0.1540 - accuracy: 0.9555 - val_loss: 0.1446 - val_accuracy: 0.9590 Epoch 10/10 55000/55000 [==============================] - 2s 35us/sample - loss: 0.1448 - accuracy: 0.9583 - val_loss: 0.1414 - val_accuracy: 0.9608 . &lt;tensorflow.python.keras.callbacks.History at 0x12f58f908&gt; . model_version = &quot;0002&quot; model_name = &quot;my_mnist_model&quot; model_path = os.path.join(model_name, model_version) model_path . &#39;my_mnist_model/0002&#39; . tf.saved_model.save(model, model_path) . for root, dirs, files in os.walk(model_name): indent = &#39; &#39; * root.count(os.sep) print(&#39;{}{}/&#39;.format(indent, os.path.basename(root))) for filename in files: print(&#39;{}{}&#39;.format(indent + &#39; &#39;, filename)) . my_mnist_model/ 0002/ saved_model.pb variables/ variables.data-00000-of-00001 variables.index assets/ 0001/ saved_model.pb variables/ variables.data-00000-of-00001 variables.index assets/ . Warning: You may need to wait a minute before the new model is loaded by TensorFlow Serving. . import requests SERVER_URL = &#39;http://localhost:8501/v1/models/my_mnist_model:predict&#39; response = requests.post(SERVER_URL, data=input_data_json) response.raise_for_status() response = response.json() . response.keys() . dict_keys([&#39;predictions&#39;]) . y_proba = np.array(response[&quot;predictions&quot;]) y_proba.round(2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0.99, 0.01, 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0.96, 0.01, 0. , 0. , 0. , 0. , 0.01, 0.01, 0. ]]) . Deploy the model to Google Cloud AI Platform . Follow the instructions in the book to deploy the model to Google Cloud AI Platform, download the service account&#39;s private key and save it to the my_service_account_private_key.json in the project directory. Also, update the project_id: . project_id = &quot;onyx-smoke-242003&quot; . import googleapiclient.discovery os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;my_service_account_private_key.json&quot; model_id = &quot;my_mnist_model&quot; model_path = &quot;projects/{}/models/{}&quot;.format(project_id, model_id) model_path += &quot;/versions/v0001/&quot; # if you want to run a specific version ml_resource = googleapiclient.discovery.build(&quot;ml&quot;, &quot;v1&quot;).projects() . def predict(X): input_data_json = {&quot;signature_name&quot;: &quot;serving_default&quot;, &quot;instances&quot;: X.tolist()} request = ml_resource.predict(name=model_path, body=input_data_json) response = request.execute() if &quot;error&quot; in response: raise RuntimeError(response[&quot;error&quot;]) return np.array([pred[output_name] for pred in response[&quot;predictions&quot;]]) . Y_probas = predict(X_new) np.round(Y_probas, 2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0.99, 0.01, 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0.96, 0.01, 0. , 0. , 0. , 0. , 0.01, 0.01, 0. ]]) . Using GPUs . tf.test.is_gpu_available() . False . tf.test.gpu_device_name() . &#39;&#39; . tf.test.is_built_with_cuda() . False . from tensorflow.python.client.device_lib import list_local_devices devices = list_local_devices() devices . [name: &#34;/device:CPU:0&#34; device_type: &#34;CPU&#34; memory_limit: 268435456 locality { } incarnation: 11178133101787456811] . Distributed Training . keras.backend.clear_session() tf.random.set_seed(42) np.random.seed(42) . def create_model(): return keras.models.Sequential([ keras.layers.Conv2D(filters=64, kernel_size=7, activation=&quot;relu&quot;, padding=&quot;same&quot;, input_shape=[28, 28, 1]), keras.layers.MaxPooling2D(pool_size=2), keras.layers.Conv2D(filters=128, kernel_size=3, activation=&quot;relu&quot;, padding=&quot;same&quot;), keras.layers.Conv2D(filters=128, kernel_size=3, activation=&quot;relu&quot;, padding=&quot;same&quot;), keras.layers.MaxPooling2D(pool_size=2), keras.layers.Flatten(), keras.layers.Dense(units=64, activation=&#39;relu&#39;), keras.layers.Dropout(0.5), keras.layers.Dense(units=10, activation=&#39;softmax&#39;), ]) . batch_size = 100 model = create_model() model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-2), metrics=[&quot;accuracy&quot;]) model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), batch_size=batch_size) . keras.backend.clear_session() tf.random.set_seed(42) np.random.seed(42) distribution = tf.distribute.MirroredStrategy() # Change the default all-reduce algorithm: #distribution = tf.distribute.MirroredStrategy( # cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) # Specify the list of GPUs to use: #distribution = tf.distribute.MirroredStrategy(devices=[&quot;/gpu:0&quot;, &quot;/gpu:1&quot;]) # Use the central storage strategy instead: #distribution = tf.distribute.experimental.CentralStorageStrategy() #resolver = tf.distribute.cluster_resolver.TPUClusterResolver() #tf.tpu.experimental.initialize_tpu_system(resolver) #distribution = tf.distribute.experimental.TPUStrategy(resolver) with distribution.scope(): model = create_model() model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-2), metrics=[&quot;accuracy&quot;]) . WARNING: Logging before flag parsing goes to stderr. W0603 15:31:26.178871 140735810999168 cross_device_ops.py:1178] There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce. . batch_size = 100 # must be divisible by the number of workers model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), batch_size=batch_size) . model.predict(X_new) . array([[0.09101252, 0.07083996, 0.06410537, 0.11957529, 0.06693752, 0.05124901, 0.04676544, 0.23180223, 0.13522181, 0.12249089], [0.08099081, 0.12387844, 0.14915964, 0.13171668, 0.05875394, 0.08834281, 0.16267018, 0.06899565, 0.07834874, 0.05714307], [0.04303756, 0.2682051 , 0.0909673 , 0.11496522, 0.06084979, 0.07125981, 0.08520001, 0.08517107, 0.09236596, 0.0879782 ]], dtype=float32) . Custom training loop: . keras.backend.clear_session() tf.random.set_seed(42) np.random.seed(42) K = keras.backend distribution = tf.distribute.MirroredStrategy() with distribution.scope(): model = create_model() optimizer = keras.optimizers.SGD() with distribution.scope(): dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat().batch(batch_size) input_iterator = distribution.make_dataset_iterator(dataset) @tf.function def train_step(): def step_fn(inputs): X, y = inputs with tf.GradientTape() as tape: Y_proba = model(X) loss = K.sum(keras.losses.sparse_categorical_crossentropy(y, Y_proba)) / batch_size grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables)) return loss per_replica_losses = distribution.experimental_run(step_fn, input_iterator) mean_loss = distribution.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None) return mean_loss n_epochs = 10 with distribution.scope(): input_iterator.initialize() for epoch in range(n_epochs): print(&quot;Epoch {}/{}&quot;.format(epoch + 1, n_epochs)) for iteration in range(len(X_train) // batch_size): print(&quot; rLoss: {:.3f}&quot;.format(train_step().numpy()), end=&quot;&quot;) print() . batch_size = 100 # must be divisible by the number of workers model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), batch_size=batch_size) . Training across multiple servers . A TensorFlow cluster is a group of TensorFlow processes running in parallel, usually on different machines, and talking to each other to complete some work, for example training or executing a neural network. Each TF process in the cluster is called a &quot;task&quot; (or a &quot;TF server&quot;). It has an IP address, a port, and a type (also called its role or its job). The type can be &quot;worker&quot;, &quot;chief&quot;, &quot;ps&quot; (parameter server) or &quot;evaluator&quot;: . Each worker performs computations, usually on a machine with one or more GPUs. | The chief performs computations as well, but it also handles extra work such as writing TensorBoard logs or saving checkpoints. There is a single chief in a cluster. If no chief is specified, then the first worker is the chief. | A parameter server (ps) only keeps track of variable values, it is usually on a CPU-only machine. | The evaluator obviously takes care of evaluation. There is usually a single evaluator in a cluster. | . The set of tasks that share the same type is often called a &quot;job&quot;. For example, the &quot;worker&quot; job is the set of all workers. . To start a TensorFlow cluster, you must first specify it. This means defining all the tasks (IP address, TCP port, and type). For example, the following cluster specification defines a cluster with 3 tasks (2 workers and 1 parameter server). It&#39;s a dictionary with one key per job, and the values are lists of task addresses: . { &quot;worker&quot;: [&quot;my-worker0.example.com:9876&quot;, &quot;my-worker1.example.com:9876&quot;], &quot;ps&quot;: [&quot;my-ps0.example.com:9876&quot;] } . Every task in the cluster may communicate with every other task in the server, so make sure to configure your firewall to authorize all communications between these machines on these ports (it&#39;s usually simpler if you use the same port on every machine). . When a task is started, it needs to be told which one it is: its type and index (the task index is also called the task id). A common way to specify everything at once (both the cluster spec and the current task&#39;s type and id) is to set the TF_CONFIG environment variable before starting the program. It must be a JSON-encoded dictionary containing a cluster specification (under the &quot;cluster&quot; key), and the type and index of the task to start (under the &quot;task&quot; key). For example, the following TF_CONFIG environment variable defines a simple cluster with 2 workers and 1 parameter server, and specifies that the task to start is the first worker: . import os import json os.environ[&quot;TF_CONFIG&quot;] = json.dumps({ &quot;cluster&quot;: { &quot;worker&quot;: [&quot;my-work0.example.com:9876&quot;, &quot;my-work1.example.com:9876&quot;], &quot;ps&quot;: [&quot;my-ps0.example.com:9876&quot;] }, &quot;task&quot;: {&quot;type&quot;: &quot;worker&quot;, &quot;index&quot;: 0} }) print(&quot;TF_CONFIG=&#39;{}&#39;&quot;.format(os.environ[&quot;TF_CONFIG&quot;])) . TF_CONFIG=&#39;{&#34;cluster&#34;: {&#34;worker&#34;: [&#34;my-work0.example.com:9876&#34;, &#34;my-work1.example.com:9876&#34;], &#34;ps&#34;: [&#34;my-ps0.example.com:9876&#34;]}, &#34;task&#34;: {&#34;type&#34;: &#34;worker&#34;, &#34;index&#34;: 0}}&#39; . Some platforms (e.g., Google Cloud ML Engine) automatically set this environment variable for you. . Then you would write a short Python script to start a task. The same script can be used on every machine, since it will load the TF_CONFIG variable, which will tell it which task to start: . import tensorflow as tf resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver() worker0 = tf.distribute.Server(resolver.cluster_spec(), job_name=resolver.task_type, task_index=resolver.task_id) . Another way to specify the cluster specification is directly in Python, rather than through an environment variable: . cluster_spec = tf.train.ClusterSpec({ &quot;worker&quot;: [&quot;127.0.0.1:9901&quot;, &quot;127.0.0.1:9902&quot;], &quot;ps&quot;: [&quot;127.0.0.1:9903&quot;] }) . You can then start a server simply by passing it the cluster spec and indicating its type and index. Let&#39;s start the two remaining tasks (remember that in general you would only start a single task per machine; we are starting 3 tasks on the localhost just for the purpose of this code example): . #worker1 = tf.distribute.Server(cluster_spec, job_name=&quot;worker&quot;, task_index=1) ps0 = tf.distribute.Server(cluster_spec, job_name=&quot;ps&quot;, task_index=0) . os.environ[&quot;TF_CONFIG&quot;] = json.dumps({ &quot;cluster&quot;: { &quot;worker&quot;: [&quot;127.0.0.1:9901&quot;, &quot;127.0.0.1:9902&quot;], &quot;ps&quot;: [&quot;127.0.0.1:9903&quot;] }, &quot;task&quot;: {&quot;type&quot;: &quot;worker&quot;, &quot;index&quot;: 1} }) print(repr(os.environ[&quot;TF_CONFIG&quot;])) . &#39;{&#34;cluster&#34;: {&#34;worker&#34;: [&#34;127.0.0.1:9901&#34;, &#34;127.0.0.1:9902&#34;], &#34;ps&#34;: [&#34;127.0.0.1:9903&#34;]}, &#34;task&#34;: {&#34;type&#34;: &#34;worker&#34;, &#34;index&#34;: 1}}&#39; . distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy() keras.backend.clear_session() tf.random.set_seed(42) np.random.seed(42) os.environ[&quot;TF_CONFIG&quot;] = json.dumps({ &quot;cluster&quot;: { &quot;worker&quot;: [&quot;127.0.0.1:9901&quot;, &quot;127.0.0.1:9902&quot;], &quot;ps&quot;: [&quot;127.0.0.1:9903&quot;] }, &quot;task&quot;: {&quot;type&quot;: &quot;worker&quot;, &quot;index&quot;: 1} }) #CUDA_VISIBLE_DEVICES=0 with distribution.scope(): model = create_model() model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-2), metrics=[&quot;accuracy&quot;]) . import tensorflow as tf from tensorflow import keras import numpy as np # At the beginning of the program (restart the kernel before running this cell) distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy() (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data() X_train_full = X_train_full[..., np.newaxis] / 255. X_test = X_test[..., np.newaxis] / 255. X_valid, X_train = X_train_full[:5000], X_train_full[5000:] y_valid, y_train = y_train_full[:5000], y_train_full[5000:] X_new = X_test[:3] n_workers = 2 batch_size = 32 * n_workers dataset = tf.data.Dataset.from_tensor_slices((X_train[..., np.newaxis], y_train)).repeat().batch(batch_size) def create_model(): return keras.models.Sequential([ keras.layers.Conv2D(filters=64, kernel_size=7, activation=&quot;relu&quot;, padding=&quot;same&quot;, input_shape=[28, 28, 1]), keras.layers.MaxPooling2D(pool_size=2), keras.layers.Conv2D(filters=128, kernel_size=3, activation=&quot;relu&quot;, padding=&quot;same&quot;), keras.layers.Conv2D(filters=128, kernel_size=3, activation=&quot;relu&quot;, padding=&quot;same&quot;), keras.layers.MaxPooling2D(pool_size=2), keras.layers.Flatten(), keras.layers.Dense(units=64, activation=&#39;relu&#39;), keras.layers.Dropout(0.5), keras.layers.Dense(units=10, activation=&#39;softmax&#39;), ]) with distribution.scope(): model = create_model() model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-2), metrics=[&quot;accuracy&quot;]) model.fit(dataset, steps_per_epoch=len(X_train)//batch_size, epochs=10) . # Hyperparameter tuning # Only talk to ps server config_proto = tf.ConfigProto(device_filters=[&#39;/job:ps&#39;, &#39;/job:worker/task:%d&#39; % tf_config[&#39;task&#39;][&#39;index&#39;]]) config = tf.estimator.RunConfig(session_config=config_proto) # default since 1.10 . strategy.num_replicas_in_sync .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/19_training_and_deploying_at_scale",
            "relUrl": "/19_training_and_deploying_at_scale",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "The Machine Learning landscape",
            "content": "Code example 1-1 . Although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead. . # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) . # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; . This function just merges the OECD&#39;s life satisfaction data and the IMF&#39;s GDP per capita data. It&#39;s a bit too long and boring and it&#39;s not specific to Machine Learning, which is why I left it out of the book. . #collapse-show def prepare_country_stats(oecd_bli, gdp_per_capita): oecd_bli = oecd_bli[oecd_bli[&quot;INEQUALITY&quot;]==&quot;TOT&quot;] oecd_bli = oecd_bli.pivot(index=&quot;Country&quot;, columns=&quot;Indicator&quot;, values=&quot;Value&quot;) gdp_per_capita.rename(columns={&quot;2015&quot;: &quot;GDP per capita&quot;}, inplace=True) gdp_per_capita.set_index(&quot;Country&quot;, inplace=True) full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita, left_index=True, right_index=True) full_country_stats.sort_values(by=&quot;GDP per capita&quot;, inplace=True) remove_indices = [0, 1, 6, 8, 33, 34, 35] keep_indices = list(set(range(36)) - set(remove_indices)) return full_country_stats[[&quot;GDP per capita&quot;, &#39;Life satisfaction&#39;]].iloc[keep_indices] . . The code in the book expects the data files to be located in the current directory. I just tweaked it here to fetch the files in datasets/lifesat. . import os datapath = os.path.join(&quot;datasets&quot;, &quot;lifesat&quot;, &quot;&quot;) . # To plot pretty figures directly within Jupyter %matplotlib inline import matplotlib as mpl mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) . # Download the data import urllib DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; os.makedirs(datapath, exist_ok=True) for filename in (&quot;oecd_bli_2015.csv&quot;, &quot;gdp_per_capita.csv&quot;): print(&quot;Downloading&quot;, filename) url = DOWNLOAD_ROOT + &quot;datasets/lifesat/&quot; + filename urllib.request.urlretrieve(url, datapath + filename) . Downloading oecd_bli_2015.csv Downloading gdp_per_capita.csv . # Code example import matplotlib.pyplot as plt import numpy as np import pandas as pd import sklearn.linear_model # Load the data oecd_bli = pd.read_csv(datapath + &quot;oecd_bli_2015.csv&quot;, thousands=&#39;,&#39;) gdp_per_capita = pd.read_csv(datapath + &quot;gdp_per_capita.csv&quot;,thousands=&#39;,&#39;,delimiter=&#39; t&#39;, encoding=&#39;latin1&#39;, na_values=&quot;n/a&quot;) # Prepare the data country_stats = prepare_country_stats(oecd_bli, gdp_per_capita) X = np.c_[country_stats[&quot;GDP per capita&quot;]] y = np.c_[country_stats[&quot;Life satisfaction&quot;]] # Visualize the data country_stats.plot(kind=&#39;scatter&#39;, x=&quot;GDP per capita&quot;, y=&#39;Life satisfaction&#39;) plt.show() # Select a linear model model = sklearn.linear_model.LinearRegression() # Train the model model.fit(X, y) # Make a prediction for Cyprus X_new = [[22587]] # Cyprus&#39; GDP per capita print(model.predict(X_new)) # outputs [[ 5.96242338]] . [[5.96242338]] . /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/sklearn/linear_model/base.py:485: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to &#39;gelss&#39; driver. linalg.lstsq(X, y) . Note: you can ignore the rest of this notebook, it just generates many of the figures in chapter 1. . Create a function to save the figures. . # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;fundamentals&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . Make this notebook&#39;s output stable across runs: . np.random.seed(42) . Load and prepare Life satisfaction data . If you want, you can get fresh data from the OECD&#39;s website. Download the CSV from http://stats.oecd.org/index.aspx?DataSetCode=BLI and save it to datasets/lifesat/. . oecd_bli = pd.read_csv(datapath + &quot;oecd_bli_2015.csv&quot;, thousands=&#39;,&#39;) oecd_bli = oecd_bli[oecd_bli[&quot;INEQUALITY&quot;]==&quot;TOT&quot;] oecd_bli = oecd_bli.pivot(index=&quot;Country&quot;, columns=&quot;Indicator&quot;, values=&quot;Value&quot;) oecd_bli.head(2) . Indicator Air pollution Assault rate Consultation on rule-making Dwellings without basic facilities Educational attainment Employees working very long hours Employment rate Homicide rate Household net adjusted disposable income Household net financial wealth ... Long-term unemployment rate Personal earnings Quality of support network Rooms per person Self-reported health Student skills Time devoted to leisure and personal care Voter turnout Water quality Years in education . Country . Australia 13.0 | 2.1 | 10.5 | 1.1 | 76.0 | 14.02 | 72.0 | 0.8 | 31588.0 | 47657.0 | ... | 1.08 | 50449.0 | 92.0 | 2.3 | 85.0 | 512.0 | 14.41 | 93.0 | 91.0 | 19.4 | . Austria 27.0 | 3.4 | 7.1 | 1.0 | 83.0 | 7.61 | 72.0 | 0.4 | 31173.0 | 49887.0 | ... | 1.19 | 45199.0 | 89.0 | 1.6 | 69.0 | 500.0 | 14.46 | 75.0 | 94.0 | 17.0 | . 2 rows Ã— 24 columns . oecd_bli[&quot;Life satisfaction&quot;].head() . Country Australia 7.3 Austria 6.9 Belgium 6.9 Brazil 7.0 Canada 7.3 Name: Life satisfaction, dtype: float64 . Load and prepare GDP per capita data . Just like above, you can update the GDP per capita data if you want. Just download data from http://goo.gl/j1MSKe (=&gt; imf.org) and save it to datasets/lifesat/. . gdp_per_capita = pd.read_csv(datapath+&quot;gdp_per_capita.csv&quot;, thousands=&#39;,&#39;, delimiter=&#39; t&#39;, encoding=&#39;latin1&#39;, na_values=&quot;n/a&quot;) gdp_per_capita.rename(columns={&quot;2015&quot;: &quot;GDP per capita&quot;}, inplace=True) gdp_per_capita.set_index(&quot;Country&quot;, inplace=True) gdp_per_capita.head(2) . Subject Descriptor Units Scale Country/Series-specific Notes GDP per capita Estimates Start After . Country . Afghanistan Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 599.994 | 2013.0 | . Albania Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 3995.383 | 2010.0 | . full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita, left_index=True, right_index=True) full_country_stats.sort_values(by=&quot;GDP per capita&quot;, inplace=True) full_country_stats . Air pollution Assault rate Consultation on rule-making Dwellings without basic facilities Educational attainment Employees working very long hours Employment rate Homicide rate Household net adjusted disposable income Household net financial wealth ... Time devoted to leisure and personal care Voter turnout Water quality Years in education Subject Descriptor Units Scale Country/Series-specific Notes GDP per capita Estimates Start After . Country . Brazil 18.0 | 7.9 | 4.0 | 6.7 | 45.0 | 10.41 | 67.0 | 25.5 | 11664.0 | 6844.0 | ... | 14.97 | 79.0 | 72.0 | 16.3 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 8669.998 | 2014.0 | . Mexico 30.0 | 12.8 | 9.0 | 4.2 | 37.0 | 28.83 | 61.0 | 23.4 | 13085.0 | 9056.0 | ... | 13.89 | 63.0 | 67.0 | 14.4 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 9009.280 | 2015.0 | . Russia 15.0 | 3.8 | 2.5 | 15.1 | 94.0 | 0.16 | 69.0 | 12.8 | 19292.0 | 3412.0 | ... | 14.97 | 65.0 | 56.0 | 16.0 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 9054.914 | 2015.0 | . Turkey 35.0 | 5.0 | 5.5 | 12.7 | 34.0 | 40.86 | 50.0 | 1.2 | 14095.0 | 3251.0 | ... | 13.42 | 88.0 | 62.0 | 16.4 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 9437.372 | 2013.0 | . Hungary 15.0 | 3.6 | 7.9 | 4.8 | 82.0 | 3.19 | 58.0 | 1.3 | 15442.0 | 13277.0 | ... | 15.04 | 62.0 | 77.0 | 17.6 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 12239.894 | 2015.0 | . Poland 33.0 | 1.4 | 10.8 | 3.2 | 90.0 | 7.41 | 60.0 | 0.9 | 17852.0 | 10919.0 | ... | 14.20 | 55.0 | 79.0 | 18.4 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 12495.334 | 2014.0 | . Chile 46.0 | 6.9 | 2.0 | 9.4 | 57.0 | 15.42 | 62.0 | 4.4 | 14533.0 | 17733.0 | ... | 14.41 | 49.0 | 73.0 | 16.5 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 13340.905 | 2014.0 | . Slovak Republic 13.0 | 3.0 | 6.6 | 0.6 | 92.0 | 7.02 | 60.0 | 1.2 | 17503.0 | 8663.0 | ... | 14.99 | 59.0 | 81.0 | 16.3 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 15991.736 | 2015.0 | . Czech Republic 16.0 | 2.8 | 6.8 | 0.9 | 92.0 | 6.98 | 68.0 | 0.8 | 18404.0 | 17299.0 | ... | 14.98 | 59.0 | 85.0 | 18.1 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 17256.918 | 2015.0 | . Estonia 9.0 | 5.5 | 3.3 | 8.1 | 90.0 | 3.30 | 68.0 | 4.8 | 15167.0 | 7680.0 | ... | 14.90 | 64.0 | 79.0 | 17.5 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 17288.083 | 2014.0 | . Greece 27.0 | 3.7 | 6.5 | 0.7 | 68.0 | 6.16 | 49.0 | 1.6 | 18575.0 | 14579.0 | ... | 14.91 | 64.0 | 69.0 | 18.6 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 18064.288 | 2014.0 | . Portugal 18.0 | 5.7 | 6.5 | 0.9 | 38.0 | 9.62 | 61.0 | 1.1 | 20086.0 | 31245.0 | ... | 14.95 | 58.0 | 86.0 | 17.6 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 19121.592 | 2014.0 | . Slovenia 26.0 | 3.9 | 10.3 | 0.5 | 85.0 | 5.63 | 63.0 | 0.4 | 19326.0 | 18465.0 | ... | 14.62 | 52.0 | 88.0 | 18.4 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 20732.482 | 2015.0 | . Spain 24.0 | 4.2 | 7.3 | 0.1 | 55.0 | 5.89 | 56.0 | 0.6 | 22477.0 | 24774.0 | ... | 16.06 | 69.0 | 71.0 | 17.6 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 25864.721 | 2014.0 | . Korea 30.0 | 2.1 | 10.4 | 4.2 | 82.0 | 18.72 | 64.0 | 1.1 | 19510.0 | 29091.0 | ... | 14.63 | 76.0 | 78.0 | 17.5 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 27195.197 | 2014.0 | . Italy 21.0 | 4.7 | 5.0 | 1.1 | 57.0 | 3.66 | 56.0 | 0.7 | 25166.0 | 54987.0 | ... | 14.98 | 75.0 | 71.0 | 16.8 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 29866.581 | 2015.0 | . Japan 24.0 | 1.4 | 7.3 | 6.4 | 94.0 | 22.26 | 72.0 | 0.3 | 26111.0 | 86764.0 | ... | 14.93 | 53.0 | 85.0 | 16.3 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 32485.545 | 2015.0 | . Israel 21.0 | 6.4 | 2.5 | 3.7 | 85.0 | 16.03 | 67.0 | 2.3 | 22104.0 | 52933.0 | ... | 14.48 | 68.0 | 68.0 | 15.8 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 35343.336 | 2015.0 | . New Zealand 11.0 | 2.2 | 10.3 | 0.2 | 74.0 | 13.87 | 73.0 | 1.2 | 23815.0 | 28290.0 | ... | 14.87 | 77.0 | 89.0 | 18.1 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 37044.891 | 2015.0 | . France 12.0 | 5.0 | 3.5 | 0.5 | 73.0 | 8.15 | 64.0 | 0.6 | 28799.0 | 48741.0 | ... | 15.33 | 80.0 | 82.0 | 16.4 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 37675.006 | 2015.0 | . Belgium 21.0 | 6.6 | 4.5 | 2.0 | 72.0 | 4.57 | 62.0 | 1.1 | 28307.0 | 83876.0 | ... | 15.71 | 89.0 | 87.0 | 18.9 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 40106.632 | 2014.0 | . Germany 16.0 | 3.6 | 4.5 | 0.1 | 86.0 | 5.25 | 73.0 | 0.5 | 31252.0 | 50394.0 | ... | 15.31 | 72.0 | 95.0 | 18.2 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 40996.511 | 2014.0 | . Finland 15.0 | 2.4 | 9.0 | 0.6 | 85.0 | 3.58 | 69.0 | 1.4 | 27927.0 | 18761.0 | ... | 14.89 | 69.0 | 94.0 | 19.7 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 41973.988 | 2014.0 | . Canada 15.0 | 1.3 | 10.5 | 0.2 | 89.0 | 3.94 | 72.0 | 1.5 | 29365.0 | 67913.0 | ... | 14.25 | 61.0 | 91.0 | 17.2 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 43331.961 | 2015.0 | . Netherlands 30.0 | 4.9 | 6.1 | 0.0 | 73.0 | 0.45 | 74.0 | 0.9 | 27888.0 | 77961.0 | ... | 15.44 | 75.0 | 92.0 | 18.7 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 43603.115 | 2014.0 | . Austria 27.0 | 3.4 | 7.1 | 1.0 | 83.0 | 7.61 | 72.0 | 0.4 | 31173.0 | 49887.0 | ... | 14.46 | 75.0 | 94.0 | 17.0 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 43724.031 | 2015.0 | . United Kingdom 13.0 | 1.9 | 11.5 | 0.2 | 78.0 | 12.70 | 71.0 | 0.3 | 27029.0 | 60778.0 | ... | 14.83 | 66.0 | 88.0 | 16.4 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 43770.688 | 2015.0 | . Sweden 10.0 | 5.1 | 10.9 | 0.0 | 88.0 | 1.13 | 74.0 | 0.7 | 29185.0 | 60328.0 | ... | 15.11 | 86.0 | 95.0 | 19.3 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 49866.266 | 2014.0 | . Iceland 18.0 | 2.7 | 5.1 | 0.4 | 71.0 | 12.25 | 82.0 | 0.3 | 23965.0 | 43045.0 | ... | 14.61 | 81.0 | 97.0 | 19.8 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 50854.583 | 2014.0 | . Australia 13.0 | 2.1 | 10.5 | 1.1 | 76.0 | 14.02 | 72.0 | 0.8 | 31588.0 | 47657.0 | ... | 14.41 | 93.0 | 91.0 | 19.4 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 50961.865 | 2014.0 | . Ireland 13.0 | 2.6 | 9.0 | 0.2 | 75.0 | 4.20 | 60.0 | 0.8 | 23917.0 | 31580.0 | ... | 15.19 | 70.0 | 80.0 | 17.6 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 51350.744 | 2014.0 | . Denmark 15.0 | 3.9 | 7.0 | 0.9 | 78.0 | 2.03 | 73.0 | 0.3 | 26491.0 | 44488.0 | ... | 16.06 | 88.0 | 94.0 | 19.4 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 52114.165 | 2015.0 | . United States 18.0 | 1.5 | 8.3 | 0.1 | 89.0 | 11.30 | 67.0 | 5.2 | 41355.0 | 145769.0 | ... | 14.27 | 68.0 | 85.0 | 17.2 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 55805.204 | 2015.0 | . Norway 16.0 | 3.3 | 8.1 | 0.3 | 82.0 | 2.82 | 75.0 | 0.6 | 33492.0 | 8797.0 | ... | 15.56 | 78.0 | 94.0 | 17.9 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 74822.106 | 2015.0 | . Switzerland 20.0 | 4.2 | 8.4 | 0.0 | 86.0 | 6.72 | 80.0 | 0.5 | 33491.0 | 108823.0 | ... | 14.98 | 49.0 | 96.0 | 17.3 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 80675.308 | 2015.0 | . Luxembourg 12.0 | 4.3 | 6.0 | 0.1 | 78.0 | 3.47 | 66.0 | 0.4 | 38951.0 | 61765.0 | ... | 15.12 | 91.0 | 86.0 | 15.1 | Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 101994.093 | 2014.0 | . 36 rows Ã— 30 columns . full_country_stats[[&quot;GDP per capita&quot;, &#39;Life satisfaction&#39;]].loc[&quot;United States&quot;] . GDP per capita 55805.204 Life satisfaction 7.200 Name: United States, dtype: float64 . remove_indices = [0, 1, 6, 8, 33, 34, 35] keep_indices = list(set(range(36)) - set(remove_indices)) sample_data = full_country_stats[[&quot;GDP per capita&quot;, &#39;Life satisfaction&#39;]].iloc[keep_indices] missing_data = full_country_stats[[&quot;GDP per capita&quot;, &#39;Life satisfaction&#39;]].iloc[remove_indices] . sample_data.plot(kind=&#39;scatter&#39;, x=&quot;GDP per capita&quot;, y=&#39;Life satisfaction&#39;, figsize=(5,3)) plt.axis([0, 60000, 0, 10]) position_text = { &quot;Hungary&quot;: (5000, 1), &quot;Korea&quot;: (18000, 1.7), &quot;France&quot;: (29000, 2.4), &quot;Australia&quot;: (40000, 3.0), &quot;United States&quot;: (52000, 3.8), } for country, pos_text in position_text.items(): pos_data_x, pos_data_y = sample_data.loc[country] country = &quot;U.S.&quot; if country == &quot;United States&quot; else country plt.annotate(country, xy=(pos_data_x, pos_data_y), xytext=pos_text, arrowprops=dict(facecolor=&#39;black&#39;, width=0.5, shrink=0.1, headwidth=5)) plt.plot(pos_data_x, pos_data_y, &quot;ro&quot;) plt.xlabel(&quot;GDP per capita (USD)&quot;) save_fig(&#39;money_happy_scatterplot&#39;) plt.show() . Saving figure money_happy_scatterplot . sample_data.to_csv(os.path.join(&quot;datasets&quot;, &quot;lifesat&quot;, &quot;lifesat.csv&quot;)) . sample_data.loc[list(position_text.keys())] . GDP per capita Life satisfaction . Country . Hungary 12239.894 | 4.9 | . Korea 27195.197 | 5.8 | . France 37675.006 | 6.5 | . Australia 50961.865 | 7.3 | . United States 55805.204 | 7.2 | . import numpy as np sample_data.plot(kind=&#39;scatter&#39;, x=&quot;GDP per capita&quot;, y=&#39;Life satisfaction&#39;, figsize=(5,3)) plt.xlabel(&quot;GDP per capita (USD)&quot;) plt.axis([0, 60000, 0, 10]) X=np.linspace(0, 60000, 1000) plt.plot(X, 2*X/100000, &quot;r&quot;) plt.text(40000, 2.7, r&quot;$ theta_0 = 0$&quot;, fontsize=14, color=&quot;r&quot;) plt.text(40000, 1.8, r&quot;$ theta_1 = 2 times 10^{-5}$&quot;, fontsize=14, color=&quot;r&quot;) plt.plot(X, 8 - 5*X/100000, &quot;g&quot;) plt.text(5000, 9.1, r&quot;$ theta_0 = 8$&quot;, fontsize=14, color=&quot;g&quot;) plt.text(5000, 8.2, r&quot;$ theta_1 = -5 times 10^{-5}$&quot;, fontsize=14, color=&quot;g&quot;) plt.plot(X, 4 + 5*X/100000, &quot;b&quot;) plt.text(5000, 3.5, r&quot;$ theta_0 = 4$&quot;, fontsize=14, color=&quot;b&quot;) plt.text(5000, 2.6, r&quot;$ theta_1 = 5 times 10^{-5}$&quot;, fontsize=14, color=&quot;b&quot;) save_fig(&#39;tweaking_model_params_plot&#39;) plt.show() . Saving figure tweaking_model_params_plot . from sklearn import linear_model lin1 = linear_model.LinearRegression() Xsample = np.c_[sample_data[&quot;GDP per capita&quot;]] ysample = np.c_[sample_data[&quot;Life satisfaction&quot;]] lin1.fit(Xsample, ysample) t0, t1 = lin1.intercept_[0], lin1.coef_[0][0] t0, t1 . (4.853052800266436, 4.911544589158484e-05) . sample_data.plot(kind=&#39;scatter&#39;, x=&quot;GDP per capita&quot;, y=&#39;Life satisfaction&#39;, figsize=(5,3)) plt.xlabel(&quot;GDP per capita (USD)&quot;) plt.axis([0, 60000, 0, 10]) X=np.linspace(0, 60000, 1000) plt.plot(X, t0 + t1*X, &quot;b&quot;) plt.text(5000, 3.1, r&quot;$ theta_0 = 4.85$&quot;, fontsize=14, color=&quot;b&quot;) plt.text(5000, 2.2, r&quot;$ theta_1 = 4.91 times 10^{-5}$&quot;, fontsize=14, color=&quot;b&quot;) save_fig(&#39;best_fit_model_plot&#39;) plt.show() . Saving figure best_fit_model_plot . cyprus_gdp_per_capita = gdp_per_capita.loc[&quot;Cyprus&quot;][&quot;GDP per capita&quot;] print(cyprus_gdp_per_capita) cyprus_predicted_life_satisfaction = lin1.predict([[cyprus_gdp_per_capita]])[0][0] cyprus_predicted_life_satisfaction . 22587.49 . 5.96244744318815 . sample_data.plot(kind=&#39;scatter&#39;, x=&quot;GDP per capita&quot;, y=&#39;Life satisfaction&#39;, figsize=(5,3), s=1) plt.xlabel(&quot;GDP per capita (USD)&quot;) X=np.linspace(0, 60000, 1000) plt.plot(X, t0 + t1*X, &quot;b&quot;) plt.axis([0, 60000, 0, 10]) plt.text(5000, 7.5, r&quot;$ theta_0 = 4.85$&quot;, fontsize=14, color=&quot;b&quot;) plt.text(5000, 6.6, r&quot;$ theta_1 = 4.91 times 10^{-5}$&quot;, fontsize=14, color=&quot;b&quot;) plt.plot([cyprus_gdp_per_capita, cyprus_gdp_per_capita], [0, cyprus_predicted_life_satisfaction], &quot;r--&quot;) plt.text(25000, 5.0, r&quot;Prediction = 5.96&quot;, fontsize=14, color=&quot;b&quot;) plt.plot(cyprus_gdp_per_capita, cyprus_predicted_life_satisfaction, &quot;ro&quot;) save_fig(&#39;cyprus_prediction_plot&#39;) plt.show() . Saving figure cyprus_prediction_plot . sample_data[7:10] . GDP per capita Life satisfaction . Country . Portugal 19121.592 | 5.1 | . Slovenia 20732.482 | 5.7 | . Spain 25864.721 | 6.5 | . (5.1+5.7+6.5)/3 . 5.766666666666667 . backup = oecd_bli, gdp_per_capita def prepare_country_stats(oecd_bli, gdp_per_capita): oecd_bli = oecd_bli[oecd_bli[&quot;INEQUALITY&quot;]==&quot;TOT&quot;] oecd_bli = oecd_bli.pivot(index=&quot;Country&quot;, columns=&quot;Indicator&quot;, values=&quot;Value&quot;) gdp_per_capita.rename(columns={&quot;2015&quot;: &quot;GDP per capita&quot;}, inplace=True) gdp_per_capita.set_index(&quot;Country&quot;, inplace=True) full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita, left_index=True, right_index=True) full_country_stats.sort_values(by=&quot;GDP per capita&quot;, inplace=True) remove_indices = [0, 1, 6, 8, 33, 34, 35] keep_indices = list(set(range(36)) - set(remove_indices)) return full_country_stats[[&quot;GDP per capita&quot;, &#39;Life satisfaction&#39;]].iloc[keep_indices] . # Code example import matplotlib.pyplot as plt import numpy as np import pandas as pd import sklearn.linear_model # Load the data oecd_bli = pd.read_csv(datapath + &quot;oecd_bli_2015.csv&quot;, thousands=&#39;,&#39;) gdp_per_capita = pd.read_csv(datapath + &quot;gdp_per_capita.csv&quot;,thousands=&#39;,&#39;,delimiter=&#39; t&#39;, encoding=&#39;latin1&#39;, na_values=&quot;n/a&quot;) # Prepare the data country_stats = prepare_country_stats(oecd_bli, gdp_per_capita) X = np.c_[country_stats[&quot;GDP per capita&quot;]] y = np.c_[country_stats[&quot;Life satisfaction&quot;]] # Visualize the data country_stats.plot(kind=&#39;scatter&#39;, x=&quot;GDP per capita&quot;, y=&#39;Life satisfaction&#39;) plt.show() # Select a linear model model = sklearn.linear_model.LinearRegression() # Train the model model.fit(X, y) # Make a prediction for Cyprus X_new = [[22587]] # Cyprus&#39; GDP per capita print(model.predict(X_new)) # outputs [[ 5.96242338]] . [[5.96242338]] . oecd_bli, gdp_per_capita = backup . missing_data . GDP per capita Life satisfaction . Country . Brazil 8669.998 | 7.0 | . Mexico 9009.280 | 6.7 | . Chile 13340.905 | 6.7 | . Czech Republic 17256.918 | 6.5 | . Norway 74822.106 | 7.4 | . Switzerland 80675.308 | 7.5 | . Luxembourg 101994.093 | 6.9 | . position_text2 = { &quot;Brazil&quot;: (1000, 9.0), &quot;Mexico&quot;: (11000, 9.0), &quot;Chile&quot;: (25000, 9.0), &quot;Czech Republic&quot;: (35000, 9.0), &quot;Norway&quot;: (60000, 3), &quot;Switzerland&quot;: (72000, 3.0), &quot;Luxembourg&quot;: (90000, 3.0), } . sample_data.plot(kind=&#39;scatter&#39;, x=&quot;GDP per capita&quot;, y=&#39;Life satisfaction&#39;, figsize=(8,3)) plt.axis([0, 110000, 0, 10]) for country, pos_text in position_text2.items(): pos_data_x, pos_data_y = missing_data.loc[country] plt.annotate(country, xy=(pos_data_x, pos_data_y), xytext=pos_text, arrowprops=dict(facecolor=&#39;black&#39;, width=0.5, shrink=0.1, headwidth=5)) plt.plot(pos_data_x, pos_data_y, &quot;rs&quot;) X=np.linspace(0, 110000, 1000) plt.plot(X, t0 + t1*X, &quot;b:&quot;) lin_reg_full = linear_model.LinearRegression() Xfull = np.c_[full_country_stats[&quot;GDP per capita&quot;]] yfull = np.c_[full_country_stats[&quot;Life satisfaction&quot;]] lin_reg_full.fit(Xfull, yfull) t0full, t1full = lin_reg_full.intercept_[0], lin_reg_full.coef_[0][0] X = np.linspace(0, 110000, 1000) plt.plot(X, t0full + t1full * X, &quot;k&quot;) plt.xlabel(&quot;GDP per capita (USD)&quot;) save_fig(&#39;representative_training_data_scatterplot&#39;) plt.show() . Saving figure representative_training_data_scatterplot . full_country_stats.plot(kind=&#39;scatter&#39;, x=&quot;GDP per capita&quot;, y=&#39;Life satisfaction&#39;, figsize=(8,3)) plt.axis([0, 110000, 0, 10]) from sklearn import preprocessing from sklearn import pipeline poly = preprocessing.PolynomialFeatures(degree=60, include_bias=False) scaler = preprocessing.StandardScaler() lin_reg2 = linear_model.LinearRegression() pipeline_reg = pipeline.Pipeline([(&#39;poly&#39;, poly), (&#39;scal&#39;, scaler), (&#39;lin&#39;, lin_reg2)]) pipeline_reg.fit(Xfull, yfull) curve = pipeline_reg.predict(X[:, np.newaxis]) plt.plot(X, curve) plt.xlabel(&quot;GDP per capita (USD)&quot;) save_fig(&#39;overfitting_model_plot&#39;) plt.show() . /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1508: RuntimeWarning: overflow encountered in multiply sqr = np.multiply(arr, arr, out=arr) /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce return ufunc.reduce(obj, axis, dtype, out, **passkwargs) . Saving figure overfitting_model_plot . full_country_stats.loc[[c for c in full_country_stats.index if &quot;W&quot; in c.upper()]][&quot;Life satisfaction&quot;] . Country New Zealand 7.3 Sweden 7.2 Norway 7.4 Switzerland 7.5 Name: Life satisfaction, dtype: float64 . gdp_per_capita.loc[[c for c in gdp_per_capita.index if &quot;W&quot; in c.upper()]].head() . Subject Descriptor Units Scale Country/Series-specific Notes GDP per capita Estimates Start After . Country . Botswana Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 6040.957 | 2008.0 | . Kuwait Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 29363.027 | 2014.0 | . Malawi Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 354.275 | 2011.0 | . New Zealand Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 37044.891 | 2015.0 | . Norway Gross domestic product per capita, current prices | U.S. dollars | Units | See notes for: Gross domestic product, curren... | 74822.106 | 2015.0 | . plt.figure(figsize=(8,3)) plt.xlabel(&quot;GDP per capita&quot;) plt.ylabel(&#39;Life satisfaction&#39;) plt.plot(list(sample_data[&quot;GDP per capita&quot;]), list(sample_data[&quot;Life satisfaction&quot;]), &quot;bo&quot;) plt.plot(list(missing_data[&quot;GDP per capita&quot;]), list(missing_data[&quot;Life satisfaction&quot;]), &quot;rs&quot;) X = np.linspace(0, 110000, 1000) plt.plot(X, t0full + t1full * X, &quot;r--&quot;, label=&quot;Linear model on all data&quot;) plt.plot(X, t0 + t1*X, &quot;b:&quot;, label=&quot;Linear model on partial data&quot;) ridge = linear_model.Ridge(alpha=10**9.5) Xsample = np.c_[sample_data[&quot;GDP per capita&quot;]] ysample = np.c_[sample_data[&quot;Life satisfaction&quot;]] ridge.fit(Xsample, ysample) t0ridge, t1ridge = ridge.intercept_[0], ridge.coef_[0][0] plt.plot(X, t0ridge + t1ridge * X, &quot;b&quot;, label=&quot;Regularized linear model on partial data&quot;) plt.legend(loc=&quot;lower right&quot;) plt.axis([0, 110000, 0, 10]) plt.xlabel(&quot;GDP per capita (USD)&quot;) save_fig(&#39;ridge_model_plot&#39;) plt.show() . Saving figure ridge_model_plot . backup = oecd_bli, gdp_per_capita def prepare_country_stats(oecd_bli, gdp_per_capita): return sample_data . # Replace this linear model: import sklearn.linear_model model = sklearn.linear_model.LinearRegression() . # with this k-neighbors regression model: import sklearn.neighbors model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3) . X = np.c_[country_stats[&quot;GDP per capita&quot;]] y = np.c_[country_stats[&quot;Life satisfaction&quot;]] # Train the model model.fit(X, y) # Make a prediction for Cyprus X_new = np.array([[22587.0]]) # Cyprus&#39; GDP per capita print(model.predict(X_new)) # outputs [[ 5.76666667]] . [[5.76666667]] .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/01_the_machine_learning_landscape",
            "relUrl": "/01_the_machine_learning_landscape",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Support Vector Machines",
            "content": "Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;svm&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . . Large margin classification . The next few code cells generate the first figures in chapter 5. The first actual code sample comes after: . #collapse-show from sklearn.svm import SVC from sklearn import datasets iris = datasets.load_iris() X = iris[&quot;data&quot;][:, (2, 3)] # petal length, petal width y = iris[&quot;target&quot;] setosa_or_versicolor = (y == 0) | (y == 1) X = X[setosa_or_versicolor] y = y[setosa_or_versicolor] # SVM Classifier model svm_clf = SVC(kernel=&quot;linear&quot;, C=float(&quot;inf&quot;)) svm_clf.fit(X, y) . . SVC(C=inf, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . #collapse-show # Bad models x0 = np.linspace(0, 5.5, 200) pred_1 = 5*x0 - 20 pred_2 = x0 - 1.8 pred_3 = 0.1 * x0 + 0.5 def plot_svc_decision_boundary(svm_clf, xmin, xmax): w = svm_clf.coef_[0] b = svm_clf.intercept_[0] # At the decision boundary, w0*x0 + w1*x1 + b = 0 # =&gt; x1 = -w0/w1 * x0 - b/w1 x0 = np.linspace(xmin, xmax, 200) decision_boundary = -w[0]/w[1] * x0 - b/w[1] margin = 1/w[1] gutter_up = decision_boundary + margin gutter_down = decision_boundary - margin svs = svm_clf.support_vectors_ plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors=&#39;#FFAAAA&#39;) plt.plot(x0, decision_boundary, &quot;k-&quot;, linewidth=2) plt.plot(x0, gutter_up, &quot;k--&quot;, linewidth=2) plt.plot(x0, gutter_down, &quot;k--&quot;, linewidth=2) fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True) plt.sca(axes[0]) plt.plot(x0, pred_1, &quot;g--&quot;, linewidth=2) plt.plot(x0, pred_2, &quot;m-&quot;, linewidth=2) plt.plot(x0, pred_3, &quot;r-&quot;, linewidth=2) plt.plot(X[:, 0][y==1], X[:, 1][y==1], &quot;bs&quot;, label=&quot;Iris versicolor&quot;) plt.plot(X[:, 0][y==0], X[:, 1][y==0], &quot;yo&quot;, label=&quot;Iris setosa&quot;) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.legend(loc=&quot;upper left&quot;, fontsize=14) plt.axis([0, 5.5, 0, 2]) plt.sca(axes[1]) plot_svc_decision_boundary(svm_clf, 0, 5.5) plt.plot(X[:, 0][y==1], X[:, 1][y==1], &quot;bs&quot;) plt.plot(X[:, 0][y==0], X[:, 1][y==0], &quot;yo&quot;) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.axis([0, 5.5, 0, 2]) save_fig(&quot;large_margin_classification_plot&quot;) plt.show() . . Saving figure large_margin_classification_plot . Sensitivity to feature scales . #collapse-show Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64) ys = np.array([0, 0, 1, 1]) svm_clf = SVC(kernel=&quot;linear&quot;, C=100) svm_clf.fit(Xs, ys) plt.figure(figsize=(9,2.7)) plt.subplot(121) plt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], &quot;bo&quot;) plt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], &quot;ms&quot;) plot_svc_decision_boundary(svm_clf, 0, 6) plt.xlabel(&quot;$x_0$&quot;, fontsize=20) plt.ylabel(&quot;$x_1$Â Â Â Â &quot;, fontsize=20, rotation=0) plt.title(&quot;Unscaled&quot;, fontsize=16) plt.axis([0, 6, 0, 90]) from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_scaled = scaler.fit_transform(Xs) svm_clf.fit(X_scaled, ys) plt.subplot(122) plt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], &quot;bo&quot;) plt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], &quot;ms&quot;) plot_svc_decision_boundary(svm_clf, -2, 2) plt.xlabel(&quot;$x_0$&quot;, fontsize=20) plt.ylabel(&quot;$x&#39;_1$ &quot;, fontsize=20, rotation=0) plt.title(&quot;Scaled&quot;, fontsize=16) plt.axis([-2, 2, -2, 2]) save_fig(&quot;sensitivity_to_feature_scales_plot&quot;) . . Saving figure sensitivity_to_feature_scales_plot . Sensitivity to outliers . #collapse-show X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]]) y_outliers = np.array([0, 0]) Xo1 = np.concatenate([X, X_outliers[:1]], axis=0) yo1 = np.concatenate([y, y_outliers[:1]], axis=0) Xo2 = np.concatenate([X, X_outliers[1:]], axis=0) yo2 = np.concatenate([y, y_outliers[1:]], axis=0) svm_clf2 = SVC(kernel=&quot;linear&quot;, C=10**9) svm_clf2.fit(Xo2, yo2) fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True) plt.sca(axes[0]) plt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], &quot;bs&quot;) plt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], &quot;yo&quot;) plt.text(0.3, 1.0, &quot;Impossible!&quot;, fontsize=24, color=&quot;red&quot;) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.annotate(&quot;Outlier&quot;, xy=(X_outliers[0][0], X_outliers[0][1]), xytext=(2.5, 1.7), ha=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.1), fontsize=16, ) plt.axis([0, 5.5, 0, 2]) plt.sca(axes[1]) plt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], &quot;bs&quot;) plt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], &quot;yo&quot;) plot_svc_decision_boundary(svm_clf2, 0, 5.5) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.annotate(&quot;Outlier&quot;, xy=(X_outliers[1][0], X_outliers[1][1]), xytext=(3.2, 0.08), ha=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.1), fontsize=16, ) plt.axis([0, 5.5, 0, 2]) save_fig(&quot;sensitivity_to_outliers_plot&quot;) plt.show() . . Saving figure sensitivity_to_outliers_plot . Large margin vs margin violations . This is the first code example in chapter 5: . #collapse-show import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[&quot;data&quot;][:, (2, 3)] # petal length, petal width y = (iris[&quot;target&quot;] == 2).astype(np.float64) # Iris virginica svm_clf = Pipeline([ (&quot;scaler&quot;, StandardScaler()), (&quot;linear_svc&quot;, LinearSVC(C=1, loss=&quot;hinge&quot;, random_state=42)), ]) svm_clf.fit(X, y) . . Pipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;linear_svc&#39;, LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=42, tol=0.0001, verbose=0))]) . svm_clf.predict([[5.5, 1.7]]) . array([1.]) . Now let&#39;s generate the graph comparing different regularization settings: . #collapse-show scaler = StandardScaler() svm_clf1 = LinearSVC(C=1, loss=&quot;hinge&quot;, random_state=42) svm_clf2 = LinearSVC(C=100, loss=&quot;hinge&quot;, random_state=42) scaled_svm_clf1 = Pipeline([ (&quot;scaler&quot;, scaler), (&quot;linear_svc&quot;, svm_clf1), ]) scaled_svm_clf2 = Pipeline([ (&quot;scaler&quot;, scaler), (&quot;linear_svc&quot;, svm_clf2), ]) scaled_svm_clf1.fit(X, y) scaled_svm_clf2.fit(X, y) . . /Users/ageron/miniconda3/envs/tf2b/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. &#34;the number of iterations.&#34;, ConvergenceWarning) . Pipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;linear_svc&#39;, LinearSVC(C=100, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=42, tol=0.0001, verbose=0))]) . #collapse-show # Convert to unscaled parameters b1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_]) b2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_]) w1 = svm_clf1.coef_[0] / scaler.scale_ w2 = svm_clf2.coef_[0] / scaler.scale_ svm_clf1.intercept_ = np.array([b1]) svm_clf2.intercept_ = np.array([b2]) svm_clf1.coef_ = np.array([w1]) svm_clf2.coef_ = np.array([w2]) # Find support vectors (LinearSVC does not do this automatically) t = y * 2 - 1 support_vectors_idx1 = (t * (X.dot(w1) + b1) &lt; 1).ravel() support_vectors_idx2 = (t * (X.dot(w2) + b2) &lt; 1).ravel() svm_clf1.support_vectors_ = X[support_vectors_idx1] svm_clf2.support_vectors_ = X[support_vectors_idx2] . . #collapse-show fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True) plt.sca(axes[0]) plt.plot(X[:, 0][y==1], X[:, 1][y==1], &quot;g^&quot;, label=&quot;Iris virginica&quot;) plt.plot(X[:, 0][y==0], X[:, 1][y==0], &quot;bs&quot;, label=&quot;Iris versicolor&quot;) plot_svc_decision_boundary(svm_clf1, 4, 5.9) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.legend(loc=&quot;upper left&quot;, fontsize=14) plt.title(&quot;$C = {}$&quot;.format(svm_clf1.C), fontsize=16) plt.axis([4, 5.9, 0.8, 2.8]) plt.sca(axes[1]) plt.plot(X[:, 0][y==1], X[:, 1][y==1], &quot;g^&quot;) plt.plot(X[:, 0][y==0], X[:, 1][y==0], &quot;bs&quot;) plot_svc_decision_boundary(svm_clf2, 4, 5.99) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.title(&quot;$C = {}$&quot;.format(svm_clf2.C), fontsize=16) plt.axis([4, 5.9, 0.8, 2.8]) save_fig(&quot;regularization_plot&quot;) . . Saving figure regularization_plot . Non-linear classification . #collapse-show X1D = np.linspace(-4, 4, 9).reshape(-1, 1) X2D = np.c_[X1D, X1D**2] y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0]) plt.figure(figsize=(10, 3)) plt.subplot(121) plt.grid(True, which=&#39;both&#39;) plt.axhline(y=0, color=&#39;k&#39;) plt.plot(X1D[:, 0][y==0], np.zeros(4), &quot;bs&quot;) plt.plot(X1D[:, 0][y==1], np.zeros(5), &quot;g^&quot;) plt.gca().get_yaxis().set_ticks([]) plt.xlabel(r&quot;$x_1$&quot;, fontsize=20) plt.axis([-4.5, 4.5, -0.2, 0.2]) plt.subplot(122) plt.grid(True, which=&#39;both&#39;) plt.axhline(y=0, color=&#39;k&#39;) plt.axvline(x=0, color=&#39;k&#39;) plt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], &quot;bs&quot;) plt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], &quot;g^&quot;) plt.xlabel(r&quot;$x_1$&quot;, fontsize=20) plt.ylabel(r&quot;$x_2$Â Â &quot;, fontsize=20, rotation=0) plt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16]) plt.plot([-4.5, 4.5], [6.5, 6.5], &quot;r--&quot;, linewidth=3) plt.axis([-4.5, 4.5, -1, 17]) plt.subplots_adjust(right=1) save_fig(&quot;higher_dimensions_plot&quot;, tight_layout=False) plt.show() . . Saving figure higher_dimensions_plot . from sklearn.datasets import make_moons X, y = make_moons(n_samples=100, noise=0.15, random_state=42) def plot_dataset(X, y, axes): plt.plot(X[:, 0][y==0], X[:, 1][y==0], &quot;bs&quot;) plt.plot(X[:, 0][y==1], X[:, 1][y==1], &quot;g^&quot;) plt.axis(axes) plt.grid(True, which=&#39;both&#39;) plt.xlabel(r&quot;$x_1$&quot;, fontsize=20) plt.ylabel(r&quot;$x_2$&quot;, fontsize=20, rotation=0) plot_dataset(X, y, [-1.5, 2.5, -1, 1.5]) plt.show() . from sklearn.datasets import make_moons from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures polynomial_svm_clf = Pipeline([ (&quot;poly_features&quot;, PolynomialFeatures(degree=3)), (&quot;scaler&quot;, StandardScaler()), (&quot;svm_clf&quot;, LinearSVC(C=10, loss=&quot;hinge&quot;, random_state=42)) ]) polynomial_svm_clf.fit(X, y) . Pipeline(memory=None, steps=[(&#39;poly_features&#39;, PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;svm_clf&#39;, LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=42, tol=0.0001, verbose=0))]) . def plot_predictions(clf, axes): x0s = np.linspace(axes[0], axes[1], 100) x1s = np.linspace(axes[2], axes[3], 100) x0, x1 = np.meshgrid(x0s, x1s) X = np.c_[x0.ravel(), x1.ravel()] y_pred = clf.predict(X).reshape(x0.shape) y_decision = clf.decision_function(X).reshape(x0.shape) plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2) plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1) plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5]) plot_dataset(X, y, [-1.5, 2.5, -1, 1.5]) save_fig(&quot;moons_polynomial_svc_plot&quot;) plt.show() . Saving figure moons_polynomial_svc_plot . from sklearn.svm import SVC poly_kernel_svm_clf = Pipeline([ (&quot;scaler&quot;, StandardScaler()), (&quot;svm_clf&quot;, SVC(kernel=&quot;poly&quot;, degree=3, coef0=1, C=5)) ]) poly_kernel_svm_clf.fit(X, y) . Pipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;svm_clf&#39;, SVC(C=5, cache_size=200, class_weight=None, coef0=1, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;poly&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))]) . poly100_kernel_svm_clf = Pipeline([ (&quot;scaler&quot;, StandardScaler()), (&quot;svm_clf&quot;, SVC(kernel=&quot;poly&quot;, degree=10, coef0=100, C=5)) ]) poly100_kernel_svm_clf.fit(X, y) . Pipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;svm_clf&#39;, SVC(C=5, cache_size=200, class_weight=None, coef0=100, decision_function_shape=&#39;ovr&#39;, degree=10, gamma=&#39;auto_deprecated&#39;, kernel=&#39;poly&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))]) . #collapse-show fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True) plt.sca(axes[0]) plot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5]) plot_dataset(X, y, [-1.5, 2.4, -1, 1.5]) plt.title(r&quot;$d=3, r=1, C=5$&quot;, fontsize=18) plt.sca(axes[1]) plot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5]) plot_dataset(X, y, [-1.5, 2.4, -1, 1.5]) plt.title(r&quot;$d=10, r=100, C=5$&quot;, fontsize=18) plt.ylabel(&quot;&quot;) save_fig(&quot;moons_kernelized_polynomial_svc_plot&quot;) plt.show() . . Saving figure moons_kernelized_polynomial_svc_plot . #collapse-show def gaussian_rbf(x, landmark, gamma): return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2) gamma = 0.3 x1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1) x2s = gaussian_rbf(x1s, -2, gamma) x3s = gaussian_rbf(x1s, 1, gamma) XK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)] yk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0]) plt.figure(figsize=(10.5, 4)) plt.subplot(121) plt.grid(True, which=&#39;both&#39;) plt.axhline(y=0, color=&#39;k&#39;) plt.scatter(x=[-2, 1], y=[0, 0], s=150, alpha=0.5, c=&quot;red&quot;) plt.plot(X1D[:, 0][yk==0], np.zeros(4), &quot;bs&quot;) plt.plot(X1D[:, 0][yk==1], np.zeros(5), &quot;g^&quot;) plt.plot(x1s, x2s, &quot;g--&quot;) plt.plot(x1s, x3s, &quot;b:&quot;) plt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1]) plt.xlabel(r&quot;$x_1$&quot;, fontsize=20) plt.ylabel(r&quot;Similarity&quot;, fontsize=14) plt.annotate(r&#39;$ mathbf{x}$&#39;, xy=(X1D[3, 0], 0), xytext=(-0.5, 0.20), ha=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.1), fontsize=18, ) plt.text(-2, 0.9, &quot;$x_2$&quot;, ha=&quot;center&quot;, fontsize=20) plt.text(1, 0.9, &quot;$x_3$&quot;, ha=&quot;center&quot;, fontsize=20) plt.axis([-4.5, 4.5, -0.1, 1.1]) plt.subplot(122) plt.grid(True, which=&#39;both&#39;) plt.axhline(y=0, color=&#39;k&#39;) plt.axvline(x=0, color=&#39;k&#39;) plt.plot(XK[:, 0][yk==0], XK[:, 1][yk==0], &quot;bs&quot;) plt.plot(XK[:, 0][yk==1], XK[:, 1][yk==1], &quot;g^&quot;) plt.xlabel(r&quot;$x_2$&quot;, fontsize=20) plt.ylabel(r&quot;$x_3$Â Â &quot;, fontsize=20, rotation=0) plt.annotate(r&#39;$ phi left( mathbf{x} right)$&#39;, xy=(XK[3, 0], XK[3, 1]), xytext=(0.65, 0.50), ha=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.1), fontsize=18, ) plt.plot([-0.1, 1.1], [0.57, -0.1], &quot;r--&quot;, linewidth=3) plt.axis([-0.1, 1.1, -0.1, 1.1]) plt.subplots_adjust(right=1) save_fig(&quot;kernel_method_plot&quot;) plt.show() . . Saving figure kernel_method_plot . x1_example = X1D[3, 0] for landmark in (-2, 1): k = gaussian_rbf(np.array([[x1_example]]), np.array([[landmark]]), gamma) print(&quot;Phi({}, {}) = {}&quot;.format(x1_example, landmark, k)) . Phi(-1.0, -2) = [0.74081822] Phi(-1.0, 1) = [0.30119421] . rbf_kernel_svm_clf = Pipeline([ (&quot;scaler&quot;, StandardScaler()), (&quot;svm_clf&quot;, SVC(kernel=&quot;rbf&quot;, gamma=5, C=0.001)) ]) rbf_kernel_svm_clf.fit(X, y) . Pipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;svm_clf&#39;, SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=5, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))]) . #collapse-show from sklearn.svm import SVC gamma1, gamma2 = 0.1, 5 C1, C2 = 0.001, 1000 hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2) svm_clfs = [] for gamma, C in hyperparams: rbf_kernel_svm_clf = Pipeline([ (&quot;scaler&quot;, StandardScaler()), (&quot;svm_clf&quot;, SVC(kernel=&quot;rbf&quot;, gamma=gamma, C=C)) ]) rbf_kernel_svm_clf.fit(X, y) svm_clfs.append(rbf_kernel_svm_clf) fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True) for i, svm_clf in enumerate(svm_clfs): plt.sca(axes[i // 2, i % 2]) plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5]) plot_dataset(X, y, [-1.5, 2.45, -1, 1.5]) gamma, C = hyperparams[i] plt.title(r&quot;$ gamma = {}, C = {}$&quot;.format(gamma, C), fontsize=16) if i in (0, 1): plt.xlabel(&quot;&quot;) if i in (1, 3): plt.ylabel(&quot;&quot;) save_fig(&quot;moons_rbf_svc_plot&quot;) plt.show() . . Saving figure moons_rbf_svc_plot . Regression . np.random.seed(42) m = 50 X = 2 * np.random.rand(m, 1) y = (4 + 3 * X + np.random.randn(m, 1)).ravel() . from sklearn.svm import LinearSVR svm_reg = LinearSVR(epsilon=1.5, random_state=42) svm_reg.fit(X, y) . LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True, intercept_scaling=1.0, loss=&#39;epsilon_insensitive&#39;, max_iter=1000, random_state=42, tol=0.0001, verbose=0) . svm_reg1 = LinearSVR(epsilon=1.5, random_state=42) svm_reg2 = LinearSVR(epsilon=0.5, random_state=42) svm_reg1.fit(X, y) svm_reg2.fit(X, y) def find_support_vectors(svm_reg, X, y): y_pred = svm_reg.predict(X) off_margin = (np.abs(y - y_pred) &gt;= svm_reg.epsilon) return np.argwhere(off_margin) svm_reg1.support_ = find_support_vectors(svm_reg1, X, y) svm_reg2.support_ = find_support_vectors(svm_reg2, X, y) eps_x1 = 1 eps_y_pred = svm_reg1.predict([[eps_x1]]) . #collapse-show def plot_svm_regression(svm_reg, X, y, axes): x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1) y_pred = svm_reg.predict(x1s) plt.plot(x1s, y_pred, &quot;k-&quot;, linewidth=2, label=r&quot;$ hat{y}$&quot;) plt.plot(x1s, y_pred + svm_reg.epsilon, &quot;k--&quot;) plt.plot(x1s, y_pred - svm_reg.epsilon, &quot;k--&quot;) plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors=&#39;#FFAAAA&#39;) plt.plot(X, y, &quot;bo&quot;) plt.xlabel(r&quot;$x_1$&quot;, fontsize=18) plt.legend(loc=&quot;upper left&quot;, fontsize=18) plt.axis(axes) fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True) plt.sca(axes[0]) plot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11]) plt.title(r&quot;$ epsilon = {}$&quot;.format(svm_reg1.epsilon), fontsize=18) plt.ylabel(r&quot;$y$&quot;, fontsize=18, rotation=0) #plt.plot([eps_x1, eps_x1], [eps_y_pred, eps_y_pred - svm_reg1.epsilon], &quot;k-&quot;, linewidth=2) plt.annotate( &#39;&#39;, xy=(eps_x1, eps_y_pred), xycoords=&#39;data&#39;, xytext=(eps_x1, eps_y_pred - svm_reg1.epsilon), textcoords=&#39;data&#39;, arrowprops={&#39;arrowstyle&#39;: &#39;&lt;-&gt;&#39;, &#39;linewidth&#39;: 1.5} ) plt.text(0.91, 5.6, r&quot;$ epsilon$&quot;, fontsize=20) plt.sca(axes[1]) plot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11]) plt.title(r&quot;$ epsilon = {}$&quot;.format(svm_reg2.epsilon), fontsize=18) save_fig(&quot;svm_regression_plot&quot;) plt.show() . . Saving figure svm_regression_plot . np.random.seed(42) m = 100 X = 2 * np.random.rand(m, 1) - 1 y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel() . Note: to be future-proof, we set gamma=&quot;scale&quot;, as this will be the default value in Scikit-Learn 0.22. . from sklearn.svm import SVR svm_poly_reg = SVR(kernel=&quot;poly&quot;, degree=2, C=100, epsilon=0.1, gamma=&quot;scale&quot;) svm_poly_reg.fit(X, y) . SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma=&#39;scale&#39;, kernel=&#39;poly&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False) . from sklearn.svm import SVR svm_poly_reg1 = SVR(kernel=&quot;poly&quot;, degree=2, C=100, epsilon=0.1, gamma=&quot;scale&quot;) svm_poly_reg2 = SVR(kernel=&quot;poly&quot;, degree=2, C=0.01, epsilon=0.1, gamma=&quot;scale&quot;) svm_poly_reg1.fit(X, y) svm_poly_reg2.fit(X, y) . SVR(C=0.01, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma=&#39;scale&#39;, kernel=&#39;poly&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False) . fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True) plt.sca(axes[0]) plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1]) plt.title(r&quot;$degree={}, C={}, epsilon = {}$&quot;.format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon), fontsize=18) plt.ylabel(r&quot;$y$&quot;, fontsize=18, rotation=0) plt.sca(axes[1]) plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1]) plt.title(r&quot;$degree={}, C={}, epsilon = {}$&quot;.format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon), fontsize=18) save_fig(&quot;svm_with_polynomial_kernel_plot&quot;) plt.show() . Saving figure svm_with_polynomial_kernel_plot . Under the hood . iris = datasets.load_iris() X = iris[&quot;data&quot;][:, (2, 3)] # petal length, petal width y = (iris[&quot;target&quot;] == 2).astype(np.float64) # Iris virginica . #collapse-show from mpl_toolkits.mplot3d import Axes3D def plot_3D_decision_function(ax, w, b, x1_lim=[4, 6], x2_lim=[0.8, 2.8]): x1_in_bounds = (X[:, 0] &gt; x1_lim[0]) &amp; (X[:, 0] &lt; x1_lim[1]) X_crop = X[x1_in_bounds] y_crop = y[x1_in_bounds] x1s = np.linspace(x1_lim[0], x1_lim[1], 20) x2s = np.linspace(x2_lim[0], x2_lim[1], 20) x1, x2 = np.meshgrid(x1s, x2s) xs = np.c_[x1.ravel(), x2.ravel()] df = (xs.dot(w) + b).reshape(x1.shape) m = 1 / np.linalg.norm(w) boundary_x2s = -x1s*(w[0]/w[1])-b/w[1] margin_x2s_1 = -x1s*(w[0]/w[1])-(b-1)/w[1] margin_x2s_2 = -x1s*(w[0]/w[1])-(b+1)/w[1] ax.plot_surface(x1s, x2, np.zeros_like(x1), color=&quot;b&quot;, alpha=0.2, cstride=100, rstride=100) ax.plot(x1s, boundary_x2s, 0, &quot;k-&quot;, linewidth=2, label=r&quot;$h=0$&quot;) ax.plot(x1s, margin_x2s_1, 0, &quot;k--&quot;, linewidth=2, label=r&quot;$h= pm 1$&quot;) ax.plot(x1s, margin_x2s_2, 0, &quot;k--&quot;, linewidth=2) ax.plot(X_crop[:, 0][y_crop==1], X_crop[:, 1][y_crop==1], 0, &quot;g^&quot;) ax.plot_wireframe(x1, x2, df, alpha=0.3, color=&quot;k&quot;) ax.plot(X_crop[:, 0][y_crop==0], X_crop[:, 1][y_crop==0], 0, &quot;bs&quot;) ax.axis(x1_lim + x2_lim) ax.text(4.5, 2.5, 3.8, &quot;Decision function $h$&quot;, fontsize=16) ax.set_xlabel(r&quot;Petal length&quot;, fontsize=16, labelpad=10) ax.set_ylabel(r&quot;Petal width&quot;, fontsize=16, labelpad=10) ax.set_zlabel(r&quot;$h = mathbf{w}^T mathbf{x} + b$&quot;, fontsize=18, labelpad=5) ax.legend(loc=&quot;upper left&quot;, fontsize=16) fig = plt.figure(figsize=(11, 6)) ax1 = fig.add_subplot(111, projection=&#39;3d&#39;) plot_3D_decision_function(ax1, w=svm_clf2.coef_[0], b=svm_clf2.intercept_[0]) save_fig(&quot;iris_3D_plot&quot;) plt.show() . . Saving figure iris_3D_plot . Small weight vector results in a large margin . #collapse-show def plot_2D_decision_function(w, b, ylabel=True, x1_lim=[-3, 3]): x1 = np.linspace(x1_lim[0], x1_lim[1], 200) y = w * x1 + b m = 1 / w plt.plot(x1, y) plt.plot(x1_lim, [1, 1], &quot;k:&quot;) plt.plot(x1_lim, [-1, -1], &quot;k:&quot;) plt.axhline(y=0, color=&#39;k&#39;) plt.axvline(x=0, color=&#39;k&#39;) plt.plot([m, m], [0, 1], &quot;k--&quot;) plt.plot([-m, -m], [0, -1], &quot;k--&quot;) plt.plot([-m, m], [0, 0], &quot;k-o&quot;, linewidth=3) plt.axis(x1_lim + [-2, 2]) plt.xlabel(r&quot;$x_1$&quot;, fontsize=16) if ylabel: plt.ylabel(r&quot;$w_1 x_1$Â Â &quot;, rotation=0, fontsize=16) plt.title(r&quot;$w_1 = {}$&quot;.format(w), fontsize=16) fig, axes = plt.subplots(ncols=2, figsize=(9, 3.2), sharey=True) plt.sca(axes[0]) plot_2D_decision_function(1, 0) plt.sca(axes[1]) plot_2D_decision_function(0.5, 0, ylabel=False) save_fig(&quot;small_w_large_margin_plot&quot;) plt.show() . . Saving figure small_w_large_margin_plot . from sklearn.svm import SVC from sklearn import datasets iris = datasets.load_iris() X = iris[&quot;data&quot;][:, (2, 3)] # petal length, petal width y = (iris[&quot;target&quot;] == 2).astype(np.float64) # Iris virginica svm_clf = SVC(kernel=&quot;linear&quot;, C=1) svm_clf.fit(X, y) svm_clf.predict([[5.3, 1.3]]) . array([1.]) . Hinge loss . t = np.linspace(-2, 4, 200) h = np.where(1 - t &lt; 0, 0, 1 - t) # max(0, 1-t) plt.figure(figsize=(5,2.8)) plt.plot(t, h, &quot;b-&quot;, linewidth=2, label=&quot;$max(0, 1 - t)$&quot;) plt.grid(True, which=&#39;both&#39;) plt.axhline(y=0, color=&#39;k&#39;) plt.axvline(x=0, color=&#39;k&#39;) plt.yticks(np.arange(-1, 2.5, 1)) plt.xlabel(&quot;$t$&quot;, fontsize=16) plt.axis([-2, 4, -1, 2.5]) plt.legend(loc=&quot;upper right&quot;, fontsize=16) save_fig(&quot;hinge_plot&quot;) plt.show() . Saving figure hinge_plot . Extra material . Training time . X, y = make_moons(n_samples=1000, noise=0.4, random_state=42) plt.plot(X[:, 0][y==0], X[:, 1][y==0], &quot;bs&quot;) plt.plot(X[:, 0][y==1], X[:, 1][y==1], &quot;g^&quot;) . [&lt;matplotlib.lines.Line2D at 0x7f985051ef90&gt;] . #collapse-show import time tol = 0.1 tols = [] times = [] for i in range(10): svm_clf = SVC(kernel=&quot;poly&quot;, gamma=3, C=10, tol=tol, verbose=1) t1 = time.time() svm_clf.fit(X, y) t2 = time.time() times.append(t2-t1) tols.append(tol) print(i, tol, t2-t1) tol /= 10 plt.semilogx(tols, times, &quot;bo-&quot;) plt.xlabel(&quot;Tolerance&quot;, fontsize=16) plt.ylabel(&quot;Time (seconds)&quot;, fontsize=16) plt.grid(True) plt.show() . . [LibSVM]0 0.1 0.19860410690307617 [LibSVM]1 0.01 0.18686413764953613 [LibSVM]2 0.001 0.20957088470458984 [LibSVM]3 0.0001 0.3814828395843506 [LibSVM]4 1e-05 0.6392810344696045 [LibSVM]5 1.0000000000000002e-06 0.5971472263336182 [LibSVM]6 1.0000000000000002e-07 0.6291971206665039 [LibSVM]7 1.0000000000000002e-08 0.6234230995178223 [LibSVM]8 1.0000000000000003e-09 0.6174850463867188 [LibSVM]9 1.0000000000000003e-10 0.6262409687042236 . Linear SVM classifier implementation using Batch Gradient Descent . # Training set X = iris[&quot;data&quot;][:, (2, 3)] # petal length, petal width y = (iris[&quot;target&quot;] == 2).astype(np.float64).reshape(-1, 1) # Iris virginica . #collapse-show from sklearn.base import BaseEstimator class MyLinearSVC(BaseEstimator): def __init__(self, C=1, eta0=1, eta_d=10000, n_epochs=1000, random_state=None): self.C = C self.eta0 = eta0 self.n_epochs = n_epochs self.random_state = random_state self.eta_d = eta_d def eta(self, epoch): return self.eta0 / (epoch + self.eta_d) def fit(self, X, y): # Random initialization if self.random_state: np.random.seed(self.random_state) w = np.random.randn(X.shape[1], 1) # n feature weights b = 0 m = len(X) t = y * 2 - 1 # -1 if t==0, +1 if t==1 X_t = X * t self.Js=[] # Training for epoch in range(self.n_epochs): support_vectors_idx = (X_t.dot(w) + t * b &lt; 1).ravel() X_t_sv = X_t[support_vectors_idx] t_sv = t[support_vectors_idx] J = 1/2 * np.sum(w * w) + self.C * (np.sum(1 - X_t_sv.dot(w)) - b * np.sum(t_sv)) self.Js.append(J) w_gradient_vector = w - self.C * np.sum(X_t_sv, axis=0).reshape(-1, 1) b_derivative = -C * np.sum(t_sv) w = w - self.eta(epoch) * w_gradient_vector b = b - self.eta(epoch) * b_derivative self.intercept_ = np.array([b]) self.coef_ = np.array([w]) support_vectors_idx = (X_t.dot(w) + t * b &lt; 1).ravel() self.support_vectors_ = X[support_vectors_idx] return self def decision_function(self, X): return X.dot(self.coef_[0]) + self.intercept_[0] def predict(self, X): return (self.decision_function(X) &gt;= 0).astype(np.float64) C=2 svm_clf = MyLinearSVC(C=C, eta0 = 10, eta_d = 1000, n_epochs=60000, random_state=2) svm_clf.fit(X, y) svm_clf.predict(np.array([[5, 2], [4, 1]])) . . array([[1.], [0.]]) . plt.plot(range(svm_clf.n_epochs), svm_clf.Js) plt.axis([0, svm_clf.n_epochs, 0, 100]) . [0, 60000, 0, 100] . print(svm_clf.intercept_, svm_clf.coef_) . [-15.56761653] [[[2.28120287] [2.71621742]]] . svm_clf2 = SVC(kernel=&quot;linear&quot;, C=C) svm_clf2.fit(X, y.ravel()) print(svm_clf2.intercept_, svm_clf2.coef_) . [-15.51721253] [[2.27128546 2.71287145]] . #collapse-show yr = y.ravel() fig, axes = plt.subplots(ncols=2, figsize=(11, 3.2), sharey=True) plt.sca(axes[0]) plt.plot(X[:, 0][yr==1], X[:, 1][yr==1], &quot;g^&quot;, label=&quot;Iris virginica&quot;) plt.plot(X[:, 0][yr==0], X[:, 1][yr==0], &quot;bs&quot;, label=&quot;Not Iris virginica&quot;) plot_svc_decision_boundary(svm_clf, 4, 6) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.title(&quot;MyLinearSVC&quot;, fontsize=14) plt.axis([4, 6, 0.8, 2.8]) plt.legend(loc=&quot;upper left&quot;) plt.sca(axes[1]) plt.plot(X[:, 0][yr==1], X[:, 1][yr==1], &quot;g^&quot;) plt.plot(X[:, 0][yr==0], X[:, 1][yr==0], &quot;bs&quot;) plot_svc_decision_boundary(svm_clf2, 4, 6) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.title(&quot;SVC&quot;, fontsize=14) plt.axis([4, 6, 0.8, 2.8]) . . [4, 6, 0.8, 2.8] . from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(loss=&quot;hinge&quot;, alpha=0.017, max_iter=1000, tol=1e-3, random_state=42) sgd_clf.fit(X, y.ravel()) m = len(X) t = y * 2 - 1 # -1 if t==0, +1 if t==1 X_b = np.c_[np.ones((m, 1)), X] # Add bias input x0=1 X_b_t = X_b * t sgd_theta = np.r_[sgd_clf.intercept_[0], sgd_clf.coef_[0]] print(sgd_theta) support_vectors_idx = (X_b_t.dot(sgd_theta) &lt; 1).ravel() sgd_clf.support_vectors_ = X[support_vectors_idx] sgd_clf.C = C plt.figure(figsize=(5.5,3.2)) plt.plot(X[:, 0][yr==1], X[:, 1][yr==1], &quot;g^&quot;) plt.plot(X[:, 0][yr==0], X[:, 1][yr==0], &quot;bs&quot;) plot_svc_decision_boundary(sgd_clf, 4, 6) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.title(&quot;SGDClassifier&quot;, fontsize=14) plt.axis([4, 6, 0.8, 2.8]) . [-14.32885908 2.27493198 1.98084355] . [4, 6, 0.8, 2.8] . Exercise solutions . 1. to 7. . See appendix A. . 8. . Exercise: train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model. . Let&#39;s use the Iris dataset: the Iris Setosa and Iris Versicolor classes are linearly separable. . from sklearn import datasets iris = datasets.load_iris() X = iris[&quot;data&quot;][:, (2, 3)] # petal length, petal width y = iris[&quot;target&quot;] setosa_or_versicolor = (y == 0) | (y == 1) X = X[setosa_or_versicolor] y = y[setosa_or_versicolor] . #collapse-show from sklearn.svm import SVC, LinearSVC from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler C = 5 alpha = 1 / (C * len(X)) lin_clf = LinearSVC(loss=&quot;hinge&quot;, C=C, random_state=42) svm_clf = SVC(kernel=&quot;linear&quot;, C=C) sgd_clf = SGDClassifier(loss=&quot;hinge&quot;, learning_rate=&quot;constant&quot;, eta0=0.001, alpha=alpha, max_iter=1000, tol=1e-3, random_state=42) scaler = StandardScaler() X_scaled = scaler.fit_transform(X) lin_clf.fit(X_scaled, y) svm_clf.fit(X_scaled, y) sgd_clf.fit(X_scaled, y) print(&quot;LinearSVC: &quot;, lin_clf.intercept_, lin_clf.coef_) print(&quot;SVC: &quot;, svm_clf.intercept_, svm_clf.coef_) print(&quot;SGDClassifier(alpha={:.5f}):&quot;.format(sgd_clf.alpha), sgd_clf.intercept_, sgd_clf.coef_) . . LinearSVC: [0.28474532] [[1.05364923 1.09903601]] SVC: [0.31896852] [[1.1203284 1.02625193]] SGDClassifier(alpha=0.00200): [0.117] [[0.77666262 0.72787608]] . Let&#39;s plot the decision boundaries of these three models: . #collapse-show # Compute the slope and bias of each decision boundary w1 = -lin_clf.coef_[0, 0]/lin_clf.coef_[0, 1] b1 = -lin_clf.intercept_[0]/lin_clf.coef_[0, 1] w2 = -svm_clf.coef_[0, 0]/svm_clf.coef_[0, 1] b2 = -svm_clf.intercept_[0]/svm_clf.coef_[0, 1] w3 = -sgd_clf.coef_[0, 0]/sgd_clf.coef_[0, 1] b3 = -sgd_clf.intercept_[0]/sgd_clf.coef_[0, 1] # Transform the decision boundary lines back to the original scale line1 = scaler.inverse_transform([[-10, -10 * w1 + b1], [10, 10 * w1 + b1]]) line2 = scaler.inverse_transform([[-10, -10 * w2 + b2], [10, 10 * w2 + b2]]) line3 = scaler.inverse_transform([[-10, -10 * w3 + b3], [10, 10 * w3 + b3]]) # Plot all three decision boundaries plt.figure(figsize=(11, 4)) plt.plot(line1[:, 0], line1[:, 1], &quot;k:&quot;, label=&quot;LinearSVC&quot;) plt.plot(line2[:, 0], line2[:, 1], &quot;b--&quot;, linewidth=2, label=&quot;SVC&quot;) plt.plot(line3[:, 0], line3[:, 1], &quot;r-&quot;, label=&quot;SGDClassifier&quot;) plt.plot(X[:, 0][y==1], X[:, 1][y==1], &quot;bs&quot;) # label=&quot;Iris versicolor&quot; plt.plot(X[:, 0][y==0], X[:, 1][y==0], &quot;yo&quot;) # label=&quot;Iris setosa&quot; plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.legend(loc=&quot;upper center&quot;, fontsize=14) plt.axis([0, 5.5, 0, 2]) plt.show() . . Close enough! . 9. . Exercise: train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach? . First, let&#39;s load the dataset and split it into a training set and a test set. We could use train_test_split() but people usually just take the first 60,000 instances for the training set, and the last 10,000 instances for the test set (this makes it possible to compare your model&#39;s performance with others): . from sklearn.datasets import fetch_openml mnist = fetch_openml(&#39;mnist_784&#39;, version=1, cache=True) X = mnist[&quot;data&quot;] y = mnist[&quot;target&quot;].astype(np.uint8) X_train = X[:60000] y_train = y[:60000] X_test = X[60000:] y_test = y[60000:] . Many training algorithms are sensitive to the order of the training instances, so it&#39;s generally good practice to shuffle them first. However, the dataset is already shuffled, so we do not need to do it. . Let&#39;s start simple, with a linear SVM classifier. It will automatically use the One-vs-All (also called One-vs-the-Rest, OvR) strategy, so there&#39;s nothing special we need to do. Easy! . Warning: this may take a few minutes depending on your hardware. . lin_clf = LinearSVC(random_state=42) lin_clf.fit(X_train, y_train) . /Users/ageron/miniconda3/envs/tf2b/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. &#34;the number of iterations.&#34;, ConvergenceWarning) . LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=42, tol=0.0001, verbose=0) . Let&#39;s make predictions on the training set and measure the accuracy (we don&#39;t want to measure it on the test set yet, since we have not selected and trained the final model yet): . from sklearn.metrics import accuracy_score y_pred = lin_clf.predict(X_train) accuracy_score(y_train, y_pred) . 0.895 . Okay, 89.5% accuracy on MNIST is pretty bad. This linear model is certainly too simple for MNIST, but perhaps we just needed to scale the data first: . scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train.astype(np.float32)) X_test_scaled = scaler.transform(X_test.astype(np.float32)) . Warning: this may take a few minutes depending on your hardware. . lin_clf = LinearSVC(random_state=42) lin_clf.fit(X_train_scaled, y_train) . /Users/ageron/miniconda3/envs/tf2b/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. &#34;the number of iterations.&#34;, ConvergenceWarning) . LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=42, tol=0.0001, verbose=0) . y_pred = lin_clf.predict(X_train_scaled) accuracy_score(y_train, y_pred) . 0.92105 . That&#39;s much better (we cut the error rate by about 25%), but still not great at all for MNIST. If we want to use an SVM, we will have to use a kernel. Let&#39;s try an SVC with an RBF kernel (the default). . Note: to be future-proof we set gamma=&quot;scale&quot; since it will be the default value in Scikit-Learn 0.22. . svm_clf = SVC(gamma=&quot;scale&quot;) svm_clf.fit(X_train_scaled[:10000], y_train[:10000]) . SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . y_pred = svm_clf.predict(X_train_scaled) accuracy_score(y_train, y_pred) . 0.9455333333333333 . That&#39;s promising, we get better performance even though we trained the model on 6 times less data. Let&#39;s tune the hyperparameters by doing a randomized search with cross validation. We will do this on a small dataset just to speed up the process: . from sklearn.model_selection import RandomizedSearchCV from scipy.stats import reciprocal, uniform param_distributions = {&quot;gamma&quot;: reciprocal(0.001, 0.1), &quot;C&quot;: uniform(1, 10)} rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2, cv=3) rnd_search_cv.fit(X_train_scaled[:1000], y_train[:1000]) . Fitting 3 folds for each of 10 candidates, totalling 30 fits [CV] C=4.205364371116072, gamma=0.0020363543195523162 ................ . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . [CV] . C=4.205364371116072, gamma=0.0020363543195523162, total= 0.6s [CV] C=4.205364371116072, gamma=0.0020363543195523162 ................ . [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 0.9s remaining: 0.0s . [CV] . C=4.205364371116072, gamma=0.0020363543195523162, total= 0.6s [CV] C=4.205364371116072, gamma=0.0020363543195523162 ................ [CV] . C=4.205364371116072, gamma=0.0020363543195523162, total= 0.6s [CV] C=7.988626913672013, gamma=0.0017374050727190405 ................ [CV] . C=7.988626913672013, gamma=0.0017374050727190405, total= 0.6s [CV] C=7.988626913672013, gamma=0.0017374050727190405 ................ [CV] . C=7.988626913672013, gamma=0.0017374050727190405, total= 0.6s [CV] C=7.988626913672013, gamma=0.0017374050727190405 ................ [CV] . C=7.988626913672013, gamma=0.0017374050727190405, total= 0.6s [CV] C=5.85175909599865, gamma=0.01842788363564703 ................... [CV] .... C=5.85175909599865, gamma=0.01842788363564703, total= 0.7s [CV] C=5.85175909599865, gamma=0.01842788363564703 ................... [CV] .... C=5.85175909599865, gamma=0.01842788363564703, total= 0.7s [CV] C=5.85175909599865, gamma=0.01842788363564703 ................... [CV] .... C=5.85175909599865, gamma=0.01842788363564703, total= 0.7s [CV] C=9.182267213548876, gamma=0.02323014864755092 .................. [CV] ... C=9.182267213548876, gamma=0.02323014864755092, total= 0.7s [CV] C=9.182267213548876, gamma=0.02323014864755092 .................. [CV] ... C=9.182267213548876, gamma=0.02323014864755092, total= 0.7s [CV] C=9.182267213548876, gamma=0.02323014864755092 .................. [CV] ... C=9.182267213548876, gamma=0.02323014864755092, total= 0.7s [CV] C=5.98561170053319, gamma=0.014913993724076518 .................. [CV] ... C=5.98561170053319, gamma=0.014913993724076518, total= 0.7s [CV] C=5.98561170053319, gamma=0.014913993724076518 .................. [CV] ... C=5.98561170053319, gamma=0.014913993724076518, total= 0.7s [CV] C=5.98561170053319, gamma=0.014913993724076518 .................. [CV] ... C=5.98561170053319, gamma=0.014913993724076518, total= 0.7s [CV] C=8.197542323569591, gamma=0.003288487329556284 ................. [CV] .. C=8.197542323569591, gamma=0.003288487329556284, total= 0.7s [CV] C=8.197542323569591, gamma=0.003288487329556284 ................. [CV] .. C=8.197542323569591, gamma=0.003288487329556284, total= 0.7s [CV] C=8.197542323569591, gamma=0.003288487329556284 ................. [CV] .. C=8.197542323569591, gamma=0.003288487329556284, total= 0.7s [CV] C=6.46207319902158, gamma=0.006525528106404709 .................. [CV] ... C=6.46207319902158, gamma=0.006525528106404709, total= 0.7s [CV] C=6.46207319902158, gamma=0.006525528106404709 .................. [CV] ... C=6.46207319902158, gamma=0.006525528106404709, total= 0.7s [CV] C=6.46207319902158, gamma=0.006525528106404709 .................. [CV] ... C=6.46207319902158, gamma=0.006525528106404709, total= 0.7s [CV] C=2.7698462366793652, gamma=0.08694904406662167 ................. [CV] .. C=2.7698462366793652, gamma=0.08694904406662167, total= 0.7s [CV] C=2.7698462366793652, gamma=0.08694904406662167 ................. [CV] .. C=2.7698462366793652, gamma=0.08694904406662167, total= 0.7s [CV] C=2.7698462366793652, gamma=0.08694904406662167 ................. [CV] .. C=2.7698462366793652, gamma=0.08694904406662167, total= 0.8s [CV] C=3.970183571076878, gamma=0.0037647629143775273 ................ [CV] . C=3.970183571076878, gamma=0.0037647629143775273, total= 0.7s [CV] C=3.970183571076878, gamma=0.0037647629143775273 ................ [CV] . C=3.970183571076878, gamma=0.0037647629143775273, total= 0.7s [CV] C=3.970183571076878, gamma=0.0037647629143775273 ................ [CV] . C=3.970183571076878, gamma=0.0037647629143775273, total= 0.7s [CV] C=2.1619331759609963, gamma=0.0023091602640281797 ............... [CV] C=2.1619331759609963, gamma=0.0023091602640281797, total= 0.6s [CV] C=2.1619331759609963, gamma=0.0023091602640281797 ............... [CV] C=2.1619331759609963, gamma=0.0023091602640281797, total= 0.6s [CV] C=2.1619331759609963, gamma=0.0023091602640281797 ............... [CV] C=2.1619331759609963, gamma=0.0023091602640281797, total= 0.6s . [Parallel(n_jobs=1)]: Done 30 out of 30 | elapsed: 28.9s finished /Users/ageron/miniconda3/envs/tf2b/lib/python3.7/site-packages/sklearn/model_selection/_search.py:842: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal. DeprecationWarning) . RandomizedSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), fit_params=None, iid=&#39;warn&#39;, n_iter=10, n_jobs=None, param_distributions={&#39;gamma&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f9850502d90&gt;, &#39;C&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f98b046a550&gt;}, pre_dispatch=&#39;2*n_jobs&#39;, random_state=None, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=2) . rnd_search_cv.best_estimator_ . SVC(C=7.988626913672013, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=0.0017374050727190405, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . rnd_search_cv.best_score_ . 0.86 . This looks pretty low but remember we only trained the model on 1,000 instances. Let&#39;s retrain the best estimator on the whole training set (run this at night, it will take hours): . rnd_search_cv.best_estimator_.fit(X_train_scaled, y_train) . SVC(C=7.988626913672013, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=0.0017374050727190405, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled) accuracy_score(y_train, y_pred) . 0.9995 . Ah, this looks good! Let&#39;s select this model. Now we can test it on the test set: . y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled) accuracy_score(y_test, y_pred) . 0.9713 . Not too bad, but apparently the model is overfitting slightly. It&#39;s tempting to tweak the hyperparameters a bit more (e.g. decreasing C and/or gamma), but we would run the risk of overfitting the test set. Other people have found that the hyperparameters C=5 and gamma=0.005 yield even better performance (over 98% accuracy). By running the randomized search for longer and on a larger part of the training set, you may be able to find this as well. . 10. . Exercise: train an SVM regressor on the California housing dataset. . Let&#39;s load the dataset using Scikit-Learn&#39;s fetch_california_housing() function: . from sklearn.datasets import fetch_california_housing housing = fetch_california_housing() X = housing[&quot;data&quot;] y = housing[&quot;target&quot;] . Split it into a training set and a test set: . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . Don&#39;t forget to scale the data: . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) . Let&#39;s train a simple LinearSVR first: . from sklearn.svm import LinearSVR lin_svr = LinearSVR(random_state=42) lin_svr.fit(X_train_scaled, y_train) . /Users/ageron/miniconda3/envs/tf2b/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. &#34;the number of iterations.&#34;, ConvergenceWarning) . LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True, intercept_scaling=1.0, loss=&#39;epsilon_insensitive&#39;, max_iter=1000, random_state=42, tol=0.0001, verbose=0) . Let&#39;s see how it performs on the training set: . from sklearn.metrics import mean_squared_error y_pred = lin_svr.predict(X_train_scaled) mse = mean_squared_error(y_train, y_pred) mse . 0.954517044073374 . Let&#39;s look at the RMSE: . np.sqrt(mse) . 0.976993881287582 . In this training set, the targets are tens of thousands of dollars. The RMSE gives a rough idea of the kind of error you should expect (with a higher weight for large errors): so with this model we can expect errors somewhere around $10,000. Not great. Let&#39;s see if we can do better with an RBF Kernel. We will use randomized search with cross validation to find the appropriate hyperparameter values for C and gamma: . from sklearn.svm import SVR from sklearn.model_selection import RandomizedSearchCV from scipy.stats import reciprocal, uniform param_distributions = {&quot;gamma&quot;: reciprocal(0.001, 0.1), &quot;C&quot;: uniform(1, 10)} rnd_search_cv = RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, cv=3, random_state=42) rnd_search_cv.fit(X_train_scaled, y_train) . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . Fitting 3 folds for each of 10 candidates, totalling 30 fits [CV] C=4.745401188473625, gamma=0.07969454818643928 .................. [CV] ... C=4.745401188473625, gamma=0.07969454818643928, total= 5.4s [CV] C=4.745401188473625, gamma=0.07969454818643928 .................. . [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 7.3s remaining: 0.0s . [CV] ... C=4.745401188473625, gamma=0.07969454818643928, total= 5.5s [CV] C=4.745401188473625, gamma=0.07969454818643928 .................. [CV] ... C=4.745401188473625, gamma=0.07969454818643928, total= 5.5s [CV] C=8.31993941811405, gamma=0.015751320499779724 .................. [CV] ... C=8.31993941811405, gamma=0.015751320499779724, total= 5.3s [CV] C=8.31993941811405, gamma=0.015751320499779724 .................. [CV] ... C=8.31993941811405, gamma=0.015751320499779724, total= 5.2s [CV] C=8.31993941811405, gamma=0.015751320499779724 .................. [CV] ... C=8.31993941811405, gamma=0.015751320499779724, total= 5.1s [CV] C=2.560186404424365, gamma=0.002051110418843397 ................. [CV] .. C=2.560186404424365, gamma=0.002051110418843397, total= 4.6s [CV] C=2.560186404424365, gamma=0.002051110418843397 ................. [CV] .. C=2.560186404424365, gamma=0.002051110418843397, total= 4.6s [CV] C=2.560186404424365, gamma=0.002051110418843397 ................. [CV] .. C=2.560186404424365, gamma=0.002051110418843397, total= 4.6s [CV] C=1.5808361216819946, gamma=0.05399484409787431 ................. [CV] .. C=1.5808361216819946, gamma=0.05399484409787431, total= 4.7s [CV] C=1.5808361216819946, gamma=0.05399484409787431 ................. [CV] .. C=1.5808361216819946, gamma=0.05399484409787431, total= 4.8s [CV] C=1.5808361216819946, gamma=0.05399484409787431 ................. [CV] .. C=1.5808361216819946, gamma=0.05399484409787431, total= 4.8s [CV] C=7.011150117432088, gamma=0.026070247583707663 ................. [CV] .. C=7.011150117432088, gamma=0.026070247583707663, total= 5.4s [CV] C=7.011150117432088, gamma=0.026070247583707663 ................. [CV] .. C=7.011150117432088, gamma=0.026070247583707663, total= 5.2s [CV] C=7.011150117432088, gamma=0.026070247583707663 ................. [CV] .. C=7.011150117432088, gamma=0.026070247583707663, total= 5.4s [CV] C=1.2058449429580245, gamma=0.0870602087830485 .................. [CV] ... C=1.2058449429580245, gamma=0.0870602087830485, total= 4.8s [CV] C=1.2058449429580245, gamma=0.0870602087830485 .................. [CV] ... C=1.2058449429580245, gamma=0.0870602087830485, total= 4.8s [CV] C=1.2058449429580245, gamma=0.0870602087830485 .................. [CV] ... C=1.2058449429580245, gamma=0.0870602087830485, total= 4.9s [CV] C=9.324426408004218, gamma=0.0026587543983272693 ................ [CV] . C=9.324426408004218, gamma=0.0026587543983272693, total= 5.0s [CV] C=9.324426408004218, gamma=0.0026587543983272693 ................ [CV] . C=9.324426408004218, gamma=0.0026587543983272693, total= 4.9s [CV] C=9.324426408004218, gamma=0.0026587543983272693 ................ [CV] . C=9.324426408004218, gamma=0.0026587543983272693, total= 5.0s [CV] C=2.818249672071006, gamma=0.0023270677083837795 ................ [CV] . C=2.818249672071006, gamma=0.0023270677083837795, total= 4.9s [CV] C=2.818249672071006, gamma=0.0023270677083837795 ................ [CV] . C=2.818249672071006, gamma=0.0023270677083837795, total= 4.9s [CV] C=2.818249672071006, gamma=0.0023270677083837795 ................ [CV] . C=2.818249672071006, gamma=0.0023270677083837795, total= 4.8s [CV] C=4.042422429595377, gamma=0.011207606211860567 ................. [CV] .. C=4.042422429595377, gamma=0.011207606211860567, total= 4.9s [CV] C=4.042422429595377, gamma=0.011207606211860567 ................. [CV] .. C=4.042422429595377, gamma=0.011207606211860567, total= 5.1s [CV] C=4.042422429595377, gamma=0.011207606211860567 ................. [CV] .. C=4.042422429595377, gamma=0.011207606211860567, total= 4.8s [CV] C=5.319450186421157, gamma=0.003823475224675185 ................. [CV] .. C=5.319450186421157, gamma=0.003823475224675185, total= 4.8s [CV] C=5.319450186421157, gamma=0.003823475224675185 ................. [CV] .. C=5.319450186421157, gamma=0.003823475224675185, total= 4.7s [CV] C=5.319450186421157, gamma=0.003823475224675185 ................. [CV] .. C=5.319450186421157, gamma=0.003823475224675185, total= 5.0s . [Parallel(n_jobs=1)]: Done 30 out of 30 | elapsed: 3.5min finished . RandomizedSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False), fit_params=None, iid=&#39;warn&#39;, n_iter=10, n_jobs=None, param_distributions={&#39;gamma&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x12fc2e390&gt;, &#39;C&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x12fc2eac8&gt;}, pre_dispatch=&#39;2*n_jobs&#39;, random_state=42, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=2) . rnd_search_cv.best_estimator_ . SVR(C=4.745401188473625, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.07969454818643928, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False) . Now let&#39;s measure the RMSE on the training set: . y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled) mse = mean_squared_error(y_train, y_pred) np.sqrt(mse) . 0.5727524770785359 . Looks much better than the linear model. Let&#39;s select this model and evaluate it on the test set: . y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled) mse = mean_squared_error(y_test, y_pred) np.sqrt(mse) . 0.5929168385528734 .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/05_support_vector_machines",
            "relUrl": "/05_support_vector_machines",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Reinforcement Learning",
            "content": "This notebook contains all the sample code in chapter 18. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20 and TensorFlow â‰¥2.0. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x !apt update &amp;&amp; apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb !pip install -q -U tf-agents-nightly pyvirtualdisplay gym[atari] IS_COLAB = True except Exception: IS_COLAB = False # TensorFlow â‰¥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; if not tf.test.is_gpu_available(): print(&quot;No GPU was detected. CNNs can be very slow without a GPU.&quot;) if IS_COLAB: print(&quot;Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.&quot;) # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) tf.random.set_seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # To get smooth animations import matplotlib.animation as animation mpl.rc(&#39;animation&#39;, html=&#39;jshtml&#39;) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;rl&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . . Introduction to OpenAI gym . In this notebook we will be using OpenAI gym, a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning agents to interact with. Let&#39;s start by importing gym: . import gym . Let&#39;s list all the available environments: . gym.envs.registry.all() . dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v2), EnvSpec(BipedalWalkerHardcore-v2), EnvSpec(CarRacing-v0), EnvSpec(Blackjack-v0), EnvSpec(KellyCoinflip-v0), EnvSpec(KellyCoinflipGeneralized-v0), EnvSpec(FrozenLake-v0), EnvSpec(FrozenLake8x8-v0), EnvSpec(CliffWalking-v0), EnvSpec(NChain-v0), EnvSpec(Roulette-v0), EnvSpec(Taxi-v2), EnvSpec(GuessingGame-v0), EnvSpec(HotterColder-v0), EnvSpec(Reacher-v2), EnvSpec(Pusher-v2), EnvSpec(Thrower-v2), EnvSpec(Striker-v2), EnvSpec(InvertedPendulum-v2), EnvSpec(InvertedDoublePendulum-v2), EnvSpec(HalfCheetah-v2), EnvSpec(HalfCheetah-v3), EnvSpec(Hopper-v2), EnvSpec(Hopper-v3), EnvSpec(Swimmer-v2), EnvSpec(Swimmer-v3), EnvSpec(Walker2d-v2), EnvSpec(Walker2d-v3), EnvSpec(Ant-v2), EnvSpec(Ant-v3), EnvSpec(Humanoid-v2), EnvSpec(Humanoid-v3), EnvSpec(HumanoidStandup-v2), EnvSpec(FetchSlide-v1), EnvSpec(FetchPickAndPlace-v1), EnvSpec(FetchReach-v1), EnvSpec(FetchPush-v1), EnvSpec(HandReach-v0), EnvSpec(HandManipulateBlockRotateZ-v0), EnvSpec(HandManipulateBlockRotateParallel-v0), EnvSpec(HandManipulateBlockRotateXYZ-v0), EnvSpec(HandManipulateBlockFull-v0), EnvSpec(HandManipulateBlock-v0), EnvSpec(HandManipulateBlockTouchSensors-v0), EnvSpec(HandManipulateEggRotate-v0), EnvSpec(HandManipulateEggFull-v0), EnvSpec(HandManipulateEgg-v0), EnvSpec(HandManipulateEggTouchSensors-v0), EnvSpec(HandManipulatePenRotate-v0), EnvSpec(HandManipulatePenFull-v0), EnvSpec(HandManipulatePen-v0), EnvSpec(HandManipulatePenTouchSensors-v0), EnvSpec(FetchSlideDense-v1), EnvSpec(FetchPickAndPlaceDense-v1), EnvSpec(FetchReachDense-v1), EnvSpec(FetchPushDense-v1), EnvSpec(HandReachDense-v0), EnvSpec(HandManipulateBlockRotateZDense-v0), EnvSpec(HandManipulateBlockRotateParallelDense-v0), EnvSpec(HandManipulateBlockRotateXYZDense-v0), EnvSpec(HandManipulateBlockFullDense-v0), EnvSpec(HandManipulateBlockDense-v0), EnvSpec(HandManipulateBlockTouchSensorsDense-v0), EnvSpec(HandManipulateEggRotateDense-v0), EnvSpec(HandManipulateEggFullDense-v0), EnvSpec(HandManipulateEggDense-v0), EnvSpec(HandManipulateEggTouchSensorsDense-v0), EnvSpec(HandManipulatePenRotateDense-v0), EnvSpec(HandManipulatePenFullDense-v0), EnvSpec(HandManipulatePenDense-v0), EnvSpec(HandManipulatePenTouchSensorsDense-v0), EnvSpec(Adventure-v0), EnvSpec(Adventure-v4), EnvSpec(AdventureDeterministic-v0), EnvSpec(AdventureDeterministic-v4), EnvSpec(AdventureNoFrameskip-v0), EnvSpec(AdventureNoFrameskip-v4), EnvSpec(Adventure-ram-v0), EnvSpec(Adventure-ram-v4), EnvSpec(Adventure-ramDeterministic-v0), EnvSpec(Adventure-ramDeterministic-v4), EnvSpec(Adventure-ramNoFrameskip-v0), EnvSpec(Adventure-ramNoFrameskip-v4), EnvSpec(AirRaid-v0), EnvSpec(AirRaid-v4), EnvSpec(AirRaidDeterministic-v0), EnvSpec(AirRaidDeterministic-v4), EnvSpec(AirRaidNoFrameskip-v0), EnvSpec(AirRaidNoFrameskip-v4), EnvSpec(AirRaid-ram-v0), EnvSpec(AirRaid-ram-v4), EnvSpec(AirRaid-ramDeterministic-v0), EnvSpec(AirRaid-ramDeterministic-v4), EnvSpec(AirRaid-ramNoFrameskip-v0), EnvSpec(AirRaid-ramNoFrameskip-v4), EnvSpec(Alien-v0), EnvSpec(Alien-v4), EnvSpec(AlienDeterministic-v0), EnvSpec(AlienDeterministic-v4), EnvSpec(AlienNoFrameskip-v0), EnvSpec(AlienNoFrameskip-v4), EnvSpec(Alien-ram-v0), EnvSpec(Alien-ram-v4), EnvSpec(Alien-ramDeterministic-v0), EnvSpec(Alien-ramDeterministic-v4), EnvSpec(Alien-ramNoFrameskip-v0), EnvSpec(Alien-ramNoFrameskip-v4), EnvSpec(Amidar-v0), EnvSpec(Amidar-v4), EnvSpec(AmidarDeterministic-v0), EnvSpec(AmidarDeterministic-v4), EnvSpec(AmidarNoFrameskip-v0), EnvSpec(AmidarNoFrameskip-v4), EnvSpec(Amidar-ram-v0), EnvSpec(Amidar-ram-v4), EnvSpec(Amidar-ramDeterministic-v0), EnvSpec(Amidar-ramDeterministic-v4), EnvSpec(Amidar-ramNoFrameskip-v0), EnvSpec(Amidar-ramNoFrameskip-v4), EnvSpec(Assault-v0), EnvSpec(Assault-v4), EnvSpec(AssaultDeterministic-v0), EnvSpec(AssaultDeterministic-v4), EnvSpec(AssaultNoFrameskip-v0), EnvSpec(AssaultNoFrameskip-v4), EnvSpec(Assault-ram-v0), EnvSpec(Assault-ram-v4), EnvSpec(Assault-ramDeterministic-v0), EnvSpec(Assault-ramDeterministic-v4), EnvSpec(Assault-ramNoFrameskip-v0), EnvSpec(Assault-ramNoFrameskip-v4), EnvSpec(Asterix-v0), EnvSpec(Asterix-v4), EnvSpec(AsterixDeterministic-v0), EnvSpec(AsterixDeterministic-v4), EnvSpec(AsterixNoFrameskip-v0), EnvSpec(AsterixNoFrameskip-v4), EnvSpec(Asterix-ram-v0), EnvSpec(Asterix-ram-v4), EnvSpec(Asterix-ramDeterministic-v0), EnvSpec(Asterix-ramDeterministic-v4), EnvSpec(Asterix-ramNoFrameskip-v0), EnvSpec(Asterix-ramNoFrameskip-v4), EnvSpec(Asteroids-v0), EnvSpec(Asteroids-v4), EnvSpec(AsteroidsDeterministic-v0), EnvSpec(AsteroidsDeterministic-v4), EnvSpec(AsteroidsNoFrameskip-v0), EnvSpec(AsteroidsNoFrameskip-v4), EnvSpec(Asteroids-ram-v0), EnvSpec(Asteroids-ram-v4), EnvSpec(Asteroids-ramDeterministic-v0), EnvSpec(Asteroids-ramDeterministic-v4), EnvSpec(Asteroids-ramNoFrameskip-v0), EnvSpec(Asteroids-ramNoFrameskip-v4), EnvSpec(Atlantis-v0), EnvSpec(Atlantis-v4), EnvSpec(AtlantisDeterministic-v0), EnvSpec(AtlantisDeterministic-v4), EnvSpec(AtlantisNoFrameskip-v0), EnvSpec(AtlantisNoFrameskip-v4), EnvSpec(Atlantis-ram-v0), EnvSpec(Atlantis-ram-v4), EnvSpec(Atlantis-ramDeterministic-v0), EnvSpec(Atlantis-ramDeterministic-v4), EnvSpec(Atlantis-ramNoFrameskip-v0), EnvSpec(Atlantis-ramNoFrameskip-v4), EnvSpec(BankHeist-v0), EnvSpec(BankHeist-v4), EnvSpec(BankHeistDeterministic-v0), EnvSpec(BankHeistDeterministic-v4), EnvSpec(BankHeistNoFrameskip-v0), EnvSpec(BankHeistNoFrameskip-v4), EnvSpec(BankHeist-ram-v0), EnvSpec(BankHeist-ram-v4), EnvSpec(BankHeist-ramDeterministic-v0), EnvSpec(BankHeist-ramDeterministic-v4), EnvSpec(BankHeist-ramNoFrameskip-v0), EnvSpec(BankHeist-ramNoFrameskip-v4), EnvSpec(BattleZone-v0), EnvSpec(BattleZone-v4), EnvSpec(BattleZoneDeterministic-v0), EnvSpec(BattleZoneDeterministic-v4), EnvSpec(BattleZoneNoFrameskip-v0), EnvSpec(BattleZoneNoFrameskip-v4), EnvSpec(BattleZone-ram-v0), EnvSpec(BattleZone-ram-v4), EnvSpec(BattleZone-ramDeterministic-v0), EnvSpec(BattleZone-ramDeterministic-v4), EnvSpec(BattleZone-ramNoFrameskip-v0), EnvSpec(BattleZone-ramNoFrameskip-v4), EnvSpec(BeamRider-v0), EnvSpec(BeamRider-v4), EnvSpec(BeamRiderDeterministic-v0), EnvSpec(BeamRiderDeterministic-v4), EnvSpec(BeamRiderNoFrameskip-v0), EnvSpec(BeamRiderNoFrameskip-v4), EnvSpec(BeamRider-ram-v0), EnvSpec(BeamRider-ram-v4), EnvSpec(BeamRider-ramDeterministic-v0), EnvSpec(BeamRider-ramDeterministic-v4), EnvSpec(BeamRider-ramNoFrameskip-v0), EnvSpec(BeamRider-ramNoFrameskip-v4), EnvSpec(Berzerk-v0), EnvSpec(Berzerk-v4), EnvSpec(BerzerkDeterministic-v0), EnvSpec(BerzerkDeterministic-v4), EnvSpec(BerzerkNoFrameskip-v0), EnvSpec(BerzerkNoFrameskip-v4), EnvSpec(Berzerk-ram-v0), EnvSpec(Berzerk-ram-v4), EnvSpec(Berzerk-ramDeterministic-v0), EnvSpec(Berzerk-ramDeterministic-v4), EnvSpec(Berzerk-ramNoFrameskip-v0), EnvSpec(Berzerk-ramNoFrameskip-v4), EnvSpec(Bowling-v0), EnvSpec(Bowling-v4), EnvSpec(BowlingDeterministic-v0), EnvSpec(BowlingDeterministic-v4), EnvSpec(BowlingNoFrameskip-v0), EnvSpec(BowlingNoFrameskip-v4), EnvSpec(Bowling-ram-v0), EnvSpec(Bowling-ram-v4), EnvSpec(Bowling-ramDeterministic-v0), EnvSpec(Bowling-ramDeterministic-v4), EnvSpec(Bowling-ramNoFrameskip-v0), EnvSpec(Bowling-ramNoFrameskip-v4), EnvSpec(Boxing-v0), EnvSpec(Boxing-v4), EnvSpec(BoxingDeterministic-v0), EnvSpec(BoxingDeterministic-v4), EnvSpec(BoxingNoFrameskip-v0), EnvSpec(BoxingNoFrameskip-v4), EnvSpec(Boxing-ram-v0), EnvSpec(Boxing-ram-v4), EnvSpec(Boxing-ramDeterministic-v0), EnvSpec(Boxing-ramDeterministic-v4), EnvSpec(Boxing-ramNoFrameskip-v0), EnvSpec(Boxing-ramNoFrameskip-v4), EnvSpec(Breakout-v0), EnvSpec(Breakout-v4), EnvSpec(BreakoutDeterministic-v0), EnvSpec(BreakoutDeterministic-v4), EnvSpec(BreakoutNoFrameskip-v0), EnvSpec(BreakoutNoFrameskip-v4), EnvSpec(Breakout-ram-v0), EnvSpec(Breakout-ram-v4), EnvSpec(Breakout-ramDeterministic-v0), EnvSpec(Breakout-ramDeterministic-v4), EnvSpec(Breakout-ramNoFrameskip-v0), EnvSpec(Breakout-ramNoFrameskip-v4), EnvSpec(Carnival-v0), EnvSpec(Carnival-v4), EnvSpec(CarnivalDeterministic-v0), EnvSpec(CarnivalDeterministic-v4), EnvSpec(CarnivalNoFrameskip-v0), EnvSpec(CarnivalNoFrameskip-v4), EnvSpec(Carnival-ram-v0), EnvSpec(Carnival-ram-v4), EnvSpec(Carnival-ramDeterministic-v0), EnvSpec(Carnival-ramDeterministic-v4), EnvSpec(Carnival-ramNoFrameskip-v0), EnvSpec(Carnival-ramNoFrameskip-v4), EnvSpec(Centipede-v0), EnvSpec(Centipede-v4), EnvSpec(CentipedeDeterministic-v0), EnvSpec(CentipedeDeterministic-v4), EnvSpec(CentipedeNoFrameskip-v0), EnvSpec(CentipedeNoFrameskip-v4), EnvSpec(Centipede-ram-v0), EnvSpec(Centipede-ram-v4), EnvSpec(Centipede-ramDeterministic-v0), EnvSpec(Centipede-ramDeterministic-v4), EnvSpec(Centipede-ramNoFrameskip-v0), EnvSpec(Centipede-ramNoFrameskip-v4), EnvSpec(ChopperCommand-v0), EnvSpec(ChopperCommand-v4), EnvSpec(ChopperCommandDeterministic-v0), EnvSpec(ChopperCommandDeterministic-v4), EnvSpec(ChopperCommandNoFrameskip-v0), EnvSpec(ChopperCommandNoFrameskip-v4), EnvSpec(ChopperCommand-ram-v0), EnvSpec(ChopperCommand-ram-v4), EnvSpec(ChopperCommand-ramDeterministic-v0), EnvSpec(ChopperCommand-ramDeterministic-v4), EnvSpec(ChopperCommand-ramNoFrameskip-v0), EnvSpec(ChopperCommand-ramNoFrameskip-v4), EnvSpec(CrazyClimber-v0), EnvSpec(CrazyClimber-v4), EnvSpec(CrazyClimberDeterministic-v0), EnvSpec(CrazyClimberDeterministic-v4), EnvSpec(CrazyClimberNoFrameskip-v0), EnvSpec(CrazyClimberNoFrameskip-v4), EnvSpec(CrazyClimber-ram-v0), EnvSpec(CrazyClimber-ram-v4), EnvSpec(CrazyClimber-ramDeterministic-v0), EnvSpec(CrazyClimber-ramDeterministic-v4), EnvSpec(CrazyClimber-ramNoFrameskip-v0), EnvSpec(CrazyClimber-ramNoFrameskip-v4), EnvSpec(Defender-v0), EnvSpec(Defender-v4), EnvSpec(DefenderDeterministic-v0), EnvSpec(DefenderDeterministic-v4), EnvSpec(DefenderNoFrameskip-v0), EnvSpec(DefenderNoFrameskip-v4), EnvSpec(Defender-ram-v0), EnvSpec(Defender-ram-v4), EnvSpec(Defender-ramDeterministic-v0), EnvSpec(Defender-ramDeterministic-v4), EnvSpec(Defender-ramNoFrameskip-v0), EnvSpec(Defender-ramNoFrameskip-v4), EnvSpec(DemonAttack-v0), EnvSpec(DemonAttack-v4), EnvSpec(DemonAttackDeterministic-v0), EnvSpec(DemonAttackDeterministic-v4), EnvSpec(DemonAttackNoFrameskip-v0), EnvSpec(DemonAttackNoFrameskip-v4), EnvSpec(DemonAttack-ram-v0), EnvSpec(DemonAttack-ram-v4), EnvSpec(DemonAttack-ramDeterministic-v0), EnvSpec(DemonAttack-ramDeterministic-v4), EnvSpec(DemonAttack-ramNoFrameskip-v0), EnvSpec(DemonAttack-ramNoFrameskip-v4), EnvSpec(DoubleDunk-v0), EnvSpec(DoubleDunk-v4), EnvSpec(DoubleDunkDeterministic-v0), EnvSpec(DoubleDunkDeterministic-v4), EnvSpec(DoubleDunkNoFrameskip-v0), EnvSpec(DoubleDunkNoFrameskip-v4), EnvSpec(DoubleDunk-ram-v0), EnvSpec(DoubleDunk-ram-v4), EnvSpec(DoubleDunk-ramDeterministic-v0), EnvSpec(DoubleDunk-ramDeterministic-v4), EnvSpec(DoubleDunk-ramNoFrameskip-v0), EnvSpec(DoubleDunk-ramNoFrameskip-v4), EnvSpec(ElevatorAction-v0), EnvSpec(ElevatorAction-v4), EnvSpec(ElevatorActionDeterministic-v0), EnvSpec(ElevatorActionDeterministic-v4), EnvSpec(ElevatorActionNoFrameskip-v0), EnvSpec(ElevatorActionNoFrameskip-v4), EnvSpec(ElevatorAction-ram-v0), EnvSpec(ElevatorAction-ram-v4), EnvSpec(ElevatorAction-ramDeterministic-v0), EnvSpec(ElevatorAction-ramDeterministic-v4), EnvSpec(ElevatorAction-ramNoFrameskip-v0), EnvSpec(ElevatorAction-ramNoFrameskip-v4), EnvSpec(Enduro-v0), EnvSpec(Enduro-v4), EnvSpec(EnduroDeterministic-v0), EnvSpec(EnduroDeterministic-v4), EnvSpec(EnduroNoFrameskip-v0), EnvSpec(EnduroNoFrameskip-v4), EnvSpec(Enduro-ram-v0), EnvSpec(Enduro-ram-v4), EnvSpec(Enduro-ramDeterministic-v0), EnvSpec(Enduro-ramDeterministic-v4), EnvSpec(Enduro-ramNoFrameskip-v0), EnvSpec(Enduro-ramNoFrameskip-v4), EnvSpec(FishingDerby-v0), EnvSpec(FishingDerby-v4), EnvSpec(FishingDerbyDeterministic-v0), EnvSpec(FishingDerbyDeterministic-v4), EnvSpec(FishingDerbyNoFrameskip-v0), EnvSpec(FishingDerbyNoFrameskip-v4), EnvSpec(FishingDerby-ram-v0), EnvSpec(FishingDerby-ram-v4), EnvSpec(FishingDerby-ramDeterministic-v0), EnvSpec(FishingDerby-ramDeterministic-v4), EnvSpec(FishingDerby-ramNoFrameskip-v0), EnvSpec(FishingDerby-ramNoFrameskip-v4), EnvSpec(Freeway-v0), EnvSpec(Freeway-v4), EnvSpec(FreewayDeterministic-v0), EnvSpec(FreewayDeterministic-v4), EnvSpec(FreewayNoFrameskip-v0), EnvSpec(FreewayNoFrameskip-v4), EnvSpec(Freeway-ram-v0), EnvSpec(Freeway-ram-v4), EnvSpec(Freeway-ramDeterministic-v0), EnvSpec(Freeway-ramDeterministic-v4), EnvSpec(Freeway-ramNoFrameskip-v0), EnvSpec(Freeway-ramNoFrameskip-v4), EnvSpec(Frostbite-v0), EnvSpec(Frostbite-v4), EnvSpec(FrostbiteDeterministic-v0), EnvSpec(FrostbiteDeterministic-v4), EnvSpec(FrostbiteNoFrameskip-v0), EnvSpec(FrostbiteNoFrameskip-v4), EnvSpec(Frostbite-ram-v0), EnvSpec(Frostbite-ram-v4), EnvSpec(Frostbite-ramDeterministic-v0), EnvSpec(Frostbite-ramDeterministic-v4), EnvSpec(Frostbite-ramNoFrameskip-v0), EnvSpec(Frostbite-ramNoFrameskip-v4), EnvSpec(Gopher-v0), EnvSpec(Gopher-v4), EnvSpec(GopherDeterministic-v0), EnvSpec(GopherDeterministic-v4), EnvSpec(GopherNoFrameskip-v0), EnvSpec(GopherNoFrameskip-v4), EnvSpec(Gopher-ram-v0), EnvSpec(Gopher-ram-v4), EnvSpec(Gopher-ramDeterministic-v0), EnvSpec(Gopher-ramDeterministic-v4), EnvSpec(Gopher-ramNoFrameskip-v0), EnvSpec(Gopher-ramNoFrameskip-v4), EnvSpec(Gravitar-v0), EnvSpec(Gravitar-v4), EnvSpec(GravitarDeterministic-v0), EnvSpec(GravitarDeterministic-v4), EnvSpec(GravitarNoFrameskip-v0), EnvSpec(GravitarNoFrameskip-v4), EnvSpec(Gravitar-ram-v0), EnvSpec(Gravitar-ram-v4), EnvSpec(Gravitar-ramDeterministic-v0), EnvSpec(Gravitar-ramDeterministic-v4), EnvSpec(Gravitar-ramNoFrameskip-v0), EnvSpec(Gravitar-ramNoFrameskip-v4), EnvSpec(Hero-v0), EnvSpec(Hero-v4), EnvSpec(HeroDeterministic-v0), EnvSpec(HeroDeterministic-v4), EnvSpec(HeroNoFrameskip-v0), EnvSpec(HeroNoFrameskip-v4), EnvSpec(Hero-ram-v0), EnvSpec(Hero-ram-v4), EnvSpec(Hero-ramDeterministic-v0), EnvSpec(Hero-ramDeterministic-v4), EnvSpec(Hero-ramNoFrameskip-v0), EnvSpec(Hero-ramNoFrameskip-v4), EnvSpec(IceHockey-v0), EnvSpec(IceHockey-v4), EnvSpec(IceHockeyDeterministic-v0), EnvSpec(IceHockeyDeterministic-v4), EnvSpec(IceHockeyNoFrameskip-v0), EnvSpec(IceHockeyNoFrameskip-v4), EnvSpec(IceHockey-ram-v0), EnvSpec(IceHockey-ram-v4), EnvSpec(IceHockey-ramDeterministic-v0), EnvSpec(IceHockey-ramDeterministic-v4), EnvSpec(IceHockey-ramNoFrameskip-v0), EnvSpec(IceHockey-ramNoFrameskip-v4), EnvSpec(Jamesbond-v0), EnvSpec(Jamesbond-v4), EnvSpec(JamesbondDeterministic-v0), EnvSpec(JamesbondDeterministic-v4), EnvSpec(JamesbondNoFrameskip-v0), EnvSpec(JamesbondNoFrameskip-v4), EnvSpec(Jamesbond-ram-v0), EnvSpec(Jamesbond-ram-v4), EnvSpec(Jamesbond-ramDeterministic-v0), EnvSpec(Jamesbond-ramDeterministic-v4), EnvSpec(Jamesbond-ramNoFrameskip-v0), EnvSpec(Jamesbond-ramNoFrameskip-v4), EnvSpec(JourneyEscape-v0), EnvSpec(JourneyEscape-v4), EnvSpec(JourneyEscapeDeterministic-v0), EnvSpec(JourneyEscapeDeterministic-v4), EnvSpec(JourneyEscapeNoFrameskip-v0), EnvSpec(JourneyEscapeNoFrameskip-v4), EnvSpec(JourneyEscape-ram-v0), EnvSpec(JourneyEscape-ram-v4), EnvSpec(JourneyEscape-ramDeterministic-v0), EnvSpec(JourneyEscape-ramDeterministic-v4), EnvSpec(JourneyEscape-ramNoFrameskip-v0), EnvSpec(JourneyEscape-ramNoFrameskip-v4), EnvSpec(Kangaroo-v0), EnvSpec(Kangaroo-v4), EnvSpec(KangarooDeterministic-v0), EnvSpec(KangarooDeterministic-v4), EnvSpec(KangarooNoFrameskip-v0), EnvSpec(KangarooNoFrameskip-v4), EnvSpec(Kangaroo-ram-v0), EnvSpec(Kangaroo-ram-v4), EnvSpec(Kangaroo-ramDeterministic-v0), EnvSpec(Kangaroo-ramDeterministic-v4), EnvSpec(Kangaroo-ramNoFrameskip-v0), EnvSpec(Kangaroo-ramNoFrameskip-v4), EnvSpec(Krull-v0), EnvSpec(Krull-v4), EnvSpec(KrullDeterministic-v0), EnvSpec(KrullDeterministic-v4), EnvSpec(KrullNoFrameskip-v0), EnvSpec(KrullNoFrameskip-v4), EnvSpec(Krull-ram-v0), EnvSpec(Krull-ram-v4), EnvSpec(Krull-ramDeterministic-v0), EnvSpec(Krull-ramDeterministic-v4), EnvSpec(Krull-ramNoFrameskip-v0), EnvSpec(Krull-ramNoFrameskip-v4), EnvSpec(KungFuMaster-v0), EnvSpec(KungFuMaster-v4), EnvSpec(KungFuMasterDeterministic-v0), EnvSpec(KungFuMasterDeterministic-v4), EnvSpec(KungFuMasterNoFrameskip-v0), EnvSpec(KungFuMasterNoFrameskip-v4), EnvSpec(KungFuMaster-ram-v0), EnvSpec(KungFuMaster-ram-v4), EnvSpec(KungFuMaster-ramDeterministic-v0), EnvSpec(KungFuMaster-ramDeterministic-v4), EnvSpec(KungFuMaster-ramNoFrameskip-v0), EnvSpec(KungFuMaster-ramNoFrameskip-v4), EnvSpec(MontezumaRevenge-v0), EnvSpec(MontezumaRevenge-v4), EnvSpec(MontezumaRevengeDeterministic-v0), EnvSpec(MontezumaRevengeDeterministic-v4), EnvSpec(MontezumaRevengeNoFrameskip-v0), EnvSpec(MontezumaRevengeNoFrameskip-v4), EnvSpec(MontezumaRevenge-ram-v0), EnvSpec(MontezumaRevenge-ram-v4), EnvSpec(MontezumaRevenge-ramDeterministic-v0), EnvSpec(MontezumaRevenge-ramDeterministic-v4), EnvSpec(MontezumaRevenge-ramNoFrameskip-v0), EnvSpec(MontezumaRevenge-ramNoFrameskip-v4), EnvSpec(MsPacman-v0), EnvSpec(MsPacman-v4), EnvSpec(MsPacmanDeterministic-v0), EnvSpec(MsPacmanDeterministic-v4), EnvSpec(MsPacmanNoFrameskip-v0), EnvSpec(MsPacmanNoFrameskip-v4), EnvSpec(MsPacman-ram-v0), EnvSpec(MsPacman-ram-v4), EnvSpec(MsPacman-ramDeterministic-v0), EnvSpec(MsPacman-ramDeterministic-v4), EnvSpec(MsPacman-ramNoFrameskip-v0), EnvSpec(MsPacman-ramNoFrameskip-v4), EnvSpec(NameThisGame-v0), EnvSpec(NameThisGame-v4), EnvSpec(NameThisGameDeterministic-v0), EnvSpec(NameThisGameDeterministic-v4), EnvSpec(NameThisGameNoFrameskip-v0), EnvSpec(NameThisGameNoFrameskip-v4), EnvSpec(NameThisGame-ram-v0), EnvSpec(NameThisGame-ram-v4), EnvSpec(NameThisGame-ramDeterministic-v0), EnvSpec(NameThisGame-ramDeterministic-v4), EnvSpec(NameThisGame-ramNoFrameskip-v0), EnvSpec(NameThisGame-ramNoFrameskip-v4), EnvSpec(Phoenix-v0), EnvSpec(Phoenix-v4), EnvSpec(PhoenixDeterministic-v0), EnvSpec(PhoenixDeterministic-v4), EnvSpec(PhoenixNoFrameskip-v0), EnvSpec(PhoenixNoFrameskip-v4), EnvSpec(Phoenix-ram-v0), EnvSpec(Phoenix-ram-v4), EnvSpec(Phoenix-ramDeterministic-v0), EnvSpec(Phoenix-ramDeterministic-v4), EnvSpec(Phoenix-ramNoFrameskip-v0), EnvSpec(Phoenix-ramNoFrameskip-v4), EnvSpec(Pitfall-v0), EnvSpec(Pitfall-v4), EnvSpec(PitfallDeterministic-v0), EnvSpec(PitfallDeterministic-v4), EnvSpec(PitfallNoFrameskip-v0), EnvSpec(PitfallNoFrameskip-v4), EnvSpec(Pitfall-ram-v0), EnvSpec(Pitfall-ram-v4), EnvSpec(Pitfall-ramDeterministic-v0), EnvSpec(Pitfall-ramDeterministic-v4), EnvSpec(Pitfall-ramNoFrameskip-v0), EnvSpec(Pitfall-ramNoFrameskip-v4), EnvSpec(Pong-v0), EnvSpec(Pong-v4), EnvSpec(PongDeterministic-v0), EnvSpec(PongDeterministic-v4), EnvSpec(PongNoFrameskip-v0), EnvSpec(PongNoFrameskip-v4), EnvSpec(Pong-ram-v0), EnvSpec(Pong-ram-v4), EnvSpec(Pong-ramDeterministic-v0), EnvSpec(Pong-ramDeterministic-v4), EnvSpec(Pong-ramNoFrameskip-v0), EnvSpec(Pong-ramNoFrameskip-v4), EnvSpec(Pooyan-v0), EnvSpec(Pooyan-v4), EnvSpec(PooyanDeterministic-v0), EnvSpec(PooyanDeterministic-v4), EnvSpec(PooyanNoFrameskip-v0), EnvSpec(PooyanNoFrameskip-v4), EnvSpec(Pooyan-ram-v0), EnvSpec(Pooyan-ram-v4), EnvSpec(Pooyan-ramDeterministic-v0), EnvSpec(Pooyan-ramDeterministic-v4), EnvSpec(Pooyan-ramNoFrameskip-v0), EnvSpec(Pooyan-ramNoFrameskip-v4), EnvSpec(PrivateEye-v0), EnvSpec(PrivateEye-v4), EnvSpec(PrivateEyeDeterministic-v0), EnvSpec(PrivateEyeDeterministic-v4), EnvSpec(PrivateEyeNoFrameskip-v0), EnvSpec(PrivateEyeNoFrameskip-v4), EnvSpec(PrivateEye-ram-v0), EnvSpec(PrivateEye-ram-v4), EnvSpec(PrivateEye-ramDeterministic-v0), EnvSpec(PrivateEye-ramDeterministic-v4), EnvSpec(PrivateEye-ramNoFrameskip-v0), EnvSpec(PrivateEye-ramNoFrameskip-v4), EnvSpec(Qbert-v0), EnvSpec(Qbert-v4), EnvSpec(QbertDeterministic-v0), EnvSpec(QbertDeterministic-v4), EnvSpec(QbertNoFrameskip-v0), EnvSpec(QbertNoFrameskip-v4), EnvSpec(Qbert-ram-v0), EnvSpec(Qbert-ram-v4), EnvSpec(Qbert-ramDeterministic-v0), EnvSpec(Qbert-ramDeterministic-v4), EnvSpec(Qbert-ramNoFrameskip-v0), EnvSpec(Qbert-ramNoFrameskip-v4), EnvSpec(Riverraid-v0), EnvSpec(Riverraid-v4), EnvSpec(RiverraidDeterministic-v0), EnvSpec(RiverraidDeterministic-v4), EnvSpec(RiverraidNoFrameskip-v0), EnvSpec(RiverraidNoFrameskip-v4), EnvSpec(Riverraid-ram-v0), EnvSpec(Riverraid-ram-v4), EnvSpec(Riverraid-ramDeterministic-v0), EnvSpec(Riverraid-ramDeterministic-v4), EnvSpec(Riverraid-ramNoFrameskip-v0), EnvSpec(Riverraid-ramNoFrameskip-v4), EnvSpec(RoadRunner-v0), EnvSpec(RoadRunner-v4), EnvSpec(RoadRunnerDeterministic-v0), EnvSpec(RoadRunnerDeterministic-v4), EnvSpec(RoadRunnerNoFrameskip-v0), EnvSpec(RoadRunnerNoFrameskip-v4), EnvSpec(RoadRunner-ram-v0), EnvSpec(RoadRunner-ram-v4), EnvSpec(RoadRunner-ramDeterministic-v0), EnvSpec(RoadRunner-ramDeterministic-v4), EnvSpec(RoadRunner-ramNoFrameskip-v0), EnvSpec(RoadRunner-ramNoFrameskip-v4), EnvSpec(Robotank-v0), EnvSpec(Robotank-v4), EnvSpec(RobotankDeterministic-v0), EnvSpec(RobotankDeterministic-v4), EnvSpec(RobotankNoFrameskip-v0), EnvSpec(RobotankNoFrameskip-v4), EnvSpec(Robotank-ram-v0), EnvSpec(Robotank-ram-v4), EnvSpec(Robotank-ramDeterministic-v0), EnvSpec(Robotank-ramDeterministic-v4), EnvSpec(Robotank-ramNoFrameskip-v0), EnvSpec(Robotank-ramNoFrameskip-v4), EnvSpec(Seaquest-v0), EnvSpec(Seaquest-v4), EnvSpec(SeaquestDeterministic-v0), EnvSpec(SeaquestDeterministic-v4), EnvSpec(SeaquestNoFrameskip-v0), EnvSpec(SeaquestNoFrameskip-v4), EnvSpec(Seaquest-ram-v0), EnvSpec(Seaquest-ram-v4), EnvSpec(Seaquest-ramDeterministic-v0), EnvSpec(Seaquest-ramDeterministic-v4), EnvSpec(Seaquest-ramNoFrameskip-v0), EnvSpec(Seaquest-ramNoFrameskip-v4), EnvSpec(Skiing-v0), EnvSpec(Skiing-v4), EnvSpec(SkiingDeterministic-v0), EnvSpec(SkiingDeterministic-v4), EnvSpec(SkiingNoFrameskip-v0), EnvSpec(SkiingNoFrameskip-v4), EnvSpec(Skiing-ram-v0), EnvSpec(Skiing-ram-v4), EnvSpec(Skiing-ramDeterministic-v0), EnvSpec(Skiing-ramDeterministic-v4), EnvSpec(Skiing-ramNoFrameskip-v0), EnvSpec(Skiing-ramNoFrameskip-v4), EnvSpec(Solaris-v0), EnvSpec(Solaris-v4), EnvSpec(SolarisDeterministic-v0), EnvSpec(SolarisDeterministic-v4), EnvSpec(SolarisNoFrameskip-v0), EnvSpec(SolarisNoFrameskip-v4), EnvSpec(Solaris-ram-v0), EnvSpec(Solaris-ram-v4), EnvSpec(Solaris-ramDeterministic-v0), EnvSpec(Solaris-ramDeterministic-v4), EnvSpec(Solaris-ramNoFrameskip-v0), EnvSpec(Solaris-ramNoFrameskip-v4), EnvSpec(SpaceInvaders-v0), EnvSpec(SpaceInvaders-v4), EnvSpec(SpaceInvadersDeterministic-v0), EnvSpec(SpaceInvadersDeterministic-v4), EnvSpec(SpaceInvadersNoFrameskip-v0), EnvSpec(SpaceInvadersNoFrameskip-v4), EnvSpec(SpaceInvaders-ram-v0), EnvSpec(SpaceInvaders-ram-v4), EnvSpec(SpaceInvaders-ramDeterministic-v0), EnvSpec(SpaceInvaders-ramDeterministic-v4), EnvSpec(SpaceInvaders-ramNoFrameskip-v0), EnvSpec(SpaceInvaders-ramNoFrameskip-v4), EnvSpec(StarGunner-v0), EnvSpec(StarGunner-v4), EnvSpec(StarGunnerDeterministic-v0), EnvSpec(StarGunnerDeterministic-v4), EnvSpec(StarGunnerNoFrameskip-v0), EnvSpec(StarGunnerNoFrameskip-v4), EnvSpec(StarGunner-ram-v0), EnvSpec(StarGunner-ram-v4), EnvSpec(StarGunner-ramDeterministic-v0), EnvSpec(StarGunner-ramDeterministic-v4), EnvSpec(StarGunner-ramNoFrameskip-v0), EnvSpec(StarGunner-ramNoFrameskip-v4), EnvSpec(Tennis-v0), EnvSpec(Tennis-v4), EnvSpec(TennisDeterministic-v0), EnvSpec(TennisDeterministic-v4), EnvSpec(TennisNoFrameskip-v0), EnvSpec(TennisNoFrameskip-v4), EnvSpec(Tennis-ram-v0), EnvSpec(Tennis-ram-v4), EnvSpec(Tennis-ramDeterministic-v0), EnvSpec(Tennis-ramDeterministic-v4), EnvSpec(Tennis-ramNoFrameskip-v0), EnvSpec(Tennis-ramNoFrameskip-v4), EnvSpec(TimePilot-v0), EnvSpec(TimePilot-v4), EnvSpec(TimePilotDeterministic-v0), EnvSpec(TimePilotDeterministic-v4), EnvSpec(TimePilotNoFrameskip-v0), EnvSpec(TimePilotNoFrameskip-v4), EnvSpec(TimePilot-ram-v0), EnvSpec(TimePilot-ram-v4), EnvSpec(TimePilot-ramDeterministic-v0), EnvSpec(TimePilot-ramDeterministic-v4), EnvSpec(TimePilot-ramNoFrameskip-v0), EnvSpec(TimePilot-ramNoFrameskip-v4), EnvSpec(Tutankham-v0), EnvSpec(Tutankham-v4), EnvSpec(TutankhamDeterministic-v0), EnvSpec(TutankhamDeterministic-v4), EnvSpec(TutankhamNoFrameskip-v0), EnvSpec(TutankhamNoFrameskip-v4), EnvSpec(Tutankham-ram-v0), EnvSpec(Tutankham-ram-v4), EnvSpec(Tutankham-ramDeterministic-v0), EnvSpec(Tutankham-ramDeterministic-v4), EnvSpec(Tutankham-ramNoFrameskip-v0), EnvSpec(Tutankham-ramNoFrameskip-v4), EnvSpec(UpNDown-v0), EnvSpec(UpNDown-v4), EnvSpec(UpNDownDeterministic-v0), EnvSpec(UpNDownDeterministic-v4), EnvSpec(UpNDownNoFrameskip-v0), EnvSpec(UpNDownNoFrameskip-v4), EnvSpec(UpNDown-ram-v0), EnvSpec(UpNDown-ram-v4), EnvSpec(UpNDown-ramDeterministic-v0), EnvSpec(UpNDown-ramDeterministic-v4), EnvSpec(UpNDown-ramNoFrameskip-v0), EnvSpec(UpNDown-ramNoFrameskip-v4), EnvSpec(Venture-v0), EnvSpec(Venture-v4), EnvSpec(VentureDeterministic-v0), EnvSpec(VentureDeterministic-v4), EnvSpec(VentureNoFrameskip-v0), EnvSpec(VentureNoFrameskip-v4), EnvSpec(Venture-ram-v0), EnvSpec(Venture-ram-v4), EnvSpec(Venture-ramDeterministic-v0), EnvSpec(Venture-ramDeterministic-v4), EnvSpec(Venture-ramNoFrameskip-v0), EnvSpec(Venture-ramNoFrameskip-v4), EnvSpec(VideoPinball-v0), EnvSpec(VideoPinball-v4), EnvSpec(VideoPinballDeterministic-v0), EnvSpec(VideoPinballDeterministic-v4), EnvSpec(VideoPinballNoFrameskip-v0), EnvSpec(VideoPinballNoFrameskip-v4), EnvSpec(VideoPinball-ram-v0), EnvSpec(VideoPinball-ram-v4), EnvSpec(VideoPinball-ramDeterministic-v0), EnvSpec(VideoPinball-ramDeterministic-v4), EnvSpec(VideoPinball-ramNoFrameskip-v0), EnvSpec(VideoPinball-ramNoFrameskip-v4), EnvSpec(WizardOfWor-v0), EnvSpec(WizardOfWor-v4), EnvSpec(WizardOfWorDeterministic-v0), EnvSpec(WizardOfWorDeterministic-v4), EnvSpec(WizardOfWorNoFrameskip-v0), EnvSpec(WizardOfWorNoFrameskip-v4), EnvSpec(WizardOfWor-ram-v0), EnvSpec(WizardOfWor-ram-v4), EnvSpec(WizardOfWor-ramDeterministic-v0), EnvSpec(WizardOfWor-ramDeterministic-v4), EnvSpec(WizardOfWor-ramNoFrameskip-v0), EnvSpec(WizardOfWor-ramNoFrameskip-v4), EnvSpec(YarsRevenge-v0), EnvSpec(YarsRevenge-v4), EnvSpec(YarsRevengeDeterministic-v0), EnvSpec(YarsRevengeDeterministic-v4), EnvSpec(YarsRevengeNoFrameskip-v0), EnvSpec(YarsRevengeNoFrameskip-v4), EnvSpec(YarsRevenge-ram-v0), EnvSpec(YarsRevenge-ram-v4), EnvSpec(YarsRevenge-ramDeterministic-v0), EnvSpec(YarsRevenge-ramDeterministic-v4), EnvSpec(YarsRevenge-ramNoFrameskip-v0), EnvSpec(YarsRevenge-ramNoFrameskip-v4), EnvSpec(Zaxxon-v0), EnvSpec(Zaxxon-v4), EnvSpec(ZaxxonDeterministic-v0), EnvSpec(ZaxxonDeterministic-v4), EnvSpec(ZaxxonNoFrameskip-v0), EnvSpec(ZaxxonNoFrameskip-v4), EnvSpec(Zaxxon-ram-v0), EnvSpec(Zaxxon-ram-v4), EnvSpec(Zaxxon-ramDeterministic-v0), EnvSpec(Zaxxon-ramDeterministic-v4), EnvSpec(Zaxxon-ramNoFrameskip-v0), EnvSpec(Zaxxon-ramNoFrameskip-v4), EnvSpec(CubeCrash-v0), EnvSpec(CubeCrashSparse-v0), EnvSpec(CubeCrashScreenBecomesBlack-v0), EnvSpec(MemorizeDigits-v0)]) . The Cart-Pole is a very simple environment composed of a cart that can move left or right, and pole placed vertically on top of it. The agent must move the cart left or right to keep the pole upright. . env = gym.make(&#39;CartPole-v1&#39;) . Let&#39;s initialize the environment by calling is reset() method. This returns an observation: . env.seed(42) obs = env.reset() . Observations vary depending on the environment. In this case it is a 1D NumPy array composed of 4 floats: they represent the cart&#39;s horizontal position, its velocity, the angle of the pole (0 = vertical), and the angular velocity. . obs . array([-0.01258566, -0.00156614, 0.04207708, -0.00180545]) . An environment can be visualized by calling its render() method, and you can pick the rendering mode (the rendering options depend on the environment). . Warning: some environments (including the Cart-Pole) require access to your display, which opens up a separate window, even if you specify mode=&quot;rgb_array&quot;. In general you can safely ignore that window. However, if Jupyter is running on a headless server (ie. without a screen) it will raise an exception. One way to avoid this is to install a fake X server like Xvfb. On Debian or Ubuntu: . $ apt update $ apt install -y xvfb . You can then start Jupyter using the xvfb-run command: . $ xvfb-run -s &quot;-screen 0 1400x900x24&quot; jupyter notebook . Alternatively, you can install the pyvirtualdisplay Python library which wraps Xvfb: . python3 -m pip install -U pyvirtualdisplay . And run the following code: . try: import pyvirtualdisplay display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start() except ImportError: pass . env.render() . True . In this example we will set mode=&quot;rgb_array&quot; to get an image of the environment as a NumPy array: . img = env.render(mode=&quot;rgb_array&quot;) img.shape . (800, 1200, 3) . def plot_environment(env, figsize=(5,4)): plt.figure(figsize=figsize) img = env.render(mode=&quot;rgb_array&quot;) plt.imshow(img) plt.axis(&quot;off&quot;) return img . plot_environment(env) plt.show() . Let&#39;s see how to interact with an environment. Your agent will need to select an action from an &quot;action space&quot; (the set of possible actions). Let&#39;s see what this environment&#39;s action space looks like: . env.action_space . Discrete(2) . Yep, just two possible actions: accelerate towards the left or towards the right. . Since the pole is leaning toward the right (obs[2] &gt; 0), let&#39;s accelerate the cart toward the right: . action = 1 # accelerate right obs, reward, done, info = env.step(action) obs . array([-0.01261699, 0.19292789, 0.04204097, -0.28092127]) . Notice that the cart is now moving toward the right (obs[1] &gt; 0). The pole is still tilted toward the right (obs[2] &gt; 0), but its angular velocity is now negative (obs[3] &lt; 0), so it will likely be tilted toward the left after the next step. . plot_environment(env) save_fig(&quot;cart_pole_plot&quot;) . Saving figure cart_pole_plot . Looks like it&#39;s doing what we&#39;re telling it to do! . The environment also tells the agent how much reward it got during the last step: . reward . 1.0 . When the game is over, the environment returns done=True: . done . False . Finally, info is an environment-specific dictionary that can provide some extra information that you may find useful for debugging or for training. For example, in some games it may indicate how many lives the agent has. . info . {} . The sequence of steps between the moment the environment is reset until it is done is called an &quot;episode&quot;. At the end of an episode (i.e., when step() returns done=True), you should reset the environment before you continue to use it. . if done: obs = env.reset() . Now how can we make the poll remain upright? We will need to define a policy for that. This is the strategy that the agent will use to select an action at each step. It can use all the past actions and observations to decide what to do. . A simple hard-coded policy . Let&#39;s hard code a simple strategy: if the pole is tilting to the left, then push the cart to the left, and vice versa. Let&#39;s see if that works: . env.seed(42) def basic_policy(obs): angle = obs[2] return 0 if angle &lt; 0 else 1 totals = [] for episode in range(500): episode_rewards = 0 obs = env.reset() for step in range(200): action = basic_policy(obs) obs, reward, done, info = env.step(action) episode_rewards += reward if done: break totals.append(episode_rewards) . np.mean(totals), np.std(totals), np.min(totals), np.max(totals) . (41.718, 8.858356280936096, 24.0, 68.0) . Well, as expected, this strategy is a bit too basic: the best it did was to keep the poll up for only 68 steps. This environment is considered solved when the agent keeps the poll up for 200 steps. . Let&#39;s visualize one episode: . env.seed(42) frames = [] obs = env.reset() for step in range(200): img = env.render(mode=&quot;rgb_array&quot;) frames.append(img) action = basic_policy(obs) obs, reward, done, info = env.step(action) if done: break . Now show the animation: . def update_scene(num, frames, patch): patch.set_data(frames[num]) return patch, def plot_animation(frames, repeat=False, interval=40): fig = plt.figure() patch = plt.imshow(frames[0]) plt.axis(&#39;off&#39;) anim = animation.FuncAnimation( fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval) plt.close() return anim . plot_animation(frames) . Clearly the system is unstable and after just a few wobbles, the pole ends up too tilted: game over. We will need to be smarter than that! . Neural Network Policies . Let&#39;s create a neural network that will take observations as inputs, and output the action to take for each observation. To choose an action, the network will estimate a probability for each action, then we will select an action randomly according to the estimated probabilities. In the case of the Cart-Pole environment, there are just two possible actions (left or right), so we only need one output neuron: it will output the probability p of the action 0 (left), and of course the probability of action 1 (right) will be 1 - p. . tf.random.set_seed(42) np.random.seed(42) n_inputs = 4 # == env.observation_space.shape[0] model = keras.models.Sequential([ keras.layers.Dense(5, activation=&quot;elu&quot;, input_shape=[n_inputs]), keras.layers.Dense(1, activation=&quot;sigmoid&quot;), ]) . In this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment&#39;s full state. If there were some hidden state then you may need to consider past actions and observations in order to try to infer the hidden state of the environment. For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. Another example is if the observations are noisy: you may want to use the past few observations to estimate the most likely current state. Our problem is thus as simple as can be: the current observation is noise-free and contains the environment&#39;s full state. . You may wonder why we plan to pick a random action based on the probability given by the policy network, rather than just picking the action with the highest probability. This approach lets the agent find the right balance between exploring new actions and exploiting the actions that are known to work well. Here&#39;s an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing so you randomly pick one. If it turns out to be good, you can increase the probability to order it next time, but you shouldn&#39;t increase that probability to 100%, or else you will never try out the other dishes, some of which may be even better than the one you tried. . Let&#39;s write a small function that will run the model to play one episode, and return the frames so we can display an animation: . def render_policy_net(model, n_max_steps=200, seed=42): frames = [] env = gym.make(&quot;CartPole-v1&quot;) env.seed(seed) np.random.seed(seed) obs = env.reset() for step in range(n_max_steps): frames.append(env.render(mode=&quot;rgb_array&quot;)) left_proba = model.predict(obs.reshape(1, -1)) action = int(np.random.rand() &gt; left_proba) obs, reward, done, info = env.step(action) if done: break env.close() return frames . Now let&#39;s look at how well this randomly initialized policy network performs: . frames = render_policy_net(model) plot_animation(frames) . Yeah... pretty bad. The neural network will have to learn to do better. First let&#39;s see if it is capable of learning the basic policy we used earlier: go left if the pole is tilting left, and go right if it is tilting right. . We can make the same net play in 50 different environments in parallel (this will give us a diverse training batch at each step), and train for 5000 iterations. We also reset environments when they are done. We train the model using a custom training loop so we can easily use the predictions at each training step to advance the environments. . n_environments = 50 n_iterations = 5000 envs = [gym.make(&quot;CartPole-v1&quot;) for _ in range(n_environments)] for index, env in enumerate(envs): env.seed(index) np.random.seed(42) observations = [env.reset() for env in envs] optimizer = keras.optimizers.RMSprop() loss_fn = keras.losses.binary_crossentropy for iteration in range(n_iterations): # if angle &lt; 0, we want proba(left) = 1., or else proba(left) = 0. target_probas = np.array([([1.] if obs[2] &lt; 0 else [0.]) for obs in observations]) with tf.GradientTape() as tape: left_probas = model(np.array(observations)) loss = tf.reduce_mean(loss_fn(target_probas, left_probas)) print(&quot; rIteration: {}, Loss: {:.3f}&quot;.format(iteration, loss.numpy()), end=&quot;&quot;) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables)) actions = (np.random.rand(n_environments, 1) &gt; left_probas.numpy()).astype(np.int32) for env_index, env in enumerate(envs): obs, reward, done, info = env.step(actions[env_index][0]) observations[env_index] = obs if not done else env.reset() for env in envs: env.close() . WARNING: Logging before flag parsing goes to stderr. W0526 22:48:21.481136 140735810999168 deprecation.py:323] From /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where . Iteration: 4999, Loss: 0.094 . frames = render_policy_net(model) plot_animation(frames) . Looks like it learned the policy correctly. Now let&#39;s see if it can learn a better policy on its own. One that does not wobble as much. . Policy Gradients . To train this neural network we will need to define the target probabilities y. If an action is good we should increase its probability, and conversely if it is bad we should reduce it. But how do we know whether an action is good or bad? The problem is that most actions have delayed effects, so when you win or lose points in an episode, it is not clear which actions contributed to this result: was it just the last action? Or the last 10? Or just one action 50 steps earlier? This is called the credit assignment problem. . The Policy Gradients algorithm tackles this problem by first playing multiple episodes, then making the actions in good episodes slightly more likely, while actions in bad episodes are made slightly less likely. First we play, then we go back and think about what we did. . Let&#39;s start by creating a function to play a single step using the model. We will also pretend for now that whatever action it takes is the right one, so we can compute the loss and its gradients (we will just save these gradients for now, and modify them later depending on how good or bad the action turned out to be): . def play_one_step(env, obs, model, loss_fn): with tf.GradientTape() as tape: left_proba = model(obs[np.newaxis]) action = (tf.random.uniform([1, 1]) &gt; left_proba) y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) loss = tf.reduce_mean(loss_fn(y_target, left_proba)) grads = tape.gradient(loss, model.trainable_variables) obs, reward, done, info = env.step(int(action[0, 0].numpy())) return obs, reward, done, grads . If left_proba is high, then action will most likely be False (since a random number uniformally sampled between 0 and 1 will probably not be greater than left_proba). And False means 0 when you cast it to a number, so y_target would be equal to 1 - 0 = 1. In other words, we set the target to 1, meaning we pretend that the probability of going left should have been 100% (so we took the right action). . Now let&#39;s create another function that will rely on the play_one_step() function to play multiple episodes, returning all the rewards and gradients, for each episode and each step: . def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn): all_rewards = [] all_grads = [] for episode in range(n_episodes): current_rewards = [] current_grads = [] obs = env.reset() for step in range(n_max_steps): obs, reward, done, grads = play_one_step(env, obs, model, loss_fn) current_rewards.append(reward) current_grads.append(grads) if done: break all_rewards.append(current_rewards) all_grads.append(current_grads) return all_rewards, all_grads . The Policy Gradients algorithm uses the model to play the episode several times (e.g., 10 times), then it goes back and looks at all the rewards, discounts them and normalizes them. So let&#39;s create couple functions for that: the first will compute discounted rewards; the second will normalize the discounted rewards across many episodes. . def discount_rewards(rewards, discount_rate): discounted = np.array(rewards) for step in range(len(rewards) - 2, -1, -1): discounted[step] += discounted[step + 1] * discount_rate return discounted def discount_and_normalize_rewards(all_rewards, discount_rate): all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards] flat_rewards = np.concatenate(all_discounted_rewards) reward_mean = flat_rewards.mean() reward_std = flat_rewards.std() return [(discounted_rewards - reward_mean) / reward_std for discounted_rewards in all_discounted_rewards] . Say there were 3 actions, and after each action there was a reward: first 10, then 0, then -50. If we use a discount factor of 80%, then the 3rd action will get -50 (full credit for the last reward), but the 2nd action will only get -40 (80% credit for the last reward), and the 1st action will get 80% of -40 (-32) plus full credit for the first reward (+10), which leads to a discounted reward of -22: . discount_rewards([10, 0, -50], discount_rate=0.8) . array([-22, -40, -50]) . To normalize all discounted rewards across all episodes, we compute the mean and standard deviation of all the discounted rewards, and we subtract the mean from each discounted reward, and divide by the standard deviation: . discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8) . [array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])] . n_iterations = 150 n_episodes_per_update = 10 n_max_steps = 200 discount_rate = 0.95 . optimizer = keras.optimizers.Adam(lr=0.01) loss_fn = keras.losses.binary_crossentropy . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.Dense(5, activation=&quot;elu&quot;, input_shape=[4]), keras.layers.Dense(1, activation=&quot;sigmoid&quot;), ]) . env = gym.make(&quot;CartPole-v1&quot;) env.seed(42); for iteration in range(n_iterations): all_rewards, all_grads = play_multiple_episodes( env, n_episodes_per_update, n_max_steps, model, loss_fn) total_rewards = sum(map(sum, all_rewards)) # Not shown in the book print(&quot; rIteration: {}, mean rewards: {:.1f}&quot;.format( # Not shown iteration, total_rewards / n_episodes_per_update), end=&quot;&quot;) # Not shown all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_rate) all_mean_grads = [] for var_index in range(len(model.trainable_variables)): mean_grads = tf.reduce_mean( [final_reward * all_grads[episode_index][step][var_index] for episode_index, final_rewards in enumerate(all_final_rewards) for step, final_reward in enumerate(final_rewards)], axis=0) all_mean_grads.append(mean_grads) optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables)) env.close() . Iteration: 149, mean rewards: 191.3 . frames = render_policy_net(model) plot_animation(frames) . Markov Chains . np.random.seed(42) transition_probabilities = [ # shape=[s, s&#39;] [0.7, 0.2, 0.0, 0.1], # from s0 to s0, s1, s2, s3 [0.0, 0.0, 0.9, 0.1], # from s1 to ... [0.0, 1.0, 0.0, 0.0], # from s2 to ... [0.0, 0.0, 0.0, 1.0]] # from s3 to ... n_max_steps = 50 def print_sequence(): current_state = 0 print(&quot;States:&quot;, end=&quot; &quot;) for step in range(n_max_steps): print(current_state, end=&quot; &quot;) if current_state == 3: break current_state = np.random.choice(range(4), p=transition_probabilities[current_state]) else: print(&quot;...&quot;, end=&quot;&quot;) print() for _ in range(10): print_sequence() . States: 0 0 3 States: 0 1 2 1 2 1 2 1 2 1 3 States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 States: 0 3 States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 States: 0 1 3 States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 ... States: 0 0 3 States: 0 0 0 1 2 1 2 1 3 States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 . Markov Decision Process . Let&#39;s define some transition probabilities, rewards and possible actions. For example, in state s0, if action a0 is chosen then with proba 0.7 we will go to state s0 with reward +10, with probability 0.3 we will go to state s1 with no reward, and with never go to state s2 (so the transition probabilities are [0.7, 0.3, 0.0], and the rewards are [+10, 0, 0]): . transition_probabilities = [ # shape=[s, a, s&#39;] [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]], [None, [0.8, 0.1, 0.1], None]] rewards = [ # shape=[s, a, s&#39;] [[+10, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, -50]], [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]] possible_actions = [[0, 1, 2], [0, 2], [1]] . Q-Value Iteration . Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions for state, actions in enumerate(possible_actions): Q_values[state, actions] = 0.0 # for all possible actions . gamma = 0.90 # the discount factor history1 = [] # Not shown in the book (for the figure below) for iteration in range(50): Q_prev = Q_values.copy() history1.append(Q_prev) # Not shown for s in range(3): for a in possible_actions[s]: Q_values[s, a] = np.sum([ transition_probabilities[s][a][sp] * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp])) for sp in range(3)]) history1 = np.array(history1) # Not shown . Q_values . array([[18.91891892, 17.02702702, 13.62162162], [ 0. , -inf, -4.87971488], [ -inf, 50.13365013, -inf]]) . np.argmax(Q_values, axis=1) . array([0, 0, 1]) . The optimal policy for this MDP, when using a discount factor of 0.90, is to choose action a0 when in state s0, and choose action a0 when in state s1, and finally choose action a1 (the only possible action) when in state s2. . Let&#39;s try again with a discount factor of 0.95: . Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions for state, actions in enumerate(possible_actions): Q_values[state, actions] = 0.0 # for all possible actions . gamma = 0.95 # the discount factor for iteration in range(50): Q_prev = Q_values.copy() for s in range(3): for a in possible_actions[s]: Q_values[s, a] = np.sum([ transition_probabilities[s][a][sp] * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp])) for sp in range(3)]) . Q_values . array([[21.73304188, 20.63807938, 16.70138772], [ 0.95462106, -inf, 1.01361207], [ -inf, 53.70728682, -inf]]) . np.argmax(Q_values, axis=1) . array([0, 2, 1]) . Now the policy has changed! In state s1, we now prefer to go through the fire (choose action a2). This is because the discount factor is larger so the agent values the future more, and it is therefore ready to pay an immediate penalty in order to get more future rewards. . Q-Learning . Q-Learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-Values. Once it has accurate Q-Value estimates (or close enough), then the optimal policy consists in choosing the action that has the highest Q-Value (i.e., the greedy policy). . We will need to simulate an agent moving around in the environment, so let&#39;s define a function to perform some action and get the new state and a reward: . def step(state, action): probas = transition_probabilities[state][action] next_state = np.random.choice([0, 1, 2], p=probas) reward = rewards[state][action][next_state] return next_state, reward . We also need an exploration policy, which can be any policy, as long as it visits every possible state many times. We will just use a random policy, since the state space is very small: . def exploration_policy(state): return np.random.choice(possible_actions[state]) . Now let&#39;s initialize the Q-Values like earlier, and run the Q-Learning algorithm: . np.random.seed(42) Q_values = np.full((3, 3), -np.inf) for state, actions in enumerate(possible_actions): Q_values[state][actions] = 0 alpha0 = 0.05 # initial learning rate decay = 0.005 # learning rate decay gamma = 0.90 # discount factor state = 0 # initial state history2 = [] # Not shown in the book for iteration in range(10000): history2.append(Q_values.copy()) # Not shown action = exploration_policy(state) next_state, reward = step(state, action) next_value = np.max(Q_values[next_state]) # greedy policy at the next step alpha = alpha0 / (1 + iteration * decay) Q_values[state, action] *= 1 - alpha Q_values[state, action] += alpha * (reward + gamma * next_value) state = next_state history2 = np.array(history2) # Not shown . Q_values . array([[18.77621289, 17.2238872 , 13.74543343], [ 0. , -inf, -8.00485647], [ -inf, 49.40208921, -inf]]) . np.argmax(Q_values, axis=1) # optimal action for each state . array([0, 0, 1]) . true_Q_value = history1[-1, 0, 0] fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True) axes[0].set_ylabel(&quot;Q-Value$(s_0, a_0)$&quot;, fontsize=14) axes[0].set_title(&quot;Q-Value Iteration&quot;, fontsize=14) axes[1].set_title(&quot;Q-Learning&quot;, fontsize=14) for ax, width, history in zip(axes, (50, 10000), (history1, history2)): ax.plot([0, width], [true_Q_value, true_Q_value], &quot;k--&quot;) ax.plot(np.arange(width), history[:, 0, 0], &quot;b-&quot;, linewidth=2) ax.set_xlabel(&quot;Iterations&quot;, fontsize=14) ax.axis([0, width, 0, 24]) save_fig(&quot;q_value_plot&quot;) . Saving figure q_value_plot . Deep Q-Network . Let&#39;s build the DQN. Given a state, it will estimate, for each possible action, the sum of discounted future rewards it can expect after it plays that action (but before it sees its outcome): . tf.random.set_seed(42) np.random.seed(42) env = gym.make(&quot;CartPole-v1&quot;) input_shape = [4] # == env.observation_space.shape n_outputs = 2 # == env.action_space.n model = keras.models.Sequential([ keras.layers.Dense(32, activation=&quot;elu&quot;, input_shape=input_shape), keras.layers.Dense(32, activation=&quot;elu&quot;), keras.layers.Dense(n_outputs) ]) . To select an action using this DQN, we just pick the action with the largest predicted Q-value. However, to ensure that the agent explores the environment, we choose a random action with probability epsilon. . def epsilon_greedy_policy(state, epsilon=0): if np.random.rand() &lt; epsilon: return np.random.randint(2) else: Q_values = model.predict(state[np.newaxis]) return np.argmax(Q_values[0]) . We will also need a replay memory. It will contain the agent&#39;s experiences, in the form of tuples: (obs, action, reward, next_obs, done). We can use the deque class for that: . from collections import deque replay_memory = deque(maxlen=2000) . And let&#39;s create a function to sample experiences from the replay memory. It will return 5 NumPy arrays: [obs, actions, rewards, next_obs, dones]. . def sample_experiences(batch_size): indices = np.random.randint(len(replay_memory), size=batch_size) batch = [replay_memory[index] for index in indices] states, actions, rewards, next_states, dones = [ np.array([experience[field_index] for experience in batch]) for field_index in range(5)] return states, actions, rewards, next_states, dones . Now we can create a function that will use the DQN to play one step, and record its experience in the replay memory: . def play_one_step(env, state, epsilon): action = epsilon_greedy_policy(state, epsilon) next_state, reward, done, info = env.step(action) replay_memory.append((state, action, reward, next_state, done)) return next_state, reward, done, info . Lastly, let&#39;s create a function that will sample some experiences from the replay memory and perform a training step: . batch_size = 32 discount_rate = 0.95 optimizer = keras.optimizers.Adam(lr=1e-3) loss_fn = keras.losses.mean_squared_error def training_step(batch_size): experiences = sample_experiences(batch_size) states, actions, rewards, next_states, dones = experiences next_Q_values = model.predict(next_states) max_next_Q_values = np.max(next_Q_values, axis=1) target_Q_values = rewards + (1 - dones) * discount_rate * max_next_Q_values mask = tf.one_hot(actions, n_outputs) with tf.GradientTape() as tape: all_Q_values = model(states) Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True) loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values)) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables)) . And now, let&#39;s train the model! . env.seed(42) np.random.seed(42) tf.random.set_seed(42) rewards = [] best_score = 0 . for episode in range(600): obs = env.reset() for step in range(200): epsilon = max(1 - episode / 500, 0.01) obs, reward, done, info = play_one_step(env, obs, epsilon) if done: break rewards.append(step) # Not shown in the book if step &gt; best_score: # Not shown best_weights = model.get_weights() # Not shown best_score = step # Not shown print(&quot; rEpisode: {}, Steps: {}, eps: {:.3f}&quot;.format(episode, step + 1, epsilon), end=&quot;&quot;) # Not shown if episode &gt; 50: training_step(batch_size) model.set_weights(best_weights) . Episode: 599, Steps: 91, eps: 0.0100 . plt.figure(figsize=(8, 4)) plt.plot(rewards) plt.xlabel(&quot;Episode&quot;, fontsize=14) plt.ylabel(&quot;Sum of rewards&quot;, fontsize=14) save_fig(&quot;dqn_rewards_plot&quot;) plt.show() . Saving figure dqn_rewards_plot . env.seed(42) state = env.reset() frames = [] for step in range(200): action = epsilon_greedy_policy(state) state, reward, done, info = env.step(action) if done: break img = env.render(mode=&quot;rgb_array&quot;) frames.append(img) plot_animation(frames) . Not bad at all! . Double DQN . tf.random.set_seed(42) np.random.seed(42) model = keras.models.Sequential([ keras.layers.Dense(32, activation=&quot;elu&quot;, input_shape=[4]), keras.layers.Dense(32, activation=&quot;elu&quot;), keras.layers.Dense(n_outputs) ]) target = keras.models.clone_model(model) target.set_weights(model.get_weights()) . batch_size = 32 discount_rate = 0.95 optimizer = keras.optimizers.Adam(lr=1e-3) loss_fn = keras.losses.Huber() def training_step(batch_size): experiences = sample_experiences(batch_size) states, actions, rewards, next_states, dones = experiences next_Q_values = model.predict(next_states) best_next_actions = np.argmax(next_Q_values, axis=1) next_mask = tf.one_hot(best_next_actions, n_outputs).numpy() next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1) target_Q_values = rewards + (1 - dones) * discount_rate * next_best_Q_values mask = tf.one_hot(actions, n_outputs) with tf.GradientTape() as tape: all_Q_values = model(states) Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True) loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values)) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables)) . replay_memory = deque(maxlen=2000) . env.seed(42) np.random.seed(42) tf.random.set_seed(42) rewards = [] best_score = 0 for episode in range(600): obs = env.reset() for step in range(200): epsilon = max(1 - episode / 500, 0.01) obs, reward, done, info = play_one_step(env, obs, epsilon) if done: break rewards.append(step) if step &gt; best_score: best_weights = model.get_weights() best_score = step print(&quot; rEpisode: {}, Steps: {}, eps: {:.3f}&quot;.format(episode, step + 1, epsilon), end=&quot;&quot;) if episode &gt; 50: training_step(batch_size) if episode % 50 == 0: target.set_weights(model.get_weights()) # Alternatively, you can do soft updates at each step: #if episode &gt; 50: #target_weights = target.get_weights() #online_weights = model.get_weights() #for index in range(len(target_weights)): # target_weights[index] = 0.99 * target_weights[index] + 0.01 * target_weights[index] #target.set_weights(target_weights) model.set_weights(best_weights) . Episode: 599, Steps: 12, eps: 0.0104 . plt.figure(figsize=(8, 4)) plt.plot(rewards) plt.xlabel(&quot;Episode&quot;, fontsize=14) plt.ylabel(&quot;Sum of rewards&quot;, fontsize=14) save_fig(&quot;double_dqn_rewards_plot&quot;) plt.show() . Saving figure dqn_rewards_plot . env.seed(42) state = env.reset() frames = [] for step in range(200): action = epsilon_greedy_policy(state) state, reward, done, info = env.step(action) if done: break img = env.render(mode=&quot;rgb_array&quot;) frames.append(img) plot_animation(frames) . Dueling Double DQN . tf.random.set_seed(42) np.random.seed(42) K = keras.backend input_states = keras.layers.Input(shape=[4]) hidden1 = keras.layers.Dense(32, activation=&quot;elu&quot;)(input_states) hidden2 = keras.layers.Dense(32, activation=&quot;elu&quot;)(hidden1) state_values = keras.layers.Dense(1)(hidden2) raw_advantages = keras.layers.Dense(n_outputs)(hidden2) advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True) Q_values = state_values + advantages model = keras.models.Model(inputs=[input_states], outputs=[Q_values]) target = keras.models.clone_model(model) target.set_weights(model.get_weights()) . batch_size = 32 discount_rate = 0.95 optimizer = keras.optimizers.Adam(lr=1e-2) loss_fn = keras.losses.Huber() def training_step(batch_size): experiences = sample_experiences(batch_size) states, actions, rewards, next_states, dones = experiences next_Q_values = model.predict(next_states) best_next_actions = np.argmax(next_Q_values, axis=1) next_mask = tf.one_hot(best_next_actions, n_outputs).numpy() next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1) target_Q_values = rewards + (1 - dones) * discount_rate * next_best_Q_values mask = tf.one_hot(actions, n_outputs) with tf.GradientTape() as tape: all_Q_values = model(states) Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True) loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values)) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables)) . replay_memory = deque(maxlen=2000) . env.seed(42) np.random.seed(42) tf.random.set_seed(42) rewards = [] best_score = 0 for episode in range(600): obs = env.reset() for step in range(200): epsilon = max(1 - episode / 500, 0.01) obs, reward, done, info = play_one_step(env, obs, epsilon) if done: break rewards.append(step) if step &gt; best_score: best_weights = model.get_weights() best_score = step print(&quot; rEpisode: {}, Steps: {}, eps: {:.3f}&quot;.format(episode, step + 1, epsilon), end=&quot;&quot;) if episode &gt; 50: training_step(batch_size) if episode % 200 == 0: target.set_weights(model.get_weights()) model.set_weights(best_weights) . Episode: 599, Steps: 10, eps: 0.0100 . plt.plot(rewards) plt.xlabel(&quot;Episode&quot;) plt.ylabel(&quot;Sum of rewards&quot;) plt.show() . env.seed(42) state = env.reset() frames = [] for step in range(200): action = epsilon_greedy_policy(state) state, reward, done, info = env.step(action) if done: break img = env.render(mode=&quot;rgb_array&quot;) frames.append(img) plot_animation(frames) . This looks like a pretty robust agent! . env.close() . Using TF-Agents to Beat Breakout . Let&#39;s use TF-Agents to create an agent that will learn to play Breakout. We will use the Deep Q-Learning algorithm, so you can easily compare the components with the previous implementation, but TF-Agents implements many other (and more sophisticated) algorithms! . TF-Agents Environments . tf.random.set_seed(42) np.random.seed(42) . from tf_agents.environments import suite_gym env = suite_gym.load(&quot;Breakout-v4&quot;) env . &lt;tf_agents.environments.wrappers.TimeLimit at 0x10b3788d0&gt; . env.gym . &lt;gym.envs.atari.atari_env.AtariEnv at 0x10a9d8518&gt; . env.seed(42) env.reset() . TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], ..., [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]], dtype=float32)) . env.step(1) # Fire . TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], ..., [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]], dtype=float32)) . img = env.render(mode=&quot;rgb_array&quot;) plt.figure(figsize=(6, 8)) plt.imshow(img) plt.axis(&quot;off&quot;) save_fig(&quot;breakout_plot&quot;) plt.show() . Saving figure breakout_plot . env.current_time_step() . TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], ..., [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], ..., [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]], dtype=float32)) . Environment Specifications . env.observation_spec() . BoundedArraySpec(shape=(210, 160, 3), dtype=dtype(&#39;float32&#39;), name=None, minimum=[[[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] ... [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]]], maximum=[[[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]] [[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]] [[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]] ... [[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]] [[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]] [[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]]]) . env.action_spec() . BoundedArraySpec(shape=(), dtype=dtype(&#39;int64&#39;), name=None, minimum=0, maximum=3) . env.time_step_spec() . TimeStep(step_type=ArraySpec(shape=(), dtype=dtype(&#39;int32&#39;), name=&#39;step_type&#39;), reward=ArraySpec(shape=(), dtype=dtype(&#39;float32&#39;), name=&#39;reward&#39;), discount=BoundedArraySpec(shape=(), dtype=dtype(&#39;float32&#39;), name=&#39;discount&#39;, minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(210, 160, 3), dtype=dtype(&#39;float32&#39;), name=None, minimum=[[[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] ... [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.] ... [0. 0. 0.] [0. 0. 0.] [0. 0. 0.]]], maximum=[[[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]] [[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]] [[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]] ... [[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]] [[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]] [[255. 255. 255.] [255. 255. 255.] [255. 255. 255.] ... [255. 255. 255.] [255. 255. 255.] [255. 255. 255.]]])) . Environment Wrappers . You can wrap a TF-Agents environments in a TF-Agents wrapper: . from tf_agents.environments.wrappers import ActionRepeat repeating_env = ActionRepeat(env, times=4) repeating_env . &lt;tf_agents.environments.wrappers.ActionRepeat at 0x134eb87b8&gt; . repeating_env.unwrapped . &lt;gym.envs.atari.atari_env.AtariEnv at 0x10a9d8518&gt; . Here is the list of available wrappers: . import tf_agents.environments.wrappers for name in dir(tf_agents.environments.wrappers): obj = getattr(tf_agents.environments.wrappers, name) if hasattr(obj, &quot;__base__&quot;) and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper): print(&quot;{:27s} {}&quot;.format(name, obj.__doc__.split(&quot; n&quot;)[0])) . ActionClipWrapper Wraps an environment and clips actions to spec before applying. ActionDiscretizeWrapper Wraps an environment with continuous actions and discretizes them. ActionOffsetWrapper Offsets actions to be zero-based. ActionRepeat Repeates actions over n-steps while acummulating the received reward. FlattenObservationsWrapper Wraps an environment and flattens nested multi-dimensional observations. GoalReplayEnvWrapper Adds a goal to the observation, used for HER (Hindsight Experience Replay). PyEnvironmentBaseWrapper PyEnvironment wrapper forwards calls to the given environment. RunStats Wrapper that accumulates run statistics as the environment iterates. TimeLimit End episodes after specified number of steps. . The suite_gym.load() function can create an env and wrap it for you, both with TF-Agents environment wrappers and Gym environment wrappers (the latter are applied first). . from functools import partial from gym.wrappers import TimeLimit limited_repeating_env = suite_gym.load( &quot;Breakout-v4&quot;, gym_env_wrappers=[partial(TimeLimit, max_episode_steps=10000)], env_wrappers=[partial(ActionRepeat, times=4)], ) . limited_repeating_env . &lt;tf_agents.environments.wrappers.ActionRepeat at 0x135fa0400&gt; . Create an Atari Breakout environment, and wrap it to apply the default Atari preprocessing steps: . limited_repeating_env.unwrapped . &lt;gym.envs.atari.atari_env.AtariEnv at 0x135fa0588&gt; . from tf_agents.environments import suite_atari from tf_agents.environments.atari_preprocessing import AtariPreprocessing from tf_agents.environments.atari_wrappers import FrameStack4 max_episode_steps = 27000 # &lt;=&gt; 108k ALE frames since 1 step = 4 frames environment_name = &quot;BreakoutNoFrameskip-v4&quot; env = suite_atari.load( environment_name, max_episode_steps=max_episode_steps, gym_env_wrappers=[AtariPreprocessing, FrameStack4]) . env . &lt;tf_agents.environments.atari_wrappers.AtariTimeLimit at 0x135fc08d0&gt; . Play a few steps just to see what happens: . env.seed(42) env.reset() time_step = env.step(1) # FIRE for _ in range(4): time_step = env.step(3) # LEFT . /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: WARN: &lt;class &#39;tf_agents.environments.atari_wrappers.FrameStack4&#39;&gt; doesn&#39;t implement &#39;reset&#39; method, but it implements deprecated &#39;_reset&#39; method. warnings.warn(colorize(&#39;%s: %s&#39;%(&#39;WARN&#39;, msg % args), &#39;yellow&#39;)) . def plot_observation(obs): # Since there are only 3 color channels, you cannot display 4 frames # with one primary color per frame. So this code computes the delta between # the current frame and the mean of the other frames, and it adds this delta # to the red and blue channels to get a pink color for the current frame. obs = obs.astype(np.float32) img = obs[..., :3] current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.) img[..., 0] += current_frame_delta img[..., 2] += current_frame_delta img = np.clip(img / 150, 0, 1) plt.imshow(img) plt.axis(&quot;off&quot;) . plt.figure(figsize=(6, 6)) plot_observation(time_step.observation) save_fig(&quot;preprocessed_breakout_plot&quot;) plt.show() . Saving figure preprocessed_breakout_plot . Convert the Python environment to a TF environment: . from tf_agents.environments.tf_py_environment import TFPyEnvironment tf_env = TFPyEnvironment(env) . Creating the DQN . Create a small class to normalize the observations. Images are stored using bytes from 0 to 255 to use less RAM, but we want to pass floats from 0.0 to 1.0 to the neural network: . Create the Q-Network: . from tf_agents.networks.q_network import QNetwork preprocessing_layer = keras.layers.Lambda( lambda obs: tf.cast(obs, np.float32) / 255.) conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)] fc_layer_params=[512] q_net = QNetwork( tf_env.observation_spec(), tf_env.action_spec(), preprocessing_layers=preprocessing_layer, conv_layer_params=conv_layer_params, fc_layer_params=fc_layer_params) . Create the DQN Agent: . from tf_agents.agents.dqn.dqn_agent import DqnAgent # see TF-agents issue #113 #optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0, # epsilon=0.00001, centered=True) train_step = tf.Variable(0) update_period = 4 # run a training step every 4 collect steps optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=2.5e-4, decay=0.95, momentum=0.0, epsilon=0.00001, centered=True) epsilon_fn = keras.optimizers.schedules.PolynomialDecay( initial_learning_rate=1.0, # initial Îµ decay_steps=250000 // update_period, # &lt;=&gt; 1,000,000 ALE frames end_learning_rate=0.01) # final Îµ agent = DqnAgent(tf_env.time_step_spec(), tf_env.action_spec(), q_network=q_net, optimizer=optimizer, target_update_period=2000, # &lt;=&gt; 32,000 ALE frames td_errors_loss_fn=keras.losses.Huber(reduction=&quot;none&quot;), gamma=0.99, # discount factor train_step_counter=train_step, epsilon_greedy=lambda: epsilon_fn(train_step)) agent.initialize() . Create the replay buffer: . from tf_agents.replay_buffers import tf_uniform_replay_buffer replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer( data_spec=agent.collect_data_spec, batch_size=tf_env.batch_size, max_length=1000000) replay_buffer_observer = replay_buffer.add_batch . Create a simple custom observer that counts and displays the number of times it is called (except when it is passed a trajectory that represents the boundary between two episodes, as this does not count as a step): . class ShowProgress: def __init__(self, total): self.counter = 0 self.total = total def __call__(self, trajectory): if not trajectory.is_boundary(): self.counter += 1 if self.counter % 100 == 0: print(&quot; r{}/{}&quot;.format(self.counter, self.total), end=&quot;&quot;) . Let&#39;s add some training metrics: . from tf_agents.metrics import tf_metrics train_metrics = [ tf_metrics.NumberOfEpisodes(), tf_metrics.EnvironmentSteps(), tf_metrics.AverageReturnMetric(), tf_metrics.AverageEpisodeLengthMetric(), ] . train_metrics[0].result() . &lt;tf.Tensor: id=469, shape=(), dtype=int64, numpy=0&gt; . from tf_agents.eval.metric_utils import log_metrics import logging logging.getLogger().setLevel(logging.INFO) log_metrics(train_metrics) . WARNING: Logging before flag parsing goes to stderr. I0528 08:47:44.704986 140735810999168 metric_utils.py:47] NumberOfEpisodes = 0 EnvironmentSteps = 0 AverageReturn = 0.0 AverageEpisodeLength = 0.0 . Create the collect driver: . from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver collect_driver = DynamicStepDriver( tf_env, agent.collect_policy, observers=[replay_buffer_observer] + train_metrics, num_steps=update_period) # collect 4 steps for each training iteration . Collect the initial experiences, before training: . from tf_agents.policies.random_tf_policy import RandomTFPolicy initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec()) init_driver = DynamicStepDriver( tf_env, initial_collect_policy, observers=[replay_buffer.add_batch, ShowProgress(20000)], num_steps=20000) # &lt;=&gt; 80,000 ALE frames final_time_step, final_policy_state = init_driver.run() . W0528 08:47:44.747640 140735810999168 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64 W0528 08:47:44.761187 140735810999168 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64 W0528 08:47:44.765793 140735810999168 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64 W0528 08:47:44.770788 140735810999168 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64 W0528 08:47:44.775924 140735810999168 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64 . 20000/20000 . Let&#39;s sample 2 sub-episodes, with 3 time steps each and display them: . tf.random.set_seed(888) # chosen to show an example of trajectory at the end of an episode trajectories, buffer_info = replay_buffer.get_next( sample_batch_size=2, num_steps=3) . trajectories._fields . (&#39;step_type&#39;, &#39;observation&#39;, &#39;action&#39;, &#39;policy_info&#39;, &#39;next_step_type&#39;, &#39;reward&#39;, &#39;discount&#39;) . trajectories.observation.shape . TensorShape([2, 3, 84, 84, 4]) . from tf_agents.trajectories.trajectory import to_transition time_steps, action_steps, next_time_steps = to_transition(trajectories) time_steps.observation.shape . TensorShape([2, 2, 84, 84, 4]) . trajectories.step_type.numpy() . array([[1, 1, 1], [1, 1, 1]], dtype=int32) . plt.figure(figsize=(10, 6.8)) for row in range(2): for col in range(3): plt.subplot(2, 3, row * 3 + col + 1) plot_observation(trajectories.observation[row, col].numpy()) plt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02) save_fig(&quot;sub_episodes_plot&quot;) plt.show() . Saving figure sub_episodes_plot . Now let&#39;s create the dataset: . dataset = replay_buffer.as_dataset( sample_batch_size=64, num_steps=2, num_parallel_calls=3).prefetch(3) . Convert the main functions to TF Functions for better performance: . from tf_agents.utils.common import function collect_driver.run = function(collect_driver.run) agent.train = function(agent.train) . And now we are ready to run the main loop! . def train_agent(n_iterations): time_step = None policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size) iterator = iter(dataset) for iteration in range(n_iterations): time_step, policy_state = collect_driver.run(time_step, policy_state) trajectories, buffer_info = next(iterator) train_loss = agent.train(trajectories) print(&quot; r{} loss:{:.5f}&quot;.format( iteration, train_loss.loss.numpy()), end=&quot;&quot;) if iteration % 1000 == 0: log_metrics(train_metrics) . Run the next cell to train the agent for 10,000 steps. Then look at its behavior by running the following cell. You can run these two cells as many times as you wish. The agent will keep improving! . train_agent(n_iterations=10000) . W0528 08:49:00.697262 140735810999168 deprecation.py:323] From /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Deprecated in favor of operator or tf.math.divide. W0528 08:49:01.475340 140735810999168 deprecation.py:323] From /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where I0528 08:49:02.463025 140735810999168 metric_utils.py:47] NumberOfEpisodes = 0 EnvironmentSteps = 4 AverageReturn = 0.0 AverageEpisodeLength = 0.0 . 997 loss:0.01551 . I0528 08:50:16.405580 140735810999168 metric_utils.py:47] NumberOfEpisodes = 24 EnvironmentSteps = 4004 AverageReturn = 1.399999976158142 AverageEpisodeLength = 180.5 . 2000 loss:0.00024 . I0528 08:51:28.353239 140735810999168 metric_utils.py:47] NumberOfEpisodes = 47 EnvironmentSteps = 8004 AverageReturn = 0.8999999761581421 AverageEpisodeLength = 165.89999389648438 . 2997 loss:0.00010 . I0528 08:52:36.316717 140735810999168 metric_utils.py:47] NumberOfEpisodes = 69 EnvironmentSteps = 12004 AverageReturn = 0.800000011920929 AverageEpisodeLength = 162.3000030517578 . 3999 loss:0.00751 . I0528 08:53:47.764101 140735810999168 metric_utils.py:47] NumberOfEpisodes = 92 EnvironmentSteps = 16004 AverageReturn = 0.699999988079071 AverageEpisodeLength = 161.89999389648438 . 4997 loss:0.00032 . I0528 08:54:57.040647 140735810999168 metric_utils.py:47] NumberOfEpisodes = 111 EnvironmentSteps = 20004 AverageReturn = 1.0 AverageEpisodeLength = 181.60000610351562 . 6000 loss:0.00006 . I0528 08:56:07.210252 140735810999168 metric_utils.py:47] NumberOfEpisodes = 131 EnvironmentSteps = 24004 AverageReturn = 1.7000000476837158 AverageEpisodeLength = 206.39999389648438 . 6999 loss:0.00784 . I0528 08:57:18.494511 140735810999168 metric_utils.py:47] NumberOfEpisodes = 154 EnvironmentSteps = 28004 AverageReturn = 1.100000023841858 AverageEpisodeLength = 182.6999969482422 . 7997 loss:0.00002 . I0528 08:58:35.320452 140735810999168 metric_utils.py:47] NumberOfEpisodes = 175 EnvironmentSteps = 32004 AverageReturn = 1.7999999523162842 AverageEpisodeLength = 196.60000610351562 . 8997 loss:0.00749 . I0528 08:59:51.332596 140735810999168 metric_utils.py:47] NumberOfEpisodes = 195 EnvironmentSteps = 36004 AverageReturn = 1.100000023841858 AverageEpisodeLength = 185.8000030517578 . 9999 loss:0.00001 . frames = [] def save_frames(trajectory): global frames frames.append(tf_env.pyenv.envs[0].render(mode=&quot;rgb_array&quot;)) prev_lives = tf_env.pyenv.envs[0].ale.lives() def reset_and_fire_on_life_lost(trajectory): global prev_lives lives = tf_env.pyenv.envs[0].ale.lives() if prev_lives != lives: tf_env.reset() tf_env.pyenv.envs[0].step(1) prev_lives = lives watch_driver = DynamicStepDriver( tf_env, agent.policy, observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(1000)], num_steps=1000) final_time_step, final_policy_state = watch_driver.run() plot_animation(frames) . If you want to save an animated GIF to show off your agent to your friends, here&#39;s one way to do it: . import PIL image_path = os.path.join(&quot;images&quot;, &quot;rl&quot;, &quot;breakout.gif&quot;) frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]] frame_images[0].save(image_path, format=&#39;GIF&#39;, append_images=frame_images[1:], save_all=True, duration=30, loop=0) . %%html {% include image.html file=&quot;/images/copied_from_nb/images/rl/breakout.gif&quot; %} . Extra material . Deque vs Rotating List . The deque class offers fast append, but fairly slow random access (for large replay memories): . from collections import deque np.random.seed(42) mem = deque(maxlen=1000000) for i in range(1000000): mem.append(i) [mem[i] for i in np.random.randint(1000000, size=5)] . [121958, 671155, 131932, 365838, 259178] . %timeit mem.append(1) . 76.8 ns Â± 0.31 ns per loop (mean Â± std. dev. of 7 runs, 10000000 loops each) . %timeit [mem[i] for i in np.random.randint(1000000, size=5)] . 320 Âµs Â± 23 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each) . Alternatively, you could use a rotating list like this ReplayMemory class. This would make random access faster for large replay memories: . class ReplayMemory: def __init__(self, max_size): self.buffer = np.empty(max_size, dtype=np.object) self.max_size = max_size self.index = 0 self.size = 0 def append(self, obj): self.buffer[self.index] = obj self.size = min(self.size + 1, self.max_size) self.index = (self.index + 1) % self.max_size def sample(self, batch_size): indices = np.random.randint(self.size, size=batch_size) return self.buffer[indices] . mem = ReplayMemory(max_size=1000000) for i in range(1000000): mem.append(i) mem.sample(5) . array([757386, 904203, 190588, 595754, 865356], dtype=object) . %timeit mem.append(1) . 761 ns Â± 17.6 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each) . %timeit mem.sample(5) . 2.97 Âµs Â± 16.4 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each) . Creating a Custom TF-Agents Environment . To create a custom TF-Agent environment, you just need to write a class that inherits from the PyEnvironment class and implements a few methods. For example, the following minimal environment represents a simple 4x4 grid. The agent starts in one corner (0,0) and must move to the opposite corner (3,3). The episode is done if the agent reaches the goal (it gets a +10 reward) or if the agent goes out of bounds (-1 reward). The actions are up (0), down (1), left (2) and right (3). . class MyEnvironment(tf_agents.environments.py_environment.PyEnvironment): def __init__(self, discount=1.0): super().__init__() self._action_spec = tf_agents.specs.BoundedArraySpec( shape=(), dtype=np.int32, name=&quot;action&quot;, minimum=0, maximum=3) self._observation_spec = tf_agents.specs.BoundedArraySpec( shape=(4, 4), dtype=np.int32, name=&quot;observation&quot;, minimum=0, maximum=1) self.discount = discount def action_spec(self): return self._action_spec def observation_spec(self): return self._observation_spec def _reset(self): self._state = np.zeros(2, dtype=np.int32) obs = np.zeros((4, 4), dtype=np.int32) obs[self._state[0], self._state[1]] = 1 return tf_agents.trajectories.time_step.restart(obs) def _step(self, action): self._state += [(-1, 0), (+1, 0), (0, -1), (0, +1)][action] reward = 0 obs = np.zeros((4, 4), dtype=np.int32) done = (self._state.min() &lt; 0 or self._state.max() &gt; 3) if not done: obs[self._state[0], self._state[1]] = 1 if done or np.all(self._state == np.array([3, 3])): reward = -1 if done else +10 return tf_agents.trajectories.time_step.termination(obs, reward) else: return tf_agents.trajectories.time_step.transition(obs, reward, self.discount) . The action and observation specs will generally be instances of the ArraySpec or BoundedArraySpec classes from the tf_agents.specs package (check out the other specs in this package as well). Optionally, you can also define a render() method, a close() method to free resources, as well as a time_step_spec() method if you don&#39;t want the reward and discount to be 32-bit float scalars. Note that the base class takes care of keeping track of the current time step, which is why we must implement _reset() and _step() rather than reset() and step(). . my_env = MyEnvironment() time_step = my_env.reset() time_step . TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], dtype=int32)) . time_step = my_env.step(1) time_step . TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[0, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], dtype=int32)) .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/18_reinforcement_learning",
            "relUrl": "/18_reinforcement_learning",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Processing Sequences Using RNNs and CNNs",
            "content": "This notebook contains all the sample code in chapter 15. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20 and TensorFlow â‰¥2.0. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x IS_COLAB = True except Exception: IS_COLAB = False # TensorFlow â‰¥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; if not tf.test.is_gpu_available(): print(&quot;No GPU was detected. LSTMs and CNNs can be very slow without a GPU.&quot;) if IS_COLAB: print(&quot;Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.&quot;) # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) tf.random.set_seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;rnn&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . . Basic RNNs . Generate the Dataset . def generate_time_series(batch_size, n_steps): freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1) time = np.linspace(0, 1, n_steps) series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1 series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2 series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise return series[..., np.newaxis].astype(np.float32) . np.random.seed(42) n_steps = 50 series = generate_time_series(10000, n_steps + 1) X_train, y_train = series[:7000, :n_steps], series[:7000, -1] X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1] X_test, y_test = series[9000:, :n_steps], series[9000:, -1] . X_train.shape, y_train.shape . ((7000, 50, 1), (7000, 1)) . def plot_series(series, y=None, y_pred=None, x_label=&quot;$t$&quot;, y_label=&quot;$x(t)$&quot;): plt.plot(series, &quot;.-&quot;) if y is not None: plt.plot(n_steps, y, &quot;bx&quot;, markersize=10) if y_pred is not None: plt.plot(n_steps, y_pred, &quot;ro&quot;) plt.grid(True) if x_label: plt.xlabel(x_label, fontsize=16) if y_label: plt.ylabel(y_label, fontsize=16, rotation=0) plt.hlines(0, 0, 100, linewidth=1) plt.axis([0, n_steps + 1, -1, 1]) fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4)) for col in range(3): plt.sca(axes[col]) plot_series(X_valid[col, :, 0], y_valid[col, 0], y_label=(&quot;$x(t)$&quot; if col==0 else None)) save_fig(&quot;time_series_plot&quot;) plt.show() . Saving figure time_series_plot . Computing Some Baselines . Naive predictions (just predict the last observed value): . y_pred = X_valid[:, -1] np.mean(keras.losses.mean_squared_error(y_valid, y_pred)) . 0.020211367 . plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0]) plt.show() . Linear predictions: . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[50, 1]), keras.layers.Dense(1) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;) history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)) . WARNING: Logging before flag parsing goes to stderr. W0803 10:31:33.783297 140735810999168 deprecation.py:323] From /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:468: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version. Instructions for updating: Apply a constraint manually following the optimizer update step. . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 0s 44us/sample - loss: 0.1015 - val_loss: 0.0551 Epoch 2/20 7000/7000 [==============================] - 0s 27us/sample - loss: 0.0384 - val_loss: 0.0267 Epoch 3/20 7000/7000 [==============================] - 0s 27us/sample - loss: 0.0205 - val_loss: 0.0161 Epoch 4/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0133 - val_loss: 0.0119 Epoch 5/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0104 - val_loss: 0.0099 Epoch 6/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0090 - val_loss: 0.0088 Epoch 7/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0081 - val_loss: 0.0079 Epoch 8/20 7000/7000 [==============================] - 0s 27us/sample - loss: 0.0073 - val_loss: 0.0074 Epoch 9/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0067 - val_loss: 0.0066 Epoch 10/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0062 - val_loss: 0.0062 Epoch 11/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0058 - val_loss: 0.0058 Epoch 12/20 7000/7000 [==============================] - 0s 27us/sample - loss: 0.0055 - val_loss: 0.0054 Epoch 13/20 7000/7000 [==============================] - 0s 27us/sample - loss: 0.0052 - val_loss: 0.0052 Epoch 14/20 7000/7000 [==============================] - 0s 27us/sample - loss: 0.0050 - val_loss: 0.0050 Epoch 15/20 7000/7000 [==============================] - 0s 27us/sample - loss: 0.0048 - val_loss: 0.0048 Epoch 16/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0047 - val_loss: 0.0046 Epoch 17/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0045 - val_loss: 0.0047 Epoch 18/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0044 - val_loss: 0.0044 Epoch 19/20 7000/7000 [==============================] - 0s 28us/sample - loss: 0.0043 - val_loss: 0.0042 Epoch 20/20 7000/7000 [==============================] - 0s 27us/sample - loss: 0.0042 - val_loss: 0.0042 . model.evaluate(X_valid, y_valid) . 2000/2000 [==============================] - 0s 14us/sample - loss: 0.0042 . 0.004174048032611608 . def plot_learning_curves(loss, val_loss): plt.plot(np.arange(len(loss)) + 0.5, loss, &quot;b.-&quot;, label=&quot;Training loss&quot;) plt.plot(np.arange(len(val_loss)) + 1, val_loss, &quot;r.-&quot;, label=&quot;Validation loss&quot;) plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True)) plt.axis([1, 20, 0, 0.05]) plt.legend(fontsize=14) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Loss&quot;) plt.grid(True) plot_learning_curves(history.history[&quot;loss&quot;], history.history[&quot;val_loss&quot;]) plt.show() . y_pred = model.predict(X_valid) plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0]) plt.show() . Using a Simple RNN . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.SimpleRNN(1, input_shape=[None, 1]) ]) optimizer = keras.optimizers.Adam(lr=0.005) model.compile(loss=&quot;mse&quot;, optimizer=optimizer) history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 3s 465us/sample - loss: 0.0960 - val_loss: 0.0482 Epoch 2/20 7000/7000 [==============================] - 3s 367us/sample - loss: 0.0366 - val_loss: 0.0293 Epoch 3/20 7000/7000 [==============================] - 3s 376us/sample - loss: 0.0251 - val_loss: 0.0216 Epoch 4/20 7000/7000 [==============================] - 3s 369us/sample - loss: 0.0196 - val_loss: 0.0175 Epoch 5/20 7000/7000 [==============================] - 3s 381us/sample - loss: 0.0165 - val_loss: 0.0150 Epoch 6/20 7000/7000 [==============================] - 3s 380us/sample - loss: 0.0145 - val_loss: 0.0133 Epoch 7/20 7000/7000 [==============================] - 3s 376us/sample - loss: 0.0132 - val_loss: 0.0123 Epoch 8/20 7000/7000 [==============================] - 3s 364us/sample - loss: 0.0123 - val_loss: 0.0116 Epoch 9/20 7000/7000 [==============================] - 3s 369us/sample - loss: 0.0118 - val_loss: 0.0112 Epoch 10/20 7000/7000 [==============================] - 3s 369us/sample - loss: 0.0116 - val_loss: 0.0110 Epoch 11/20 7000/7000 [==============================] - 3s 371us/sample - loss: 0.0114 - val_loss: 0.0109 Epoch 12/20 7000/7000 [==============================] - 3s 380us/sample - loss: 0.0114 - val_loss: 0.0109 Epoch 13/20 7000/7000 [==============================] - 3s 377us/sample - loss: 0.0114 - val_loss: 0.0109 Epoch 14/20 7000/7000 [==============================] - 3s 381us/sample - loss: 0.0114 - val_loss: 0.0109 Epoch 15/20 7000/7000 [==============================] - 3s 373us/sample - loss: 0.0114 - val_loss: 0.0109 Epoch 16/20 7000/7000 [==============================] - 3s 379us/sample - loss: 0.0114 - val_loss: 0.0109 Epoch 17/20 7000/7000 [==============================] - 3s 372us/sample - loss: 0.0114 - val_loss: 0.0109 Epoch 18/20 7000/7000 [==============================] - 3s 380us/sample - loss: 0.0114 - val_loss: 0.0110 Epoch 19/20 7000/7000 [==============================] - 3s 375us/sample - loss: 0.0114 - val_loss: 0.0109 Epoch 20/20 7000/7000 [==============================] - 3s 374us/sample - loss: 0.0114 - val_loss: 0.0108 . model.evaluate(X_valid, y_valid) . 2000/2000 [==============================] - 0s 194us/sample - loss: 0.0108 . 0.010848855562508107 . plot_learning_curves(history.history[&quot;loss&quot;], history.history[&quot;val_loss&quot;]) plt.show() . y_pred = model.predict(X_valid) plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0]) plt.show() . Deep RNNs . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20, return_sequences=True), keras.layers.SimpleRNN(1) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;) history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 10s 1ms/sample - loss: 0.0502 - val_loss: 0.0093 Epoch 2/20 7000/7000 [==============================] - 9s 1ms/sample - loss: 0.0069 - val_loss: 0.0055 Epoch 3/20 7000/7000 [==============================] - 9s 1ms/sample - loss: 0.0050 - val_loss: 0.0044 Epoch 4/20 7000/7000 [==============================] - 9s 1ms/sample - loss: 0.0048 - val_loss: 0.0040 Epoch 5/20 7000/7000 [==============================] - 9s 1ms/sample - loss: 0.0041 - val_loss: 0.0037 Epoch 6/20 7000/7000 [==============================] - 9s 1ms/sample - loss: 0.0041 - val_loss: 0.0043 Epoch 7/20 7000/7000 [==============================] - 9s 1ms/sample - loss: 0.0038 - val_loss: 0.0036 Epoch 8/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0037 - val_loss: 0.0032 Epoch 9/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0036 - val_loss: 0.0036 Epoch 10/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0035 - val_loss: 0.0031 Epoch 11/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0034 - val_loss: 0.0032 Epoch 12/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0034 - val_loss: 0.0036 Epoch 13/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0033 - val_loss: 0.0031 Epoch 14/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0033 - val_loss: 0.0030 Epoch 15/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0033 - val_loss: 0.0031 Epoch 16/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0033 - val_loss: 0.0029 Epoch 17/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0032 - val_loss: 0.0029 Epoch 18/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0032 - val_loss: 0.0029 Epoch 19/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0032 - val_loss: 0.0027 Epoch 20/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0031 - val_loss: 0.0029 . model.evaluate(X_valid, y_valid) . 2000/2000 [==============================] - 1s 567us/sample - loss: 0.0029 . 0.002889613825827837 . plot_learning_curves(history.history[&quot;loss&quot;], history.history[&quot;val_loss&quot;]) plt.show() . y_pred = model.predict(X_valid) plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0]) plt.show() . Make the second SimpleRNN layer return only the last output: . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20), keras.layers.Dense(1) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;) history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 7s 976us/sample - loss: 0.0212 - val_loss: 0.0053 Epoch 2/20 7000/7000 [==============================] - 6s 811us/sample - loss: 0.0041 - val_loss: 0.0034 Epoch 3/20 7000/7000 [==============================] - 6s 809us/sample - loss: 0.0034 - val_loss: 0.0031 Epoch 4/20 7000/7000 [==============================] - 6s 804us/sample - loss: 0.0033 - val_loss: 0.0030 Epoch 5/20 7000/7000 [==============================] - 6s 823us/sample - loss: 0.0031 - val_loss: 0.0030 Epoch 6/20 7000/7000 [==============================] - 6s 842us/sample - loss: 0.0031 - val_loss: 0.0028 Epoch 7/20 7000/7000 [==============================] - 6s 839us/sample - loss: 0.0030 - val_loss: 0.0028 Epoch 8/20 7000/7000 [==============================] - 6s 806us/sample - loss: 0.0030 - val_loss: 0.0028 Epoch 9/20 7000/7000 [==============================] - 6s 810us/sample - loss: 0.0030 - val_loss: 0.0031 Epoch 10/20 7000/7000 [==============================] - 6s 805us/sample - loss: 0.0030 - val_loss: 0.0027 Epoch 11/20 7000/7000 [==============================] - 6s 802us/sample - loss: 0.0029 - val_loss: 0.0027 Epoch 12/20 7000/7000 [==============================] - 6s 807us/sample - loss: 0.0030 - val_loss: 0.0029 Epoch 13/20 7000/7000 [==============================] - 6s 801us/sample - loss: 0.0029 - val_loss: 0.0029 Epoch 14/20 7000/7000 [==============================] - 6s 818us/sample - loss: 0.0029 - val_loss: 0.0030 Epoch 15/20 7000/7000 [==============================] - 6s 806us/sample - loss: 0.0029 - val_loss: 0.0028 Epoch 16/20 7000/7000 [==============================] - 6s 820us/sample - loss: 0.0028 - val_loss: 0.0026 Epoch 17/20 7000/7000 [==============================] - 6s 809us/sample - loss: 0.0028 - val_loss: 0.0026 Epoch 18/20 7000/7000 [==============================] - 6s 821us/sample - loss: 0.0029 - val_loss: 0.0029 Epoch 19/20 7000/7000 [==============================] - 6s 806us/sample - loss: 0.0028 - val_loss: 0.0025 Epoch 20/20 7000/7000 [==============================] - 6s 816us/sample - loss: 0.0028 - val_loss: 0.0027 . model.evaluate(X_valid, y_valid) . 2000/2000 [==============================] - 1s 342us/sample - loss: 0.0027 . 0.002677031420171261 . plot_learning_curves(history.history[&quot;loss&quot;], history.history[&quot;val_loss&quot;]) plt.show() . y_pred = model.predict(X_valid) plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0]) plt.show() . Forecasting Several Steps Ahead . np.random.seed(43) # not 42, as it would give the first series in the train set series = generate_time_series(1, n_steps + 10) X_new, Y_new = series[:, :n_steps], series[:, n_steps:] X = X_new for step_ahead in range(10): y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :] X = np.concatenate([X, y_pred_one], axis=1) Y_pred = X[:, n_steps:] . Y_pred.shape . (1, 10, 1) . def plot_multiple_forecasts(X, Y, Y_pred): n_steps = X.shape[1] ahead = Y.shape[1] plot_series(X[0, :, 0]) plt.plot(np.arange(n_steps, n_steps + ahead), Y[0, :, 0], &quot;ro-&quot;, label=&quot;Actual&quot;) plt.plot(np.arange(n_steps, n_steps + ahead), Y_pred[0, :, 0], &quot;bx-&quot;, label=&quot;Forecast&quot;, markersize=10) plt.axis([0, n_steps + ahead, -1, 1]) plt.legend(fontsize=14) plot_multiple_forecasts(X_new, Y_new, Y_pred) save_fig(&quot;forecast_ahead_plot&quot;) plt.show() . Saving figure forecast_ahead_plot . Now let&#39;s use this model to predict the next 10 values. We first need to regenerate the sequences with 9 more time steps. . np.random.seed(42) n_steps = 50 series = generate_time_series(10000, n_steps + 10) X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0] X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0] X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0] . Now let&#39;s predict the next 10 values one by one: . X = X_valid for step_ahead in range(10): y_pred_one = model.predict(X)[:, np.newaxis, :] X = np.concatenate([X, y_pred_one], axis=1) Y_pred = X[:, n_steps:, 0] . Y_pred.shape . (2000, 10) . np.mean(keras.metrics.mean_squared_error(Y_valid, Y_pred)) . 0.0285489 . Let&#39;s compare this performance with some baselines: naive predictions and a simple linear model: . Y_naive_pred = Y_valid[:, -1:] np.mean(keras.metrics.mean_squared_error(Y_valid, Y_naive_pred)) . 0.22278848 . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[50, 1]), keras.layers.Dense(10) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;) history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 0s 46us/sample - loss: 0.1334 - val_loss: 0.0605 Epoch 2/20 7000/7000 [==============================] - 0s 30us/sample - loss: 0.0498 - val_loss: 0.0424 Epoch 3/20 7000/7000 [==============================] - 0s 30us/sample - loss: 0.0385 - val_loss: 0.0356 Epoch 4/20 7000/7000 [==============================] - 0s 30us/sample - loss: 0.0332 - val_loss: 0.0313 Epoch 5/20 7000/7000 [==============================] - 0s 32us/sample - loss: 0.0298 - val_loss: 0.0284 Epoch 6/20 7000/7000 [==============================] - 0s 31us/sample - loss: 0.0274 - val_loss: 0.0264 Epoch 7/20 7000/7000 [==============================] - 0s 30us/sample - loss: 0.0256 - val_loss: 0.0248 Epoch 8/20 7000/7000 [==============================] - 0s 31us/sample - loss: 0.0244 - val_loss: 0.0239 Epoch 9/20 7000/7000 [==============================] - 0s 32us/sample - loss: 0.0234 - val_loss: 0.0229 Epoch 10/20 7000/7000 [==============================] - 0s 30us/sample - loss: 0.0226 - val_loss: 0.0223 Epoch 11/20 7000/7000 [==============================] - 0s 31us/sample - loss: 0.0220 - val_loss: 0.0216 Epoch 12/20 7000/7000 [==============================] - 0s 32us/sample - loss: 0.0215 - val_loss: 0.0213 Epoch 13/20 7000/7000 [==============================] - 0s 32us/sample - loss: 0.0210 - val_loss: 0.0207 Epoch 14/20 7000/7000 [==============================] - 0s 31us/sample - loss: 0.0207 - val_loss: 0.0204 Epoch 15/20 7000/7000 [==============================] - 0s 30us/sample - loss: 0.0203 - val_loss: 0.0201 Epoch 16/20 7000/7000 [==============================] - 0s 30us/sample - loss: 0.0199 - val_loss: 0.0196 Epoch 17/20 7000/7000 [==============================] - 0s 32us/sample - loss: 0.0196 - val_loss: 0.0196 Epoch 18/20 7000/7000 [==============================] - 0s 30us/sample - loss: 0.0194 - val_loss: 0.0191 Epoch 19/20 7000/7000 [==============================] - 0s 30us/sample - loss: 0.0191 - val_loss: 0.0189 Epoch 20/20 7000/7000 [==============================] - 0s 33us/sample - loss: 0.0188 - val_loss: 0.0188 . Now let&#39;s create an RNN that predicts all 10 next values at once: . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20), keras.layers.Dense(10) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;) history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0670 - val_loss: 0.0341 Epoch 2/20 7000/7000 [==============================] - 6s 867us/sample - loss: 0.0267 - val_loss: 0.0195 Epoch 3/20 7000/7000 [==============================] - 7s 961us/sample - loss: 0.0189 - val_loss: 0.0195 Epoch 4/20 7000/7000 [==============================] - 6s 903us/sample - loss: 0.0152 - val_loss: 0.0129 Epoch 5/20 7000/7000 [==============================] - 6s 870us/sample - loss: 0.0136 - val_loss: 0.0125 Epoch 6/20 7000/7000 [==============================] - 6s 914us/sample - loss: 0.0126 - val_loss: 0.0128 Epoch 7/20 7000/7000 [==============================] - 6s 860us/sample - loss: 0.0121 - val_loss: 0.0107 Epoch 8/20 7000/7000 [==============================] - 6s 840us/sample - loss: 0.0112 - val_loss: 0.0109 Epoch 9/20 7000/7000 [==============================] - 6s 886us/sample - loss: 0.0109 - val_loss: 0.0106 Epoch 10/20 7000/7000 [==============================] - 6s 857us/sample - loss: 0.0114 - val_loss: 0.0100 Epoch 11/20 7000/7000 [==============================] - 6s 841us/sample - loss: 0.0105 - val_loss: 0.0109 Epoch 12/20 7000/7000 [==============================] - 6s 824us/sample - loss: 0.0104 - val_loss: 0.0095 Epoch 13/20 7000/7000 [==============================] - 6s 823us/sample - loss: 0.0098 - val_loss: 0.0098 Epoch 14/20 7000/7000 [==============================] - 6s 839us/sample - loss: 0.0099 - val_loss: 0.0104 Epoch 15/20 7000/7000 [==============================] - 6s 818us/sample - loss: 0.0096 - val_loss: 0.0089 Epoch 16/20 7000/7000 [==============================] - 6s 821us/sample - loss: 0.0096 - val_loss: 0.0088 Epoch 17/20 7000/7000 [==============================] - 6s 827us/sample - loss: 0.0094 - val_loss: 0.0092 Epoch 18/20 7000/7000 [==============================] - 6s 840us/sample - loss: 0.0090 - val_loss: 0.0083 Epoch 19/20 7000/7000 [==============================] - 6s 812us/sample - loss: 0.0090 - val_loss: 0.0113 Epoch 20/20 7000/7000 [==============================] - 6s 808us/sample - loss: 0.0098 - val_loss: 0.0082 . np.random.seed(43) series = generate_time_series(1, 50 + 10) X_new, Y_new = series[:, :50, :], series[:, -10:, :] Y_pred = model.predict(X_new)[..., np.newaxis] . plot_multiple_forecasts(X_new, Y_new, Y_pred) plt.show() . Now let&#39;s create an RNN that predicts the next 10 steps at each time step. That is, instead of just forecasting time steps 50 to 59 based on time steps 0 to 49, it will forecast time steps 1 to 10 at time step 0, then time steps 2 to 11 at time step 1, and so on, and finally it will forecast time steps 50 to 59 at the last time step. Notice that the model is causal: when it makes predictions at any time step, it can only see past time steps. . np.random.seed(42) n_steps = 50 series = generate_time_series(10000, n_steps + 10) X_train = series[:7000, :n_steps] X_valid = series[7000:9000, :n_steps] X_test = series[9000:, :n_steps] Y = np.empty((10000, n_steps, 10)) for step_ahead in range(1, 10 + 1): Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0] Y_train = Y[:7000] Y_valid = Y[7000:9000] Y_test = Y[9000:] . X_train.shape, Y_train.shape . ((7000, 50, 1), (7000, 50, 10)) . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10)) ]) def last_time_step_mse(Y_true, Y_pred): return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1]) model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.Adam(lr=0.01), metrics=[last_time_step_mse]) history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0505 - last_time_step_mse: 0.0395 - val_loss: 0.0422 - val_last_time_step_mse: 0.0321 Epoch 2/20 7000/7000 [==============================] - 6s 889us/sample - loss: 0.0408 - last_time_step_mse: 0.0306 - val_loss: 0.0421 - val_last_time_step_mse: 0.0350 Epoch 3/20 7000/7000 [==============================] - 6s 897us/sample - loss: 0.0376 - last_time_step_mse: 0.0291 - val_loss: 0.0345 - val_last_time_step_mse: 0.0270 Epoch 4/20 7000/7000 [==============================] - 6s 891us/sample - loss: 0.0317 - last_time_step_mse: 0.0215 - val_loss: 0.0274 - val_last_time_step_mse: 0.0155 Epoch 5/20 7000/7000 [==============================] - 6s 879us/sample - loss: 0.0279 - last_time_step_mse: 0.0166 - val_loss: 0.0259 - val_last_time_step_mse: 0.0136 Epoch 6/20 7000/7000 [==============================] - 6s 875us/sample - loss: 0.0244 - last_time_step_mse: 0.0120 - val_loss: 0.0233 - val_last_time_step_mse: 0.0105 Epoch 7/20 7000/7000 [==============================] - 6s 880us/sample - loss: 0.0225 - last_time_step_mse: 0.0101 - val_loss: 0.0213 - val_last_time_step_mse: 0.0091 Epoch 8/20 7000/7000 [==============================] - 6s 900us/sample - loss: 0.0210 - last_time_step_mse: 0.0086 - val_loss: 0.0201 - val_last_time_step_mse: 0.0082 Epoch 9/20 7000/7000 [==============================] - 6s 895us/sample - loss: 0.0203 - last_time_step_mse: 0.0080 - val_loss: 0.0212 - val_last_time_step_mse: 0.0097 Epoch 10/20 7000/7000 [==============================] - 6s 845us/sample - loss: 0.0198 - last_time_step_mse: 0.0076 - val_loss: 0.0196 - val_last_time_step_mse: 0.0080 Epoch 11/20 7000/7000 [==============================] - 6s 843us/sample - loss: 0.0196 - last_time_step_mse: 0.0074 - val_loss: 0.0188 - val_last_time_step_mse: 0.0068 Epoch 12/20 7000/7000 [==============================] - 6s 837us/sample - loss: 0.0191 - last_time_step_mse: 0.0071 - val_loss: 0.0190 - val_last_time_step_mse: 0.0067 Epoch 13/20 7000/7000 [==============================] - 6s 881us/sample - loss: 0.0189 - last_time_step_mse: 0.0069 - val_loss: 0.0184 - val_last_time_step_mse: 0.0068 Epoch 14/20 7000/7000 [==============================] - 6s 855us/sample - loss: 0.0193 - last_time_step_mse: 0.0076 - val_loss: 0.0198 - val_last_time_step_mse: 0.0079 Epoch 15/20 7000/7000 [==============================] - 6s 847us/sample - loss: 0.0189 - last_time_step_mse: 0.0070 - val_loss: 0.0181 - val_last_time_step_mse: 0.0065 Epoch 16/20 7000/7000 [==============================] - 6s 849us/sample - loss: 0.0189 - last_time_step_mse: 0.0072 - val_loss: 0.0180 - val_last_time_step_mse: 0.0058 Epoch 17/20 7000/7000 [==============================] - 6s 837us/sample - loss: 0.0186 - last_time_step_mse: 0.0068 - val_loss: 0.0173 - val_last_time_step_mse: 0.0056 Epoch 18/20 7000/7000 [==============================] - 6s 831us/sample - loss: 0.0181 - last_time_step_mse: 0.0063 - val_loss: 0.0184 - val_last_time_step_mse: 0.0072 Epoch 19/20 7000/7000 [==============================] - 6s 830us/sample - loss: 0.0184 - last_time_step_mse: 0.0067 - val_loss: 0.0180 - val_last_time_step_mse: 0.0066 Epoch 20/20 7000/7000 [==============================] - 6s 848us/sample - loss: 0.0181 - last_time_step_mse: 0.0066 - val_loss: 0.0198 - val_last_time_step_mse: 0.0095 . np.random.seed(43) series = generate_time_series(1, 50 + 10) X_new, Y_new = series[:, :50, :], series[:, 50:, :] Y_pred = model.predict(X_new)[:, -1][..., np.newaxis] . plot_multiple_forecasts(X_new, Y_new, Y_pred) plt.show() . Deep RNN with Batch Norm . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.BatchNormalization(), keras.layers.SimpleRNN(20, return_sequences=True), keras.layers.BatchNormalization(), keras.layers.TimeDistributed(keras.layers.Dense(10)) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;, metrics=[last_time_step_mse]) history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.1939 - last_time_step_mse: 0.1922 - val_loss: 0.0876 - val_last_time_step_mse: 0.0834 Epoch 2/20 7000/7000 [==============================] - 6s 840us/sample - loss: 0.0536 - last_time_step_mse: 0.0448 - val_loss: 0.0553 - val_last_time_step_mse: 0.0467 Epoch 3/20 7000/7000 [==============================] - 6s 854us/sample - loss: 0.0472 - last_time_step_mse: 0.0377 - val_loss: 0.0450 - val_last_time_step_mse: 0.0353 Epoch 4/20 7000/7000 [==============================] - 6s 843us/sample - loss: 0.0438 - last_time_step_mse: 0.0338 - val_loss: 0.0423 - val_last_time_step_mse: 0.0331 Epoch 5/20 7000/7000 [==============================] - 6s 849us/sample - loss: 0.0408 - last_time_step_mse: 0.0304 - val_loss: 0.0394 - val_last_time_step_mse: 0.0285 Epoch 6/20 7000/7000 [==============================] - 6s 835us/sample - loss: 0.0385 - last_time_step_mse: 0.0277 - val_loss: 0.0381 - val_last_time_step_mse: 0.0274 Epoch 7/20 7000/7000 [==============================] - 6s 831us/sample - loss: 0.0365 - last_time_step_mse: 0.0253 - val_loss: 0.0363 - val_last_time_step_mse: 0.0244 Epoch 8/20 7000/7000 [==============================] - 6s 829us/sample - loss: 0.0350 - last_time_step_mse: 0.0236 - val_loss: 0.0348 - val_last_time_step_mse: 0.0242 Epoch 9/20 7000/7000 [==============================] - 6s 830us/sample - loss: 0.0339 - last_time_step_mse: 0.0223 - val_loss: 0.0333 - val_last_time_step_mse: 0.0212 Epoch 10/20 7000/7000 [==============================] - 6s 830us/sample - loss: 0.0330 - last_time_step_mse: 0.0215 - val_loss: 0.0345 - val_last_time_step_mse: 0.0241 Epoch 11/20 7000/7000 [==============================] - 6s 835us/sample - loss: 0.0321 - last_time_step_mse: 0.0206 - val_loss: 0.0319 - val_last_time_step_mse: 0.0203 Epoch 12/20 7000/7000 [==============================] - 6s 843us/sample - loss: 0.0316 - last_time_step_mse: 0.0199 - val_loss: 0.0314 - val_last_time_step_mse: 0.0200 Epoch 13/20 7000/7000 [==============================] - 6s 842us/sample - loss: 0.0312 - last_time_step_mse: 0.0195 - val_loss: 0.0313 - val_last_time_step_mse: 0.0195 Epoch 14/20 7000/7000 [==============================] - 6s 844us/sample - loss: 0.0307 - last_time_step_mse: 0.0190 - val_loss: 0.0339 - val_last_time_step_mse: 0.0227 Epoch 15/20 7000/7000 [==============================] - 6s 845us/sample - loss: 0.0303 - last_time_step_mse: 0.0183 - val_loss: 0.0306 - val_last_time_step_mse: 0.0184 Epoch 16/20 7000/7000 [==============================] - 6s 846us/sample - loss: 0.0297 - last_time_step_mse: 0.0178 - val_loss: 0.0292 - val_last_time_step_mse: 0.0172 Epoch 17/20 7000/7000 [==============================] - 6s 840us/sample - loss: 0.0294 - last_time_step_mse: 0.0174 - val_loss: 0.0291 - val_last_time_step_mse: 0.0168 Epoch 18/20 7000/7000 [==============================] - 6s 845us/sample - loss: 0.0291 - last_time_step_mse: 0.0172 - val_loss: 0.0286 - val_last_time_step_mse: 0.0160 Epoch 19/20 7000/7000 [==============================] - 6s 841us/sample - loss: 0.0288 - last_time_step_mse: 0.0169 - val_loss: 0.0294 - val_last_time_step_mse: 0.0174 Epoch 20/20 7000/7000 [==============================] - 6s 838us/sample - loss: 0.0282 - last_time_step_mse: 0.0161 - val_loss: 0.0289 - val_last_time_step_mse: 0.0171 . Deep RNNs with Layer Norm . from tensorflow.keras.layers import LayerNormalization . class LNSimpleRNNCell(keras.layers.Layer): def __init__(self, units, activation=&quot;tanh&quot;, **kwargs): super().__init__(**kwargs) self.state_size = units self.output_size = units self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None) self.layer_norm = LayerNormalization() self.activation = keras.activations.get(activation) def get_initial_state(self, inputs=None, batch_size=None, dtype=None): if inputs is not None: batch_size = tf.shape(inputs)[0] dtype = inputs.dtype return [tf.zeros([batch_size, self.state_size], dtype=dtype)] def call(self, inputs, states): outputs, new_states = self.simple_rnn_cell(inputs, states) norm_outputs = self.activation(self.layer_norm(outputs)) return norm_outputs, [norm_outputs] . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]), keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10)) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;, metrics=[last_time_step_mse]) history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 16s 2ms/sample - loss: 0.1636 - last_time_step_mse: 0.1493 - val_loss: 0.0725 - val_last_time_step_mse: 0.0675 Epoch 2/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0629 - last_time_step_mse: 0.0548 - val_loss: 0.0562 - val_last_time_step_mse: 0.0448 Epoch 3/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0528 - last_time_step_mse: 0.0415 - val_loss: 0.0487 - val_last_time_step_mse: 0.0357 Epoch 4/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0466 - last_time_step_mse: 0.0344 - val_loss: 0.0443 - val_last_time_step_mse: 0.0315 Epoch 5/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0428 - last_time_step_mse: 0.0306 - val_loss: 0.0408 - val_last_time_step_mse: 0.0284 Epoch 6/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0398 - last_time_step_mse: 0.0275 - val_loss: 0.0377 - val_last_time_step_mse: 0.0246 Epoch 7/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0376 - last_time_step_mse: 0.0257 - val_loss: 0.0368 - val_last_time_step_mse: 0.0254 Epoch 8/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0356 - last_time_step_mse: 0.0237 - val_loss: 0.0351 - val_last_time_step_mse: 0.0240 Epoch 9/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0343 - last_time_step_mse: 0.0223 - val_loss: 0.0333 - val_last_time_step_mse: 0.0210 Epoch 10/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0331 - last_time_step_mse: 0.0211 - val_loss: 0.0321 - val_last_time_step_mse: 0.0199 Epoch 11/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0323 - last_time_step_mse: 0.0203 - val_loss: 0.0316 - val_last_time_step_mse: 0.0193 Epoch 12/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0317 - last_time_step_mse: 0.0196 - val_loss: 0.0311 - val_last_time_step_mse: 0.0198 Epoch 13/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0309 - last_time_step_mse: 0.0188 - val_loss: 0.0301 - val_last_time_step_mse: 0.0169 Epoch 14/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0304 - last_time_step_mse: 0.0181 - val_loss: 0.0296 - val_last_time_step_mse: 0.0171 Epoch 15/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0299 - last_time_step_mse: 0.0174 - val_loss: 0.0295 - val_last_time_step_mse: 0.0168 Epoch 16/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0295 - last_time_step_mse: 0.0170 - val_loss: 0.0293 - val_last_time_step_mse: 0.0171 Epoch 17/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0292 - last_time_step_mse: 0.0166 - val_loss: 0.0288 - val_last_time_step_mse: 0.0162 Epoch 18/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0288 - last_time_step_mse: 0.0160 - val_loss: 0.0294 - val_last_time_step_mse: 0.0175 Epoch 19/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0284 - last_time_step_mse: 0.0158 - val_loss: 0.0277 - val_last_time_step_mse: 0.0151 Epoch 20/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0281 - last_time_step_mse: 0.0153 - val_loss: 0.0275 - val_last_time_step_mse: 0.0149 . Creating a Custom RNN Class . class MyRNN(keras.layers.Layer): def __init__(self, cell, return_sequences=False, **kwargs): super().__init__(**kwargs) self.cell = cell self.return_sequences = return_sequences self.get_initial_state = getattr( self.cell, &quot;get_initial_state&quot;, self.fallback_initial_state) def fallback_initial_state(self, inputs): return [tf.zeros([self.cell.state_size], dtype=inputs.dtype)] @tf.function def call(self, inputs): states = self.get_initial_state(inputs) n_steps = tf.shape(inputs)[1] if self.return_sequences: sequences = tf.TensorArray(inputs.dtype, size=n_steps) outputs = tf.zeros(shape=[n_steps, self.cell.output_size], dtype=inputs.dtype) for step in tf.range(n_steps): outputs, states = self.cell(inputs[:, step], states) if self.return_sequences: sequences = sequences.write(step, outputs) if self.return_sequences: return sequences.stack() else: return outputs . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ MyRNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]), MyRNN(LNSimpleRNNCell(20), return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10)) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;, metrics=[last_time_step_mse]) history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 16s 2ms/sample - loss: 0.2296 - last_time_step_mse: 0.2117 - val_loss: 0.1150 - val_last_time_step_mse: 0.1047 Epoch 2/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0869 - last_time_step_mse: 0.0769 - val_loss: 0.0637 - val_last_time_step_mse: 0.0535 Epoch 3/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0562 - last_time_step_mse: 0.0452 - val_loss: 0.0514 - val_last_time_step_mse: 0.0397 Epoch 4/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0475 - last_time_step_mse: 0.0342 - val_loss: 0.0433 - val_last_time_step_mse: 0.0279 Epoch 5/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0414 - last_time_step_mse: 0.0268 - val_loss: 0.0393 - val_last_time_step_mse: 0.0244 Epoch 6/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0380 - last_time_step_mse: 0.0232 - val_loss: 0.0366 - val_last_time_step_mse: 0.0215 Epoch 7/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0357 - last_time_step_mse: 0.0210 - val_loss: 0.0342 - val_last_time_step_mse: 0.0192 Epoch 8/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0347 - last_time_step_mse: 0.0202 - val_loss: 0.0361 - val_last_time_step_mse: 0.0214 Epoch 9/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0339 - last_time_step_mse: 0.0192 - val_loss: 0.0329 - val_last_time_step_mse: 0.0175 Epoch 10/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0327 - last_time_step_mse: 0.0181 - val_loss: 0.0320 - val_last_time_step_mse: 0.0171 Epoch 11/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0319 - last_time_step_mse: 0.0176 - val_loss: 0.0315 - val_last_time_step_mse: 0.0170 Epoch 12/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0311 - last_time_step_mse: 0.0171 - val_loss: 0.0307 - val_last_time_step_mse: 0.0161 Epoch 13/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0307 - last_time_step_mse: 0.0166 - val_loss: 0.0304 - val_last_time_step_mse: 0.0166 Epoch 14/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0302 - last_time_step_mse: 0.0163 - val_loss: 0.0300 - val_last_time_step_mse: 0.0159 Epoch 15/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0297 - last_time_step_mse: 0.0159 - val_loss: 0.0294 - val_last_time_step_mse: 0.0154 Epoch 16/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0293 - last_time_step_mse: 0.0155 - val_loss: 0.0291 - val_last_time_step_mse: 0.0155 Epoch 17/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0288 - last_time_step_mse: 0.0151 - val_loss: 0.0283 - val_last_time_step_mse: 0.0141 Epoch 18/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0283 - last_time_step_mse: 0.0146 - val_loss: 0.0287 - val_last_time_step_mse: 0.0153 Epoch 19/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0280 - last_time_step_mse: 0.0144 - val_loss: 0.0275 - val_last_time_step_mse: 0.0134 Epoch 20/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0275 - last_time_step_mse: 0.0138 - val_loss: 0.0276 - val_last_time_step_mse: 0.0139 . LSTMs . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]), keras.layers.LSTM(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10)) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;, metrics=[last_time_step_mse]) history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0759 - last_time_step_mse: 0.0615 - val_loss: 0.0551 - val_last_time_step_mse: 0.0364 Epoch 2/20 7000/7000 [==============================] - 11s 2ms/sample - loss: 0.0477 - last_time_step_mse: 0.0275 - val_loss: 0.0421 - val_last_time_step_mse: 0.0211 Epoch 3/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0390 - last_time_step_mse: 0.0183 - val_loss: 0.0364 - val_last_time_step_mse: 0.0151 Epoch 4/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0351 - last_time_step_mse: 0.0152 - val_loss: 0.0336 - val_last_time_step_mse: 0.0132 Epoch 5/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0327 - last_time_step_mse: 0.0135 - val_loss: 0.0314 - val_last_time_step_mse: 0.0121 Epoch 6/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0311 - last_time_step_mse: 0.0126 - val_loss: 0.0302 - val_last_time_step_mse: 0.0114 Epoch 7/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0300 - last_time_step_mse: 0.0120 - val_loss: 0.0293 - val_last_time_step_mse: 0.0111 Epoch 8/20 7000/7000 [==============================] - 11s 2ms/sample - loss: 0.0292 - last_time_step_mse: 0.0115 - val_loss: 0.0283 - val_last_time_step_mse: 0.0103 Epoch 9/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0283 - last_time_step_mse: 0.0110 - val_loss: 0.0289 - val_last_time_step_mse: 0.0112 Epoch 10/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0277 - last_time_step_mse: 0.0105 - val_loss: 0.0272 - val_last_time_step_mse: 0.0102 Epoch 11/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0271 - last_time_step_mse: 0.0104 - val_loss: 0.0268 - val_last_time_step_mse: 0.0101 Epoch 12/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0266 - last_time_step_mse: 0.0100 - val_loss: 0.0268 - val_last_time_step_mse: 0.0102 Epoch 13/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0262 - last_time_step_mse: 0.0098 - val_loss: 0.0260 - val_last_time_step_mse: 0.0100 Epoch 14/20 7000/7000 [==============================] - 12s 2ms/sample - loss: 0.0259 - last_time_step_mse: 0.0098 - val_loss: 0.0254 - val_last_time_step_mse: 0.0095 Epoch 15/20 7000/7000 [==============================] - 11s 2ms/sample - loss: 0.0255 - last_time_step_mse: 0.0097 - val_loss: 0.0259 - val_last_time_step_mse: 0.0112 Epoch 16/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0252 - last_time_step_mse: 0.0095 - val_loss: 0.0250 - val_last_time_step_mse: 0.0098 Epoch 17/20 7000/7000 [==============================] - 15s 2ms/sample - loss: 0.0248 - last_time_step_mse: 0.0093 - val_loss: 0.0245 - val_last_time_step_mse: 0.0091 Epoch 18/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0245 - last_time_step_mse: 0.0091 - val_loss: 0.0247 - val_last_time_step_mse: 0.0101 Epoch 19/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0242 - last_time_step_mse: 0.0090 - val_loss: 0.0240 - val_last_time_step_mse: 0.0090 Epoch 20/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0239 - last_time_step_mse: 0.0089 - val_loss: 0.0237 - val_last_time_step_mse: 0.0085 . model.evaluate(X_valid, Y_valid) . 2000/2000 [==============================] - 2s 753us/sample - loss: 0.0237 - last_time_step_mse: 0.0085 . [0.023689542233943938, 0.008518972] . plot_learning_curves(history.history[&quot;loss&quot;], history.history[&quot;val_loss&quot;]) plt.show() . np.random.seed(43) series = generate_time_series(1, 50 + 10) X_new, Y_new = series[:, :50, :], series[:, 50:, :] Y_pred = model.predict(X_new)[:, -1][..., np.newaxis] . plot_multiple_forecasts(X_new, Y_new, Y_pred) plt.show() . GRUs . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]), keras.layers.GRU(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10)) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;, metrics=[last_time_step_mse]) history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 21s 3ms/sample - loss: 0.0742 - last_time_step_mse: 0.0667 - val_loss: 0.0528 - val_last_time_step_mse: 0.0414 Epoch 2/20 7000/7000 [==============================] - 15s 2ms/sample - loss: 0.0476 - last_time_step_mse: 0.0365 - val_loss: 0.0440 - val_last_time_step_mse: 0.0326 Epoch 3/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0418 - last_time_step_mse: 0.0303 - val_loss: 0.0394 - val_last_time_step_mse: 0.0271 Epoch 4/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0373 - last_time_step_mse: 0.0249 - val_loss: 0.0359 - val_last_time_step_mse: 0.0226 Epoch 5/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0328 - last_time_step_mse: 0.0181 - val_loss: 0.0316 - val_last_time_step_mse: 0.0168 Epoch 6/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0308 - last_time_step_mse: 0.0157 - val_loss: 0.0299 - val_last_time_step_mse: 0.0146 Epoch 7/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0298 - last_time_step_mse: 0.0149 - val_loss: 0.0289 - val_last_time_step_mse: 0.0137 Epoch 8/20 7000/7000 [==============================] - 15s 2ms/sample - loss: 0.0288 - last_time_step_mse: 0.0140 - val_loss: 0.0282 - val_last_time_step_mse: 0.0136 Epoch 9/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0280 - last_time_step_mse: 0.0135 - val_loss: 0.0274 - val_last_time_step_mse: 0.0126 Epoch 10/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0272 - last_time_step_mse: 0.0126 - val_loss: 0.0268 - val_last_time_step_mse: 0.0126 Epoch 11/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0269 - last_time_step_mse: 0.0126 - val_loss: 0.0262 - val_last_time_step_mse: 0.0120 Epoch 12/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0264 - last_time_step_mse: 0.0122 - val_loss: 0.0264 - val_last_time_step_mse: 0.0128 Epoch 13/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0261 - last_time_step_mse: 0.0120 - val_loss: 0.0260 - val_last_time_step_mse: 0.0123 Epoch 14/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0258 - last_time_step_mse: 0.0119 - val_loss: 0.0254 - val_last_time_step_mse: 0.0116 Epoch 15/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0256 - last_time_step_mse: 0.0117 - val_loss: 0.0258 - val_last_time_step_mse: 0.0127 Epoch 16/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0254 - last_time_step_mse: 0.0116 - val_loss: 0.0253 - val_last_time_step_mse: 0.0116 Epoch 17/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0252 - last_time_step_mse: 0.0116 - val_loss: 0.0250 - val_last_time_step_mse: 0.0118 Epoch 18/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0248 - last_time_step_mse: 0.0111 - val_loss: 0.0250 - val_last_time_step_mse: 0.0120 Epoch 19/20 7000/7000 [==============================] - 13s 2ms/sample - loss: 0.0246 - last_time_step_mse: 0.0110 - val_loss: 0.0243 - val_last_time_step_mse: 0.0109 Epoch 20/20 7000/7000 [==============================] - 14s 2ms/sample - loss: 0.0245 - last_time_step_mse: 0.0110 - val_loss: 0.0246 - val_last_time_step_mse: 0.0111 . model.evaluate(X_valid, Y_valid) . 2000/2000 [==============================] - 2s 777us/sample - loss: 0.0246 - last_time_step_mse: 0.0111 . [0.024557730346918105, 0.011060879] . plot_learning_curves(history.history[&quot;loss&quot;], history.history[&quot;val_loss&quot;]) plt.show() . np.random.seed(43) series = generate_time_series(1, 50 + 10) X_new, Y_new = series[:, :50, :], series[:, 50:, :] Y_pred = model.predict(X_new)[:, -1][..., np.newaxis] . plot_multiple_forecasts(X_new, Y_new, Y_pred) plt.show() . Using One-Dimensional Convolutional Layers to Process Sequences . 1D conv layer with kernel size 4, stride 2, VALID padding: |--2--| |--5...| |--23-| |--1--| |--4--| ... |--22-| |--0-| |--3--| |...|--21-| X: 0 1 2 3 4 5 6 7 8 9 10 11 12 ... 42 43 44 45 46 47 48 49 Y: 1 2 3 4 5 6 7 8 9 10 11 12 13 ... 43 44 45 46 47 48 49 50 /10 11 12 13 14 15 16 17 18 19 20 21 22 ... 52 53 54 55 56 57 58 59 Output: X: 0/3 2/5 4/7 6/9 8/11 10/13 .../43 42/45 44/47 46/49 Y: 4/13 6/15 8/17 10/19 12/21 14/23 .../53 46/55 48/57 50/59 . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential([ keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=&quot;valid&quot;, input_shape=[None, 1]), keras.layers.GRU(20, return_sequences=True), keras.layers.GRU(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10)) ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;, metrics=[last_time_step_mse]) history = model.fit(X_train, Y_train[:, 3::2], epochs=20, validation_data=(X_valid, Y_valid[:, 3::2])) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0681 - last_time_step_mse: 0.0608 - val_loss: 0.0477 - val_last_time_step_mse: 0.0397 Epoch 2/20 7000/7000 [==============================] - 7s 962us/sample - loss: 0.0411 - last_time_step_mse: 0.0338 - val_loss: 0.0367 - val_last_time_step_mse: 0.0287 Epoch 3/20 7000/7000 [==============================] - 7s 966us/sample - loss: 0.0339 - last_time_step_mse: 0.0259 - val_loss: 0.0310 - val_last_time_step_mse: 0.0217 Epoch 4/20 7000/7000 [==============================] - 7s 941us/sample - loss: 0.0284 - last_time_step_mse: 0.0186 - val_loss: 0.0260 - val_last_time_step_mse: 0.0151 Epoch 5/20 7000/7000 [==============================] - 7s 943us/sample - loss: 0.0248 - last_time_step_mse: 0.0140 - val_loss: 0.0243 - val_last_time_step_mse: 0.0137 Epoch 6/20 7000/7000 [==============================] - 6s 928us/sample - loss: 0.0234 - last_time_step_mse: 0.0126 - val_loss: 0.0228 - val_last_time_step_mse: 0.0118 Epoch 7/20 7000/7000 [==============================] - 7s 939us/sample - loss: 0.0227 - last_time_step_mse: 0.0119 - val_loss: 0.0222 - val_last_time_step_mse: 0.0112 Epoch 8/20 7000/7000 [==============================] - 7s 974us/sample - loss: 0.0221 - last_time_step_mse: 0.0112 - val_loss: 0.0217 - val_last_time_step_mse: 0.0109 Epoch 9/20 7000/7000 [==============================] - 7s 946us/sample - loss: 0.0217 - last_time_step_mse: 0.0109 - val_loss: 0.0215 - val_last_time_step_mse: 0.0108 Epoch 10/20 7000/7000 [==============================] - 6s 925us/sample - loss: 0.0212 - last_time_step_mse: 0.0105 - val_loss: 0.0210 - val_last_time_step_mse: 0.0102 Epoch 11/20 7000/7000 [==============================] - 7s 934us/sample - loss: 0.0210 - last_time_step_mse: 0.0105 - val_loss: 0.0208 - val_last_time_step_mse: 0.0098 Epoch 12/20 7000/7000 [==============================] - 7s 973us/sample - loss: 0.0207 - last_time_step_mse: 0.0101 - val_loss: 0.0206 - val_last_time_step_mse: 0.0100 Epoch 13/20 7000/7000 [==============================] - 7s 949us/sample - loss: 0.0204 - last_time_step_mse: 0.0099 - val_loss: 0.0205 - val_last_time_step_mse: 0.0099 Epoch 14/20 7000/7000 [==============================] - 7s 962us/sample - loss: 0.0202 - last_time_step_mse: 0.0098 - val_loss: 0.0200 - val_last_time_step_mse: 0.0094 Epoch 15/20 7000/7000 [==============================] - 7s 930us/sample - loss: 0.0200 - last_time_step_mse: 0.0096 - val_loss: 0.0200 - val_last_time_step_mse: 0.0094 Epoch 16/20 7000/7000 [==============================] - 7s 949us/sample - loss: 0.0197 - last_time_step_mse: 0.0094 - val_loss: 0.0200 - val_last_time_step_mse: 0.0094 Epoch 17/20 7000/7000 [==============================] - 7s 932us/sample - loss: 0.0196 - last_time_step_mse: 0.0093 - val_loss: 0.0195 - val_last_time_step_mse: 0.0093 Epoch 18/20 7000/7000 [==============================] - 7s 951us/sample - loss: 0.0193 - last_time_step_mse: 0.0090 - val_loss: 0.0195 - val_last_time_step_mse: 0.0090 Epoch 19/20 7000/7000 [==============================] - 7s 968us/sample - loss: 0.0190 - last_time_step_mse: 0.0088 - val_loss: 0.0188 - val_last_time_step_mse: 0.0084 Epoch 20/20 7000/7000 [==============================] - 7s 930us/sample - loss: 0.0187 - last_time_step_mse: 0.0085 - val_loss: 0.0186 - val_last_time_step_mse: 0.0081 . WaveNet . C2 / / / / / / / / / / / / / .../ / / / / / / / / / / / / / / / / / / C1 / / / / / / / / / / / / /.../ / / / / / / X: 0 1 2 3 4 5 6 7 8 9 10 11 12 ... 43 44 45 46 47 48 49 Y: 1 2 3 4 5 6 7 8 9 10 11 12 13 ... 44 45 46 47 48 49 50 /10 11 12 13 14 15 16 17 18 19 20 21 22 ... 53 54 55 56 57 58 59 . np.random.seed(42) tf.random.set_seed(42) model = keras.models.Sequential() model.add(keras.layers.InputLayer(input_shape=[None, 1])) for rate in (1, 2, 4, 8) * 2: model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=&quot;causal&quot;, activation=&quot;relu&quot;, dilation_rate=rate)) model.add(keras.layers.Conv1D(filters=10, kernel_size=1)) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;, metrics=[last_time_step_mse]) history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/20 7000/7000 [==============================] - 3s 451us/sample - loss: 0.0668 - last_time_step_mse: 0.0546 - val_loss: 0.0376 - val_last_time_step_mse: 0.0249 Epoch 2/20 7000/7000 [==============================] - 2s 302us/sample - loss: 0.0325 - last_time_step_mse: 0.0202 - val_loss: 0.0296 - val_last_time_step_mse: 0.0175 Epoch 3/20 7000/7000 [==============================] - 2s 286us/sample - loss: 0.0284 - last_time_step_mse: 0.0170 - val_loss: 0.0269 - val_last_time_step_mse: 0.0153 Epoch 4/20 7000/7000 [==============================] - 2s 305us/sample - loss: 0.0264 - last_time_step_mse: 0.0149 - val_loss: 0.0256 - val_last_time_step_mse: 0.0138 Epoch 5/20 7000/7000 [==============================] - 2s 303us/sample - loss: 0.0251 - last_time_step_mse: 0.0135 - val_loss: 0.0244 - val_last_time_step_mse: 0.0128 Epoch 6/20 7000/7000 [==============================] - 2s 301us/sample - loss: 0.0244 - last_time_step_mse: 0.0129 - val_loss: 0.0238 - val_last_time_step_mse: 0.0122 Epoch 7/20 7000/7000 [==============================] - 2s 309us/sample - loss: 0.0238 - last_time_step_mse: 0.0124 - val_loss: 0.0233 - val_last_time_step_mse: 0.0117 Epoch 8/20 7000/7000 [==============================] - 2s 307us/sample - loss: 0.0231 - last_time_step_mse: 0.0116 - val_loss: 0.0227 - val_last_time_step_mse: 0.0113 Epoch 9/20 7000/7000 [==============================] - 2s 326us/sample - loss: 0.0227 - last_time_step_mse: 0.0113 - val_loss: 0.0228 - val_last_time_step_mse: 0.0114 Epoch 10/20 7000/7000 [==============================] - 2s 349us/sample - loss: 0.0225 - last_time_step_mse: 0.0112 - val_loss: 0.0222 - val_last_time_step_mse: 0.0108 Epoch 11/20 7000/7000 [==============================] - 2s 317us/sample - loss: 0.0221 - last_time_step_mse: 0.0107 - val_loss: 0.0225 - val_last_time_step_mse: 0.0116 Epoch 12/20 7000/7000 [==============================] - 2s 306us/sample - loss: 0.0219 - last_time_step_mse: 0.0104 - val_loss: 0.0215 - val_last_time_step_mse: 0.0101 Epoch 13/20 7000/7000 [==============================] - 2s 296us/sample - loss: 0.0216 - last_time_step_mse: 0.0102 - val_loss: 0.0213 - val_last_time_step_mse: 0.0099 Epoch 14/20 7000/7000 [==============================] - 2s 306us/sample - loss: 0.0215 - last_time_step_mse: 0.0102 - val_loss: 0.0214 - val_last_time_step_mse: 0.0100 Epoch 15/20 7000/7000 [==============================] - 2s 320us/sample - loss: 0.0212 - last_time_step_mse: 0.0098 - val_loss: 0.0212 - val_last_time_step_mse: 0.0102 Epoch 16/20 7000/7000 [==============================] - 2s 300us/sample - loss: 0.0210 - last_time_step_mse: 0.0096 - val_loss: 0.0209 - val_last_time_step_mse: 0.0096 Epoch 17/20 7000/7000 [==============================] - 2s 298us/sample - loss: 0.0210 - last_time_step_mse: 0.0096 - val_loss: 0.0205 - val_last_time_step_mse: 0.0091 Epoch 18/20 7000/7000 [==============================] - 2s 299us/sample - loss: 0.0206 - last_time_step_mse: 0.0093 - val_loss: 0.0207 - val_last_time_step_mse: 0.0096 Epoch 19/20 7000/7000 [==============================] - 2s 303us/sample - loss: 0.0205 - last_time_step_mse: 0.0092 - val_loss: 0.0206 - val_last_time_step_mse: 0.0093 Epoch 20/20 7000/7000 [==============================] - 2s 330us/sample - loss: 0.0204 - last_time_step_mse: 0.0091 - val_loss: 0.0207 - val_last_time_step_mse: 0.0095 . Here is the original WaveNet defined in the paper: it uses Gated Activation Units instead of ReLU and parametrized skip connections, plus it pads with zeros on the left to avoid getting shorter and shorter sequences: . class GatedActivationUnit(keras.layers.Layer): def __init__(self, activation=&quot;tanh&quot;, **kwargs): super().__init__(**kwargs) self.activation = keras.activations.get(activation) def call(self, inputs): n_filters = inputs.shape[-1] // 2 linear_output = self.activation(inputs[..., :n_filters]) gate = keras.activations.sigmoid(inputs[..., n_filters:]) return self.activation(linear_output) * gate . def wavenet_residual_block(inputs, n_filters, dilation_rate): z = keras.layers.Conv1D(2 * n_filters, kernel_size=2, padding=&quot;causal&quot;, dilation_rate=dilation_rate)(inputs) z = GatedActivationUnit()(z) z = keras.layers.Conv1D(n_filters, kernel_size=1)(z) return keras.layers.Add()([z, inputs]), z . keras.backend.clear_session() np.random.seed(42) tf.random.set_seed(42) n_layers_per_block = 3 # 10 in the paper n_blocks = 1 # 3 in the paper n_filters = 32 # 128 in the paper n_outputs = 10 # 256 in the paper inputs = keras.layers.Input(shape=[None, 1]) z = keras.layers.Conv1D(n_filters, kernel_size=2, padding=&quot;causal&quot;)(inputs) skip_to_last = [] for dilation_rate in [2**i for i in range(n_layers_per_block)] * n_blocks: z, skip = wavenet_residual_block(z, n_filters, dilation_rate) skip_to_last.append(skip) z = keras.activations.relu(keras.layers.Add()(skip_to_last)) z = keras.layers.Conv1D(n_filters, kernel_size=1, activation=&quot;relu&quot;)(z) Y_proba = keras.layers.Conv1D(n_outputs, kernel_size=1, activation=&quot;softmax&quot;)(z) model = keras.models.Model(inputs=[inputs], outputs=[Y_proba]) . model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;, metrics=[last_time_step_mse]) history = model.fit(X_train, Y_train, epochs=2, validation_data=(X_valid, Y_valid)) . Train on 7000 samples, validate on 2000 samples Epoch 1/2 7000/7000 [==============================] - 4s 546us/sample - loss: 0.1300 - last_time_step_mse: 0.1261 - val_loss: 0.1230 - val_last_time_step_mse: 0.1200 Epoch 2/2 7000/7000 [==============================] - 3s 410us/sample - loss: 0.1222 - last_time_step_mse: 0.1178 - val_loss: 0.1217 - val_last_time_step_mse: 0.1189 . In this chapter we explored the fundamentals of RNNs and used them to process sequences (namely, time series). In the process we also looked at other ways to process sequences, including CNNs. In the next chapter we will use RNNs for Natural Language Processing, and we will learn more about RNNs (bidirectional RNNs, stateful vs stateless RNNs, Encoderâ€“Decoders, and Attention-augmented Encoder-Decoders). We will also look at the Transformer, an Attention-only architecture. . Exercise solutions . 1. to 6. . See Appendix A. . 7. Embedded Reber Grammars . First we need to build a function that generates strings based on a grammar. The grammar will be represented as a list of possible transitions for each state. A transition specifies the string to output (or a grammar to generate it) and the next state. . np.random.seed(42) default_reber_grammar = [ [(&quot;B&quot;, 1)], # (state 0) =B=&gt;(state 1) [(&quot;T&quot;, 2), (&quot;P&quot;, 3)], # (state 1) =T=&gt;(state 2) or =P=&gt;(state 3) [(&quot;S&quot;, 2), (&quot;X&quot;, 4)], # (state 2) =S=&gt;(state 2) or =X=&gt;(state 4) [(&quot;T&quot;, 3), (&quot;V&quot;, 5)], # and so on... [(&quot;X&quot;, 3), (&quot;S&quot;, 6)], [(&quot;P&quot;, 4), (&quot;V&quot;, 6)], [(&quot;E&quot;, None)]] # (state 6) =E=&gt;(terminal state) embedded_reber_grammar = [ [(&quot;B&quot;, 1)], [(&quot;T&quot;, 2), (&quot;P&quot;, 3)], [(default_reber_grammar, 4)], [(default_reber_grammar, 5)], [(&quot;T&quot;, 6)], [(&quot;P&quot;, 6)], [(&quot;E&quot;, None)]] def generate_string(grammar): state = 0 output = [] while state is not None: index = np.random.randint(len(grammar[state])) production, state = grammar[state][index] if isinstance(production, list): production = generate_string(grammar=production) output.append(production) return &quot;&quot;.join(output) . Let&#39;s generate a few strings based on the default Reber grammar: . for _ in range(25): print(generate_string(default_reber_grammar), end=&quot; &quot;) . BTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE . Looks good. Now let&#39;s generate a few strings based on the embedded Reber grammar: . for _ in range(25): print(generate_string(embedded_reber_grammar), end=&quot; &quot;) . BTBPVVETE BTBTSSSSSSSXXVVETE BPBTSSSXXTTTTVPSEPE BTBPTTVVETE BPBTXXTVVEPE BTBTXSETE BPBTSSSSSXXTTVPXVPXTTTVVEPE BPBTSSXXTVPSEPE BPBPTTTTTTTVPSEPE BTBTSXSETE BPBPTVPXVVEPE BPBPVVEPE BPBPTVVEPE BTBPTTVPXTTVPSETE BTBTSSXSETE BTBTXXTTVVETE BPBTSXSEPE BPBPTVPSEPE BTBPVVETE BPBTXXTTTVPXTVVEPE BPBPTTVPXTVVEPE BTBPVVETE BPBPTVPXVPXTVVEPE BTBPVVETE BPBTSXSEPE . Okay, now we need a function to generate strings that do not respect the grammar. We could generate a random string, but the task would be a bit too easy, so instead we will generate a string that respects the grammar, and we will corrupt it by changing just one character: . def generate_corrupted_string(grammar, chars=&quot;BEPSTVX&quot;): good_string = generate_string(grammar) index = np.random.randint(len(good_string)) good_char = good_string[index] bad_char = np.random.choice(sorted(set(chars) - set(good_char))) return good_string[:index] + bad_char + good_string[index + 1:] . Let&#39;s look at a few corrupted strings: . for _ in range(25): print(generate_corrupted_string(embedded_reber_grammar), end=&quot; &quot;) . BTTTXXVVETE BPBTXXSPXTVVEPE BTBTXSPTE BPTTSXXTVPXVVEPE PPBPVPSEPE BTBPTVETE BPTTSSSSSXSEPE BPBSVPSEPE BTBPVVESE BPBTXSEPS BEBTXSETE XPBTXXTVPSEPE BTBPVVEPE BTXPTVVETE BTBPVXETE BVBTXSETE BPTTXXVPXVPSEPE BTBPXVPSETE STBPTTVPXVPXTVPSETE BPBPTVPSESE BPBPVEEPE ETBTXSETE BTBTXSVTE BPBTXXVPSEPP BTBTXXVPSETS . To be continued... .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/15_processing_sequences_using_rnns_and_cnns",
            "relUrl": "/15_processing_sequences_using_rnns_and_cnns",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Natural Language Processing with RNNs and Attention",
            "content": "This notebook contains all the sample code in chapter 16. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20 and TensorFlow â‰¥2.0. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x !pip install -q -U tensorflow-addons IS_COLAB = True except Exception: IS_COLAB = False # TensorFlow â‰¥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; if not tf.test.is_gpu_available(): print(&quot;No GPU was detected. LSTMs and CNNs can be very slow without a GPU.&quot;) if IS_COLAB: print(&quot;Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.&quot;) # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) tf.random.set_seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;nlp&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . . Char-RNN . Splitting a sequence into batches of shuffled windows . For example, let&#39;s split the sequence 0 to 14 into windows of length 5, each shifted by 2 (e.g.,[0, 1, 2, 3, 4], [2, 3, 4, 5, 6], etc.), then shuffle them, and split them into inputs (the first 4 steps) and targets (the last 4 steps) (e.g., [2, 3, 4, 5, 6] would be split into [[2, 3, 4, 5], [3, 4, 5, 6]]), then create batches of 3 such input/target pairs: . np.random.seed(42) tf.random.set_seed(42) n_steps = 5 dataset = tf.data.Dataset.from_tensor_slices(tf.range(15)) dataset = dataset.window(n_steps, shift=2, drop_remainder=True) dataset = dataset.flat_map(lambda window: window.batch(n_steps)) dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:])) dataset = dataset.batch(3).prefetch(1) for index, (X_batch, Y_batch) in enumerate(dataset): print(&quot;_&quot; * 20, &quot;Batch&quot;, index, &quot; nX_batch&quot;) print(X_batch.numpy()) print(&quot;=&quot; * 5, &quot; nY_batch&quot;) print(Y_batch.numpy()) . ____________________ Batch 0 X_batch [[6 7 8 9] [2 3 4 5] [4 5 6 7]] ===== Y_batch [[ 7 8 9 10] [ 3 4 5 6] [ 5 6 7 8]] ____________________ Batch 1 X_batch [[ 0 1 2 3] [ 8 9 10 11] [10 11 12 13]] ===== Y_batch [[ 1 2 3 4] [ 9 10 11 12] [11 12 13 14]] . Loading the Data and Preparing the Dataset . shakespeare_url = &quot;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&quot; filepath = keras.utils.get_file(&quot;shakespeare.txt&quot;, shakespeare_url) with open(filepath) as f: shakespeare_text = f.read() . print(shakespeare_text[:148]) . First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? . &quot;&quot;.join(sorted(set(shakespeare_text.lower()))) . &#34; n !$&amp;&#39;,-.3:;?abcdefghijklmnopqrstuvwxyz&#34; . tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) tokenizer.fit_on_texts(shakespeare_text) . tokenizer.texts_to_sequences([&quot;First&quot;]) . [[20, 6, 9, 8, 3]] . tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]]) . [&#39;f i r s t&#39;] . max_id = len(tokenizer.word_index) # number of distinct characters dataset_size = tokenizer.document_count # total number of characters . [encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1 train_size = dataset_size * 90 // 100 dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size]) . n_steps = 100 window_length = n_steps + 1 # target = input shifted 1 character ahead dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True) . dataset = dataset.flat_map(lambda window: window.batch(window_length)) . np.random.seed(42) tf.random.set_seed(42) . batch_size = 32 dataset = dataset.shuffle(10000).batch(batch_size) dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:])) . dataset = dataset.map( lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)) . dataset = dataset.prefetch(1) . for X_batch, Y_batch in dataset.take(1): print(X_batch.shape, Y_batch.shape) . (32, 100, 39) (32, 100) . Creating and Training the Model . model = keras.models.Sequential([ keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], # no dropout in stateful RNN (https://github.com/ageron/handson-ml2/issues/32) # dropout=0.2, recurrent_dropout=0.2, ), keras.layers.GRU(128, return_sequences=True, # dropout=0.2, recurrent_dropout=0.2 ), keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=&quot;softmax&quot;)) ]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;adam&quot;) history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10) . Epoch 1/10 31370/31370 [==============================] - 6063s 193ms/step - loss: 1.7662 Epoch 2/10 31370/31370 [==============================] - 5744s 183ms/step - loss: 1.6649 Epoch 3/10 31370/31370 [==============================] - 5320s 170ms/step - loss: 1.6508 Epoch 4/10 31370/31370 [==============================] - 5318s 170ms/step - loss: 1.6400 Epoch 5/10 31370/31370 [==============================] - 5318s 170ms/step - loss: 1.6359 Epoch 6/10 31370/31370 [==============================] - 5316s 169ms/step - loss: 1.6344 Epoch 7/10 31370/31370 [==============================] - 5489s 175ms/step - loss: 1.6336 Epoch 8/10 31370/31370 [==============================] - 5638s 180ms/step - loss: 1.6277 Epoch 9/10 31370/31370 [==============================] - 5709s 182ms/step - loss: 1.6309 Epoch 10/10 31370/31370 [==============================] - 6107s 195ms/step - loss: 1.6317 . Using the Model to Generate Text . def preprocess(texts): X = np.array(tokenizer.texts_to_sequences(texts)) - 1 return tf.one_hot(X, max_id) . X_new = preprocess([&quot;How are yo&quot;]) Y_pred = model.predict_classes(X_new) tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char . &#39;u&#39; . tf.random.set_seed(42) tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy() . array([[0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 1, 0, 2, 1, 0, 1, 2, 1, 1, 1, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2]]) . def next_char(text, temperature=1): X_new = preprocess([text]) y_proba = model.predict(X_new)[0, -1:, :] rescaled_logits = tf.math.log(y_proba) / temperature char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1 return tokenizer.sequences_to_texts(char_id.numpy())[0] . tf.random.set_seed(42) next_char(&quot;How are yo&quot;, temperature=1) . &#39;u&#39; . def complete_text(text, n_chars=50, temperature=1): for _ in range(n_chars): text += next_char(text, temperature) return text . tf.random.set_seed(42) print(complete_text(&quot;t&quot;, temperature=0.2)) . the belly the great and who shall be the belly the . print(complete_text(&quot;t&quot;, temperature=1)) . thing? or why you gremio. who make which the first . print(complete_text(&quot;t&quot;, temperature=2)) . th no cce: yeolg-hormer firi. a play asks. fol rusb . Stateful RNN . tf.random.set_seed(42) . dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size]) dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True) dataset = dataset.flat_map(lambda window: window.batch(window_length)) dataset = dataset.repeat().batch(1) dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:])) dataset = dataset.map( lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)) dataset = dataset.prefetch(1) . batch_size = 32 encoded_parts = np.array_split(encoded[:train_size], batch_size) datasets = [] for encoded_part in encoded_parts: dataset = tf.data.Dataset.from_tensor_slices(encoded_part) dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True) dataset = dataset.flat_map(lambda window: window.batch(window_length)) datasets.append(dataset) dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows)) dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:])) dataset = dataset.map( lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)) dataset = dataset.prefetch(1) . model = keras.models.Sequential([ keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2, batch_input_shape=[batch_size, None, max_id]), keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2), keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=&quot;softmax&quot;)) ]) . class ResetStatesCallback(keras.callbacks.Callback): def on_epoch_begin(self, epoch, logs): self.model.reset_states() . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;adam&quot;) steps_per_epoch = train_size // batch_size // n_steps model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50, callbacks=[ResetStatesCallback()]) . Epoch 1/50 313/313 [==============================] - 101s 322ms/step - loss: 2.6180 Epoch 2/50 313/313 [==============================] - 98s 312ms/step - loss: 2.2312 Epoch 3/50 313/313 [==============================] - 96s 306ms/step - loss: 2.2992 Epoch 4/50 313/313 [==============================] - 96s 308ms/step - loss: 2.4599 Epoch 5/50 313/313 [==============================] - 97s 309ms/step - loss: 2.4062 Epoch 6/50 313/313 [==============================] - 97s 310ms/step - loss: 2.0630 Epoch 7/50 313/313 [==============================] - 97s 311ms/step - loss: 2.0933 Epoch 8/50 313/313 [==============================] - 97s 309ms/step - loss: 2.0784 Epoch 9/50 313/313 [==============================] - 95s 304ms/step - loss: 2.0101 Epoch 10/50 313/313 [==============================] - 95s 302ms/step - loss: 1.9146 Epoch 11/50 313/313 [==============================] - 97s 309ms/step - loss: 1.9204 Epoch 12/50 313/313 [==============================] - 95s 305ms/step - loss: 1.9049 Epoch 13/50 313/313 [==============================] - 92s 295ms/step - loss: 1.8894 Epoch 14/50 313/313 [==============================] - 93s 296ms/step - loss: 1.8397 Epoch 15/50 313/313 [==============================] - 93s 296ms/step - loss: 1.8147 Epoch 16/50 313/313 [==============================] - 92s 293ms/step - loss: 1.8147 Epoch 17/50 313/313 [==============================] - 92s 295ms/step - loss: 1.7741 Epoch 18/50 &lt;&lt;30 more lines&gt;&gt; 313/313 [==============================] - 93s 298ms/step - loss: 1.6102 Epoch 34/50 313/313 [==============================] - 93s 298ms/step - loss: 1.6063 Epoch 35/50 313/313 [==============================] - 96s 306ms/step - loss: 1.6022 Epoch 36/50 313/313 [==============================] - 91s 291ms/step - loss: 1.5984 Epoch 37/50 313/313 [==============================] - 91s 291ms/step - loss: 1.5964 Epoch 38/50 313/313 [==============================] - 92s 293ms/step - loss: 1.5924 Epoch 39/50 313/313 [==============================] - 97s 310ms/step - loss: 1.5903 Epoch 40/50 313/313 [==============================] - 93s 298ms/step - loss: 1.5882 Epoch 41/50 313/313 [==============================] - 95s 303ms/step - loss: 1.5867 Epoch 42/50 313/313 [==============================] - 92s 294ms/step - loss: 1.5826 Epoch 43/50 313/313 [==============================] - 92s 294ms/step - loss: 1.5817 Epoch 44/50 313/313 [==============================] - 92s 295ms/step - loss: 1.5796 Epoch 45/50 313/313 [==============================] - 92s 295ms/step - loss: 1.5765 Epoch 46/50 313/313 [==============================] - 92s 294ms/step - loss: 1.5741 Epoch 47/50 313/313 [==============================] - 92s 295ms/step - loss: 1.5733 Epoch 48/50 313/313 [==============================] - 92s 293ms/step - loss: 1.5706 Epoch 49/50 313/313 [==============================] - 90s 289ms/step - loss: 1.5703 Epoch 50/50 313/313 [==============================] - 90s 288ms/step - loss: 1.5666 . &lt;tensorflow.python.keras.callbacks.History at 0xd37b57908&gt; . To use the model with different batch sizes, we need to create a stateless copy. We can get rid of dropout since it is only used during training: . stateless_model = keras.models.Sequential([ keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]), keras.layers.GRU(128, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=&quot;softmax&quot;)) ]) . To set the weights, we first need to build the model (so the weights get created): . stateless_model.build(tf.TensorShape([None, None, max_id])) . stateless_model.set_weights(model.get_weights()) model = stateless_model . tf.random.set_seed(42) print(complete_text(&quot;t&quot;)) . torp: unto most breathe blood him sight, which rest . Sentiment Analysis . tf.random.set_seed(42) . You can load the IMDB dataset easily: . (X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data() . X_train[0][:10] . [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65] . word_index = keras.datasets.imdb.get_word_index() id_to_word = {id_ + 3: word for word, id_ in word_index.items()} for id_, token in enumerate((&quot;&lt;pad&gt;&quot;, &quot;&lt;sos&gt;&quot;, &quot;&lt;unk&gt;&quot;)): id_to_word[id_] = token &quot; &quot;.join([id_to_word[id_] for id_ in X_train[0][:10]]) . &#39;&lt;sos&gt; this film was just brilliant casting location scenery story&#39; . import tensorflow_datasets as tfds datasets, info = tfds.load(&quot;imdb_reviews&quot;, as_supervised=True, with_info=True) . datasets.keys() . dict_keys([&#39;test&#39;, &#39;train&#39;]) . train_size = info.splits[&quot;train&quot;].num_examples test_size = info.splits[&quot;test&quot;].num_examples . train_size, test_size . (25000, 25000) . for X_batch, y_batch in datasets[&quot;train&quot;].batch(2).take(1): for review, label in zip(X_batch.numpy(), y_batch.numpy()): print(&quot;Review:&quot;, review.decode(&quot;utf-8&quot;)[:200], &quot;...&quot;) print(&quot;Label:&quot;, label, &quot;= Positive&quot; if label else &quot;= Negative&quot;) print() . Review: This was soul-provoking! I am an Iranian, and living in th 21st century, I didn&#39;t know that such big tribes have been living in such conditions at the time of my grandfather!&lt;br /&gt;&lt;br /&gt;You see that t ... Label: 1 = Positive Review: A very close and sharp discription of the bubbling and dynamic emotional world of specialy one 18year old guy, that makes his first experiences in his gay love to an other boy, during an vacation with ... Label: 1 = Positive . def preprocess(X_batch, y_batch): X_batch = tf.strings.substr(X_batch, 0, 300) X_batch = tf.strings.regex_replace(X_batch, rb&quot;&lt;br s*/?&gt;&quot;, b&quot; &quot;) X_batch = tf.strings.regex_replace(X_batch, b&quot;[^a-zA-Z&#39;]&quot;, b&quot; &quot;) X_batch = tf.strings.split(X_batch) return X_batch.to_tensor(default_value=b&quot;&lt;pad&gt;&quot;), y_batch . preprocess(X_batch, y_batch) . (&lt;tf.Tensor: id=235, shape=(2, 57), dtype=string, numpy= array([[b&#39;This&#39;, b&#39;was&#39;, b&#39;soul&#39;, b&#39;provoking&#39;, b&#39;I&#39;, b&#39;am&#39;, b&#39;an&#39;, b&#39;Iranian&#39;, b&#39;and&#39;, b&#39;living&#39;, b&#39;in&#39;, b&#39;th&#39;, b&#39;st&#39;, b&#39;century&#39;, b&#39;I&#39;, b&#34;didn&#39;t&#34;, b&#39;know&#39;, b&#39;that&#39;, b&#39;such&#39;, b&#39;big&#39;, b&#39;tribes&#39;, b&#39;have&#39;, b&#39;been&#39;, b&#39;living&#39;, b&#39;in&#39;, b&#39;such&#39;, b&#39;conditions&#39;, b&#39;at&#39;, b&#39;the&#39;, b&#39;time&#39;, b&#39;of&#39;, b&#39;my&#39;, b&#39;grandfather&#39;, b&#39;You&#39;, b&#39;see&#39;, b&#39;that&#39;, b&#39;today&#39;, b&#39;or&#39;, b&#39;even&#39;, b&#39;in&#39;, b&#39;on&#39;, b&#39;one&#39;, b&#39;side&#39;, b&#39;of&#39;, b&#39;the&#39;, b&#39;world&#39;, b&#39;a&#39;, b&#39;lady&#39;, b&#39;or&#39;, b&#39;a&#39;, b&#39;baby&#39;, b&#39;could&#39;, b&#39;have&#39;, b&#39;everything&#39;, b&#39;served&#39;, b&#39;for&#39;, b&#39;hi&#39;], [b&#39;A&#39;, b&#39;very&#39;, b&#39;close&#39;, b&#39;and&#39;, b&#39;sharp&#39;, b&#39;discription&#39;, b&#39;of&#39;, b&#39;the&#39;, b&#39;bubbling&#39;, b&#39;and&#39;, b&#39;dynamic&#39;, b&#39;emotional&#39;, b&#39;world&#39;, b&#39;of&#39;, b&#39;specialy&#39;, b&#39;one&#39;, b&#39;year&#39;, b&#39;old&#39;, b&#39;guy&#39;, b&#39;that&#39;, b&#39;makes&#39;, b&#39;his&#39;, b&#39;first&#39;, b&#39;experiences&#39;, b&#39;in&#39;, b&#39;his&#39;, b&#39;gay&#39;, b&#39;love&#39;, b&#39;to&#39;, b&#39;an&#39;, b&#39;other&#39;, b&#39;boy&#39;, b&#39;during&#39;, b&#39;an&#39;, b&#39;vacation&#39;, b&#39;with&#39;, b&#39;a&#39;, b&#39;part&#39;, b&#39;of&#39;, b&#39;his&#39;, b&#39;family&#39;, b&#39;I&#39;, b&#39;liked&#39;, b&#39;this&#39;, b&#39;film&#39;, b&#39;because&#39;, b&#39;of&#39;, b&#39;his&#39;, b&#39;extremly&#39;, b&#39;clear&#39;, b&#39;and&#39;, b&#39;surrogated&#39;, b&#39;sto&#39;, b&#39;&lt;pad&gt;&#39;, b&#39;&lt;pad&gt;&#39;, b&#39;&lt;pad&gt;&#39;, b&#39;&lt;pad&gt;&#39;]], dtype=object)&gt;, &lt;tf.Tensor: id=128, shape=(2,), dtype=int64, numpy=array([1, 1])&gt;) . from collections import Counter vocabulary = Counter() for X_batch, y_batch in datasets[&quot;train&quot;].batch(32).map(preprocess): for review in X_batch: vocabulary.update(list(review.numpy())) . vocabulary.most_common()[:3] . [(b&#39;&lt;pad&gt;&#39;, 214077), (b&#39;the&#39;, 61137), (b&#39;a&#39;, 38564)] . len(vocabulary) . 53893 . vocab_size = 10000 truncated_vocabulary = [ word for word, count in vocabulary.most_common()[:vocab_size]] . word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)} for word in b&quot;This movie was faaaaaantastic&quot;.split(): print(word_to_id.get(word) or vocab_size) . 22 12 11 10000 . words = tf.constant(truncated_vocabulary) word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64) vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids) num_oov_buckets = 1000 table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets) . table.lookup(tf.constant([b&quot;This movie was faaaaaantastic&quot;.split()])) . &lt;tf.Tensor: id=126936, shape=(1, 4), dtype=int64, numpy=array([[ 22, 12, 11, 10053]])&gt; . def encode_words(X_batch, y_batch): return table.lookup(X_batch), y_batch train_set = datasets[&quot;train&quot;].repeat().batch(32).map(preprocess) train_set = train_set.map(encode_words).prefetch(1) . for X_batch, y_batch in train_set.take(1): print(X_batch) print(y_batch) . tf.Tensor( [[ 6 98 9 ... 0 0 0] [ 185 2 10865 ... 0 0 0] [ 719 2410 5630 ... 0 0 0] ... [ 6 94 13 ... 0 0 0] [ 14 498 16 ... 0 0 0] [ 168 1 1633 ... 0 0 0]], shape=(32, 64), dtype=int64) tf.Tensor([1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0], shape=(32,), dtype=int64) . embed_size = 128 model = keras.models.Sequential([ keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, mask_zero=True, # not shown in the book input_shape=[None]), keras.layers.GRU(128, return_sequences=True), keras.layers.GRU(128), keras.layers.Dense(1, activation=&quot;sigmoid&quot;) ]) model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5) . Epoch 1/5 781/781 [==============================] - 102s 131ms/step - loss: 0.5378 - accuracy: 0.7238 Epoch 2/5 781/781 [==============================] - 87s 111ms/step - loss: 0.3485 - accuracy: 0.8567 Epoch 3/5 781/781 [==============================] - 87s 111ms/step - loss: 0.1877 - accuracy: 0.9332 Epoch 4/5 781/781 [==============================] - 87s 111ms/step - loss: 0.1236 - accuracy: 0.9573 Epoch 5/5 781/781 [==============================] - 87s 111ms/step - loss: 0.0964 - accuracy: 0.9667 . Or using manual masking: . K = keras.backend embed_size = 128 inputs = keras.layers.Input(shape=[None]) mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs) z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs) z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask) z = keras.layers.GRU(128)(z, mask=mask) outputs = keras.layers.Dense(1, activation=&quot;sigmoid&quot;)(z) model = keras.models.Model(inputs=[inputs], outputs=[outputs]) model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5) . Epoch 1/5 781/781 [==============================] - 102s 131ms/step - loss: 0.5436 - accuracy: 0.7172 Epoch 2/5 781/781 [==============================] - 88s 113ms/step - loss: 0.3519 - accuracy: 0.8564 Epoch 3/5 781/781 [==============================] - 87s 111ms/step - loss: 0.1950 - accuracy: 0.9306 Epoch 4/5 781/781 [==============================] - 87s 111ms/step - loss: 0.1226 - accuracy: 0.9579 Epoch 5/5 781/781 [==============================] - 86s 110ms/step - loss: 0.0922 - accuracy: 0.9679 . Reusing Pretrained Embeddings . tf.random.set_seed(42) . TFHUB_CACHE_DIR = os.path.join(os.curdir, &quot;my_tfhub_cache&quot;) os.environ[&quot;TFHUB_CACHE_DIR&quot;] = TFHUB_CACHE_DIR . import tensorflow_hub as hub model = keras.Sequential([ hub.KerasLayer(&quot;https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1&quot;, dtype=tf.string, input_shape=[], output_shape=[50]), keras.layers.Dense(128, activation=&quot;relu&quot;), keras.layers.Dense(1, activation=&quot;sigmoid&quot;) ]) model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) . for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR): for filename in filenames: print(os.path.join(dirpath, filename)) . ./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt ./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/saved_model.pb ./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.data-00000-of-00001 ./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.index ./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/assets/tokens.txt . import tensorflow_datasets as tfds datasets, info = tfds.load(&quot;imdb_reviews&quot;, as_supervised=True, with_info=True) train_size = info.splits[&quot;train&quot;].num_examples batch_size = 32 train_set = datasets[&quot;train&quot;].repeat().batch(batch_size).prefetch(1) history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5) . Epoch 1/5 781/781 [==============================] - 119s 152ms/step - loss: 0.5499 - accuracy: 0.7230 Epoch 2/5 781/781 [==============================] - 119s 152ms/step - loss: 0.5133 - accuracy: 0.7486 Epoch 3/5 781/781 [==============================] - 117s 150ms/step - loss: 0.5078 - accuracy: 0.7518 Epoch 4/5 781/781 [==============================] - 118s 151ms/step - loss: 0.5042 - accuracy: 0.7540 Epoch 5/5 781/781 [==============================] - 122s 156ms/step - loss: 0.5010 - accuracy: 0.7574 . Automatic Translation . tf.random.set_seed(42) . vocab_size = 100 embed_size = 10 . import tensorflow_addons as tfa encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32) decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32) sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32) embeddings = keras.layers.Embedding(vocab_size, embed_size) encoder_embeddings = embeddings(encoder_inputs) decoder_embeddings = embeddings(decoder_inputs) encoder = keras.layers.LSTM(512, return_state=True) encoder_outputs, state_h, state_c = encoder(encoder_embeddings) encoder_state = [state_h, state_c] sampler = tfa.seq2seq.sampler.TrainingSampler() decoder_cell = keras.layers.LSTMCell(512) output_layer = keras.layers.Dense(vocab_size) decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer) final_outputs, final_state, final_sequence_lengths = decoder( decoder_embeddings, initial_state=encoder_state, sequence_length=sequence_lengths) Y_proba = tf.nn.softmax(final_outputs.rnn_output) model = keras.models.Model( inputs=[encoder_inputs, decoder_inputs, sequence_lengths], outputs=[Y_proba]) . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;adam&quot;) . X = np.random.randint(100, size=10*1000).reshape(1000, 10) Y = np.random.randint(100, size=15*1000).reshape(1000, 15) X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]] seq_lengths = np.full([1000], 15) history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2) . Epoch 1/2 1000/1000 [==============================] - 6s 6ms/sample - loss: 4.6054 Epoch 2/2 1000/1000 [==============================] - 5s 5ms/sample - loss: 4.6041 . Bidirectional Recurrent Layers . model = keras.models.Sequential([ keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]), keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True)) ]) model.summary() . Model: &#34;sequential_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= gru_4 (GRU) (None, None, 10) 660 _________________________________________________________________ bidirectional (Bidirectional (None, None, 20) 1320 ================================================================= Total params: 1,980 Trainable params: 1,980 Non-trainable params: 0 _________________________________________________________________ . Positional Encoding . class PositionalEncoding(keras.layers.Layer): def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs): super().__init__(dtype=dtype, **kwargs) if max_dims % 2 == 1: max_dims += 1 # max_dims must be even p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2)) pos_emb = np.empty((1, max_steps, max_dims)) pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T self.positional_embedding = tf.constant(pos_emb.astype(self.dtype)) def call(self, inputs): shape = tf.shape(inputs) return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]] . max_steps = 201 max_dims = 512 pos_emb = PositionalEncoding(max_steps, max_dims) PE = pos_emb(np.zeros((1, max_steps, max_dims), np.float32))[0].numpy() . i1, i2, crop_i = 100, 101, 150 p1, p2, p3 = 22, 60, 35 fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5)) ax1.plot([p1, p1], [-1, 1], &quot;k--&quot;, label=&quot;$p = {}$&quot;.format(p1)) ax1.plot([p2, p2], [-1, 1], &quot;k--&quot;, label=&quot;$p = {}$&quot;.format(p2), alpha=0.5) ax1.plot(p3, PE[p3, i1], &quot;bx&quot;, label=&quot;$p = {}$&quot;.format(p3)) ax1.plot(PE[:,i1], &quot;b-&quot;, label=&quot;$i = {}$&quot;.format(i1)) ax1.plot(PE[:,i2], &quot;r-&quot;, label=&quot;$i = {}$&quot;.format(i2)) ax1.plot([p1, p2], [PE[p1, i1], PE[p2, i1]], &quot;bo&quot;) ax1.plot([p1, p2], [PE[p1, i2], PE[p2, i2]], &quot;ro&quot;) ax1.legend(loc=&quot;center right&quot;, fontsize=14, framealpha=0.95) ax1.set_ylabel(&quot;$P_{(p,i)}$&quot;, rotation=0, fontsize=16) ax1.grid(True, alpha=0.3) ax1.hlines(0, 0, max_steps - 1, color=&quot;k&quot;, linewidth=1, alpha=0.3) ax1.axis([0, max_steps - 1, -1, 1]) ax2.imshow(PE.T[:crop_i], cmap=&quot;gray&quot;, interpolation=&quot;bilinear&quot;, aspect=&quot;auto&quot;) ax2.hlines(i1, 0, max_steps - 1, color=&quot;b&quot;) cheat = 2 # need to raise the red line a bit, or else it hides the blue one ax2.hlines(i2+cheat, 0, max_steps - 1, color=&quot;r&quot;) ax2.plot([p1, p1], [0, crop_i], &quot;k--&quot;) ax2.plot([p2, p2], [0, crop_i], &quot;k--&quot;, alpha=0.5) ax2.plot([p1, p2], [i2+cheat, i2+cheat], &quot;ro&quot;) ax2.plot([p1, p2], [i1, i1], &quot;bo&quot;) ax2.axis([0, max_steps - 1, 0, crop_i]) ax2.set_xlabel(&quot;$p$&quot;, fontsize=16) ax2.set_ylabel(&quot;$i$&quot;, rotation=0, fontsize=16) plt.savefig(&quot;positional_embedding_plot&quot;) plt.show() . embed_size = 512; max_steps = 500; vocab_size = 10000 encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32) decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32) embeddings = keras.layers.Embedding(vocab_size, embed_size) encoder_embeddings = embeddings(encoder_inputs) decoder_embeddings = embeddings(decoder_inputs) positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size) encoder_in = positional_encoding(encoder_embeddings) decoder_in = positional_encoding(decoder_embeddings) . Here is a (very) simplified Transformer (the actual architecture has skip connections, layer norm, dense nets, and most importantly it uses Multi-Head Attention instead of regular Attention): . Z = encoder_in for N in range(6): Z = keras.layers.Attention(use_scale=True)([Z, Z]) encoder_outputs = Z Z = decoder_in for N in range(6): Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z]) Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs]) outputs = keras.layers.TimeDistributed( keras.layers.Dense(vocab_size, activation=&quot;softmax&quot;))(Z) . Here&#39;s a basic implementation of the MultiHeadAttention layer. One will likely be added to keras.layers in the near future. Note that Conv1D layers with kernel_size=1 (and the default padding=&quot;valid&quot; and strides=1) is equivalent to a TimeDistributed(Dense(...)) layer. . K = keras.backend class MultiHeadAttention(keras.layers.Layer): def __init__(self, n_heads, causal=False, use_scale=False, **kwargs): self.n_heads = n_heads self.causal = causal self.use_scale = use_scale super().__init__(**kwargs) def build(self, batch_input_shape): self.dims = batch_input_shape[0][-1] self.q_dims, self.v_dims, self.k_dims = [self.dims // self.n_heads] * 3 # could be hyperparameters instead self.q_linear = keras.layers.Conv1D(self.n_heads * self.q_dims, kernel_size=1, use_bias=False) self.v_linear = keras.layers.Conv1D(self.n_heads * self.v_dims, kernel_size=1, use_bias=False) self.k_linear = keras.layers.Conv1D(self.n_heads * self.k_dims, kernel_size=1, use_bias=False) self.attention = keras.layers.Attention(causal=self.causal, use_scale=self.use_scale) self.out_linear = keras.layers.Conv1D(self.dims, kernel_size=1, use_bias=False) super().build(batch_input_shape) def _multi_head_linear(self, inputs, linear): shape = K.concatenate([K.shape(inputs)[:-1], [self.n_heads, -1]]) projected = K.reshape(linear(inputs), shape) perm = K.permute_dimensions(projected, [0, 2, 1, 3]) return K.reshape(perm, [shape[0] * self.n_heads, shape[1], -1]) def call(self, inputs): q = inputs[0] v = inputs[1] k = inputs[2] if len(inputs) &gt; 2 else v shape = K.shape(q) q_proj = self._multi_head_linear(q, self.q_linear) v_proj = self._multi_head_linear(v, self.v_linear) k_proj = self._multi_head_linear(k, self.k_linear) multi_attended = self.attention([q_proj, v_proj, k_proj]) shape_attended = K.shape(multi_attended) reshaped_attended = K.reshape(multi_attended, [shape[0], self.n_heads, shape_attended[1], shape_attended[2]]) perm = K.permute_dimensions(reshaped_attended, [0, 2, 1, 3]) concat = K.reshape(perm, [shape[0], shape_attended[1], -1]) return self.out_linear(concat) . Q = np.random.rand(2, 50, 512) V = np.random.rand(2, 80, 512) multi_attn = MultiHeadAttention(8) multi_attn([Q, V]).shape . TensorShape([2, 50, 512]) .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/16_nlp_with_rnns_and_attention",
            "relUrl": "/16_nlp_with_rnns_and_attention",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Introduction to Artificial Neural Networks with Keras",
            "content": "Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20 and TensorFlow â‰¥2.0. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass # TensorFlow â‰¥2.0 is required import tensorflow as tf assert tf.__version__ &gt;= &quot;2.0&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;ann&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) # Ignore useless warnings (see SciPy issue #5998) import warnings warnings.filterwarnings(action=&quot;ignore&quot;, message=&quot;^internal gelsd&quot;) . . Perceptrons . Note: we set max_iter and tol explicitly to avoid warnings about the fact that their default value will change in future versions of Scikit-Learn. . import numpy as np from sklearn.datasets import load_iris from sklearn.linear_model import Perceptron iris = load_iris() X = iris.data[:, (2, 3)] # petal length, petal width y = (iris.target == 0).astype(np.int) per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42) per_clf.fit(X, y) y_pred = per_clf.predict([[2, 0.5]]) . y_pred . array([1]) . #collapse-show a = -per_clf.coef_[0][0] / per_clf.coef_[0][1] b = -per_clf.intercept_ / per_clf.coef_[0][1] axes = [0, 5, 0, 2] x0, x1 = np.meshgrid( np.linspace(axes[0], axes[1], 500).reshape(-1, 1), np.linspace(axes[2], axes[3], 200).reshape(-1, 1), ) X_new = np.c_[x0.ravel(), x1.ravel()] y_predict = per_clf.predict(X_new) zz = y_predict.reshape(x0.shape) plt.figure(figsize=(10, 4)) plt.plot(X[y==0, 0], X[y==0, 1], &quot;bs&quot;, label=&quot;Not Iris-Setosa&quot;) plt.plot(X[y==1, 0], X[y==1, 1], &quot;yo&quot;, label=&quot;Iris-Setosa&quot;) plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], &quot;k-&quot;, linewidth=3) from matplotlib.colors import ListedColormap custom_cmap = ListedColormap([&#39;#9898ff&#39;, &#39;#fafab0&#39;]) plt.contourf(x0, x1, zz, cmap=custom_cmap) plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) plt.legend(loc=&quot;lower right&quot;, fontsize=14) plt.axis(axes) save_fig(&quot;perceptron_iris_plot&quot;) plt.show() . . Saving figure perceptron_iris_plot . Activation functions . def sigmoid(z): return 1 / (1 + np.exp(-z)) def relu(z): return np.maximum(0, z) def derivative(f, z, eps=0.000001): return (f(z + eps) - f(z - eps))/(2 * eps) . #collapse-show z = np.linspace(-5, 5, 200) plt.figure(figsize=(11,4)) plt.subplot(121) plt.plot(z, np.sign(z), &quot;r-&quot;, linewidth=1, label=&quot;Step&quot;) plt.plot(z, sigmoid(z), &quot;g--&quot;, linewidth=2, label=&quot;Sigmoid&quot;) plt.plot(z, np.tanh(z), &quot;b-&quot;, linewidth=2, label=&quot;Tanh&quot;) plt.plot(z, relu(z), &quot;m-.&quot;, linewidth=2, label=&quot;ReLU&quot;) plt.grid(True) plt.legend(loc=&quot;center right&quot;, fontsize=14) plt.title(&quot;Activation functions&quot;, fontsize=14) plt.axis([-5, 5, -1.2, 1.2]) plt.subplot(122) plt.plot(z, derivative(np.sign, z), &quot;r-&quot;, linewidth=1, label=&quot;Step&quot;) plt.plot(0, 0, &quot;ro&quot;, markersize=5) plt.plot(0, 0, &quot;rx&quot;, markersize=10) plt.plot(z, derivative(sigmoid, z), &quot;g--&quot;, linewidth=2, label=&quot;Sigmoid&quot;) plt.plot(z, derivative(np.tanh, z), &quot;b-&quot;, linewidth=2, label=&quot;Tanh&quot;) plt.plot(z, derivative(relu, z), &quot;m-.&quot;, linewidth=2, label=&quot;ReLU&quot;) plt.grid(True) #plt.legend(loc=&quot;center right&quot;, fontsize=14) plt.title(&quot;Derivatives&quot;, fontsize=14) plt.axis([-5, 5, -0.2, 1.2]) save_fig(&quot;activation_functions_plot&quot;) plt.show() . . Saving figure activation_functions_plot . def heaviside(z): return (z &gt;= 0).astype(z.dtype) def mlp_xor(x1, x2, activation=heaviside): return activation(-activation(x1 + x2 - 1.5) + activation(x1 + x2 - 0.5) - 0.5) . #collapse-show x1s = np.linspace(-0.2, 1.2, 100) x2s = np.linspace(-0.2, 1.2, 100) x1, x2 = np.meshgrid(x1s, x2s) z1 = mlp_xor(x1, x2, activation=heaviside) z2 = mlp_xor(x1, x2, activation=sigmoid) plt.figure(figsize=(10,4)) plt.subplot(121) plt.contourf(x1, x2, z1) plt.plot([0, 1], [0, 1], &quot;gs&quot;, markersize=20) plt.plot([0, 1], [1, 0], &quot;y^&quot;, markersize=20) plt.title(&quot;Activation function: heaviside&quot;, fontsize=14) plt.grid(True) plt.subplot(122) plt.contourf(x1, x2, z2) plt.plot([0, 1], [0, 1], &quot;gs&quot;, markersize=20) plt.plot([0, 1], [1, 0], &quot;y^&quot;, markersize=20) plt.title(&quot;Activation function: sigmoid&quot;, fontsize=14) plt.grid(True) . . Building an Image Classifier . First let&#39;s import TensorFlow and Keras. . import tensorflow as tf from tensorflow import keras . tf.__version__ . &#39;2.1.0&#39; . keras.__version__ . &#39;2.2.4-tf&#39; . Let&#39;s start by loading the fashion MNIST dataset. Keras has a number of functions to load popular datasets in keras.datasets. The dataset is already split for you between a training set and a test set, but it can be useful to split the training set further to have a validation set: . fashion_mnist = keras.datasets.fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data() . The training set contains 60,000 grayscale images, each 28x28 pixels: . X_train_full.shape . (60000, 28, 28) . Each pixel intensity is represented as a byte (0 to 255): . X_train_full.dtype . dtype(&#39;uint8&#39;) . Let&#39;s split the full training set into a validation set and a (smaller) training set. We also scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255. . X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255. y_valid, y_train = y_train_full[:5000], y_train_full[5000:] X_test = X_test / 255. . You can plot an image using Matplotlib&#39;s imshow() function, with a &#39;binary&#39; color map: . plt.imshow(X_train[0], cmap=&quot;binary&quot;) plt.axis(&#39;off&#39;) plt.show() . The labels are the class IDs (represented as uint8), from 0 to 9: . y_train . array([4, 0, 7, ..., 3, 0, 5], dtype=uint8) . Here are the corresponding class names: . class_names = [&quot;T-shirt/top&quot;, &quot;Trouser&quot;, &quot;Pullover&quot;, &quot;Dress&quot;, &quot;Coat&quot;, &quot;Sandal&quot;, &quot;Shirt&quot;, &quot;Sneaker&quot;, &quot;Bag&quot;, &quot;Ankle boot&quot;] . So the first image in the training set is a coat: . class_names[y_train[0]] . &#39;Coat&#39; . The validation set contains 5,000 images, and the test set contains 10,000 images: . X_valid.shape . (5000, 28, 28) . X_test.shape . (10000, 28, 28) . Let&#39;s take a look at a sample of the images in the dataset: . n_rows = 4 n_cols = 10 plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2)) for row in range(n_rows): for col in range(n_cols): index = n_cols * row + col plt.subplot(n_rows, n_cols, index + 1) plt.imshow(X_train[index], cmap=&quot;binary&quot;, interpolation=&quot;nearest&quot;) plt.axis(&#39;off&#39;) plt.title(class_names[y_train[index]], fontsize=12) plt.subplots_adjust(wspace=0.2, hspace=0.5) save_fig(&#39;fashion_mnist_plot&#39;, tight_layout=False) plt.show() . Saving figure fashion_mnist_plot . model = keras.models.Sequential() model.add(keras.layers.Flatten(input_shape=[28, 28])) model.add(keras.layers.Dense(300, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(100, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;)) . keras.backend.clear_session() np.random.seed(42) tf.random.set_seed(42) . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;relu&quot;), keras.layers.Dense(100, activation=&quot;relu&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) . model.layers . [&lt;tensorflow.python.keras.layers.core.Flatten at 0x7ff370af5780&gt;, &lt;tensorflow.python.keras.layers.core.Dense at 0x7ff370af5c88&gt;, &lt;tensorflow.python.keras.layers.core.Dense at 0x7ff330ab36d8&gt;, &lt;tensorflow.python.keras.layers.core.Dense at 0x7ff330ab3828&gt;] . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 300) 235500 _________________________________________________________________ dense_1 (Dense) (None, 100) 30100 _________________________________________________________________ dense_2 (Dense) (None, 10) 1010 ================================================================= Total params: 266,610 Trainable params: 266,610 Non-trainable params: 0 _________________________________________________________________ . keras.utils.plot_model(model, &quot;my_fashion_mnist_model.png&quot;, show_shapes=True) . hidden1 = model.layers[1] hidden1.name . &#39;dense&#39; . model.get_layer(hidden1.name) is hidden1 . True . weights, biases = hidden1.get_weights() . weights . array([[ 0.02448617, -0.00877795, -0.02189048, ..., -0.02766046, 0.03859074, -0.06889391], [ 0.00476504, -0.03105379, -0.0586676 , ..., 0.00602964, -0.02763776, -0.04165364], [-0.06189284, -0.06901957, 0.07102345, ..., -0.04238207, 0.07121518, -0.07331658], ..., [-0.03048757, 0.02155137, -0.05400612, ..., -0.00113463, 0.00228987, 0.05581069], [ 0.07061854, -0.06960931, 0.07038955, ..., -0.00384101, 0.00034875, 0.02878492], [-0.06022581, 0.01577859, -0.02585464, ..., -0.00527829, 0.00272203, -0.06793761]], dtype=float32) . weights.shape . (784, 300) . biases . array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32) . biases.shape . (300,) . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;sgd&quot;, metrics=[&quot;accuracy&quot;]) . This is equivalent to: . model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=keras.optimizers.SGD(), metrics=[keras.metrics.sparse_categorical_accuracy]) . history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid)) . Train on 55000 samples, validate on 5000 samples Epoch 1/30 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7226 - accuracy: 0.7641 - val_loss: 0.5073 - val_accuracy: 0.8320 Epoch 2/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.4844 - accuracy: 0.8321 - val_loss: 0.4541 - val_accuracy: 0.8478 Epoch 3/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.4414 - accuracy: 0.8464 - val_loss: 0.4373 - val_accuracy: 0.8508 Epoch 4/30 55000/55000 [==============================] - 2s 40us/sample - loss: 0.4129 - accuracy: 0.8549 - val_loss: 0.4170 - val_accuracy: 0.8562 Epoch 5/30 55000/55000 [==============================] - 2s 38us/sample - loss: 0.3927 - accuracy: 0.8616 - val_loss: 0.3825 - val_accuracy: 0.8646 Epoch 6/30 55000/55000 [==============================] - 2s 38us/sample - loss: 0.3772 - accuracy: 0.8665 - val_loss: 0.3736 - val_accuracy: 0.8680 Epoch 7/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.3630 - accuracy: 0.8726 - val_loss: 0.3713 - val_accuracy: 0.8698 Epoch 8/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.3523 - accuracy: 0.8746 - val_loss: 0.3657 - val_accuracy: 0.8710 Epoch 9/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.3424 - accuracy: 0.8776 - val_loss: 0.3442 - val_accuracy: 0.8784 Epoch 10/30 55000/55000 [==============================] - 2s 38us/sample - loss: 0.3329 - accuracy: 0.8809 - val_loss: 0.3523 - val_accuracy: 0.8774 Epoch 11/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.3243 - accuracy: 0.8835 - val_loss: 0.3363 - val_accuracy: 0.8820 Epoch 12/30 55000/55000 [==============================] - 2s 40us/sample - loss: 0.3163 - accuracy: 0.8868 - val_loss: 0.3313 - val_accuracy: 0.8842 Epoch 13/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.3075 - accuracy: 0.8898 - val_loss: 0.3320 - val_accuracy: 0.8814 Epoch 14/30 55000/55000 [==============================] - 2s 38us/sample - loss: 0.3019 - accuracy: 0.8921 - val_loss: 0.3237 - val_accuracy: 0.8874 Epoch 15/30 55000/55000 [==============================] - 2s 38us/sample - loss: 0.2956 - accuracy: 0.8938 - val_loss: 0.3173 - val_accuracy: 0.8898 Epoch 16/30 55000/55000 [==============================] - 2s 38us/sample - loss: 0.2899 - accuracy: 0.8963 - val_loss: 0.3247 - val_accuracy: 0.8872 Epoch 17/30 55000/55000 [==============================] - 2s 38us/sample - loss: 0.2836 - accuracy: 0.8985 - val_loss: 0.3175 - val_accuracy: 0.8920 Epoch 18/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.2783 - accuracy: 0.8996 - val_loss: 0.3095 - val_accuracy: 0.8910 Epoch 19/30 55000/55000 [==============================] - 2s 38us/sample - loss: 0.2730 - accuracy: 0.9021 - val_loss: 0.3185 - val_accuracy: 0.8856 Epoch 20/30 55000/55000 [==============================] - 2s 37us/sample - loss: 0.2681 - accuracy: 0.9042 - val_loss: 0.3208 - val_accuracy: 0.8850 Epoch 21/30 55000/55000 [==============================] - 2s 43us/sample - loss: 0.2635 - accuracy: 0.9047 - val_loss: 0.3005 - val_accuracy: 0.8946 Epoch 22/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.2576 - accuracy: 0.9077 - val_loss: 0.3107 - val_accuracy: 0.8878 Epoch 23/30 55000/55000 [==============================] - 2s 43us/sample - loss: 0.2540 - accuracy: 0.9082 - val_loss: 0.3020 - val_accuracy: 0.8896 Epoch 24/30 55000/55000 [==============================] - 2s 38us/sample - loss: 0.2492 - accuracy: 0.9102 - val_loss: 0.3105 - val_accuracy: 0.8850 Epoch 25/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.2453 - accuracy: 0.9126 - val_loss: 0.3100 - val_accuracy: 0.8906 Epoch 26/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.2408 - accuracy: 0.9145 - val_loss: 0.3278 - val_accuracy: 0.8846 Epoch 27/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.2367 - accuracy: 0.9155 - val_loss: 0.3130 - val_accuracy: 0.8856 Epoch 28/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.2323 - accuracy: 0.9178 - val_loss: 0.2954 - val_accuracy: 0.8926 Epoch 29/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.2287 - accuracy: 0.9184 - val_loss: 0.2998 - val_accuracy: 0.8924 Epoch 30/30 55000/55000 [==============================] - 2s 39us/sample - loss: 0.2256 - accuracy: 0.9195 - val_loss: 0.3049 - val_accuracy: 0.8882 . history.params . {&#39;batch_size&#39;: 32, &#39;epochs&#39;: 30, &#39;steps&#39;: 1719, &#39;samples&#39;: 55000, &#39;verbose&#39;: 0, &#39;do_validation&#39;: True, &#39;metrics&#39;: [&#39;loss&#39;, &#39;accuracy&#39;, &#39;val_loss&#39;, &#39;val_accuracy&#39;]} . print(history.epoch) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] . history.history.keys() . dict_keys([&#39;loss&#39;, &#39;accuracy&#39;, &#39;val_loss&#39;, &#39;val_accuracy&#39;]) . import pandas as pd pd.DataFrame(history.history).plot(figsize=(8, 5)) plt.grid(True) plt.gca().set_ylim(0, 1) save_fig(&quot;keras_learning_curves_plot&quot;) plt.show() . Saving figure keras_learning_curves_plot . model.evaluate(X_test, y_test) . 10000/10000 [==============================] - 0s 21us/sample - loss: 0.3378 - accuracy: 0.8781 . [0.33780701770782473, 0.8781] . X_new = X_test[:3] y_proba = model.predict(X_new) y_proba.round(2) . array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99], [0. , 0. , 0.99, 0. , 0.01, 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]], dtype=float32) . y_pred = model.predict_classes(X_new) y_pred . array([9, 2, 1]) . np.array(class_names)[y_pred] . array([&#39;Ankle boot&#39;, &#39;Pullover&#39;, &#39;Trouser&#39;], dtype=&#39;&lt;U11&#39;) . y_new = y_test[:3] y_new . array([9, 2, 1], dtype=uint8) . plt.figure(figsize=(7.2, 2.4)) for index, image in enumerate(X_new): plt.subplot(1, 3, index + 1) plt.imshow(image, cmap=&quot;binary&quot;, interpolation=&quot;nearest&quot;) plt.axis(&#39;off&#39;) plt.title(class_names[y_test[index]], fontsize=12) plt.subplots_adjust(wspace=0.2, hspace=0.5) save_fig(&#39;fashion_mnist_images_plot&#39;, tight_layout=False) plt.show() . Saving figure fashion_mnist_images_plot . Regression MLP . Let&#39;s load, split and scale the California housing dataset (the original one, not the modified one as in chapter 2): . from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler housing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42) X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_valid = scaler.transform(X_valid) X_test = scaler.transform(X_test) . np.random.seed(42) tf.random.set_seed(42) . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;relu&quot;, input_shape=X_train.shape[1:]), keras.layers.Dense(1) ]) model.compile(loss=&quot;mean_squared_error&quot;, optimizer=keras.optimizers.SGD(lr=1e-3)) history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)) mse_test = model.evaluate(X_test, y_test) X_new = X_test[:3] y_pred = model.predict(X_new) . Train on 11610 samples, validate on 3870 samples Epoch 1/20 11610/11610 [==============================] - 1s 44us/sample - loss: 1.6205 - val_loss: 2.0374 Epoch 2/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.7162 - val_loss: 0.6571 Epoch 3/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6356 - val_loss: 0.5996 Epoch 4/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5989 - val_loss: 0.5662 Epoch 5/20 11610/11610 [==============================] - 0s 28us/sample - loss: 0.5713 - val_loss: 0.5489 Epoch 6/20 11610/11610 [==============================] - 0s 28us/sample - loss: 0.5491 - val_loss: 0.5204 Epoch 7/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5301 - val_loss: 0.5018 Epoch 8/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.5142 - val_loss: 0.4815 Epoch 9/20 11610/11610 [==============================] - 0s 27us/sample - loss: 0.5004 - val_loss: 0.4695 Epoch 10/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4883 - val_loss: 0.4605 Epoch 11/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4786 - val_loss: 0.4495 Epoch 12/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4697 - val_loss: 0.4382 Epoch 13/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4621 - val_loss: 0.4309 Epoch 14/20 11610/11610 [==============================] - 0s 27us/sample - loss: 0.4556 - val_loss: 0.4247 Epoch 15/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4497 - val_loss: 0.4200 Epoch 16/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4443 - val_loss: 0.4149 Epoch 17/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4397 - val_loss: 0.4108 Epoch 18/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4354 - val_loss: 0.4059 Epoch 19/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4315 - val_loss: 0.4003 Epoch 20/20 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4281 - val_loss: 0.3981 5160/5160 [==============================] - 0s 15us/sample - loss: 0.4218 . plt.plot(pd.DataFrame(history.history)) plt.grid(True) plt.gca().set_ylim(0, 1) plt.show() . y_pred . array([[0.37310064], [1.6790789 ], [3.0817137 ]], dtype=float32) . Functional API . Not all neural network models are simply sequential. Some may have complex topologies. Some may have multiple inputs and/or multiple outputs. For example, a Wide &amp; Deep neural network (see paper) connects all or part of the inputs directly to the output layer. . np.random.seed(42) tf.random.set_seed(42) . input_ = keras.layers.Input(shape=X_train.shape[1:]) hidden1 = keras.layers.Dense(30, activation=&quot;relu&quot;)(input_) hidden2 = keras.layers.Dense(30, activation=&quot;relu&quot;)(hidden1) concat = keras.layers.concatenate([input_, hidden2]) output = keras.layers.Dense(1)(concat) model = keras.models.Model(inputs=[input_], outputs=[output]) . model.summary() . Model: &#34;model&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 8)] 0 __________________________________________________________________________________________________ dense_5 (Dense) (None, 30) 270 input_1[0][0] __________________________________________________________________________________________________ dense_6 (Dense) (None, 30) 930 dense_5[0][0] __________________________________________________________________________________________________ concatenate (Concatenate) (None, 38) 0 input_1[0][0] dense_6[0][0] __________________________________________________________________________________________________ dense_7 (Dense) (None, 1) 39 concatenate[0][0] ================================================================================================== Total params: 1,239 Trainable params: 1,239 Non-trainable params: 0 __________________________________________________________________________________________________ . model.compile(loss=&quot;mean_squared_error&quot;, optimizer=keras.optimizers.SGD(lr=1e-3)) history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)) mse_test = model.evaluate(X_test, y_test) y_pred = model.predict(X_new) . Train on 11610 samples, validate on 3870 samples Epoch 1/20 11610/11610 [==============================] - 1s 47us/sample - loss: 1.2390 - val_loss: 0.6566 Epoch 2/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6312 - val_loss: 0.6734 Epoch 3/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5886 - val_loss: 0.5574 Epoch 4/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5595 - val_loss: 0.5235 Epoch 5/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5361 - val_loss: 0.5011 Epoch 6/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5178 - val_loss: 0.5065 Epoch 7/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5016 - val_loss: 0.4699 Epoch 8/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4888 - val_loss: 0.4745 Epoch 9/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4772 - val_loss: 0.4425 Epoch 10/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4673 - val_loss: 0.4384 Epoch 11/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4586 - val_loss: 0.4533 Epoch 12/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4504 - val_loss: 0.4179 Epoch 13/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4435 - val_loss: 0.4137 Epoch 14/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4376 - val_loss: 0.4062 Epoch 15/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4318 - val_loss: 0.4541 Epoch 16/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4266 - val_loss: 0.3952 Epoch 17/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4221 - val_loss: 0.3910 Epoch 18/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4173 - val_loss: 0.4205 Epoch 19/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4132 - val_loss: 0.3830 Epoch 20/20 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4096 - val_loss: 0.3923 5160/5160 [==============================] - 0s 15us/sample - loss: 0.4042 . What if you want to send different subsets of input features through the wide or deep paths? We will send 5 features (features 0 to 4), and 6 through the deep path (features 2 to 7). Note that 3 features will go through both (features 2, 3 and 4). . np.random.seed(42) tf.random.set_seed(42) . input_A = keras.layers.Input(shape=[5], name=&quot;wide_input&quot;) input_B = keras.layers.Input(shape=[6], name=&quot;deep_input&quot;) hidden1 = keras.layers.Dense(30, activation=&quot;relu&quot;)(input_B) hidden2 = keras.layers.Dense(30, activation=&quot;relu&quot;)(hidden1) concat = keras.layers.concatenate([input_A, hidden2]) output = keras.layers.Dense(1, name=&quot;output&quot;)(concat) model = keras.models.Model(inputs=[input_A, input_B], outputs=[output]) . model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(lr=1e-3)) X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:] X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:] X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:] X_new_A, X_new_B = X_test_A[:3], X_test_B[:3] history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid)) mse_test = model.evaluate((X_test_A, X_test_B), y_test) y_pred = model.predict((X_new_A, X_new_B)) . Train on 11610 samples, validate on 3870 samples Epoch 1/20 11610/11610 [==============================] - 1s 50us/sample - loss: 1.8127 - val_loss: 2.1165 Epoch 2/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.6852 - val_loss: 0.6178 Epoch 3/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5965 - val_loss: 0.5600 Epoch 4/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5587 - val_loss: 0.5269 Epoch 5/20 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5321 - val_loss: 0.5185 Epoch 6/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5129 - val_loss: 0.4803 Epoch 7/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4959 - val_loss: 0.4689 Epoch 8/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4837 - val_loss: 0.4498 Epoch 9/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4734 - val_loss: 0.4387 Epoch 10/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4646 - val_loss: 0.4306 Epoch 11/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4571 - val_loss: 0.4262 Epoch 12/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4507 - val_loss: 0.4173 Epoch 13/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4456 - val_loss: 0.4124 Epoch 14/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4422 - val_loss: 0.4084 Epoch 15/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4386 - val_loss: 0.4351 Epoch 16/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4361 - val_loss: 0.4017 Epoch 17/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4326 - val_loss: 0.3990 Epoch 18/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4296 - val_loss: 0.4148 Epoch 19/20 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4278 - val_loss: 0.3957 Epoch 20/20 11610/11610 [==============================] - 0s 33us/sample - loss: 0.4259 - val_loss: 0.3976 5160/5160 [==============================] - 0s 16us/sample - loss: 0.4202 . Adding an auxiliary output for regularization: . np.random.seed(42) tf.random.set_seed(42) . input_A = keras.layers.Input(shape=[5], name=&quot;wide_input&quot;) input_B = keras.layers.Input(shape=[6], name=&quot;deep_input&quot;) hidden1 = keras.layers.Dense(30, activation=&quot;relu&quot;)(input_B) hidden2 = keras.layers.Dense(30, activation=&quot;relu&quot;)(hidden1) concat = keras.layers.concatenate([input_A, hidden2]) output = keras.layers.Dense(1, name=&quot;main_output&quot;)(concat) aux_output = keras.layers.Dense(1, name=&quot;aux_output&quot;)(hidden2) model = keras.models.Model(inputs=[input_A, input_B], outputs=[output, aux_output]) . model.compile(loss=[&quot;mse&quot;, &quot;mse&quot;], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3)) . history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20, validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid])) . Train on 11610 samples, validate on 3870 samples Epoch 1/20 11610/11610 [==============================] - 1s 68us/sample - loss: 2.1346 - main_output_loss: 1.9194 - aux_output_loss: 4.0632 - val_loss: 2.9120 - val_main_output_loss: 2.2555 - val_aux_output_loss: 8.8088 Epoch 2/20 11610/11610 [==============================] - 0s 37us/sample - loss: 0.8954 - main_output_loss: 0.7048 - aux_output_loss: 2.6119 - val_loss: 1.4135 - val_main_output_loss: 0.6348 - val_aux_output_loss: 8.4172 Epoch 3/20 11610/11610 [==============================] - 0s 37us/sample - loss: 0.7400 - main_output_loss: 0.6077 - aux_output_loss: 1.9305 - val_loss: 1.3594 - val_main_output_loss: 0.5885 - val_aux_output_loss: 8.2925 Epoch 4/20 11610/11610 [==============================] - 0s 37us/sample - loss: 0.6749 - main_output_loss: 0.5690 - aux_output_loss: 1.6264 - val_loss: 1.2789 - val_main_output_loss: 0.5611 - val_aux_output_loss: 7.7340 Epoch 5/20 11610/11610 [==============================] - 0s 37us/sample - loss: 0.6351 - main_output_loss: 0.5420 - aux_output_loss: 1.4729 - val_loss: 1.1841 - val_main_output_loss: 0.5656 - val_aux_output_loss: 6.7464 Epoch 6/20 11610/11610 [==============================] - 0s 37us/sample - loss: 0.6068 - main_output_loss: 0.5213 - aux_output_loss: 1.3763 - val_loss: 1.0614 - val_main_output_loss: 0.5202 - val_aux_output_loss: 5.9282 Epoch 7/20 11610/11610 [==============================] - 0s 37us/sample - loss: 0.5832 - main_output_loss: 0.5028 - aux_output_loss: 1.3072 - val_loss: 0.9555 - val_main_output_loss: 0.5111 - val_aux_output_loss: 4.9515 Epoch 8/20 11610/11610 [==============================] - 0s 38us/sample - loss: 0.5651 - main_output_loss: 0.4892 - aux_output_loss: 1.2463 - val_loss: 0.8426 - val_main_output_loss: 0.4713 - val_aux_output_loss: 4.1805 Epoch 9/20 11610/11610 [==============================] - 0s 36us/sample - loss: 0.5497 - main_output_loss: 0.4772 - aux_output_loss: 1.2017 - val_loss: 0.7632 - val_main_output_loss: 0.4633 - val_aux_output_loss: 3.4589 Epoch 10/20 11610/11610 [==============================] - 0s 38us/sample - loss: 0.5368 - main_output_loss: 0.4671 - aux_output_loss: 1.1631 - val_loss: 0.6954 - val_main_output_loss: 0.4467 - val_aux_output_loss: 2.9307 Epoch 11/20 11610/11610 [==============================] - 0s 36us/sample - loss: 0.5257 - main_output_loss: 0.4586 - aux_output_loss: 1.1297 - val_loss: 0.6413 - val_main_output_loss: 0.4292 - val_aux_output_loss: 2.5478 Epoch 12/20 11610/11610 [==============================] - 0s 36us/sample - loss: 0.5162 - main_output_loss: 0.4516 - aux_output_loss: 1.0989 - val_loss: 0.5950 - val_main_output_loss: 0.4232 - val_aux_output_loss: 2.1396 Epoch 13/20 11610/11610 [==============================] - 0s 36us/sample - loss: 0.5082 - main_output_loss: 0.4454 - aux_output_loss: 1.0729 - val_loss: 0.5608 - val_main_output_loss: 0.4203 - val_aux_output_loss: 1.8236 Epoch 14/20 11610/11610 [==============================] - 0s 37us/sample - loss: 0.5022 - main_output_loss: 0.4416 - aux_output_loss: 1.0487 - val_loss: 0.5324 - val_main_output_loss: 0.4142 - val_aux_output_loss: 1.5944 Epoch 15/20 11610/11610 [==============================] - 0s 37us/sample - loss: 0.4964 - main_output_loss: 0.4376 - aux_output_loss: 1.0240 - val_loss: 0.5206 - val_main_output_loss: 0.4118 - val_aux_output_loss: 1.4985 Epoch 16/20 11610/11610 [==============================] - 0s 37us/sample - loss: 0.4914 - main_output_loss: 0.4344 - aux_output_loss: 1.0054 - val_loss: 0.4939 - val_main_output_loss: 0.4017 - val_aux_output_loss: 1.3221 Epoch 17/20 11610/11610 [==============================] - 0s 37us/sample - loss: 0.4865 - main_output_loss: 0.4312 - aux_output_loss: 0.9841 - val_loss: 0.4803 - val_main_output_loss: 0.3993 - val_aux_output_loss: 1.2082 Epoch 18/20 11610/11610 [==============================] - 0s 38us/sample - loss: 0.4820 - main_output_loss: 0.4281 - aux_output_loss: 0.9663 - val_loss: 0.4820 - val_main_output_loss: 0.4053 - val_aux_output_loss: 1.1710 Epoch 19/20 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4786 - main_output_loss: 0.4264 - aux_output_loss: 0.9490 - val_loss: 0.4661 - val_main_output_loss: 0.3979 - val_aux_output_loss: 1.0790 Epoch 20/20 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4753 - main_output_loss: 0.4245 - aux_output_loss: 0.9320 - val_loss: 0.4598 - val_main_output_loss: 0.3968 - val_aux_output_loss: 1.0257 . total_loss, main_loss, aux_loss = model.evaluate( [X_test_A, X_test_B], [y_test, y_test]) y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B]) . 5160/5160 [==============================] - 0s 19us/sample - loss: 0.4656 - main_output_loss: 0.4165 - aux_output_loss: 0.9111 . The subclassing API . class WideAndDeepModel(keras.models.Model): def __init__(self, units=30, activation=&quot;relu&quot;, **kwargs): super().__init__(**kwargs) self.hidden1 = keras.layers.Dense(units, activation=activation) self.hidden2 = keras.layers.Dense(units, activation=activation) self.main_output = keras.layers.Dense(1) self.aux_output = keras.layers.Dense(1) def call(self, inputs): input_A, input_B = inputs hidden1 = self.hidden1(input_B) hidden2 = self.hidden2(hidden1) concat = keras.layers.concatenate([input_A, hidden2]) main_output = self.main_output(concat) aux_output = self.aux_output(hidden2) return main_output, aux_output model = WideAndDeepModel(30, activation=&quot;relu&quot;) . model.compile(loss=&quot;mse&quot;, loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3)) history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10, validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid))) total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test)) y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B)) . Train on 11610 samples, validate on 3870 samples Epoch 1/10 11610/11610 [==============================] - 1s 78us/sample - loss: 2.2719 - output_1_loss: 2.1554 - output_2_loss: 3.3117 - val_loss: 4.3377 - val_output_1_loss: 2.7732 - val_output_2_loss: 18.3999 Epoch 2/10 11610/11610 [==============================] - 0s 36us/sample - loss: 0.9891 - output_1_loss: 0.8653 - output_2_loss: 2.1062 - val_loss: 2.0073 - val_output_1_loss: 0.7581 - val_output_2_loss: 13.2427 Epoch 3/10 11610/11610 [==============================] - 0s 36us/sample - loss: 0.8320 - output_1_loss: 0.7303 - output_2_loss: 1.7468 - val_loss: 1.7215 - val_output_1_loss: 0.7114 - val_output_2_loss: 10.8051 Epoch 4/10 11610/11610 [==============================] - 0s 36us/sample - loss: 0.7658 - output_1_loss: 0.6760 - output_2_loss: 1.5726 - val_loss: 1.4708 - val_output_1_loss: 0.6454 - val_output_2_loss: 8.8938 Epoch 5/10 11610/11610 [==============================] - 0s 36us/sample - loss: 0.7223 - output_1_loss: 0.6394 - output_2_loss: 1.4683 - val_loss: 1.3057 - val_output_1_loss: 0.6852 - val_output_2_loss: 6.8846 Epoch 6/10 11610/11610 [==============================] - 0s 36us/sample - loss: 0.6909 - output_1_loss: 0.6132 - output_2_loss: 1.3901 - val_loss: 1.1005 - val_output_1_loss: 0.5915 - val_output_2_loss: 5.6773 Epoch 7/10 11610/11610 [==============================] - 0s 37us/sample - loss: 0.6636 - output_1_loss: 0.5894 - output_2_loss: 1.3330 - val_loss: 0.9605 - val_output_1_loss: 0.5611 - val_output_2_loss: 4.5516 Epoch 8/10 11610/11610 [==============================] - 0s 37us/sample - loss: 0.6406 - output_1_loss: 0.5691 - output_2_loss: 1.2833 - val_loss: 0.8480 - val_output_1_loss: 0.5263 - val_output_2_loss: 3.7399 Epoch 9/10 11610/11610 [==============================] - 0s 36us/sample - loss: 0.6199 - output_1_loss: 0.5507 - output_2_loss: 1.2416 - val_loss: 0.7650 - val_output_1_loss: 0.5085 - val_output_2_loss: 3.0718 Epoch 10/10 11610/11610 [==============================] - 0s 37us/sample - loss: 0.6019 - output_1_loss: 0.5351 - output_2_loss: 1.2026 - val_loss: 0.7004 - val_output_1_loss: 0.4916 - val_output_2_loss: 2.5772 5160/5160 [==============================] - 0s 18us/sample - loss: 0.5819 - output_1_loss: 0.5174 - output_2_loss: 1.1749 . model = WideAndDeepModel(30, activation=&quot;relu&quot;) . Saving and Restoring . np.random.seed(42) tf.random.set_seed(42) . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;relu&quot;, input_shape=[8]), keras.layers.Dense(30, activation=&quot;relu&quot;), keras.layers.Dense(1) ]) . model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(lr=1e-3)) history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) mse_test = model.evaluate(X_test, y_test) . Train on 11610 samples, validate on 3870 samples Epoch 1/10 11610/11610 [==============================] - 1s 46us/sample - loss: 1.8423 - val_loss: 5.2165 Epoch 2/10 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6876 - val_loss: 0.7732 Epoch 3/10 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5954 - val_loss: 0.5446 Epoch 4/10 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5553 - val_loss: 0.5425 Epoch 5/10 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5268 - val_loss: 0.5539 Epoch 6/10 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5049 - val_loss: 0.4701 Epoch 7/10 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4852 - val_loss: 0.4562 Epoch 8/10 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4706 - val_loss: 0.4452 Epoch 9/10 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4576 - val_loss: 0.4406 Epoch 10/10 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4476 - val_loss: 0.4185 5160/5160 [==============================] - 0s 15us/sample - loss: 0.4376 . model.save(&quot;my_keras_model.h5&quot;) . model = keras.models.load_model(&quot;my_keras_model.h5&quot;) . model.predict(X_new) . array([[0.551559 ], [1.6555369], [3.0014234]], dtype=float32) . model.save_weights(&quot;my_keras_weights.ckpt&quot;) . model.load_weights(&quot;my_keras_weights.ckpt&quot;) . &lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff383d586d8&gt; . Using Callbacks during Training . keras.backend.clear_session() np.random.seed(42) tf.random.set_seed(42) . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;relu&quot;, input_shape=[8]), keras.layers.Dense(30, activation=&quot;relu&quot;), keras.layers.Dense(1) ]) . model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(lr=1e-3)) checkpoint_cb = keras.callbacks.ModelCheckpoint(&quot;my_keras_model.h5&quot;, save_best_only=True) history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb]) model = keras.models.load_model(&quot;my_keras_model.h5&quot;) # rollback to best model mse_test = model.evaluate(X_test, y_test) . Train on 11610 samples, validate on 3870 samples Epoch 1/10 11610/11610 [==============================] - 1s 47us/sample - loss: 1.8423 - val_loss: 5.2165 Epoch 2/10 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6876 - val_loss: 0.7732 Epoch 3/10 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5954 - val_loss: 0.5446 Epoch 4/10 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5553 - val_loss: 0.5425 Epoch 5/10 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5268 - val_loss: 0.5539 Epoch 6/10 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5049 - val_loss: 0.4701 Epoch 7/10 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4852 - val_loss: 0.4562 Epoch 8/10 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4706 - val_loss: 0.4452 Epoch 9/10 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4576 - val_loss: 0.4406 Epoch 10/10 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4476 - val_loss: 0.4185 5160/5160 [==============================] - 0s 22us/sample - loss: 0.4376 . model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(lr=1e-3)) early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb]) mse_test = model.evaluate(X_test, y_test) . Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 1s 47us/sample - loss: 0.4385 - val_loss: 0.4287 Epoch 2/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4319 - val_loss: 0.4117 Epoch 3/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4252 - val_loss: 0.3975 Epoch 4/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4199 - val_loss: 0.3943 Epoch 5/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4150 - val_loss: 0.3964 Epoch 6/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4110 - val_loss: 0.3907 Epoch 7/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4070 - val_loss: 0.3823 Epoch 8/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4036 - val_loss: 0.3786 Epoch 9/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4003 - val_loss: 0.3739 Epoch 10/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3973 - val_loss: 0.3724 Epoch 11/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3944 - val_loss: 0.3697 Epoch 12/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3916 - val_loss: 0.3670 Epoch 13/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3892 - val_loss: 0.3638 Epoch 14/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3869 - val_loss: 0.3633 Epoch 15/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3845 - val_loss: 0.4051 Epoch 16/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3826 - val_loss: 0.3662 Epoch 17/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3805 - val_loss: 0.3554 &lt;&lt;132 more lines&gt;&gt; Epoch 84/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3312 - val_loss: 0.3282 Epoch 85/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3307 - val_loss: 0.3379 Epoch 86/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3306 - val_loss: 0.3163 Epoch 87/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3302 - val_loss: 0.3377 Epoch 88/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3300 - val_loss: 0.3340 Epoch 89/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3295 - val_loss: 0.3158 Epoch 90/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3294 - val_loss: 0.3519 Epoch 91/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3289 - val_loss: 0.3142 Epoch 92/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3286 - val_loss: 0.3619 Epoch 93/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3282 - val_loss: 0.3270 Epoch 94/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3282 - val_loss: 0.4632 Epoch 95/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3284 - val_loss: 0.3371 Epoch 96/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3275 - val_loss: 0.4659 Epoch 97/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3278 - val_loss: 0.3156 Epoch 98/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3265 - val_loss: 0.3259 Epoch 99/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3262 - val_loss: 0.3407 Epoch 100/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3264 - val_loss: 0.3176 5160/5160 [==============================] - 0s 15us/sample - loss: 0.3271 . class PrintValTrainRatioCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs): print(&quot; nval/train: {:.2f}&quot;.format(logs[&quot;val_loss&quot;] / logs[&quot;loss&quot;])) . val_train_ratio_cb = PrintValTrainRatioCallback() history = model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid), callbacks=[val_train_ratio_cb]) . Train on 11610 samples, validate on 3870 samples 10912/11610 [===========================&gt;..] - ETA: 0s - loss: 0.3231 val/train: 1.16 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3256 - val_loss: 0.3785 . TensorBoard . root_logdir = os.path.join(os.curdir, &quot;my_logs&quot;) . def get_run_logdir(): import time run_id = time.strftime(&quot;run_%Y_%m_%d-%H_%M_%S&quot;) return os.path.join(root_logdir, run_id) run_logdir = get_run_logdir() run_logdir . &#39;./my_logs/run_2020_01_27-10_15_44&#39; . keras.backend.clear_session() np.random.seed(42) tf.random.set_seed(42) . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;relu&quot;, input_shape=[8]), keras.layers.Dense(30, activation=&quot;relu&quot;), keras.layers.Dense(1) ]) model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(lr=1e-3)) . tensorboard_cb = keras.callbacks.TensorBoard(run_logdir) history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, tensorboard_cb]) . Train on 11610 samples, validate on 3870 samples Epoch 1/30 11610/11610 [==============================] - 1s 52us/sample - loss: 1.8423 - val_loss: 5.2165 Epoch 2/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.6876 - val_loss: 0.7732 Epoch 3/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.5954 - val_loss: 0.5446 Epoch 4/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.5553 - val_loss: 0.5425 Epoch 5/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.5268 - val_loss: 0.5539 Epoch 6/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.5049 - val_loss: 0.4701 Epoch 7/30 11610/11610 [==============================] - 0s 37us/sample - loss: 0.4852 - val_loss: 0.4562 Epoch 8/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.4706 - val_loss: 0.4452 Epoch 9/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4576 - val_loss: 0.4406 Epoch 10/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4476 - val_loss: 0.4185 Epoch 11/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4388 - val_loss: 0.4285 Epoch 12/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4313 - val_loss: 0.4071 Epoch 13/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4252 - val_loss: 0.3998 Epoch 14/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4198 - val_loss: 0.3970 Epoch 15/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4152 - val_loss: 0.4115 Epoch 16/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4109 - val_loss: 0.3849 Epoch 17/30 11610/11610 [==============================] - 0s 39us/sample - loss: 0.4073 - val_loss: 0.3862 Epoch 18/30 11610/11610 [==============================] - 0s 37us/sample - loss: 0.4037 - val_loss: 0.3907 Epoch 19/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4003 - val_loss: 0.3751 Epoch 20/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.3974 - val_loss: 0.3711 Epoch 21/30 11610/11610 [==============================] - 0s 37us/sample - loss: 0.3945 - val_loss: 0.3733 Epoch 22/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.3919 - val_loss: 0.3676 Epoch 23/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.3893 - val_loss: 0.3669 Epoch 24/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.3869 - val_loss: 0.3614 Epoch 25/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3846 - val_loss: 0.3600 Epoch 26/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.3825 - val_loss: 0.3578 Epoch 27/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.3802 - val_loss: 0.3676 Epoch 28/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.3781 - val_loss: 0.3545 Epoch 29/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3761 - val_loss: 0.3612 Epoch 30/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3742 - val_loss: 0.3555 . To start the TensorBoard server, one option is to open a terminal, if needed activate the virtualenv where you installed TensorBoard, go to this notebook&#39;s directory, then type: . $ tensorboard --logdir=./my_logs --port=6006 . You can then open your web browser to localhost:6006 and use TensorBoard. Once you are done, press Ctrl-C in the terminal window, this will shutdown the TensorBoard server. . Alternatively, you can load TensorBoard&#39;s Jupyter extension and run it like this: . %load_ext tensorboard %tensorboard --logdir=./my_logs --port=6006 . run_logdir2 = get_run_logdir() run_logdir2 . &#39;./my_logs/run_2020_01_27-10_18_25&#39; . keras.backend.clear_session() np.random.seed(42) tf.random.set_seed(42) . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;relu&quot;, input_shape=[8]), keras.layers.Dense(30, activation=&quot;relu&quot;), keras.layers.Dense(1) ]) model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(lr=0.05)) . tensorboard_cb = keras.callbacks.TensorBoard(run_logdir2) history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, tensorboard_cb]) . Train on 11610 samples, validate on 3870 samples Epoch 1/30 11610/11610 [==============================] - 1s 52us/sample - loss: 5.6341 - val_loss: 1.3205 Epoch 2/30 11610/11610 [==============================] - 0s 35us/sample - loss: 1.2704 - val_loss: 1.0757 Epoch 3/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.9370 - val_loss: 0.7769 Epoch 4/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.8854 - val_loss: 0.8254 Epoch 5/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.9501 - val_loss: 0.9415 Epoch 6/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.8283 - val_loss: 0.6111 Epoch 7/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.8095 - val_loss: 0.7394 Epoch 8/30 11610/11610 [==============================] - 0s 35us/sample - loss: 1.1857 - val_loss: 1.0356 Epoch 9/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.8920 - val_loss: 0.6564 Epoch 10/30 11610/11610 [==============================] - 0s 35us/sample - loss: 1.2295 - val_loss: 1.1277 Epoch 11/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.8992 - val_loss: 0.6219 Epoch 12/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.7697 - val_loss: 0.6959 Epoch 13/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.7375 - val_loss: 0.6471 Epoch 14/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.9825 - val_loss: 1.0278 Epoch 15/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.9653 - val_loss: 0.9092 Epoch 16/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.9008 - val_loss: 0.8451 Epoch 17/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.7838 - val_loss: 0.5818 Epoch 18/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.9960 - val_loss: 1.0598 Epoch 19/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.7887 - val_loss: 0.6405 Epoch 20/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.5706 - val_loss: 0.5093 Epoch 21/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.5192 - val_loss: 0.5457 Epoch 22/30 11610/11610 [==============================] - 0s 38us/sample - loss: 0.4971 - val_loss: 0.4644 Epoch 23/30 11610/11610 [==============================] - 0s 38us/sample - loss: 0.4815 - val_loss: 0.4120 Epoch 24/30 11610/11610 [==============================] - 0s 37us/sample - loss: 0.4678 - val_loss: 0.4853 Epoch 25/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4490 - val_loss: 0.3972 Epoch 26/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.4348 - val_loss: 0.4258 Epoch 27/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4377 - val_loss: 0.3829 Epoch 28/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.4353 - val_loss: 0.3860 Epoch 29/30 11610/11610 [==============================] - 0s 35us/sample - loss: 0.4209 - val_loss: 0.3763 Epoch 30/30 11610/11610 [==============================] - 0s 36us/sample - loss: 0.4441 - val_loss: 0.4186 . Notice how TensorBoard now sees two runs, and you can compare the learning curves. . Check out the other available logging options: . help(keras.callbacks.TensorBoard.__init__) . Help on function __init__ in module tensorflow.python.keras.callbacks: __init__(self, log_dir=&#39;logs&#39;, histogram_freq=0, write_graph=True, write_images=False, update_freq=&#39;epoch&#39;, profile_batch=2, embeddings_freq=0, embeddings_metadata=None, **kwargs) Initialize self. See help(type(self)) for accurate signature. . Hyperparameter Tuning . keras.backend.clear_session() np.random.seed(42) tf.random.set_seed(42) . def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]): model = keras.models.Sequential() model.add(keras.layers.InputLayer(input_shape=input_shape)) for layer in range(n_hidden): model.add(keras.layers.Dense(n_neurons, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(1)) optimizer = keras.optimizers.SGD(lr=learning_rate) model.compile(loss=&quot;mse&quot;, optimizer=optimizer) return model . keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model) . keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)]) . Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 43us/sample - loss: 1.1399 - val_loss: 24.4309 Epoch 2/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.7393 - val_loss: 3.2896 Epoch 3/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.5374 - val_loss: 0.6080 Epoch 4/100 11610/11610 [==============================] - 0s 28us/sample - loss: 0.4796 - val_loss: 0.4532 Epoch 5/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4536 - val_loss: 0.4221 Epoch 6/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4373 - val_loss: 0.4144 Epoch 7/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4256 - val_loss: 0.4025 Epoch 8/100 11610/11610 [==============================] - 0s 28us/sample - loss: 0.4178 - val_loss: 0.3938 Epoch 9/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4114 - val_loss: 0.4085 Epoch 10/100 11610/11610 [==============================] - 0s 28us/sample - loss: 0.4060 - val_loss: 0.3974 Epoch 11/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4018 - val_loss: 0.3847 Epoch 12/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3978 - val_loss: 0.3818 Epoch 13/100 11610/11610 [==============================] - 0s 28us/sample - loss: 0.3943 - val_loss: 0.3820 Epoch 14/100 11610/11610 [==============================] - 0s 28us/sample - loss: 0.3915 - val_loss: 0.3766 Epoch 15/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3888 - val_loss: 0.4268 Epoch 16/100 11610/11610 [==============================] - 0s 28us/sample - loss: 0.3862 - val_loss: 0.3638 Epoch 17/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3842 - val_loss: 0.3666 &lt;&lt;53 more lines&gt;&gt; 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3533 - val_loss: 0.3355 Epoch 45/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3529 - val_loss: 0.3329 Epoch 46/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3522 - val_loss: 0.3393 Epoch 47/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3510 - val_loss: 0.4172 Epoch 48/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3515 - val_loss: 0.3758 Epoch 49/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3506 - val_loss: 0.4285 Epoch 50/100 11610/11610 [==============================] - 0s 28us/sample - loss: 0.3506 - val_loss: 0.4004 Epoch 51/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3501 - val_loss: 0.3300 Epoch 52/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3495 - val_loss: 0.3300 Epoch 53/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3485 - val_loss: 0.3599 Epoch 54/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3479 - val_loss: 0.3424 Epoch 55/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3475 - val_loss: 0.3752 Epoch 56/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3472 - val_loss: 0.4231 Epoch 57/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3474 - val_loss: 0.3728 Epoch 58/100 11610/11610 [==============================] - 0s 28us/sample - loss: 0.3464 - val_loss: 0.3455 Epoch 59/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3458 - val_loss: 0.3647 Epoch 60/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3451 - val_loss: 0.4226 Epoch 61/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3456 - val_loss: 0.3635 . &lt;tensorflow.python.keras.callbacks.History at 0x7ff32141f630&gt; . mse_test = keras_reg.score(X_test, y_test) . 5160/5160 [==============================] - 0s 14us/sample - loss: 0.3464 . y_pred = keras_reg.predict(X_new) . np.random.seed(42) tf.random.set_seed(42) . #collapse-show from scipy.stats import reciprocal from sklearn.model_selection import RandomizedSearchCV param_distribs = { &quot;n_hidden&quot;: [0, 1, 2, 3], &quot;n_neurons&quot;: np.arange(1, 100), &quot;learning_rate&quot;: reciprocal(3e-4, 3e-2), } rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2) rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)]) . . Fitting 3 folds for each of 10 candidates, totalling 30 fits [CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 32/7740 [..............................] - ETA: 30s - loss: 6.7379 . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . 7740/7740 [==============================] - 0s 53us/sample - loss: 3.5574 - val_loss: 1.8536 Epoch 2/100 7740/7740 [==============================] - 0s 30us/sample - loss: 1.3316 - val_loss: 0.9380 Epoch 3/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.8573 - val_loss: 0.8545 Epoch 4/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.7344 - val_loss: 0.9545 Epoch 5/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.6943 - val_loss: 0.7248 Epoch 6/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.6682 - val_loss: 0.7356 Epoch 7/100 7740/7740 [==============================] - 0s 31us/sample - loss: 0.6494 - val_loss: 0.9732 Epoch 8/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.6408 - val_loss: 0.6175 Epoch 9/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.6256 - val_loss: 0.5877 Epoch 10/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.6149 - val_loss: 0.6164 Epoch 11/100 7740/7740 [==============================] - 0s 29us/sample - loss: 0.6058 - val_loss: 0.5851 Epoch 12/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.5961 - val_loss: 0.7040 Epoch 13/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.5905 - val_loss: 0.5594 Epoch 14/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.5797 - val_loss: 0.8668 Epoch 15/100 7740/7740 [==============================] - 0s 30us/sample - loss: 0.5764 - val_loss: 0.9031 &lt;&lt;2324 more lines&gt;&gt; 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2976 - val_loss: 0.3259 Epoch 72/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2979 - val_loss: 0.3275 Epoch 73/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2982 - val_loss: 0.3235 Epoch 74/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2971 - val_loss: 0.2947 Epoch 75/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2960 - val_loss: 0.3523 Epoch 76/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.2967 - val_loss: 0.3259 Epoch 77/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.2962 - val_loss: 0.3409 Epoch 78/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.2952 - val_loss: 0.2925 Epoch 79/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2949 - val_loss: 0.3667 Epoch 80/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2948 - val_loss: 0.2954 Epoch 81/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.2931 - val_loss: 0.3468 Epoch 82/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2951 - val_loss: 0.3064 Epoch 83/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2932 - val_loss: 0.3054 Epoch 84/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2924 - val_loss: 0.3041 Epoch 85/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2920 - val_loss: 0.3227 Epoch 86/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.2922 - val_loss: 0.2982 Epoch 87/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.2917 - val_loss: 0.3389 Epoch 88/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.2915 - val_loss: 0.3658 . RandomizedSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=&lt;tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7ff3841c6be0&gt;, iid=&#39;warn&#39;, n_iter=10, n_jobs=None, param_distributions={&#39;learning_rate&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7ff384301f60&gt;, &#39;n_hidden&#39;: [0, 1, 2, 3], &#39;n_neurons&#39;: array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,... 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])}, pre_dispatch=&#39;2*n_jobs&#39;, random_state=None, refit=True, return_train_score=False, scoring=None, verbose=2) . rnd_search_cv.best_params_ . {&#39;learning_rate&#39;: 0.0033625641252688094, &#39;n_hidden&#39;: 2, &#39;n_neurons&#39;: 42} . rnd_search_cv.best_score_ . -0.35952892616378346 . rnd_search_cv.best_estimator_ . &lt;tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor at 0x7ff384301518&gt; . rnd_search_cv.score(X_test, y_test) . 5160/5160 [==============================] - 0s 15us/sample - loss: 0.3065 . -0.30652404945026074 . model = rnd_search_cv.best_estimator_.model model . &lt;tensorflow.python.keras.engine.sequential.Sequential at 0x7ff350924668&gt; . model.evaluate(X_test, y_test) . 5160/5160 [==============================] - 0s 15us/sample - loss: 0.3065 . 0.30652404945026074 . Exercise solutions . 1. to 9. . See appendix A. . 10. . Exercise: Train a deep MLP on the MNIST dataset (you can load it using keras.datasets.mnist.load_data(). See if you can get over 98% precision. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the loss, and finding the point where the loss shoots up). Try adding all the bells and whistlesâ€”save checkpoints, use early stopping, and plot learning curves using TensorBoard. . Let&#39;s load the dataset: . (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data() . Just like for the Fashion MNIST dataset, the MNIST training set contains 60,000 grayscale images, each 28x28 pixels: . X_train_full.shape . (60000, 28, 28) . Each pixel intensity is also represented as a byte (0 to 255): . X_train_full.dtype . dtype(&#39;uint8&#39;) . Let&#39;s split the full training set into a validation set and a (smaller) training set. We also scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255, just like we did for Fashion MNIST: . X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255. y_valid, y_train = y_train_full[:5000], y_train_full[5000:] X_test = X_test / 255. . Let&#39;s plot an image using Matplotlib&#39;s imshow() function, with a &#39;binary&#39; color map: . plt.imshow(X_train[0], cmap=&quot;binary&quot;) plt.axis(&#39;off&#39;) plt.show() . The labels are the class IDs (represented as uint8), from 0 to 9. Conveniently, the class IDs correspond to the digits represented in the images, so we don&#39;t need a class_names array: . y_train . array([7, 3, 4, ..., 5, 6, 8], dtype=uint8) . The validation set contains 5,000 images, and the test set contains 10,000 images: . X_valid.shape . (5000, 28, 28) . X_test.shape . (10000, 28, 28) . Let&#39;s take a look at a sample of the images in the dataset: . n_rows = 4 n_cols = 10 plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2)) for row in range(n_rows): for col in range(n_cols): index = n_cols * row + col plt.subplot(n_rows, n_cols, index + 1) plt.imshow(X_train[index], cmap=&quot;binary&quot;, interpolation=&quot;nearest&quot;) plt.axis(&#39;off&#39;) plt.title(y_train[index], fontsize=12) plt.subplots_adjust(wspace=0.2, hspace=0.5) plt.show() . Let&#39;s build a simple dense network and find the optimal learning rate. We will need a callback to grow the learning rate at each iteration. It will also record the learning rate and the loss at each iteration: . K = keras.backend class ExponentialLearningRate(keras.callbacks.Callback): def __init__(self, factor): self.factor = factor self.rates = [] self.losses = [] def on_batch_end(self, batch, logs): self.rates.append(K.get_value(self.model.optimizer.lr)) self.losses.append(logs[&quot;loss&quot;]) K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor) . keras.backend.clear_session() np.random.seed(42) tf.random.set_seed(42) . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;relu&quot;), keras.layers.Dense(100, activation=&quot;relu&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) . We will start with a small learning rate of 1e-3, and grow it by 0.5% at each iteration: . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) expon_lr = ExponentialLearningRate(factor=1.005) . Now let&#39;s train the model for just 1 epoch: . history = model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid), callbacks=[expon_lr]) . Train on 55000 samples, validate on 5000 samples 55000/55000 [==============================] - 3s 52us/sample - loss: 55143288500710.3281 - accuracy: 0.5734 - val_loss: 2.3660 - val_accuracy: 0.1100 . We can now plot the loss as a functionof the learning rate: . plt.plot(expon_lr.rates, expon_lr.losses) plt.gca().set_xscale(&#39;log&#39;) plt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates)) plt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]]) plt.xlabel(&quot;Learning rate&quot;) plt.ylabel(&quot;Loss&quot;) . Text(0, 0.5, &#39;Loss&#39;) . The loss starts shooting back up violently around 3e-1, so let&#39;s try using 2e-1 as our learning rate: . keras.backend.clear_session() np.random.seed(42) tf.random.set_seed(42) . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;relu&quot;), keras.layers.Dense(100, activation=&quot;relu&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=2e-1), metrics=[&quot;accuracy&quot;]) . run_index = 1 # increment this at every run run_logdir = os.path.join(os.curdir, &quot;my_mnist_logs&quot;, &quot;run_{:03d}&quot;.format(run_index)) run_logdir . &#39;./my_mnist_logs/run_001&#39; . early_stopping_cb = keras.callbacks.EarlyStopping(patience=20) checkpoint_cb = keras.callbacks.ModelCheckpoint(&quot;my_mnist_model.h5&quot;, save_best_only=True) tensorboard_cb = keras.callbacks.TensorBoard(run_logdir) history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb, checkpoint_cb, tensorboard_cb]) . Train on 55000 samples, validate on 5000 samples Epoch 1/100 55000/55000 [==============================] - 3s 46us/sample - loss: 0.2361 - accuracy: 0.9280 - val_loss: 0.1183 - val_accuracy: 0.9664 Epoch 2/100 55000/55000 [==============================] - 2s 42us/sample - loss: 0.0954 - accuracy: 0.9705 - val_loss: 0.0855 - val_accuracy: 0.9768 Epoch 3/100 55000/55000 [==============================] - 2s 42us/sample - loss: 0.0642 - accuracy: 0.9796 - val_loss: 0.0822 - val_accuracy: 0.9786 Epoch 4/100 55000/55000 [==============================] - 2s 42us/sample - loss: 0.0462 - accuracy: 0.9855 - val_loss: 0.0804 - val_accuracy: 0.9770 Epoch 5/100 55000/55000 [==============================] - 2s 42us/sample - loss: 0.0333 - accuracy: 0.9894 - val_loss: 0.1907 - val_accuracy: 0.9500 Epoch 6/100 55000/55000 [==============================] - 2s 41us/sample - loss: 0.0244 - accuracy: 0.9919 - val_loss: 0.0698 - val_accuracy: 0.9828 Epoch 7/100 55000/55000 [==============================] - 2s 41us/sample - loss: 0.0205 - accuracy: 0.9929 - val_loss: 0.0809 - val_accuracy: 0.9800 Epoch 8/100 55000/55000 [==============================] - 2s 42us/sample - loss: 0.0153 - accuracy: 0.9949 - val_loss: 0.0841 - val_accuracy: 0.9824 Epoch 9/100 55000/55000 [==============================] - 2s 42us/sample - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.0899 - val_accuracy: 0.9788 Epoch 10/100 55000/55000 [==============================] - 2s 41us/sample - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.0741 - val_accuracy: 0.9844 Epoch 11/100 55000/55000 [==============================] - 2s 42us/sample - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.0729 - val_accuracy: 0.9842 Epoch 12/100 55000/55000 [==============================] - 2s 43us/sample - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0839 - val_accuracy: 0.9838 Epoch 13/100 55000/55000 [==============================] - 2s 43us/sample - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0747 - val_accuracy: 0.9858 Epoch 14/100 55000/55000 [==============================] - 2s 43us/sample - loss: 8.9780e-04 - accuracy: 0.9998 - val_loss: 0.0732 - val_accuracy: 0.9858 Epoch 15/100 55000/55000 [==============================] - 2s 42us/sample - loss: 3.7406e-04 - accuracy: 1.0000 - val_loss: 0.0771 - val_accuracy: 0.9862 Epoch 16/100 55000/55000 [==============================] - 2s 41us/sample - loss: 2.2128e-04 - accuracy: 1.0000 - val_loss: 0.0783 - val_accuracy: 0.9860 Epoch 17/100 55000/55000 [==============================] - 2s 43us/sample - loss: 1.8240e-04 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9864 Epoch 18/100 55000/55000 [==============================] - 2s 45us/sample - loss: 1.5978e-04 - accuracy: 1.0000 - val_loss: 0.0800 - val_accuracy: 0.9862 Epoch 19/100 55000/55000 [==============================] - 2s 43us/sample - loss: 1.4287e-04 - accuracy: 1.0000 - val_loss: 0.0808 - val_accuracy: 0.9862 Epoch 20/100 55000/55000 [==============================] - 2s 42us/sample - loss: 1.2992e-04 - accuracy: 1.0000 - val_loss: 0.0812 - val_accuracy: 0.9860 Epoch 21/100 55000/55000 [==============================] - 2s 43us/sample - loss: 1.2116e-04 - accuracy: 1.0000 - val_loss: 0.0816 - val_accuracy: 0.9860 Epoch 22/100 55000/55000 [==============================] - 2s 43us/sample - loss: 1.1251e-04 - accuracy: 1.0000 - val_loss: 0.0820 - val_accuracy: 0.9862 Epoch 23/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.0464e-04 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 0.9866 Epoch 24/100 55000/55000 [==============================] - 2s 43us/sample - loss: 9.8997e-05 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9864 Epoch 25/100 55000/55000 [==============================] - 2s 43us/sample - loss: 9.3607e-05 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 0.9864 Epoch 26/100 55000/55000 [==============================] - 2s 43us/sample - loss: 8.8776e-05 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 0.9864 . model = keras.models.load_model(&quot;my_mnist_model.h5&quot;) # rollback to best model model.evaluate(X_test, y_test) . 10000/10000 [==============================] - 0s 26us/sample - loss: 0.0692 - accuracy: 0.9806 . [0.06917384602149541, 0.9806] . We got over 98% accuracy. Finally, let&#39;s look at the learning curves using TensorBoard: . %tensorboard --logdir=./my_mnist_logs --port=6006 .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/10_neural_nets_with_keras",
            "relUrl": "/10_neural_nets_with_keras",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Loading and Preprocessing Data with TensorFlow",
            "content": "Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20 and TensorFlow â‰¥2.0. . # collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x !pip install -q -U tfx==0.15.0rc0 print(&quot;You can safely ignore the package incompatibility errors.&quot;) except Exception: pass # TensorFlow â‰¥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;data&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . . Datasets . X = tf.range(10) dataset = tf.data.Dataset.from_tensor_slices(X) dataset . &lt;TensorSliceDataset shapes: (), types: tf.int32&gt; . Equivalently: . dataset = tf.data.Dataset.range(10) . for item in dataset: print(item) . tf.Tensor(0, shape=(), dtype=int64) tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(2, shape=(), dtype=int64) tf.Tensor(3, shape=(), dtype=int64) tf.Tensor(4, shape=(), dtype=int64) tf.Tensor(5, shape=(), dtype=int64) tf.Tensor(6, shape=(), dtype=int64) tf.Tensor(7, shape=(), dtype=int64) tf.Tensor(8, shape=(), dtype=int64) tf.Tensor(9, shape=(), dtype=int64) . dataset = dataset.repeat(3).batch(7) for item in dataset: print(item) . tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64) tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64) tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64) tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64) tf.Tensor([8 9], shape=(2,), dtype=int64) . dataset = dataset.map(lambda x: x * 2) . for item in dataset: print(item) . tf.Tensor([ 0 2 4 6 8 10 12], shape=(7,), dtype=int64) tf.Tensor([14 16 18 0 2 4 6], shape=(7,), dtype=int64) tf.Tensor([ 8 10 12 14 16 18 0], shape=(7,), dtype=int64) tf.Tensor([ 2 4 6 8 10 12 14], shape=(7,), dtype=int64) tf.Tensor([16 18], shape=(2,), dtype=int64) . dataset = dataset.apply(tf.data.experimental.unbatch()) . dataset = dataset.filter(lambda x: x &lt; 10) # keep only items &lt; 10 . for item in dataset.take(3): print(item) . tf.Tensor(0, shape=(), dtype=int64) tf.Tensor(2, shape=(), dtype=int64) tf.Tensor(4, shape=(), dtype=int64) . dataset = tf.data.Dataset.range(10).repeat(3) dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7) for item in dataset: print(item) . tf.Tensor([0 3 4 2 1 5 8], shape=(7,), dtype=int64) tf.Tensor([6 9 7 2 3 1 4], shape=(7,), dtype=int64) tf.Tensor([6 0 7 9 0 1 2], shape=(7,), dtype=int64) tf.Tensor([8 4 5 5 3 8 9], shape=(7,), dtype=int64) tf.Tensor([7 6], shape=(2,), dtype=int64) . Split the California dataset to multiple CSV files . Let&#39;s start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it: . from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler housing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split( housing.data, housing.target.reshape(-1, 1), random_state=42) X_train, X_valid, y_train, y_valid = train_test_split( X_train_full, y_train_full, random_state=42) scaler = StandardScaler() scaler.fit(X_train) X_mean = scaler.mean_ X_std = scaler.scale_ . For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel. To demonstrate this, let&#39;s start by splitting the housing dataset and save it to 20 CSV files: . def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10): housing_dir = os.path.join(&quot;datasets&quot;, &quot;housing&quot;) os.makedirs(housing_dir, exist_ok=True) path_format = os.path.join(housing_dir, &quot;my_{}_{:02d}.csv&quot;) filepaths = [] m = len(data) for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)): part_csv = path_format.format(name_prefix, file_idx) filepaths.append(part_csv) with open(part_csv, &quot;wt&quot;, encoding=&quot;utf-8&quot;) as f: if header is not None: f.write(header) f.write(&quot; n&quot;) for row_idx in row_indices: f.write(&quot;,&quot;.join([repr(col) for col in data[row_idx]])) f.write(&quot; n&quot;) return filepaths . train_data = np.c_[X_train, y_train] valid_data = np.c_[X_valid, y_valid] test_data = np.c_[X_test, y_test] header_cols = housing.feature_names + [&quot;MedianHouseValue&quot;] header = &quot;,&quot;.join(header_cols) train_filepaths = save_to_multiple_csv_files(train_data, &quot;train&quot;, header, n_parts=20) valid_filepaths = save_to_multiple_csv_files(valid_data, &quot;valid&quot;, header, n_parts=10) test_filepaths = save_to_multiple_csv_files(test_data, &quot;test&quot;, header, n_parts=10) . Okay, now let&#39;s take a peek at the first few lines of one of these CSV files: . import pandas as pd pd.read_csv(train_filepaths[0]).head() . MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude MedianHouseValue . 0 3.5214 | 15.0 | 3.049945 | 1.106548 | 1447.0 | 1.605993 | 37.63 | -122.43 | 1.442 | . 1 5.3275 | 5.0 | 6.490060 | 0.991054 | 3464.0 | 3.443340 | 33.69 | -117.39 | 1.687 | . 2 3.1000 | 29.0 | 7.542373 | 1.591525 | 1328.0 | 2.250847 | 38.44 | -122.98 | 1.621 | . 3 7.1736 | 12.0 | 6.289003 | 0.997442 | 1054.0 | 2.695652 | 33.55 | -117.70 | 2.621 | . 4 2.0549 | 13.0 | 5.312457 | 1.085092 | 3297.0 | 2.244384 | 33.93 | -116.93 | 0.956 | . Or in text mode: . with open(train_filepaths[0]) as f: for i in range(5): print(f.readline(), end=&quot;&quot;) . MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue 3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442 5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687 3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621 7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621 . train_filepaths . [&#39;datasets/housing/my_train_00.csv&#39;, &#39;datasets/housing/my_train_01.csv&#39;, &#39;datasets/housing/my_train_02.csv&#39;, &#39;datasets/housing/my_train_03.csv&#39;, &#39;datasets/housing/my_train_04.csv&#39;, &#39;datasets/housing/my_train_05.csv&#39;, &#39;datasets/housing/my_train_06.csv&#39;, &#39;datasets/housing/my_train_07.csv&#39;, &#39;datasets/housing/my_train_08.csv&#39;, &#39;datasets/housing/my_train_09.csv&#39;, &#39;datasets/housing/my_train_10.csv&#39;, &#39;datasets/housing/my_train_11.csv&#39;, &#39;datasets/housing/my_train_12.csv&#39;, &#39;datasets/housing/my_train_13.csv&#39;, &#39;datasets/housing/my_train_14.csv&#39;, &#39;datasets/housing/my_train_15.csv&#39;, &#39;datasets/housing/my_train_16.csv&#39;, &#39;datasets/housing/my_train_17.csv&#39;, &#39;datasets/housing/my_train_18.csv&#39;, &#39;datasets/housing/my_train_19.csv&#39;] . Building an Input Pipeline . filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42) . for filepath in filepath_dataset: print(filepath) . tf.Tensor(b&#39;datasets/housing/my_train_05.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_16.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_01.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_17.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_00.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_14.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_10.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_02.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_12.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_19.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_07.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_09.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_13.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_15.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_11.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_18.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_04.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_06.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_03.csv&#39;, shape=(), dtype=string) tf.Tensor(b&#39;datasets/housing/my_train_08.csv&#39;, shape=(), dtype=string) . n_readers = 5 dataset = filepath_dataset.interleave( lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers) . for line in dataset.take(5): print(line.numpy()) . b&#39;4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782&#39; b&#39;4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215&#39; b&#39;3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625&#39; b&#39;3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526&#39; b&#39;3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442&#39; . Notice that field 4 is interpreted as a string. . record_defaults=[0, np.nan, tf.constant(np.nan, dtype=tf.float64), &quot;Hello&quot;, tf.constant([])] parsed_fields = tf.io.decode_csv(&#39;1,2,3,4,5&#39;, record_defaults) parsed_fields . [&lt;tf.Tensor: id=245, shape=(), dtype=int32, numpy=1&gt;, &lt;tf.Tensor: id=246, shape=(), dtype=float32, numpy=2.0&gt;, &lt;tf.Tensor: id=247, shape=(), dtype=float64, numpy=3.0&gt;, &lt;tf.Tensor: id=248, shape=(), dtype=string, numpy=b&#39;4&#39;&gt;, &lt;tf.Tensor: id=249, shape=(), dtype=float32, numpy=5.0&gt;] . Notice that all missing fields are replaced with their default value, when provided: . parsed_fields = tf.io.decode_csv(&#39;,,,,5&#39;, record_defaults) parsed_fields . [&lt;tf.Tensor: id=259, shape=(), dtype=int32, numpy=0&gt;, &lt;tf.Tensor: id=260, shape=(), dtype=float32, numpy=nan&gt;, &lt;tf.Tensor: id=261, shape=(), dtype=float64, numpy=nan&gt;, &lt;tf.Tensor: id=262, shape=(), dtype=string, numpy=b&#39;Hello&#39;&gt;, &lt;tf.Tensor: id=263, shape=(), dtype=float32, numpy=5.0&gt;] . The 5th field is compulsory (since we provided tf.constant([]) as the &quot;default value&quot;), so we get an exception if we do not provide it: . try: parsed_fields = tf.io.decode_csv(&#39;,,,,&#39;, record_defaults) except tf.errors.InvalidArgumentError as ex: print(ex) . Field 4 is required but missing in record 0! [Op:DecodeCSV] . The number of fields should match exactly the number of fields in the record_defaults: . try: parsed_fields = tf.io.decode_csv(&#39;1,2,3,4,5,6,7&#39;, record_defaults) except tf.errors.InvalidArgumentError as ex: print(ex) . Expect 5 fields but have 7 in record 0 [Op:DecodeCSV] . n_inputs = 8 # X_train.shape[-1] @tf.function def preprocess(line): defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)] fields = tf.io.decode_csv(line, record_defaults=defs) x = tf.stack(fields[:-1]) y = tf.stack(fields[-1:]) return (x - X_mean) / X_std, y . preprocess(b&#39;4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782&#39;) . (&lt;tf.Tensor: id=307, shape=(8,), dtype=float32, numpy= array([ 0.16579157, 1.216324 , -0.05204565, -0.39215982, -0.5277444 , -0.2633488 , 0.8543046 , -1.3072058 ], dtype=float32)&gt;, &lt;tf.Tensor: id=308, shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)&gt;) . def csv_reader_dataset(filepaths, repeat=1, n_readers=5, n_read_threads=None, shuffle_buffer_size=10000, n_parse_threads=5, batch_size=32): dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat) dataset = dataset.interleave( lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads) dataset = dataset.shuffle(shuffle_buffer_size) dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads) dataset = dataset.batch(batch_size) return dataset.prefetch(1) . train_set = csv_reader_dataset(train_filepaths, batch_size=3) for X_batch, y_batch in train_set.take(2): print(&quot;X =&quot;, X_batch) print(&quot;y =&quot;, y_batch) print() . X = tf.Tensor( [[-1.1309323 0.5834586 -1.1141092 -0.0205977 0.4861637 0.05622157 -0.72447133 0.63188833] [ 1.9003258 -0.998705 0.6331733 -0.03226789 0.05084941 -0.28879014 1.0229574 -1.2872185 ] [-0.08566543 -1.6315705 0.04572354 -0.16350564 0.18226504 0.09547261 -0.92123175 0.65187943]], shape=(3, 8), dtype=float32) y = tf.Tensor( [[2.219] [3.702] [2.875]], shape=(3, 1), dtype=float32) X = tf.Tensor( [[ 0.92681414 -1.0778131 0.194918 -0.2745752 -0.02215928 0.13869788 1.2525111 -1.5520943 ] [ 0.12441459 -2.0271113 -0.0920579 -0.17962678 -0.38263968 0.11107364 -1.3007004 1.9912548 ] [-0.10782045 1.8491895 -0.01604682 0.04678809 -0.33609664 -0.2658364 0.9807942 -1.4471437 ]], shape=(3, 8), dtype=float32) y = tf.Tensor( [[1.724] [0.832] [3.118]], shape=(3, 1), dtype=float32) . train_set = csv_reader_dataset(train_filepaths, repeat=None) valid_set = csv_reader_dataset(valid_filepaths) test_set = csv_reader_dataset(test_filepaths) . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;relu&quot;, input_shape=X_train.shape[1:]), keras.layers.Dense(1), ]) . model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(lr=1e-3)) . batch_size = 32 model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10, validation_data=valid_set) . Epoch 1/10 362/362 [==============================] - 1s 3ms/step - loss: 2.4158 - val_loss: 0.9218 Epoch 2/10 362/362 [==============================] - 1s 2ms/step - loss: 0.7571 - val_loss: 0.6992 Epoch 3/10 362/362 [==============================] - 1s 2ms/step - loss: 0.6767 - val_loss: 0.6532 Epoch 4/10 362/362 [==============================] - 1s 2ms/step - loss: 0.6406 - val_loss: 0.6751 Epoch 5/10 362/362 [==============================] - 1s 2ms/step - loss: 0.6012 - val_loss: 0.5904 Epoch 6/10 362/362 [==============================] - 1s 2ms/step - loss: 0.5792 - val_loss: 0.6392 Epoch 7/10 362/362 [==============================] - 1s 2ms/step - loss: 0.5531 - val_loss: 0.5686 Epoch 8/10 362/362 [==============================] - 1s 2ms/step - loss: 0.5050 - val_loss: 0.5086 Epoch 9/10 362/362 [==============================] - 1s 2ms/step - loss: 0.5039 - val_loss: 0.4825 Epoch 10/10 362/362 [==============================] - 1s 2ms/step - loss: 0.4930 - val_loss: 0.4761 . &lt;tensorflow.python.keras.callbacks.History at 0x131194e48&gt; . model.evaluate(test_set, steps=len(X_test) // batch_size) . 161/161 [==============================] - 0s 1ms/step - loss: 0.4804 . 0.4803512151937307 . new_set = test_set.map(lambda X, y: X) # we could instead just pass test_set, Keras would ignore the labels X_new = X_test model.predict(new_set, steps=len(X_new) // batch_size) . array([[2.5751495], [2.1568842], [2.019339 ], ..., [3.553556 ], [2.2041245], [2.366016 ]], dtype=float32) . optimizer = keras.optimizers.Nadam(lr=0.01) loss_fn = keras.losses.mean_squared_error n_epochs = 5 batch_size = 32 n_steps_per_epoch = len(X_train) // batch_size total_steps = n_epochs * n_steps_per_epoch global_step = 0 for X_batch, y_batch in train_set.take(total_steps): global_step += 1 print(&quot; rGlobal step {}/{}&quot;.format(global_step, total_steps), end=&quot;&quot;) with tf.GradientTape() as tape: y_pred = model(X_batch) main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) loss = tf.add_n([main_loss] + model.losses) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) . Global step 1810/1810 . optimizer = keras.optimizers.Nadam(lr=0.01) loss_fn = keras.losses.mean_squared_error @tf.function def train(model, n_epochs, batch_size=32, n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5): train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers, n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size, n_parse_threads=n_parse_threads, batch_size=batch_size) for X_batch, y_batch in train_set: with tf.GradientTape() as tape: y_pred = model(X_batch) main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) loss = tf.add_n([main_loss] + model.losses) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train(model, 5) . optimizer = keras.optimizers.Nadam(lr=0.01) loss_fn = keras.losses.mean_squared_error @tf.function def train(model, n_epochs, batch_size=32, n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5): train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers, n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size, n_parse_threads=n_parse_threads, batch_size=batch_size) n_steps_per_epoch = len(X_train) // batch_size total_steps = n_epochs * n_steps_per_epoch global_step = 0 for X_batch, y_batch in train_set.take(total_steps): global_step += 1 if tf.equal(global_step % 100, 0): tf.print(&quot; rGlobal step&quot;, global_step, &quot;/&quot;, total_steps) with tf.GradientTape() as tape: y_pred = model(X_batch) main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) loss = tf.add_n([main_loss] + model.losses) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train(model, 5) . Global step 100 / 1810 Global step 200 / 1810 Global step 300 / 1810 Global step 400 / 1810 Global step 500 / 1810 Global step 600 / 1810 Global step 700 / 1810 Global step 800 / 1810 Global step 900 / 1810 Global step 1000 / 1810 Global step 1100 / 1810 Global step 1200 / 1810 Global step 1300 / 1810 Global step 1400 / 1810 Global step 1500 / 1810 Global step 1600 / 1810 Global step 1700 / 1810 Global step 1800 / 1810 . Here is a short description of each method in the Dataset class: . for m in dir(tf.data.Dataset): if not (m.startswith(&quot;_&quot;) or m.endswith(&quot;_&quot;)): func = getattr(tf.data.Dataset, m) if hasattr(func, &quot;__doc__&quot;): print(&quot;â— {:21s}{}&quot;.format(m + &quot;()&quot;, func.__doc__.split(&quot; n&quot;)[0])) . â— apply() Applies a transformation function to this dataset. â— batch() Combines consecutive elements of this dataset into batches. â— cache() Caches the elements in this dataset. â— concatenate() Creates a `Dataset` by concatenating given dataset with this dataset. â— filter() Filters this dataset according to `predicate`. â— flat_map() Maps `map_func` across this dataset and flattens the result. â— from_generator() Creates a `Dataset` whose elements are generated by `generator`. â— from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors. â— from_tensors() Creates a `Dataset` with a single element, comprising the given tensors. â— interleave() Maps `map_func` across this dataset, and interleaves the results. â— list_files() A dataset of all files matching one or more glob patterns. â— map() Maps `map_func` across the elements of this dataset. â— options() Returns the options for this dataset and its inputs. â— padded_batch() Combines consecutive elements of this dataset into padded batches. â— prefetch() Creates a `Dataset` that prefetches elements from this dataset. â— range() Creates a `Dataset` of a step-separated range of values. â— reduce() Reduces the input dataset to a single element. â— repeat() Repeats this dataset `count` times. â— shard() Creates a `Dataset` that includes only 1/`num_shards` of this dataset. â— shuffle() Randomly shuffles the elements of this dataset. â— skip() Creates a `Dataset` that skips `count` elements from this dataset. â— take() Creates a `Dataset` with at most `count` elements from this dataset. â— window() Combines input elements into a dataset of windows. â— with_options() Returns a new `tf.data.Dataset` with the given options set. â— zip() Creates a `Dataset` by zipping together the given datasets. . The TFRecord binary format . A TFRecord file is just a list of binary records. You can create one using a tf.io.TFRecordWriter: . with tf.io.TFRecordWriter(&quot;my_data.tfrecord&quot;) as f: f.write(b&quot;This is the first record&quot;) f.write(b&quot;And this is the second record&quot;) . And you can read it using a tf.data.TFRecordDataset: . filepaths = [&quot;my_data.tfrecord&quot;] dataset = tf.data.TFRecordDataset(filepaths) for item in dataset: print(item) . tf.Tensor(b&#39;This is the first record&#39;, shape=(), dtype=string) tf.Tensor(b&#39;And this is the second record&#39;, shape=(), dtype=string) . You can read multiple TFRecord files with just one TFRecordDataset. By default it will read them one at a time, but if you set num_parallel_reads=3, it will read 3 at a time in parallel and interleave their records: . filepaths = [&quot;my_test_{}.tfrecord&quot;.format(i) for i in range(5)] for i, filepath in enumerate(filepaths): with tf.io.TFRecordWriter(filepath) as f: for j in range(3): f.write(&quot;File {} record {}&quot;.format(i, j).encode(&quot;utf-8&quot;)) dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=3) for item in dataset: print(item) . tf.Tensor(b&#39;File 0 record 0&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 1 record 0&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 2 record 0&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 0 record 1&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 1 record 1&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 2 record 1&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 0 record 2&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 1 record 2&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 2 record 2&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 3 record 0&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 4 record 0&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 3 record 1&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 4 record 1&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 3 record 2&#39;, shape=(), dtype=string) tf.Tensor(b&#39;File 4 record 2&#39;, shape=(), dtype=string) . options = tf.io.TFRecordOptions(compression_type=&quot;GZIP&quot;) with tf.io.TFRecordWriter(&quot;my_compressed.tfrecord&quot;, options) as f: f.write(b&quot;This is the first record&quot;) f.write(b&quot;And this is the second record&quot;) . dataset = tf.data.TFRecordDataset([&quot;my_compressed.tfrecord&quot;], compression_type=&quot;GZIP&quot;) for item in dataset: print(item) . tf.Tensor(b&#39;This is the first record&#39;, shape=(), dtype=string) tf.Tensor(b&#39;And this is the second record&#39;, shape=(), dtype=string) . A Brief Intro to Protocol Buffers . For this section you need to install protobuf. In general you will not have to do so when using TensorFlow, as it comes with functions to create and parse protocol buffers of type tf.train.Example, which are generally sufficient. However, in this section we will learn about protocol buffers by creating our own simple protobuf definition, so we need the protobuf compiler (protoc): we will use it to compile the protobuf definition to a Python module that we can then use in our code. . First let&#39;s write a simple protobuf definition: . %%writefile person.proto syntax = &quot;proto3&quot;; message Person { string name = 1; int32 id = 2; repeated string email = 3; } . Overwriting person.proto . And let&#39;s compile it (the --descriptor_set_out and --include_imports options are only required for the tf.io.decode_proto() example below): . !protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports . !ls person* . person.desc person.proto person_pb2.py . from person_pb2 import Person person = Person(name=&quot;Al&quot;, id=123, email=[&quot;a@b.com&quot;]) # create a Person print(person) # display the Person . name: &#34;Al&#34; id: 123 email: &#34;a@b.com&#34; . person.name # read a field . &#39;Al&#39; . person.name = &quot;Alice&quot; # modify a field . person.email[0] # repeated fields can be accessed like arrays . &#39;a@b.com&#39; . person.email.append(&quot;c@d.com&quot;) # add an email address . s = person.SerializeToString() # serialize to a byte string s . b&#39; n x05Alice x10{ x1a x07a@b.com x1a x07c@d.com&#39; . person2 = Person() # create a new Person person2.ParseFromString(s) # parse the byte string (27 bytes) . 27 . person == person2 # now they are equal . True . Custom protobuf . In rare cases, you may want to parse a custom protobuf (like the one we just created) in TensorFlow. For this you can use the tf.io.decode_proto() function: . person_tf = tf.io.decode_proto( bytes=s, message_type=&quot;Person&quot;, field_names=[&quot;name&quot;, &quot;id&quot;, &quot;email&quot;], output_types=[tf.string, tf.int32, tf.string], descriptor_source=&quot;person.desc&quot;) person_tf.values . [&lt;tf.Tensor: id=12, shape=(1,), dtype=string, numpy=array([b&#39;Alice&#39;], dtype=object)&gt;, &lt;tf.Tensor: id=13, shape=(1,), dtype=int32, numpy=array([123], dtype=int32)&gt;, &lt;tf.Tensor: id=14, shape=(2,), dtype=string, numpy=array([b&#39;a@b.com&#39;, b&#39;c@d.com&#39;], dtype=object)&gt;] . For more details, see the tf.io.decode_proto() documentation. . TensorFlow Protobufs . Here is the definition of the tf.train.Example protobuf: . syntax = &quot;proto3&quot;; message BytesList { repeated bytes value = 1; } message FloatList { repeated float value = 1 [packed = true]; } message Int64List { repeated int64 value = 1 [packed = true]; } message Feature { oneof kind { BytesList bytes_list = 1; FloatList float_list = 2; Int64List int64_list = 3; } }; message Features { map&lt;string, Feature&gt; feature = 1; }; message Example { Features features = 1; }; . # WARNING: there&#39;s currently a bug preventing &quot;from tensorflow.train import X&quot; # so we work around it by writing &quot;X = tf.train.X&quot; #from tensorflow.train import BytesList, FloatList, Int64List #from tensorflow.train import Feature, Features, Example BytesList = tf.train.BytesList FloatList = tf.train.FloatList Int64List = tf.train.Int64List Feature = tf.train.Feature Features = tf.train.Features Example = tf.train.Example person_example = Example( features=Features( feature={ &quot;name&quot;: Feature(bytes_list=BytesList(value=[b&quot;Alice&quot;])), &quot;id&quot;: Feature(int64_list=Int64List(value=[123])), &quot;emails&quot;: Feature(bytes_list=BytesList(value=[b&quot;a@b.com&quot;, b&quot;c@d.com&quot;])) })) with tf.io.TFRecordWriter(&quot;my_contacts.tfrecord&quot;) as f: f.write(person_example.SerializeToString()) . feature_description = { &quot;name&quot;: tf.io.FixedLenFeature([], tf.string, default_value=&quot;&quot;), &quot;id&quot;: tf.io.FixedLenFeature([], tf.int64, default_value=0), &quot;emails&quot;: tf.io.VarLenFeature(tf.string), } for serialized_example in tf.data.TFRecordDataset([&quot;my_contacts.tfrecord&quot;]): parsed_example = tf.io.parse_single_example(serialized_example, feature_description) . parsed_example . {&#39;emails&#39;: &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x102a2acc0&gt;, &#39;id&#39;: &lt;tf.Tensor: id=535953, shape=(), dtype=int64, numpy=123&gt;, &#39;name&#39;: &lt;tf.Tensor: id=535954, shape=(), dtype=string, numpy=b&#39;Alice&#39;&gt;} . parsed_example . {&#39;emails&#39;: &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x102a2acc0&gt;, &#39;id&#39;: &lt;tf.Tensor: id=535953, shape=(), dtype=int64, numpy=123&gt;, &#39;name&#39;: &lt;tf.Tensor: id=535954, shape=(), dtype=string, numpy=b&#39;Alice&#39;&gt;} . parsed_example[&quot;emails&quot;].values[0] . &lt;tf.Tensor: id=535962, shape=(), dtype=string, numpy=b&#39;a@b.com&#39;&gt; . tf.sparse.to_dense(parsed_example[&quot;emails&quot;], default_value=b&quot;&quot;) . &lt;tf.Tensor: id=535965, shape=(2,), dtype=string, numpy=array([b&#39;a@b.com&#39;, b&#39;c@d.com&#39;], dtype=object)&gt; . parsed_example[&quot;emails&quot;].values . &lt;tf.Tensor: id=535951, shape=(2,), dtype=string, numpy=array([b&#39;a@b.com&#39;, b&#39;c@d.com&#39;], dtype=object)&gt; . Putting Images in TFRecords . from sklearn.datasets import load_sample_images img = load_sample_images()[&quot;images&quot;][0] plt.imshow(img) plt.axis(&quot;off&quot;) plt.title(&quot;Original Image&quot;) plt.show() . data = tf.io.encode_jpeg(img) example_with_image = Example(features=Features(feature={ &quot;image&quot;: Feature(bytes_list=BytesList(value=[data.numpy()]))})) serialized_example = example_with_image.SerializeToString() # then save to TFRecord . feature_description = { &quot;image&quot;: tf.io.VarLenFeature(tf.string) } example_with_image = tf.io.parse_single_example(serialized_example, feature_description) decoded_img = tf.io.decode_jpeg(example_with_image[&quot;image&quot;].values[0]) . Or use decode_image() which supports BMP, GIF, JPEG and PNG formats: . decoded_img = tf.io.decode_image(example_with_image[&quot;image&quot;].values[0]) . plt.imshow(decoded_img) plt.title(&quot;Decoded Image&quot;) plt.axis(&quot;off&quot;) plt.show() . Putting Tensors and Sparse Tensors in TFRecords . Tensors can be serialized and parsed easily using tf.io.serialize_tensor() and tf.io.parse_tensor(): . t = tf.constant([[0., 1.], [2., 3.], [4., 5.]]) s = tf.io.serialize_tensor(t) s . &lt;tf.Tensor: id=536000, shape=(), dtype=string, numpy=b&#39; x08 x01 x12 x08 x12 x02 x08 x03 x12 x02 x08 x02&#34; x18 x00 x00 x00 x00 x00 x00 x80? x00 x00 x00@ x00 x00@@ x00 x00 x80@ x00 x00 xa0@&#39;&gt; . tf.io.parse_tensor(s, out_type=tf.float32) . &lt;tf.Tensor: id=536002, shape=(3, 2), dtype=float32, numpy= array([[0., 1.], [2., 3.], [4., 5.]], dtype=float32)&gt; . serialized_sparse = tf.io.serialize_sparse(parsed_example[&quot;emails&quot;]) serialized_sparse . &lt;tf.Tensor: id=536004, shape=(3,), dtype=string, numpy= array([b&#39; x08 t x12 x08 x12 x02 x08 x02 x12 x02 x08 x01&#34; x10 x00 x00 x00 x00 x00 x00 x00 x00 x01 x00 x00 x00 x00 x00 x00 x00&#39;, b&#39; x08 x07 x12 x04 x12 x02 x08 x02&#34; x10 x07 x07a@b.comc@d.com&#39;, b&#39; x08 t x12 x04 x12 x02 x08 x01&#34; x08 x02 x00 x00 x00 x00 x00 x00 x00&#39;], dtype=object)&gt; . BytesList(value=serialized_sparse.numpy()) . value: &#34; 010 t 022 010 022 002 010 002 022 002 010 001 &#34; 020 000 000 000 000 000 000 000 000 001 000 000 000 000 000 000 000&#34; value: &#34; 010 007 022 004 022 002 010 002 &#34; 020 007 007a@b.comc@d.com&#34; value: &#34; 010 t 022 004 022 002 010 001 &#34; 010 002 000 000 000 000 000 000 000&#34; . dataset = tf.data.TFRecordDataset([&quot;my_contacts.tfrecord&quot;]).batch(10) for serialized_examples in dataset: parsed_examples = tf.io.parse_example(serialized_examples, feature_description) . parsed_examples . {&#39;image&#39;: &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x129697b70&gt;} . Handling Sequential Data Using SequenceExample . syntax = &quot;proto3&quot;; message FeatureList { repeated Feature feature = 1; }; message FeatureLists { map&lt;string, FeatureList&gt; feature_list = 1; }; message SequenceExample { Features context = 1; FeatureLists feature_lists = 2; }; . # WARNING: there&#39;s currently a bug preventing &quot;from tensorflow.train import X&quot; # so we work around it by writing &quot;X = tf.train.X&quot; #from tensorflow.train import FeatureList, FeatureLists, SequenceExample FeatureList = tf.train.FeatureList FeatureLists = tf.train.FeatureLists SequenceExample = tf.train.SequenceExample context = Features(feature={ &quot;author_id&quot;: Feature(int64_list=Int64List(value=[123])), &quot;title&quot;: Feature(bytes_list=BytesList(value=[b&quot;A&quot;, b&quot;desert&quot;, b&quot;place&quot;, b&quot;.&quot;])), &quot;pub_date&quot;: Feature(int64_list=Int64List(value=[1623, 12, 25])) }) content = [[&quot;When&quot;, &quot;shall&quot;, &quot;we&quot;, &quot;three&quot;, &quot;meet&quot;, &quot;again&quot;, &quot;?&quot;], [&quot;In&quot;, &quot;thunder&quot;, &quot;,&quot;, &quot;lightning&quot;, &quot;,&quot;, &quot;or&quot;, &quot;in&quot;, &quot;rain&quot;, &quot;?&quot;]] comments = [[&quot;When&quot;, &quot;the&quot;, &quot;hurlyburly&quot;, &quot;&#39;s&quot;, &quot;done&quot;, &quot;.&quot;], [&quot;When&quot;, &quot;the&quot;, &quot;battle&quot;, &quot;&#39;s&quot;, &quot;lost&quot;, &quot;and&quot;, &quot;won&quot;, &quot;.&quot;]] def words_to_feature(words): return Feature(bytes_list=BytesList(value=[word.encode(&quot;utf-8&quot;) for word in words])) content_features = [words_to_feature(sentence) for sentence in content] comments_features = [words_to_feature(comment) for comment in comments] sequence_example = SequenceExample( context=context, feature_lists=FeatureLists(feature_list={ &quot;content&quot;: FeatureList(feature=content_features), &quot;comments&quot;: FeatureList(feature=comments_features) })) . sequence_example . context { feature { key: &#34;author_id&#34; value { int64_list { value: 123 } } } feature { key: &#34;pub_date&#34; value { int64_list { value: 1623 value: 12 value: 25 } } } feature { key: &#34;title&#34; value { bytes_list { value: &#34;A&#34; value: &#34;desert&#34; value: &#34;place&#34; value: &#34;.&#34; } } } } feature_lists { feature_list { key: &#34;comments&#34; value { feature { bytes_list { value: &#34;When&#34; value: &#34;the&#34; value: &#34;hurlyburly&#34; value: &#34; &#39;s&#34; value: &#34;done&#34; value: &#34;.&#34; } } feature { bytes_list { value: &#34;When&#34; value: &#34;the&#34; value: &#34;battle&#34; value: &#34; &#39;s&#34; value: &#34;lost&#34; value: &#34;and&#34; value: &#34;won&#34; value: &#34;.&#34; } } } } feature_list { key: &#34;content&#34; value { feature { bytes_list { value: &#34;When&#34; value: &#34;shall&#34; value: &#34;we&#34; value: &#34;three&#34; value: &#34;meet&#34; value: &#34;again&#34; value: &#34;?&#34; } } feature { bytes_list { value: &#34;In&#34; value: &#34;thunder&#34; value: &#34;,&#34; value: &#34;lightning&#34; value: &#34;,&#34; value: &#34;or&#34; value: &#34;in&#34; value: &#34;rain&#34; value: &#34;?&#34; } } } } } . serialized_sequence_example = sequence_example.SerializeToString() . context_feature_descriptions = { &quot;author_id&quot;: tf.io.FixedLenFeature([], tf.int64, default_value=0), &quot;title&quot;: tf.io.VarLenFeature(tf.string), &quot;pub_date&quot;: tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]), } sequence_feature_descriptions = { &quot;content&quot;: tf.io.VarLenFeature(tf.string), &quot;comments&quot;: tf.io.VarLenFeature(tf.string), } parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example( serialized_sequence_example, context_feature_descriptions, sequence_feature_descriptions) . parsed_context . {&#39;title&#39;: &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x134ff6c88&gt;, &#39;author_id&#39;: &lt;tf.Tensor: id=536050, shape=(), dtype=int64, numpy=123&gt;, &#39;pub_date&#39;: &lt;tf.Tensor: id=536051, shape=(3,), dtype=int64, numpy=array([1623, 12, 25])&gt;} . parsed_context[&quot;title&quot;].values . &lt;tf.Tensor: id=536048, shape=(4,), dtype=string, numpy=array([b&#39;A&#39;, b&#39;desert&#39;, b&#39;place&#39;, b&#39;.&#39;], dtype=object)&gt; . parsed_feature_lists . {&#39;comments&#39;: &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x134ff6f98&gt;, &#39;content&#39;: &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x130e23390&gt;} . print(tf.RaggedTensor.from_sparse(parsed_feature_lists[&quot;content&quot;])) . &lt;tf.RaggedTensor [[b&#39;When&#39;, b&#39;shall&#39;, b&#39;we&#39;, b&#39;three&#39;, b&#39;meet&#39;, b&#39;again&#39;, b&#39;?&#39;], [b&#39;In&#39;, b&#39;thunder&#39;, b&#39;,&#39;, b&#39;lightning&#39;, b&#39;,&#39;, b&#39;or&#39;, b&#39;in&#39;, b&#39;rain&#39;, b&#39;?&#39;]]&gt; . The Features API . Let&#39;s use the variant of the California housing dataset that we used in Chapter 2, since it contains categorical features and missing values: . import os import tarfile import urllib DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; HOUSING_PATH = os.path.join(&quot;datasets&quot;, &quot;housing&quot;) HOUSING_URL = DOWNLOAD_ROOT + &quot;datasets/housing/housing.tgz&quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): os.makedirs(housing_path, exist_ok=True) tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close() . fetch_housing_data() . import pandas as pd def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(housing_path, &quot;housing.csv&quot;) return pd.read_csv(csv_path) . housing = load_housing_data() housing.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . housing_median_age = tf.feature_column.numeric_column(&quot;housing_median_age&quot;) . age_mean, age_std = X_mean[1], X_std[1] # The median age is column in 1 housing_median_age = tf.feature_column.numeric_column( &quot;housing_median_age&quot;, normalizer_fn=lambda x: (x - age_mean) / age_std) . median_income = tf.feature_column.numeric_column(&quot;median_income&quot;) bucketized_income = tf.feature_column.bucketized_column( median_income, boundaries=[1.5, 3., 4.5, 6.]) . bucketized_income . BucketizedColumn(source_column=NumericColumn(key=&#39;median_income&#39;, shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(1.5, 3.0, 4.5, 6.0)) . ocean_prox_vocab = [&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;] ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list( &quot;ocean_proximity&quot;, ocean_prox_vocab) . ocean_proximity . VocabularyListCategoricalColumn(key=&#39;ocean_proximity&#39;, vocabulary_list=(&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;), dtype=tf.string, default_value=-1, num_oov_buckets=0) . # Just an example, it&#39;s not used later on city_hash = tf.feature_column.categorical_column_with_hash_bucket( &quot;city&quot;, hash_bucket_size=1000) city_hash . HashedCategoricalColumn(key=&#39;city&#39;, hash_bucket_size=1000, dtype=tf.string) . bucketized_age = tf.feature_column.bucketized_column( housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was scaled age_and_ocean_proximity = tf.feature_column.crossed_column( [bucketized_age, ocean_proximity], hash_bucket_size=100) . latitude = tf.feature_column.numeric_column(&quot;latitude&quot;) longitude = tf.feature_column.numeric_column(&quot;longitude&quot;) bucketized_latitude = tf.feature_column.bucketized_column( latitude, boundaries=list(np.linspace(32., 42., 20 - 1))) bucketized_longitude = tf.feature_column.bucketized_column( longitude, boundaries=list(np.linspace(-125., -114., 20 - 1))) location = tf.feature_column.crossed_column( [bucketized_latitude, bucketized_longitude], hash_bucket_size=1000) . ocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity) . ocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity, dimension=2) . Using Feature Columns for Parsing . median_house_value = tf.feature_column.numeric_column(&quot;median_house_value&quot;) . columns = [housing_median_age, median_house_value] feature_descriptions = tf.feature_column.make_parse_example_spec(columns) feature_descriptions . {&#39;housing_median_age&#39;: FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None), &#39;median_house_value&#39;: FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None)} . with tf.io.TFRecordWriter(&quot;my_data_with_features.tfrecords&quot;) as f: for x, y in zip(X_train[:, 1:2], y_train): example = Example(features=Features(feature={ &quot;housing_median_age&quot;: Feature(float_list=FloatList(value=[x])), &quot;median_house_value&quot;: Feature(float_list=FloatList(value=[y])) })) f.write(example.SerializeToString()) . def parse_examples(serialized_examples): examples = tf.io.parse_example(serialized_examples, feature_descriptions) targets = examples.pop(&quot;median_house_value&quot;) # separate the targets return examples, targets batch_size = 32 dataset = tf.data.TFRecordDataset([&quot;my_data_with_features.tfrecords&quot;]) dataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples) . Warning: the DenseFeatures layer currently does not work with the Functional API, see TF issue #27416. Hopefully this will be resolved before the final release of TF 2.0. . columns_without_target = columns[:-1] model = keras.models.Sequential([ keras.layers.DenseFeatures(feature_columns=columns_without_target), keras.layers.Dense(1) ]) model.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) model.fit(dataset, steps_per_epoch=len(X_train) // batch_size, epochs=5) . Epoch 1/5 362/362 [==============================] - 1s 3ms/step - loss: 5.0263 - accuracy: 0.0017 Epoch 2/5 362/362 [==============================] - 1s 2ms/step - loss: 2.1655 - accuracy: 0.0036 Epoch 3/5 362/362 [==============================] - 1s 2ms/step - loss: 1.5688 - accuracy: 0.0028 Epoch 4/5 362/362 [==============================] - 1s 2ms/step - loss: 1.3550 - accuracy: 0.0022 Epoch 5/5 362/362 [==============================] - 1s 2ms/step - loss: 1.3466 - accuracy: 0.0035 . &lt;tensorflow.python.keras.callbacks.History at 0x134ff5f98&gt; . some_columns = [ocean_proximity_embed, bucketized_income] dense_features = keras.layers.DenseFeatures(some_columns) dense_features({ &quot;ocean_proximity&quot;: [[&quot;NEAR OCEAN&quot;], [&quot;INLAND&quot;], [&quot;INLAND&quot;]], &quot;median_income&quot;: [[3.], [7.2], [1.]] }) . WARNING: Logging before flag parsing goes to stderr. W0314 09:13:03.443593 140735783818112 deprecation.py:323] From /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3038: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version. Instructions for updating: The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead. W0314 09:13:03.451946 140735783818112 deprecation.py:323] From /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py:1347: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.cast` instead. . &lt;tf.Tensor: id=551231, shape=(3, 7), dtype=float32, numpy= array([[ 0. , 0. , 1. , 0. , 0. , 0.30818242, -0.40305588], [ 0. , 0. , 0. , 0. , 1. , -0.7438171 , 0.19444346], [ 1. , 0. , 0. , 0. , 0. , -0.7438171 , 0.19444346]], dtype=float32)&gt; . TF Transform . try: import tensorflow_transform as tft def preprocess(inputs): # inputs is a batch of input features median_age = inputs[&quot;housing_median_age&quot;] ocean_proximity = inputs[&quot;ocean_proximity&quot;] standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age)) ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity) return { &quot;standardized_median_age&quot;: standardized_age, &quot;ocean_proximity_id&quot;: ocean_proximity_id } except ImportError: print(&quot;TF Transform is not installed. Try running: pip3 install -U tensorflow-transform&quot;) . TF Transform is not installed. Try running: pip3 install -U tensorflow-transform . TensorFlow Datasets . import tensorflow_datasets as tfds datasets = tfds.load(name=&quot;mnist&quot;) mnist_train, mnist_test = datasets[&quot;train&quot;], datasets[&quot;test&quot;] . print(tfds.list_builders()) . [&#39;bair_robot_pushing_small&#39;, &#39;cats_vs_dogs&#39;, &#39;celeb_a&#39;, &#39;celeb_a_hq&#39;, &#39;chexpert&#39;, &#39;cifar10&#39;, &#39;cifar100&#39;, &#39;coco2014&#39;, &#39;colorectal_histology&#39;, &#39;colorectal_histology_large&#39;, &#39;diabetic_retinopathy_detection&#39;, &#39;dummy_dataset_shared_generator&#39;, &#39;dummy_mnist&#39;, &#39;fashion_mnist&#39;, &#39;flores_translate_neen&#39;, &#39;flores_translate_sien&#39;, &#39;horses_or_humans&#39;, &#39;image_label_folder&#39;, &#39;imagenet2012&#39;, &#39;imdb_reviews&#39;, &#39;kmnist&#39;, &#39;lm1b&#39;, &#39;lsun&#39;, &#39;mnist&#39;, &#39;moving_mnist&#39;, &#39;multi_nli&#39;, &#39;nsynth&#39;, &#39;omniglot&#39;, &#39;open_images_v4&#39;, &#39;quickdraw_bitmap&#39;, &#39;rock_paper_scissors&#39;, &#39;squad&#39;, &#39;starcraft_video&#39;, &#39;svhn_cropped&#39;, &#39;ted_hrlr_translate&#39;, &#39;ted_multi_translate&#39;, &#39;tf_flowers&#39;, &#39;titanic&#39;, &#39;wmt_translate_ende&#39;, &#39;wmt_translate_enfr&#39;] . plt.figure(figsize=(6,3)) mnist_train = mnist_train.repeat(5).batch(32).prefetch(1) for item in mnist_train: images = item[&quot;image&quot;] labels = item[&quot;label&quot;] for index in range(5): plt.subplot(1, 5, index + 1) image = images[index, ..., 0] label = labels[index].numpy() plt.imshow(image, cmap=&quot;binary&quot;) plt.title(label) plt.axis(&quot;off&quot;) break # just showing part of the first batch . datasets = tfds.load(name=&quot;mnist&quot;) mnist_train, mnist_test = datasets[&quot;train&quot;], datasets[&quot;test&quot;] mnist_train = mnist_train.repeat(5).batch(32) mnist_train = mnist_train.map(lambda items: (items[&quot;image&quot;], items[&quot;label&quot;])) mnist_train = mnist_train.prefetch(1) for images, labels in mnist_train.take(1): print(images.shape) print(labels.numpy()) . (32, 28, 28, 1) [1 3 8 6 0 7 7 9 2 7 3 7 6 0 4 9 8 1 5 6 0 8 0 3 7 2 6 3 3 5 5 4] . datasets = tfds.load(name=&quot;mnist&quot;, batch_size=32, as_supervised=True) mnist_train = datasets[&quot;train&quot;].repeat().prefetch(1) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28, 1]), keras.layers.Lambda(lambda images: tf.cast(images, tf.float32)), keras.layers.Dense(10, activation=&quot;softmax&quot;)]) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[&quot;accuracy&quot;]) model.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5) . Epoch 1/5 1875/1875 [==============================] - 7s 4ms/step - loss: 31.4932 - accuracy: 0.8428 Epoch 2/5 1875/1875 [==============================] - 7s 4ms/step - loss: 26.2404 - accuracy: 0.8693 Epoch 3/5 1875/1875 [==============================] - 7s 4ms/step - loss: 24.9957 - accuracy: 0.8743 Epoch 4/5 1875/1875 [==============================] - 7s 4ms/step - loss: 24.0785 - accuracy: 0.8780 Epoch 5/5 1875/1875 [==============================] - 7s 4ms/step - loss: 23.8285 - accuracy: 0.8782 . &lt;tensorflow.python.keras.callbacks.History at 0x131dad240&gt; . TensorFlow Hub . import tensorflow_hub as hub hub_layer = hub.KerasLayer(&quot;https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1&quot;, output_shape=[50], input_shape=[], dtype=tf.string) model = keras.Sequential() model.add(hub_layer) model.add(keras.layers.Dense(16, activation=&#39;relu&#39;)) model.add(keras.layers.Dense(1, activation=&#39;sigmoid&#39;)) model.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer (KerasLayer) (None, 50) 48190600 _________________________________________________________________ dense_4 (Dense) (None, 16) 816 _________________________________________________________________ dense_5 (Dense) (None, 1) 17 ================================================================= Total params: 48,191,433 Trainable params: 833 Non-trainable params: 48,190,600 _________________________________________________________________ . sentences = tf.constant([&quot;It was a great movie&quot;, &quot;The actors were amazing&quot;]) embeddings = hub_layer(sentences) . embeddings . &lt;tf.Tensor: id=665258, shape=(2, 50), dtype=float32, numpy= array([[ 7.45939985e-02, 2.76720114e-02, 9.38646123e-02, 1.25124469e-01, 5.40293928e-04, -1.09435350e-01, 1.34755149e-01, -9.57818255e-02, -1.85177118e-01, -1.69703495e-02, 1.75612606e-02, -9.06603858e-02, 1.12110220e-01, 1.04646273e-01, 3.87700424e-02, -7.71859884e-02, -3.12189370e-01, 6.99466765e-02, -4.88970093e-02, -2.99049795e-01, 1.31183028e-01, -2.12630898e-01, 6.96169436e-02, 1.63592950e-01, 1.05169769e-02, 7.79720694e-02, -2.55230188e-01, -1.80790052e-01, 2.93739915e-01, 1.62875261e-02, -2.80566931e-01, 1.60284728e-01, 9.87277832e-03, 8.44555616e-04, 8.39456245e-02, 3.24002892e-01, 1.53253034e-01, -3.01048346e-02, 8.94618109e-02, -2.39153411e-02, -1.50188789e-01, -1.81733668e-02, -1.20483577e-01, 1.32937476e-01, -3.35325629e-01, -1.46504581e-01, -1.25251599e-02, -1.64428815e-01, -7.00765476e-02, 3.60923223e-02], [-1.56998575e-01, 4.24599349e-02, -5.57703003e-02, -8.08446854e-03, 1.23733155e-01, 3.89427543e-02, -4.37901802e-02, -1.86987907e-01, -2.29341656e-01, -1.27766818e-01, 3.83025259e-02, -1.07057482e-01, -6.11584112e-02, 2.49654502e-01, -1.39712945e-01, -3.91289443e-02, -1.35873526e-01, -3.58613044e-01, 2.53462754e-02, -1.58370987e-01, -1.38350084e-01, -3.90771806e-01, -6.63642734e-02, -3.24838236e-02, -2.20453963e-02, -1.68282315e-01, -7.40613639e-02, -2.49074101e-02, 2.46460736e-01, 9.87201929e-05, -1.85390845e-01, -4.92824614e-02, 1.09015472e-01, -9.54203904e-02, -1.60352528e-01, -2.59811729e-02, 1.13778859e-01, -2.09578887e-01, 2.18261331e-01, -3.11211571e-02, -6.12562597e-02, -8.66057724e-02, -1.10762455e-01, -5.73977083e-03, -1.08923554e-01, -1.72919363e-01, 1.00515485e-01, -5.64153939e-02, -4.97694984e-02, -1.07776590e-01]], dtype=float32)&gt; .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/_loading_and_preprocessing_data.html",
            "relUrl": "/2020/03/09/_loading_and_preprocessing_data.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Ensemble Learning and Random Forests",
            "content": "This notebook contains all the sample code and solutions to the exercises in chapter 7. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;ensembles&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . . Voting classifiers . heads_proba = 0.51 coin_tosses = (np.random.rand(10000, 10) &lt; heads_proba).astype(np.int32) cumulative_heads_ratio = np.cumsum(coin_tosses, axis=0) / np.arange(1, 10001).reshape(-1, 1) . #collapse-show plt.figure(figsize=(8,3.5)) plt.plot(cumulative_heads_ratio) plt.plot([0, 10000], [0.51, 0.51], &quot;k--&quot;, linewidth=2, label=&quot;51%&quot;) plt.plot([0, 10000], [0.5, 0.5], &quot;k-&quot;, label=&quot;50%&quot;) plt.xlabel(&quot;Number of coin tosses&quot;) plt.ylabel(&quot;Heads ratio&quot;) plt.legend(loc=&quot;lower right&quot;) plt.axis([0, 10000, 0.42, 0.58]) save_fig(&quot;law_of_large_numbers_plot&quot;) plt.show() . . Saving figure law_of_large_numbers_plot . from sklearn.model_selection import train_test_split from sklearn.datasets import make_moons X, y = make_moons(n_samples=500, noise=0.30, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) . Note: to be future-proof, we set solver=&quot;lbfgs&quot;, n_estimators=100, and gamma=&quot;scale&quot; since these will be the default values in upcoming Scikit-Learn versions. . from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC log_clf = LogisticRegression(solver=&quot;lbfgs&quot;, random_state=42) rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42) svm_clf = SVC(gamma=&quot;scale&quot;, random_state=42) voting_clf = VotingClassifier( estimators=[(&#39;lr&#39;, log_clf), (&#39;rf&#39;, rnd_clf), (&#39;svc&#39;, svm_clf)], voting=&#39;hard&#39;) . voting_clf.fit(X_train, y_train) . VotingClassifier(estimators=[(&#39;lr&#39;, LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False)), (&#39;rf&#39;, RandomFor...f&#39;, max_iter=-1, probability=False, random_state=42, shrinking=True, tol=0.001, verbose=False))], flatten_transform=None, n_jobs=None, voting=&#39;hard&#39;, weights=None) . from sklearn.metrics import accuracy_score for clf in (log_clf, rnd_clf, svm_clf, voting_clf): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(clf.__class__.__name__, accuracy_score(y_test, y_pred)) . LogisticRegression 0.864 RandomForestClassifier 0.896 SVC 0.896 VotingClassifier 0.912 . Soft voting: . log_clf = LogisticRegression(solver=&quot;lbfgs&quot;, random_state=42) rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42) svm_clf = SVC(gamma=&quot;scale&quot;, probability=True, random_state=42) voting_clf = VotingClassifier( estimators=[(&#39;lr&#39;, log_clf), (&#39;rf&#39;, rnd_clf), (&#39;svc&#39;, svm_clf)], voting=&#39;soft&#39;) voting_clf.fit(X_train, y_train) . VotingClassifier(estimators=[(&#39;lr&#39;, LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False)), (&#39;rf&#39;, RandomFor...bf&#39;, max_iter=-1, probability=True, random_state=42, shrinking=True, tol=0.001, verbose=False))], flatten_transform=None, n_jobs=None, voting=&#39;soft&#39;, weights=None) . from sklearn.metrics import accuracy_score for clf in (log_clf, rnd_clf, svm_clf, voting_clf): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(clf.__class__.__name__, accuracy_score(y_test, y_pred)) . LogisticRegression 0.864 RandomForestClassifier 0.896 SVC 0.896 VotingClassifier 0.92 . Bagging ensembles . from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier bag_clf = BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=500, max_samples=100, bootstrap=True, random_state=42) bag_clf.fit(X_train, y_train) y_pred = bag_clf.predict(X_test) . from sklearn.metrics import accuracy_score print(accuracy_score(y_test, y_pred)) . 0.904 . tree_clf = DecisionTreeClassifier(random_state=42) tree_clf.fit(X_train, y_train) y_pred_tree = tree_clf.predict(X_test) print(accuracy_score(y_test, y_pred_tree)) . 0.856 . #collapse-show from matplotlib.colors import ListedColormap def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True): x1s = np.linspace(axes[0], axes[1], 100) x2s = np.linspace(axes[2], axes[3], 100) x1, x2 = np.meshgrid(x1s, x2s) X_new = np.c_[x1.ravel(), x2.ravel()] y_pred = clf.predict(X_new).reshape(x1.shape) custom_cmap = ListedColormap([&#39;#fafab0&#39;,&#39;#9898ff&#39;,&#39;#a0faa0&#39;]) plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap) if contour: custom_cmap2 = ListedColormap([&#39;#7d7d58&#39;,&#39;#4c4c7f&#39;,&#39;#507d50&#39;]) plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8) plt.plot(X[:, 0][y==0], X[:, 1][y==0], &quot;yo&quot;, alpha=alpha) plt.plot(X[:, 0][y==1], X[:, 1][y==1], &quot;bs&quot;, alpha=alpha) plt.axis(axes) plt.xlabel(r&quot;$x_1$&quot;, fontsize=18) plt.ylabel(r&quot;$x_2$&quot;, fontsize=18, rotation=0) . . #collapse-show fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True) plt.sca(axes[0]) plot_decision_boundary(tree_clf, X, y) plt.title(&quot;Decision Tree&quot;, fontsize=14) plt.sca(axes[1]) plot_decision_boundary(bag_clf, X, y) plt.title(&quot;Decision Trees with Bagging&quot;, fontsize=14) plt.ylabel(&quot;&quot;) save_fig(&quot;decision_tree_without_and_with_bagging_plot&quot;) plt.show() . . Saving figure decision_tree_without_and_with_bagging_plot . Random Forests . bag_clf = BaggingClassifier( DecisionTreeClassifier(splitter=&quot;random&quot;, max_leaf_nodes=16, random_state=42), n_estimators=500, max_samples=1.0, bootstrap=True, random_state=42) . bag_clf.fit(X_train, y_train) y_pred = bag_clf.predict(X_test) . from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42) rnd_clf.fit(X_train, y_train) y_pred_rf = rnd_clf.predict(X_test) . np.sum(y_pred == y_pred_rf) / len(y_pred) # almost identical predictions . 0.976 . from sklearn.datasets import load_iris iris = load_iris() rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42) rnd_clf.fit(iris[&quot;data&quot;], iris[&quot;target&quot;]) for name, score in zip(iris[&quot;feature_names&quot;], rnd_clf.feature_importances_): print(name, score) . sepal length (cm) 0.11249225099876374 sepal width (cm) 0.023119288282510326 petal length (cm) 0.44103046436395765 petal width (cm) 0.4233579963547681 . rnd_clf.feature_importances_ . array([0.11249225, 0.02311929, 0.44103046, 0.423358 ]) . #collapse-show plt.figure(figsize=(6, 4)) for i in range(15): tree_clf = DecisionTreeClassifier(max_leaf_nodes=16, random_state=42 + i) indices_with_replacement = np.random.randint(0, len(X_train), len(X_train)) tree_clf.fit(X[indices_with_replacement], y[indices_with_replacement]) plot_decision_boundary(tree_clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.02, contour=False) plt.show() . . Out-of-Bag evaluation . bag_clf = BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=500, bootstrap=True, oob_score=True, random_state=40) bag_clf.fit(X_train, y_train) bag_clf.oob_score_ . 0.9013333333333333 . bag_clf.oob_decision_function_ . array([[0.31746032, 0.68253968], [0.34117647, 0.65882353], [1. , 0. ], [0. , 1. ], [0. , 1. ], [0.08379888, 0.91620112], [0.31693989, 0.68306011], [0.02923977, 0.97076023], [0.97687861, 0.02312139], [0.97765363, 0.02234637], [0.74404762, 0.25595238], [0. , 1. ], [0.71195652, 0.28804348], [0.83957219, 0.16042781], [0.97777778, 0.02222222], [0.0625 , 0.9375 ], [0. , 1. ], [0.97297297, 0.02702703], [0.95238095, 0.04761905], [1. , 0. ], [0.01704545, 0.98295455], [0.38947368, 0.61052632], [0.88700565, 0.11299435], [1. , 0. ], [0.96685083, 0.03314917], [0. , 1. ], [0.99428571, 0.00571429], [1. , 0. ], [0. , 1. ], [0.64804469, 0.35195531], [0. , 1. ], [1. , 0. ], [0. , 1. ], [0. , 1. ], [0.13402062, 0.86597938], [1. , 0. ], [0. , 1. ], [0.36065574, 0.63934426], [0. , 1. ], [1. , 0. ], [0.27093596, 0.72906404], [0.34146341, 0.65853659], [1. , 0. ], [1. , 0. ], [0. , 1. ], [1. , 0. ], [1. , 0. ], [0. , 1. ], [1. , 0. ], [0.00531915, 0.99468085], [0.98265896, 0.01734104], [0.91428571, 0.08571429], [0.97282609, 0.02717391], [0.97029703, 0.02970297], [0. , 1. ], [0.06134969, 0.93865031], [0.98019802, 0.01980198], [0. , 1. ], [0. , 1. ], [0. , 1. ], [0.97790055, 0.02209945], [0.79473684, 0.20526316], [0.41919192, 0.58080808], [0.99473684, 0.00526316], [0. , 1. ], [0.67613636, 0.32386364], [1. , 0. ], [1. , 0. ], [0.87356322, 0.12643678], [1. , 0. ], [0.56140351, 0.43859649], [0.16304348, 0.83695652], [0.67539267, 0.32460733], [0.90673575, 0.09326425], [0. , 1. ], [0.16201117, 0.83798883], [0.89005236, 0.10994764], [1. , 0. ], [0. , 1. ], [0.995 , 0.005 ], [0. , 1. ], [0.07272727, 0.92727273], [0.05418719, 0.94581281], [0.29533679, 0.70466321], [1. , 0. ], [0. , 1. ], [0.81871345, 0.18128655], [0.01092896, 0.98907104], [0. , 1. ], [0. , 1. ], [0.22513089, 0.77486911], [1. , 0. ], [0. , 1. ], [0. , 1. ], [0. , 1. ], [0.9368932 , 0.0631068 ], [0.76536313, 0.23463687], [0. , 1. ], [1. , 0. ], [0.17127072, 0.82872928], [0.65306122, 0.34693878], [0. , 1. ], [0.03076923, 0.96923077], [0.49444444, 0.50555556], [1. , 0. ], [0.02673797, 0.97326203], [0.98870056, 0.01129944], [0.23121387, 0.76878613], [0.5 , 0.5 ], [0.9947644 , 0.0052356 ], [0.00555556, 0.99444444], [0.98963731, 0.01036269], [0.25641026, 0.74358974], [0.92972973, 0.07027027], [1. , 0. ], [1. , 0. ], [0. , 1. ], [0. , 1. ], [0.80681818, 0.19318182], [1. , 0. ], [0.0106383 , 0.9893617 ], [1. , 0. ], [1. , 0. ], [1. , 0. ], [0.98181818, 0.01818182], [1. , 0. ], [0.01036269, 0.98963731], [0.97752809, 0.02247191], [0.99453552, 0.00546448], [0.01960784, 0.98039216], [0.18367347, 0.81632653], [0.98387097, 0.01612903], [0.29533679, 0.70466321], [0.98295455, 0.01704545], [0. , 1. ], [0.00561798, 0.99438202], [0.75138122, 0.24861878], [0.38624339, 0.61375661], [0.42708333, 0.57291667], [0.86315789, 0.13684211], [0.92964824, 0.07035176], [0.05699482, 0.94300518], [0.82802548, 0.17197452], [0.01546392, 0.98453608], [0. , 1. ], [0.02298851, 0.97701149], [0.96721311, 0.03278689], [1. , 0. ], [1. , 0. ], [0.01041667, 0.98958333], [0. , 1. ], [0.0326087 , 0.9673913 ], [0.01020408, 0.98979592], [1. , 0. ], [1. , 0. ], [0.93785311, 0.06214689], [1. , 0. ], [1. , 0. ], [0.99462366, 0.00537634], [0. , 1. ], [0.38860104, 0.61139896], [0.32065217, 0.67934783], [0. , 1. ], [0. , 1. ], [0.31182796, 0.68817204], [1. , 0. ], [1. , 0. ], [0. , 1. ], [1. , 0. ], [0.00588235, 0.99411765], [0. , 1. ], [0.98387097, 0.01612903], [0. , 1. ], [0. , 1. ], [1. , 0. ], [0. , 1. ], [0.62264151, 0.37735849], [0.92344498, 0.07655502], [0. , 1. ], [0.99526066, 0.00473934], [1. , 0. ], [0.98888889, 0.01111111], [0. , 1. ], [0. , 1. ], [1. , 0. ], [0.06451613, 0.93548387], [1. , 0. ], [0.05154639, 0.94845361], [0. , 1. ], [1. , 0. ], [0. , 1. ], [0.03278689, 0.96721311], [1. , 0. ], [0.95808383, 0.04191617], [0.79532164, 0.20467836], [0.55665025, 0.44334975], [0. , 1. ], [0.18604651, 0.81395349], [1. , 0. ], [0.93121693, 0.06878307], [0.97740113, 0.02259887], [1. , 0. ], [0.00531915, 0.99468085], [0. , 1. ], [0.44623656, 0.55376344], [0.86363636, 0.13636364], [0. , 1. ], [0. , 1. ], [1. , 0. ], [0.00558659, 0.99441341], [0. , 1. ], [0.96923077, 0.03076923], [0. , 1. ], [0.21649485, 0.78350515], [0. , 1. ], [1. , 0. ], [0. , 1. ], [0. , 1. ], [0.98477157, 0.01522843], [0.8 , 0.2 ], [0.99441341, 0.00558659], [0. , 1. ], [0.08379888, 0.91620112], [0.98984772, 0.01015228], [0.01142857, 0.98857143], [0. , 1. ], [0.02747253, 0.97252747], [1. , 0. ], [0.79144385, 0.20855615], [0. , 1. ], [0.90804598, 0.09195402], [0.98387097, 0.01612903], [0.20634921, 0.79365079], [0.19767442, 0.80232558], [1. , 0. ], [0. , 1. ], [0. , 1. ], [0. , 1. ], [0.20338983, 0.79661017], [0.98181818, 0.01818182], [0. , 1. ], [1. , 0. ], [0.98969072, 0.01030928], [0. , 1. ], [0.48663102, 0.51336898], [1. , 0. ], [0. , 1. ], [1. , 0. ], [0. , 1. ], [0. , 1. ], [0.07821229, 0.92178771], [0.11176471, 0.88823529], [0.99415205, 0.00584795], [0.03015075, 0.96984925], [1. , 0. ], [0.40837696, 0.59162304], [0.04891304, 0.95108696], [0.51595745, 0.48404255], [0.51898734, 0.48101266], [0. , 1. ], [1. , 0. ], [0. , 1. ], [0. , 1. ], [0.59903382, 0.40096618], [0. , 1. ], [1. , 0. ], [0.24157303, 0.75842697], [0.81052632, 0.18947368], [0.08717949, 0.91282051], [0.99453552, 0.00546448], [0.82142857, 0.17857143], [0. , 1. ], [0. , 1. ], [0.125 , 0.875 ], [0.04712042, 0.95287958], [0. , 1. ], [1. , 0. ], [0.89150943, 0.10849057], [0.1978022 , 0.8021978 ], [0.95238095, 0.04761905], [0.00515464, 0.99484536], [0.609375 , 0.390625 ], [0.07692308, 0.92307692], [0.99484536, 0.00515464], [0.84210526, 0.15789474], [0. , 1. ], [0.99484536, 0.00515464], [0.95876289, 0.04123711], [0. , 1. ], [0. , 1. ], [1. , 0. ], [0. , 1. ], [1. , 0. ], [0.26903553, 0.73096447], [0.98461538, 0.01538462], [1. , 0. ], [0. , 1. ], [0.00574713, 0.99425287], [0.85142857, 0.14857143], [0. , 1. ], [1. , 0. ], [0.76506024, 0.23493976], [0.8969697 , 0.1030303 ], [1. , 0. ], [0.73333333, 0.26666667], [0.47727273, 0.52272727], [0. , 1. ], [0.92473118, 0.07526882], [0. , 1. ], [1. , 0. ], [0.87709497, 0.12290503], [1. , 0. ], [1. , 0. ], [0.74752475, 0.25247525], [0.09146341, 0.90853659], [0.44329897, 0.55670103], [0.22395833, 0.77604167], [0. , 1. ], [0.87046632, 0.12953368], [0.78212291, 0.21787709], [0.00507614, 0.99492386], [1. , 0. ], [1. , 0. ], [1. , 0. ], [0. , 1. ], [0.02884615, 0.97115385], [0.96571429, 0.03428571], [0.93478261, 0.06521739], [1. , 0. ], [0.49756098, 0.50243902], [1. , 0. ], [0. , 1. ], [1. , 0. ], [0.01604278, 0.98395722], [1. , 0. ], [1. , 0. ], [1. , 0. ], [0. , 1. ], [0.96987952, 0.03012048], [0. , 1. ], [0.05747126, 0.94252874], [0. , 1. ], [0. , 1. ], [1. , 0. ], [1. , 0. ], [0. , 1. ], [0.98989899, 0.01010101], [0.01675978, 0.98324022], [1. , 0. ], [0.13541667, 0.86458333], [0. , 1. ], [0.00546448, 0.99453552], [0. , 1. ], [0.41836735, 0.58163265], [0.11309524, 0.88690476], [0.22110553, 0.77889447], [1. , 0. ], [0.97647059, 0.02352941], [0.22826087, 0.77173913], [0.98882682, 0.01117318], [0. , 1. ], [0. , 1. ], [1. , 0. ], [0.96428571, 0.03571429], [0.33507853, 0.66492147], [0.98235294, 0.01764706], [1. , 0. ], [0. , 1. ], [0.99465241, 0.00534759], [0. , 1. ], [0.06043956, 0.93956044], [0.97619048, 0.02380952], [1. , 0. ], [0.03108808, 0.96891192], [0.57291667, 0.42708333]]) . from sklearn.metrics import accuracy_score y_pred = bag_clf.predict(X_test) accuracy_score(y_test, y_pred) . 0.912 . Feature importance . from sklearn.datasets import fetch_openml mnist = fetch_openml(&#39;mnist_784&#39;, version=1) mnist.target = mnist.target.astype(np.uint8) . rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42) rnd_clf.fit(mnist[&quot;data&quot;], mnist[&quot;target&quot;]) . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . def plot_digit(data): image = data.reshape(28, 28) plt.imshow(image, cmap = mpl.cm.hot, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;) . plot_digit(rnd_clf.feature_importances_) cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()]) cbar.ax.set_yticklabels([&#39;Not important&#39;, &#39;Very important&#39;]) save_fig(&quot;mnist_feature_importance_plot&quot;) plt.show() . Saving figure mnist_feature_importance_plot . AdaBoost . from sklearn.ensemble import AdaBoostClassifier ada_clf = AdaBoostClassifier( DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=&quot;SAMME.R&quot;, learning_rate=0.5, random_state=42) ada_clf.fit(X_train, y_train) . AdaBoostClassifier(algorithm=&#39;SAMME.R&#39;, base_estimator=DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=1, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;), learning_rate=0.5, n_estimators=200, random_state=42) . plot_decision_boundary(ada_clf, X, y) . #collapse-show m = len(X_train) fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True) for subplot, learning_rate in ((0, 1), (1, 0.5)): sample_weights = np.ones(m) plt.sca(axes[subplot]) for i in range(5): svm_clf = SVC(kernel=&quot;rbf&quot;, C=0.05, gamma=&quot;scale&quot;, random_state=42) svm_clf.fit(X_train, y_train, sample_weight=sample_weights) y_pred = svm_clf.predict(X_train) sample_weights[y_pred != y_train] *= (1 + learning_rate) plot_decision_boundary(svm_clf, X, y, alpha=0.2) plt.title(&quot;learning_rate = {}&quot;.format(learning_rate), fontsize=16) if subplot == 0: plt.text(-0.7, -0.65, &quot;1&quot;, fontsize=14) plt.text(-0.6, -0.10, &quot;2&quot;, fontsize=14) plt.text(-0.5, 0.10, &quot;3&quot;, fontsize=14) plt.text(-0.4, 0.55, &quot;4&quot;, fontsize=14) plt.text(-0.3, 0.90, &quot;5&quot;, fontsize=14) else: plt.ylabel(&quot;&quot;) save_fig(&quot;boosting_plot&quot;) plt.show() . . Saving figure boosting_plot . list(m for m in dir(ada_clf) if not m.startswith(&quot;_&quot;) and m.endswith(&quot;_&quot;)) . [&#39;base_estimator_&#39;, &#39;classes_&#39;, &#39;estimator_errors_&#39;, &#39;estimator_weights_&#39;, &#39;estimators_&#39;, &#39;feature_importances_&#39;, &#39;n_classes_&#39;] . Gradient Boosting . np.random.seed(42) X = np.random.rand(100, 1) - 0.5 y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100) . from sklearn.tree import DecisionTreeRegressor tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42) tree_reg1.fit(X, y) . DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . y2 = y - tree_reg1.predict(X) tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42) tree_reg2.fit(X, y2) . DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . y3 = y2 - tree_reg2.predict(X) tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42) tree_reg3.fit(X, y3) . DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . X_new = np.array([[0.8]]) . y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3)) . y_pred . array([0.75026781]) . #collapse-show def plot_predictions(regressors, X, y, axes, label=None, style=&quot;r-&quot;, data_style=&quot;b.&quot;, data_label=None): x1 = np.linspace(axes[0], axes[1], 500) y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors) plt.plot(X[:, 0], y, data_style, label=data_label) plt.plot(x1, y_pred, style, linewidth=2, label=label) if label or data_label: plt.legend(loc=&quot;upper center&quot;, fontsize=16) plt.axis(axes) . . #collapse-show plt.figure(figsize=(11,11)) plt.subplot(321) plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=&quot;$h_1(x_1)$&quot;, style=&quot;g-&quot;, data_label=&quot;Training set&quot;) plt.ylabel(&quot;$y$&quot;, fontsize=16, rotation=0) plt.title(&quot;Residuals and tree predictions&quot;, fontsize=16) plt.subplot(322) plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=&quot;$h(x_1) = h_1(x_1)$&quot;, data_label=&quot;Training set&quot;) plt.ylabel(&quot;$y$&quot;, fontsize=16, rotation=0) plt.title(&quot;Ensemble predictions&quot;, fontsize=16) plt.subplot(323) plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=&quot;$h_2(x_1)$&quot;, style=&quot;g-&quot;, data_style=&quot;k+&quot;, data_label=&quot;Residuals&quot;) plt.ylabel(&quot;$y - h_1(x_1)$&quot;, fontsize=16) plt.subplot(324) plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=&quot;$h(x_1) = h_1(x_1) + h_2(x_1)$&quot;) plt.ylabel(&quot;$y$&quot;, fontsize=16, rotation=0) plt.subplot(325) plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=&quot;$h_3(x_1)$&quot;, style=&quot;g-&quot;, data_style=&quot;k+&quot;) plt.ylabel(&quot;$y - h_1(x_1) - h_2(x_1)$&quot;, fontsize=16) plt.xlabel(&quot;$x_1$&quot;, fontsize=16) plt.subplot(326) plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=&quot;$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$&quot;) plt.xlabel(&quot;$x_1$&quot;, fontsize=16) plt.ylabel(&quot;$y$&quot;, fontsize=16, rotation=0) save_fig(&quot;gradient_boosting_plot&quot;) plt.show() . . Saving figure gradient_boosting_plot . from sklearn.ensemble import GradientBoostingRegressor gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42) gbrt.fit(X, y) . GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=1.0, loss=&#39;ls&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=3, n_iter_no_change=None, presort=&#39;auto&#39;, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) . gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42) gbrt_slow.fit(X, y) . GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.1, loss=&#39;ls&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, presort=&#39;auto&#39;, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) . #collapse-show fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True) plt.sca(axes[0]) plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=&quot;Ensemble predictions&quot;) plt.title(&quot;learning_rate={}, n_estimators={}&quot;.format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14) plt.xlabel(&quot;$x_1$&quot;, fontsize=16) plt.ylabel(&quot;$y$&quot;, fontsize=16, rotation=0) plt.sca(axes[1]) plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8]) plt.title(&quot;learning_rate={}, n_estimators={}&quot;.format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14) plt.xlabel(&quot;$x_1$&quot;, fontsize=16) save_fig(&quot;gbrt_learning_rate_plot&quot;) plt.show() . . Saving figure gbrt_learning_rate_plot . Gradient Boosting with Early stopping . #collapse-show import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49) gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42) gbrt.fit(X_train, y_train) errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)] bst_n_estimators = np.argmin(errors) + 1 gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42) gbrt_best.fit(X_train, y_train) . . GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.1, loss=&#39;ls&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=56, n_iter_no_change=None, presort=&#39;auto&#39;, random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) . min_error = np.min(errors) . #collapse-show plt.figure(figsize=(10, 4)) plt.subplot(121) plt.plot(errors, &quot;b.-&quot;) plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], &quot;k--&quot;) plt.plot([0, 120], [min_error, min_error], &quot;k--&quot;) plt.plot(bst_n_estimators, min_error, &quot;ko&quot;) plt.text(bst_n_estimators, min_error*1.2, &quot;Minimum&quot;, ha=&quot;center&quot;, fontsize=14) plt.axis([0, 120, 0, 0.01]) plt.xlabel(&quot;Number of trees&quot;) plt.ylabel(&quot;Error&quot;, fontsize=16) plt.title(&quot;Validation error&quot;, fontsize=14) plt.subplot(122) plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8]) plt.title(&quot;Best model (%d trees)&quot; % bst_n_estimators, fontsize=14) plt.ylabel(&quot;$y$&quot;, fontsize=16, rotation=0) plt.xlabel(&quot;$x_1$&quot;, fontsize=16) save_fig(&quot;early_stopping_gbrt_plot&quot;) plt.show() . . Saving figure early_stopping_gbrt_plot . gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42) min_val_error = float(&quot;inf&quot;) error_going_up = 0 for n_estimators in range(1, 120): gbrt.n_estimators = n_estimators gbrt.fit(X_train, y_train) y_pred = gbrt.predict(X_val) val_error = mean_squared_error(y_val, y_pred) if val_error &lt; min_val_error: min_val_error = val_error error_going_up = 0 else: error_going_up += 1 if error_going_up == 5: break # early stopping . print(gbrt.n_estimators) . 61 . print(&quot;Minimum validation MSE:&quot;, min_val_error) . Minimum validation MSE: 0.002712853325235463 . Using XGBoost . try: import xgboost except ImportError as ex: print(&quot;Error: the xgboost library is not installed.&quot;) xgboost = None . if xgboost is not None: # not shown in the book xgb_reg = xgboost.XGBRegressor(random_state=42) xgb_reg.fit(X_train, y_train) y_pred = xgb_reg.predict(X_val) val_error = mean_squared_error(y_val, y_pred) # Not shown print(&quot;Validation MSE:&quot;, val_error) # Not shown . [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. Validation MSE: 0.0028512559726563943 . if xgboost is not None: # not shown in the book xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2) y_pred = xgb_reg.predict(X_val) val_error = mean_squared_error(y_val, y_pred) # Not shown print(&quot;Validation MSE:&quot;, val_error) # Not shown . [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [0] validation_0-rmse:0.286719 Will train until validation_0-rmse hasn&#39;t improved in 2 rounds. [1] validation_0-rmse:0.258221 [2] validation_0-rmse:0.232634 [3] validation_0-rmse:0.210526 [4] validation_0-rmse:0.190232 [5] validation_0-rmse:0.172196 [6] validation_0-rmse:0.156394 [7] validation_0-rmse:0.142241 [8] validation_0-rmse:0.129789 [9] validation_0-rmse:0.118752 [10] validation_0-rmse:0.108388 [11] validation_0-rmse:0.100155 [12] validation_0-rmse:0.09208 [13] validation_0-rmse:0.084791 [14] validation_0-rmse:0.078699 [15] validation_0-rmse:0.073248 [16] validation_0-rmse:0.069391 [17] validation_0-rmse:0.066277 [18] validation_0-rmse:0.063458 [19] validation_0-rmse:0.060326 [20] validation_0-rmse:0.0578 [21] validation_0-rmse:0.055643 [22] validation_0-rmse:0.053943 [23] validation_0-rmse:0.053138 [24] validation_0-rmse:0.052415 [25] validation_0-rmse:0.051821 [26] validation_0-rmse:0.051226 [27] validation_0-rmse:0.051135 [28] validation_0-rmse:0.05091 [29] validation_0-rmse:0.050893 [30] validation_0-rmse:0.050725 [31] validation_0-rmse:0.050471 [32] validation_0-rmse:0.050285 [33] validation_0-rmse:0.050492 [34] validation_0-rmse:0.050348 Stopping. Best iteration: [32] validation_0-rmse:0.050285 Validation MSE: 0.002528626115371327 . %timeit xgboost.XGBRegressor().fit(X_train, y_train) if xgboost is not None else None . [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. &lt;&lt;742 more lines&gt;&gt; [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [16:33:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. 4.29 ms Â± 46.2 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each) . %timeit GradientBoostingRegressor().fit(X_train, y_train) . 12.9 ms Â± 827 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each) . Exercise solutions . 1. to 7. . See Appendix A. . 8. Voting Classifier . Exercise: Load the MNIST data and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). . The MNIST dataset was loaded earlier. . from sklearn.model_selection import train_test_split . X_train_val, X_test, y_train_val, y_test = train_test_split( mnist.data, mnist.target, test_size=10000, random_state=42) X_train, X_val, y_train, y_val = train_test_split( X_train_val, y_train_val, test_size=10000, random_state=42) . Exercise: Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. . from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier from sklearn.svm import LinearSVC from sklearn.neural_network import MLPClassifier . random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42) extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42) svm_clf = LinearSVC(random_state=42) mlp_clf = MLPClassifier(random_state=42) . estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf] for estimator in estimators: print(&quot;Training the&quot;, estimator) estimator.fit(X_train, y_train) . Training the RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) Training the ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) Training the LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=42, tol=0.0001, verbose=0) . /Users/ageron/miniconda3/envs/tf2b/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. &#34;the number of iterations.&#34;, ConvergenceWarning) . Training the MLPClassifier(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate=&#39;constant&#39;, learning_rate_init=0.001, max_iter=200, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True, solver=&#39;adam&#39;, tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False) . [estimator.score(X_val, y_val) for estimator in estimators] . [0.9692, 0.9715, 0.8641, 0.9603] . The linear SVM is far outperformed by the other classifiers. However, let&#39;s keep it for now since it may improve the voting classifier&#39;s performance. . Exercise: Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. . from sklearn.ensemble import VotingClassifier . named_estimators = [ (&quot;random_forest_clf&quot;, random_forest_clf), (&quot;extra_trees_clf&quot;, extra_trees_clf), (&quot;svm_clf&quot;, svm_clf), (&quot;mlp_clf&quot;, mlp_clf), ] . voting_clf = VotingClassifier(named_estimators) . voting_clf.fit(X_train, y_train) . /Users/ageron/miniconda3/envs/tf2b/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. &#34;the number of iterations.&#34;, ConvergenceWarning) . VotingClassifier(estimators=[(&#39;random_forest_clf&#39;, RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, ...=True, solver=&#39;adam&#39;, tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False))], flatten_transform=None, n_jobs=None, voting=&#39;hard&#39;, weights=None) . voting_clf.score(X_val, y_val) . 0.9704 . [estimator.score(X_val, y_val) for estimator in voting_clf.estimators_] . [0.9692, 0.9715, 0.8641, 0.9603] . Let&#39;s remove the SVM to see if performance improves. It is possible to remove an estimator by setting it to None using set_params() like this: . voting_clf.set_params(svm_clf=None) . VotingClassifier(estimators=[(&#39;random_forest_clf&#39;, RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, ...=True, solver=&#39;adam&#39;, tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False))], flatten_transform=None, n_jobs=None, voting=&#39;hard&#39;, weights=None) . This updated the list of estimators: . voting_clf.estimators . [(&#39;random_forest_clf&#39;, RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False)), (&#39;extra_trees_clf&#39;, ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False)), (&#39;svm_clf&#39;, None), (&#39;mlp_clf&#39;, MLPClassifier(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate=&#39;constant&#39;, learning_rate_init=0.001, max_iter=200, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True, solver=&#39;adam&#39;, tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False))] . However, it did not update the list of trained estimators: . voting_clf.estimators_ . [RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False), ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False), LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=42, tol=0.0001, verbose=0), MLPClassifier(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate=&#39;constant&#39;, learning_rate_init=0.001, max_iter=200, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True, solver=&#39;adam&#39;, tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False)] . So we can either fit the VotingClassifier again, or just remove the SVM from the list of trained estimators: . del voting_clf.estimators_[2] . Now let&#39;s evaluate the VotingClassifier again: . voting_clf.score(X_val, y_val) . 0.9732 . A bit better! The SVM was hurting performance. Now let&#39;s try using a soft voting classifier. We do not actually need to retrain the classifier, we can just set voting to &quot;soft&quot;: . voting_clf.voting = &quot;soft&quot; . voting_clf.score(X_val, y_val) . 0.9672 . Nope, hard voting wins in this case. . Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers? . voting_clf.voting = &quot;hard&quot; voting_clf.score(X_test, y_test) . 0.9704 . [estimator.score(X_test, y_test) for estimator in voting_clf.estimators_] . [0.9645, 0.9691, 0.9602] . The voting classifier only very slightly reduced the error rate of the best model in this case. . 9. Stacking Ensemble . Exercise: Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image&#39;s class. Train a classifier on this new training set. . X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32) for index, estimator in enumerate(estimators): X_val_predictions[:, index] = estimator.predict(X_val) . X_val_predictions . array([[5., 5., 5., 5.], [8., 8., 8., 8.], [2., 2., 2., 2.], ..., [7., 7., 7., 7.], [6., 6., 6., 6.], [7., 7., 7., 7.]], dtype=float32) . rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42) rnd_forest_blender.fit(X_val_predictions, y_val) . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None, oob_score=True, random_state=42, verbose=0, warm_start=False) . rnd_forest_blender.oob_score_ . 0.9696 . You could fine-tune this blender or try other types of blenders (e.g., an MLPClassifier), then select the best one using cross-validation, as always. . Exercise: Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let&#39;s evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble&#39;s predictions. How does it compare to the voting classifier you trained earlier? . X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32) for index, estimator in enumerate(estimators): X_test_predictions[:, index] = estimator.predict(X_test) . y_pred = rnd_forest_blender.predict(X_test_predictions) . from sklearn.metrics import accuracy_score . accuracy_score(y_test, y_pred) . 0.9669 . This stacking ensemble does not perform as well as the voting classifier we trained earlier, it&#39;s not quite as good as the best individual classifier. .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/07_ensemble_learning_and_random_forests",
            "relUrl": "/07_ensemble_learning_and_random_forests",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "End-to-end Machine Learning project",
            "content": "Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; # Common imports import numpy as np import os # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;end_to_end_project&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) # Ignore useless warnings (see SciPy issue #5998) import warnings warnings.filterwarnings(action=&quot;ignore&quot;, message=&quot;^internal gelsd&quot;) . . Get the data . #collapse-show import os import tarfile import urllib DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; HOUSING_PATH = os.path.join(&quot;datasets&quot;, &quot;housing&quot;) HOUSING_URL = DOWNLOAD_ROOT + &quot;datasets/housing/housing.tgz&quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): if not os.path.isdir(housing_path): os.makedirs(housing_path) tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close() . . fetch_housing_data() . import pandas as pd def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(housing_path, &quot;housing.csv&quot;) return pd.read_csv(csv_path) . housing = load_housing_data() housing.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . housing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): longitude 20640 non-null float64 latitude 20640 non-null float64 housing_median_age 20640 non-null float64 total_rooms 20640 non-null float64 total_bedrooms 20433 non-null float64 population 20640 non-null float64 households 20640 non-null float64 median_income 20640 non-null float64 median_house_value 20640 non-null float64 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . housing[&quot;ocean_proximity&quot;].value_counts() . &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 . housing.describe() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . count 20640.000000 | 20640.000000 | 20640.000000 | 20640.000000 | 20433.000000 | 20640.000000 | 20640.000000 | 20640.000000 | 20640.000000 | . mean -119.569704 | 35.631861 | 28.639486 | 2635.763081 | 537.870553 | 1425.476744 | 499.539680 | 3.870671 | 206855.816909 | . std 2.003532 | 2.135952 | 12.585558 | 2181.615252 | 421.385070 | 1132.462122 | 382.329753 | 1.899822 | 115395.615874 | . min -124.350000 | 32.540000 | 1.000000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | 0.499900 | 14999.000000 | . 25% -121.800000 | 33.930000 | 18.000000 | 1447.750000 | 296.000000 | 787.000000 | 280.000000 | 2.563400 | 119600.000000 | . 50% -118.490000 | 34.260000 | 29.000000 | 2127.000000 | 435.000000 | 1166.000000 | 409.000000 | 3.534800 | 179700.000000 | . 75% -118.010000 | 37.710000 | 37.000000 | 3148.000000 | 647.000000 | 1725.000000 | 605.000000 | 4.743250 | 264725.000000 | . max -114.310000 | 41.950000 | 52.000000 | 39320.000000 | 6445.000000 | 35682.000000 | 6082.000000 | 15.000100 | 500001.000000 | . %matplotlib inline import matplotlib.pyplot as plt housing.hist(bins=50, figsize=(20,15)) save_fig(&quot;attribute_histogram_plots&quot;) plt.show() . Saving figure attribute_histogram_plots . # to make this notebook&#39;s output identical at every run np.random.seed(42) . #collapse-show import numpy as np # For illustration only. Sklearn has train_test_split() def split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] . . train_set, test_set = split_train_test(housing, 0.2) len(train_set) . 16512 . len(test_set) . 4128 . #collapse-show from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set] . . The implementation of test_set_check() above works fine in both Python 2 and Python 3. In earlier releases, the following implementation was proposed, which supported any hash function, but was much slower and did not support Python 2: . import hashlib def test_set_check(identifier, test_ratio, hash=hashlib.md5): return hash(np.int64(identifier)).digest()[-1] &lt; 256 * test_ratio . If you want an implementation that supports any hash function and is compatible with both Python 2 and Python 3, here is one: . def test_set_check(identifier, test_ratio, hash=hashlib.md5): return bytearray(hash(np.int64(identifier)).digest())[-1] &lt; 256 * test_ratio . housing_with_id = housing.reset_index() # adds an `index` column train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;index&quot;) . housing_with_id[&quot;id&quot;] = housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;id&quot;) . test_set.head() . index longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity id . 8 8 | -122.26 | 37.84 | 42.0 | 2555.0 | 665.0 | 1206.0 | 595.0 | 2.0804 | 226700.0 | NEAR BAY | -122222.16 | . 10 10 | -122.26 | 37.85 | 52.0 | 2202.0 | 434.0 | 910.0 | 402.0 | 3.2031 | 281500.0 | NEAR BAY | -122222.15 | . 11 11 | -122.26 | 37.85 | 52.0 | 3503.0 | 752.0 | 1504.0 | 734.0 | 3.2705 | 241800.0 | NEAR BAY | -122222.15 | . 12 12 | -122.26 | 37.85 | 52.0 | 2491.0 | 474.0 | 1098.0 | 468.0 | 3.0750 | 213500.0 | NEAR BAY | -122222.15 | . 13 13 | -122.26 | 37.84 | 52.0 | 696.0 | 191.0 | 345.0 | 174.0 | 2.6736 | 191300.0 | NEAR BAY | -122222.16 | . from sklearn.model_selection import train_test_split train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) . test_set.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 20046 -119.01 | 36.06 | 25.0 | 1505.0 | NaN | 1392.0 | 359.0 | 1.6812 | 47700.0 | INLAND | . 3024 -119.46 | 35.14 | 30.0 | 2943.0 | NaN | 1565.0 | 584.0 | 2.5313 | 45800.0 | INLAND | . 15663 -122.44 | 37.80 | 52.0 | 3830.0 | NaN | 1310.0 | 963.0 | 3.4801 | 500001.0 | NEAR BAY | . 20484 -118.72 | 34.28 | 17.0 | 3051.0 | NaN | 1705.0 | 495.0 | 5.7376 | 218600.0 | &lt;1H OCEAN | . 9814 -121.93 | 36.62 | 34.0 | 2351.0 | NaN | 1063.0 | 428.0 | 3.7250 | 278000.0 | NEAR OCEAN | . housing[&quot;median_income&quot;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11a232400&gt; . housing[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) . housing[&quot;income_cat&quot;].value_counts() . 3 7236 2 6581 4 3639 5 2362 1 822 Name: income_cat, dtype: int64 . housing[&quot;income_cat&quot;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11a307c18&gt; . #collapse-show from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] . . strat_test_set[&quot;income_cat&quot;].value_counts() / len(strat_test_set) . 3 0.350533 2 0.318798 4 0.176357 5 0.114583 1 0.039729 Name: income_cat, dtype: float64 . housing[&quot;income_cat&quot;].value_counts() / len(housing) . 3 0.350581 2 0.318847 4 0.176308 5 0.114438 1 0.039826 Name: income_cat, dtype: float64 . #collapse-show def income_cat_proportions(data): return data[&quot;income_cat&quot;].value_counts() / len(data) train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(housing), &quot;Stratified&quot;: income_cat_proportions(strat_test_set), &quot;Random&quot;: income_cat_proportions(test_set), }).sort_index() compare_props[&quot;Rand. %error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 . . compare_props . Overall Stratified Random Rand. %error Strat. %error . 1 0.039826 | 0.039729 | 0.040213 | 0.973236 | -0.243309 | . 2 0.318847 | 0.318798 | 0.324370 | 1.732260 | -0.015195 | . 3 0.350581 | 0.350533 | 0.358527 | 2.266446 | -0.013820 | . 4 0.176308 | 0.176357 | 0.167393 | -5.056334 | 0.027480 | . 5 0.114438 | 0.114583 | 0.109496 | -4.318374 | 0.127011 | . for set_ in (strat_train_set, strat_test_set): set_.drop(&quot;income_cat&quot;, axis=1, inplace=True) . Discover and visualize the data to gain insights . housing = strat_train_set.copy() . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;) save_fig(&quot;bad_visualization_plot&quot;) . Saving figure bad_visualization_plot . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.1) save_fig(&quot;better_visualization_plot&quot;) . Saving figure better_visualization_plot . The argument sharex=False fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611 ). Thanks to Wilmer Arellano for pointing it out. . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(10,7), c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, sharex=False) plt.legend() save_fig(&quot;housing_prices_scatterplot&quot;) . Saving figure housing_prices_scatterplot . # Download the California image images_path = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, &quot;end_to_end_project&quot;) os.makedirs(images_path, exist_ok=True) DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; filename = &quot;california.png&quot; print(&quot;Downloading&quot;, filename) url = DOWNLOAD_ROOT + &quot;images/end_to_end_project/&quot; + filename urllib.request.urlretrieve(url, os.path.join(images_path, filename)) . Downloading california.png . (&#39;./images/end_to_end_project/california.png&#39;, &lt;http.client.HTTPMessage at 0x7efdc6f79588&gt;) . #collapse-show import matplotlib.image as mpimg california_img=mpimg.imread(os.path.join(images_path, filename)) ax = housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, figsize=(10,7), s=housing[&#39;population&#39;]/100, label=&quot;Population&quot;, c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=False, alpha=0.4, ) plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5, cmap=plt.get_cmap(&quot;jet&quot;)) plt.ylabel(&quot;Latitude&quot;, fontsize=14) plt.xlabel(&quot;Longitude&quot;, fontsize=14) prices = housing[&quot;median_house_value&quot;] tick_values = np.linspace(prices.min(), prices.max(), 11) cbar = plt.colorbar() cbar.ax.set_yticklabels([&quot;$%dk&quot;%(round(v/1000)) for v in tick_values], fontsize=14) cbar.set_label(&#39;Median House Value&#39;, fontsize=16) plt.legend(fontsize=16) save_fig(&quot;california_housing_prices_plot&quot;) plt.show() . . Saving figure california_housing_prices_plot . corr_matrix = housing.corr() . corr_matrix[&quot;median_house_value&quot;].sort_values(ascending=False) . median_house_value 1.000000 median_income 0.687160 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population -0.026920 longitude -0.047432 latitude -0.142724 Name: median_house_value, dtype: float64 . # from pandas.tools.plotting import scatter_matrix # For older versions of Pandas from pandas.plotting import scatter_matrix attributes = [&quot;median_house_value&quot;, &quot;median_income&quot;, &quot;total_rooms&quot;, &quot;housing_median_age&quot;] scatter_matrix(housing[attributes], figsize=(12, 8)) save_fig(&quot;scatter_matrix_plot&quot;) . Saving figure scatter_matrix_plot . housing.plot(kind=&quot;scatter&quot;, x=&quot;median_income&quot;, y=&quot;median_house_value&quot;, alpha=0.1) plt.axis([0, 16, 0, 550000]) save_fig(&quot;income_vs_house_value_scatterplot&quot;) . Saving figure income_vs_house_value_scatterplot . housing[&quot;rooms_per_household&quot;] = housing[&quot;total_rooms&quot;]/housing[&quot;households&quot;] housing[&quot;bedrooms_per_room&quot;] = housing[&quot;total_bedrooms&quot;]/housing[&quot;total_rooms&quot;] housing[&quot;population_per_household&quot;]=housing[&quot;population&quot;]/housing[&quot;households&quot;] . corr_matrix = housing.corr() corr_matrix[&quot;median_house_value&quot;].sort_values(ascending=False) . median_house_value 1.000000 median_income 0.687160 rooms_per_household 0.146285 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population_per_household -0.021985 population -0.026920 longitude -0.047432 latitude -0.142724 bedrooms_per_room -0.259984 Name: median_house_value, dtype: float64 . housing.plot(kind=&quot;scatter&quot;, x=&quot;rooms_per_household&quot;, y=&quot;median_house_value&quot;, alpha=0.2) plt.axis([0, 5, 0, 520000]) plt.show() . housing.describe() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value rooms_per_household bedrooms_per_room population_per_household . count 16512.000000 | 16512.000000 | 16512.000000 | 16512.000000 | 16354.000000 | 16512.000000 | 16512.000000 | 16512.000000 | 16512.000000 | 16512.000000 | 16354.000000 | 16512.000000 | . mean -119.575834 | 35.639577 | 28.653101 | 2622.728319 | 534.973890 | 1419.790819 | 497.060380 | 3.875589 | 206990.920724 | 5.440341 | 0.212878 | 3.096437 | . std 2.001860 | 2.138058 | 12.574726 | 2138.458419 | 412.699041 | 1115.686241 | 375.720845 | 1.904950 | 115703.014830 | 2.611712 | 0.057379 | 11.584826 | . min -124.350000 | 32.540000 | 1.000000 | 6.000000 | 2.000000 | 3.000000 | 2.000000 | 0.499900 | 14999.000000 | 1.130435 | 0.100000 | 0.692308 | . 25% -121.800000 | 33.940000 | 18.000000 | 1443.000000 | 295.000000 | 784.000000 | 279.000000 | 2.566775 | 119800.000000 | 4.442040 | 0.175304 | 2.431287 | . 50% -118.510000 | 34.260000 | 29.000000 | 2119.500000 | 433.000000 | 1164.000000 | 408.000000 | 3.540900 | 179500.000000 | 5.232284 | 0.203031 | 2.817653 | . 75% -118.010000 | 37.720000 | 37.000000 | 3141.000000 | 644.000000 | 1719.250000 | 602.000000 | 4.744475 | 263900.000000 | 6.056361 | 0.239831 | 3.281420 | . max -114.310000 | 41.950000 | 52.000000 | 39320.000000 | 6210.000000 | 35682.000000 | 5358.000000 | 15.000100 | 500001.000000 | 141.909091 | 1.000000 | 1243.333333 | . Prepare the data for Machine Learning algorithms . housing = strat_train_set.drop(&quot;median_house_value&quot;, axis=1) # drop labels for training set housing_labels = strat_train_set[&quot;median_house_value&quot;].copy() . sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head() sample_incomplete_rows . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | NaN | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | NaN | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | NaN | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | NaN | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | NaN | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . sample_incomplete_rows.dropna(subset=[&quot;total_bedrooms&quot;]) # option 1 . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . sample_incomplete_rows.drop(&quot;total_bedrooms&quot;, axis=1) # option 2 . longitude latitude housing_median_age total_rooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . median = housing[&quot;total_bedrooms&quot;].median() sample_incomplete_rows[&quot;total_bedrooms&quot;].fillna(median, inplace=True) # option 3 . sample_incomplete_rows . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | 433.0 | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | 433.0 | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | 433.0 | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | 433.0 | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | 433.0 | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=&quot;median&quot;) . Remove the text attribute because median can only be calculated on numerical attributes: . housing_num = housing.drop(&quot;ocean_proximity&quot;, axis=1) # alternatively: housing_num = housing.select_dtypes(include=[np.number]) . imputer.fit(housing_num) . SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbose=0) . imputer.statistics_ . array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) . Check that this is the same as manually computing the median of each attribute: . housing_num.median().values . array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) . Transform the training set: . X = imputer.transform(housing_num) . housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing.index) . housing_tr.loc[sample_incomplete_rows.index.values] . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | 433.0 | 3296.0 | 1462.0 | 2.2708 | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | 433.0 | 3038.0 | 727.0 | 5.1762 | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | 433.0 | 999.0 | 386.0 | 4.6328 | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | 433.0 | 1039.0 | 391.0 | 1.6675 | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | 433.0 | 3468.0 | 1405.0 | 3.1662 | . imputer.strategy . &#39;median&#39; . housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) . housing_tr.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income . 17606 -121.89 | 37.29 | 38.0 | 1568.0 | 351.0 | 710.0 | 339.0 | 2.7042 | . 18632 -121.93 | 37.05 | 14.0 | 679.0 | 108.0 | 306.0 | 113.0 | 6.4214 | . 14650 -117.20 | 32.77 | 31.0 | 1952.0 | 471.0 | 936.0 | 462.0 | 2.8621 | . 3230 -119.61 | 36.31 | 25.0 | 1847.0 | 371.0 | 1460.0 | 353.0 | 1.8839 | . 3555 -118.59 | 34.23 | 17.0 | 6592.0 | 1525.0 | 4459.0 | 1463.0 | 3.0347 | . Now let&#39;s preprocess the categorical input feature, ocean_proximity: . housing_cat = housing[[&quot;ocean_proximity&quot;]] housing_cat.head(10) . ocean_proximity . 17606 &lt;1H OCEAN | . 18632 &lt;1H OCEAN | . 14650 NEAR OCEAN | . 3230 INLAND | . 3555 &lt;1H OCEAN | . 19480 INLAND | . 8879 &lt;1H OCEAN | . 13685 INLAND | . 4937 &lt;1H OCEAN | . 4861 &lt;1H OCEAN | . from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) housing_cat_encoded[:10] . array([[0.], [0.], [4.], [1.], [0.], [1.], [0.], [1.], [0.], [0.]]) . ordinal_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . from sklearn.preprocessing import OneHotEncoder cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat) housing_cat_1hot . &lt;16512x5 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 16512 stored elements in Compressed Sparse Row format&gt; . By default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method: . housing_cat_1hot.toarray() . array([[1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 0., 1.], ..., [0., 1., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 1., 0.]]) . Alternatively, you can set sparse=False when creating the OneHotEncoder: . cat_encoder = OneHotEncoder(sparse=False) housing_cat_1hot = cat_encoder.fit_transform(housing_cat) housing_cat_1hot . array([[1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 0., 1.], ..., [0., 1., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 1., 0.]]) . cat_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . Let&#39;s create a custom transformer to add extra attributes: . #collapse-show from sklearn.base import BaseEstimator, TransformerMixin # column index rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household] attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) housing_extra_attribs = attr_adder.transform(housing.values) . . housing_extra_attribs = pd.DataFrame( housing_extra_attribs, columns=list(housing.columns)+[&quot;rooms_per_household&quot;, &quot;population_per_household&quot;], index=housing.index) housing_extra_attribs.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity rooms_per_household population_per_household . 17606 -121.89 | 37.29 | 38 | 1568 | 351 | 710 | 339 | 2.7042 | &lt;1H OCEAN | 4.62537 | 2.0944 | . 18632 -121.93 | 37.05 | 14 | 679 | 108 | 306 | 113 | 6.4214 | &lt;1H OCEAN | 6.00885 | 2.70796 | . 14650 -117.2 | 32.77 | 31 | 1952 | 471 | 936 | 462 | 2.8621 | NEAR OCEAN | 4.22511 | 2.02597 | . 3230 -119.61 | 36.31 | 25 | 1847 | 371 | 1460 | 353 | 1.8839 | INLAND | 5.23229 | 4.13598 | . 3555 -118.59 | 34.23 | 17 | 6592 | 1525 | 4459 | 1463 | 3.0347 | &lt;1H OCEAN | 4.50581 | 3.04785 | . Now let&#39;s build a pipeline for preprocessing the numerical attributes: . from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler num_pipeline = Pipeline([ (&#39;imputer&#39;, SimpleImputer(strategy=&quot;median&quot;)), (&#39;attribs_adder&#39;, CombinedAttributesAdder()), (&#39;std_scaler&#39;, StandardScaler()), ]) housing_num_tr = num_pipeline.fit_transform(housing_num) . housing_num_tr . array([[-1.15604281, 0.77194962, 0.74333089, ..., -0.31205452, -0.08649871, 0.15531753], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0.21768338, -0.03353391, -0.83628902], [ 1.18684903, -1.34218285, 0.18664186, ..., -0.46531516, -0.09240499, 0.4222004 ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0.3469342 , -0.03055414, -0.52177644], [ 0.78221312, -0.85106801, 0.18664186, ..., 0.02499488, 0.06150916, -0.30340741], [-1.43579109, 0.99645926, 1.85670895, ..., -0.22852947, -0.09586294, 0.10180567]]) . #collapse-show from sklearn.compose import ColumnTransformer num_attribs = list(housing_num) cat_attribs = [&quot;ocean_proximity&quot;] full_pipeline = ColumnTransformer([ (&quot;num&quot;, num_pipeline, num_attribs), (&quot;cat&quot;, OneHotEncoder(), cat_attribs), ]) housing_prepared = full_pipeline.fit_transform(housing) . . housing_prepared . array([[-1.15604281, 0.77194962, 0.74333089, ..., 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, ..., 0. , 0. , 1. ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0. , 0. , 0. ], [ 0.78221312, -0.85106801, 0.18664186, ..., 0. , 0. , 0. ], [-1.43579109, 0.99645926, 1.85670895, ..., 0. , 1. , 0. ]]) . housing_prepared.shape . (16512, 16) . For reference, here is the old solution based on a DataFrameSelector transformer (to just select a subset of the Pandas DataFrame columns), and a FeatureUnion: . from sklearn.base import BaseEstimator, TransformerMixin # Create a class to select numerical or categorical columns class OldDataFrameSelector(BaseEstimator, TransformerMixin): def __init__(self, attribute_names): self.attribute_names = attribute_names def fit(self, X, y=None): return self def transform(self, X): return X[self.attribute_names].values . Now let&#39;s join all these components into a big pipeline that will preprocess both the numerical and the categorical features: . num_attribs = list(housing_num) cat_attribs = [&quot;ocean_proximity&quot;] old_num_pipeline = Pipeline([ (&#39;selector&#39;, OldDataFrameSelector(num_attribs)), (&#39;imputer&#39;, SimpleImputer(strategy=&quot;median&quot;)), (&#39;attribs_adder&#39;, CombinedAttributesAdder()), (&#39;std_scaler&#39;, StandardScaler()), ]) old_cat_pipeline = Pipeline([ (&#39;selector&#39;, OldDataFrameSelector(cat_attribs)), (&#39;cat_encoder&#39;, OneHotEncoder(sparse=False)), ]) . from sklearn.pipeline import FeatureUnion old_full_pipeline = FeatureUnion(transformer_list=[ (&quot;num_pipeline&quot;, old_num_pipeline), (&quot;cat_pipeline&quot;, old_cat_pipeline), ]) . old_housing_prepared = old_full_pipeline.fit_transform(housing) old_housing_prepared . array([[-1.15604281, 0.77194962, 0.74333089, ..., 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, ..., 0. , 0. , 1. ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0. , 0. , 0. ], [ 0.78221312, -0.85106801, 0.18664186, ..., 0. , 0. , 0. ], [-1.43579109, 0.99645926, 1.85670895, ..., 0. , 1. , 0. ]]) . The result is the same as with the ColumnTransformer: . np.allclose(housing_prepared, old_housing_prepared) . True . Select and train a model . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . # let&#39;s try the full preprocessing pipeline on a few training instances some_data = housing.iloc[:5] some_labels = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(&quot;Predictions:&quot;, lin_reg.predict(some_data_prepared)) . Predictions: [210644.60459286 317768.80697211 210956.43331178 59218.98886849 189747.55849879] . Compare against the actual values: . print(&quot;Labels:&quot;, list(some_labels)) . Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0] . some_data_prepared . array([[-1.15604281, 0.77194962, 0.74333089, -0.49323393, -0.44543821, -0.63621141, -0.42069842, -0.61493744, -0.31205452, -0.08649871, 0.15531753, 1. , 0. , 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , -0.90896655, -1.0369278 , -0.99833135, -1.02222705, 1.33645936, 0.21768338, -0.03353391, -0.83628902, 1. , 0. , 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, -0.31365989, -0.15334458, -0.43363936, -0.0933178 , -0.5320456 , -0.46531516, -0.09240499, 0.4222004 , 0. , 0. , 0. , 0. , 1. ], [-0.01706767, 0.31357576, -0.29052016, -0.36276217, -0.39675594, 0.03604096, -0.38343559, -1.04556555, -0.07966124, 0.08973561, -0.19645314, 0. , 1. , 0. , 0. , 0. ], [ 0.49247384, -0.65929936, -0.92673619, 1.85619316, 2.41221109, 2.72415407, 2.57097492, -0.44143679, -0.35783383, -0.00419445, 0.2699277 , 1. , 0. , 0. , 0. , 0. ]]) . from sklearn.metrics import mean_squared_error housing_predictions = lin_reg.predict(housing_prepared) lin_mse = mean_squared_error(housing_labels, housing_predictions) lin_rmse = np.sqrt(lin_mse) lin_rmse . 68628.19819848922 . from sklearn.metrics import mean_absolute_error lin_mae = mean_absolute_error(housing_labels, housing_predictions) lin_mae . 49439.89599001897 . from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor(random_state=42) tree_reg.fit(housing_prepared, housing_labels) . DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . housing_predictions = tree_reg.predict(housing_prepared) tree_mse = mean_squared_error(housing_labels, housing_predictions) tree_rmse = np.sqrt(tree_mse) tree_rmse . 0.0 . Fine-tune your model . from sklearn.model_selection import cross_val_score scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) tree_rmse_scores = np.sqrt(-scores) . def display_scores(scores): print(&quot;Scores:&quot;, scores) print(&quot;Mean:&quot;, scores.mean()) print(&quot;Standard deviation:&quot;, scores.std()) display_scores(tree_rmse_scores) . Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782 71115.88230639 75585.14172901 70262.86139133 70273.6325285 75366.87952553 71231.65726027] Mean: 71407.68766037929 Standard deviation: 2439.4345041191004 . lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores) . Scores: [66782.73843989 66960.118071 70347.95244419 74739.57052552 68031.13388938 71193.84183426 64969.63056405 68281.61137997 71552.91566558 67665.10082067] Mean: 69052.46136345083 Standard deviation: 2731.674001798348 . Note: we specify n_estimators=100 to be future-proof since the default value is going to change to 100 in Scikit-Learn 0.22 (for simplicity, this is not shown in the book). . from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor(n_estimators=100, random_state=42) forest_reg.fit(housing_prepared, housing_labels) . RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . housing_predictions = forest_reg.predict(housing_prepared) forest_mse = mean_squared_error(housing_labels, housing_predictions) forest_rmse = np.sqrt(forest_mse) forest_rmse . 18603.515021376355 . from sklearn.model_selection import cross_val_score forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) forest_rmse_scores = np.sqrt(-forest_scores) display_scores(forest_rmse_scores) . Scores: [49519.80364233 47461.9115823 50029.02762854 52325.28068953 49308.39426421 53446.37892622 48634.8036574 47585.73832311 53490.10699751 50021.5852922 ] Mean: 50182.303100336096 Standard deviation: 2097.0810550985693 . scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) pd.Series(np.sqrt(-scores)).describe() . count 10.000000 mean 69052.461363 std 2879.437224 min 64969.630564 25% 67136.363758 50% 68156.372635 75% 70982.369487 max 74739.570526 dtype: float64 . from sklearn.svm import SVR svm_reg = SVR(kernel=&quot;linear&quot;) svm_reg.fit(housing_prepared, housing_labels) housing_predictions = svm_reg.predict(housing_prepared) svm_mse = mean_squared_error(housing_labels, housing_predictions) svm_rmse = np.sqrt(svm_mse) svm_rmse . 111094.6308539982 . #collapse-show from sklearn.model_selection import GridSearchCV param_grid = [ # try 12 (3Ã—4) combinations of hyperparameters {&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, # then try 6 (2Ã—3) combinations with bootstrap set as False {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}, ] forest_reg = RandomForestRegressor(random_state=42) # train across 5 folds, that&#39;s a total of (12+6)*5=90 rounds of training grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, return_train_score=True) grid_search.fit(housing_prepared, housing_labels) . . GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid=[{&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=True, scoring=&#39;neg_mean_squared_error&#39;, verbose=0) . The best hyperparameter combination found: . grid_search.best_params_ . {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} . grid_search.best_estimator_ . RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . Let&#39;s look at the score of each hyperparameter combination tested during the grid search: . cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]): print(np.sqrt(-mean_score), params) . 63669.05791727153 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 55627.16171305252 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 53384.57867637289 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} 60965.99185930139 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 52740.98248528835 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} 50377.344409590376 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} 58663.84733372485 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} 52006.15355973719 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} 50146.465964159885 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} 57869.25504027614 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} 51711.09443660957 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} 49682.25345942335 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} 62895.088889905004 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 54658.14484390074 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 59470.399594730654 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3} 52725.01091081235 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10} 57490.612956065226 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 51009.51445842374 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} . pd.DataFrame(grid_search.cv_results_) . mean_fit_time std_fit_time mean_score_time std_score_time param_max_features param_n_estimators param_bootstrap params split0_test_score split1_test_score ... mean_test_score std_test_score rank_test_score split0_train_score split1_train_score split2_train_score split3_train_score split4_train_score mean_train_score std_train_score . 0 0.060687 | 0.001166 | 0.004219 | 0.000192 | 2 | 3 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} | -3.837622e+09 | -4.147108e+09 | ... | -4.053749e+09 | 1.519609e+08 | 18 | -1.064113e+09 | -1.105142e+09 | -1.116550e+09 | -1.112342e+09 | -1.129650e+09 | -1.105559e+09 | 2.220402e+07 | . 1 0.197437 | 0.003169 | 0.011206 | 0.000903 | 2 | 10 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} | -3.047771e+09 | -3.254861e+09 | ... | -3.094381e+09 | 1.327046e+08 | 11 | -5.927175e+08 | -5.870952e+08 | -5.776964e+08 | -5.716332e+08 | -5.802501e+08 | -5.818785e+08 | 7.345821e+06 | . 2 0.595235 | 0.004583 | 0.032281 | 0.003131 | 2 | 30 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} | -2.689185e+09 | -3.021086e+09 | ... | -2.849913e+09 | 1.626879e+08 | 9 | -4.381089e+08 | -4.391272e+08 | -4.371702e+08 | -4.376955e+08 | -4.452654e+08 | -4.394734e+08 | 2.966320e+06 | . 3 0.099332 | 0.001025 | 0.003848 | 0.000278 | 4 | 3 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} | -3.730181e+09 | -3.786886e+09 | ... | -3.716852e+09 | 1.631421e+08 | 16 | -9.865163e+08 | -1.012565e+09 | -9.169425e+08 | -1.037400e+09 | -9.707739e+08 | -9.848396e+08 | 4.084607e+07 | . 4 0.327148 | 0.002355 | 0.011608 | 0.000659 | 4 | 10 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} | -2.666283e+09 | -2.784511e+09 | ... | -2.781611e+09 | 1.268562e+08 | 8 | -5.097115e+08 | -5.162820e+08 | -4.962893e+08 | -5.436192e+08 | -5.160297e+08 | -5.163863e+08 | 1.542862e+07 | . 5 0.972486 | 0.004962 | 0.030287 | 0.000912 | 4 | 30 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} | -2.387153e+09 | -2.588448e+09 | ... | -2.537877e+09 | 1.214603e+08 | 3 | -3.838835e+08 | -3.880268e+08 | -3.790867e+08 | -4.040957e+08 | -3.845520e+08 | -3.879289e+08 | 8.571233e+06 | . 6 0.133793 | 0.003362 | 0.003705 | 0.000122 | 6 | 3 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} | -3.119657e+09 | -3.586319e+09 | ... | -3.441447e+09 | 1.893141e+08 | 14 | -9.245343e+08 | -8.886939e+08 | -9.353135e+08 | -9.009801e+08 | -8.624664e+08 | -9.023976e+08 | 2.591445e+07 | . 7 0.446336 | 0.003465 | 0.012375 | 0.000601 | 6 | 10 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} | -2.549663e+09 | -2.782039e+09 | ... | -2.704640e+09 | 1.471542e+08 | 6 | -4.980344e+08 | -5.045869e+08 | -4.994664e+08 | -4.990325e+08 | -5.055542e+08 | -5.013349e+08 | 3.100456e+06 | . 8 1.355474 | 0.004652 | 0.032426 | 0.002536 | 6 | 30 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} | -2.370010e+09 | -2.583638e+09 | ... | -2.514668e+09 | 1.285063e+08 | 2 | -3.838538e+08 | -3.804711e+08 | -3.805218e+08 | -3.856095e+08 | -3.901917e+08 | -3.841296e+08 | 3.617057e+06 | . 9 0.171477 | 0.001233 | 0.003917 | 0.000316 | 8 | 3 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} | -3.353504e+09 | -3.348552e+09 | ... | -3.348851e+09 | 1.241864e+08 | 13 | -9.228123e+08 | -8.553031e+08 | -8.603321e+08 | -8.881964e+08 | -9.151287e+08 | -8.883545e+08 | 2.750227e+07 | . 10 0.577159 | 0.002378 | 0.011366 | 0.000736 | 8 | 10 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} | -2.571970e+09 | -2.718994e+09 | ... | -2.674037e+09 | 1.392720e+08 | 5 | -4.932416e+08 | -4.815238e+08 | -4.730979e+08 | -5.155367e+08 | -4.985555e+08 | -4.923911e+08 | 1.459294e+07 | . 11 1.739312 | 0.003247 | 0.030980 | 0.002178 | 8 | 30 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} | -2.357390e+09 | -2.546640e+09 | ... | -2.468326e+09 | 1.091647e+08 | 1 | -3.841658e+08 | -3.744500e+08 | -3.773239e+08 | -3.882250e+08 | -3.810005e+08 | -3.810330e+08 | 4.871017e+06 | . 12 0.094185 | 0.001679 | 0.004966 | 0.000238 | 2 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_est... | -3.785816e+09 | -4.166012e+09 | ... | -3.955792e+09 | 1.900966e+08 | 17 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 13 0.313036 | 0.002217 | 0.013152 | 0.001291 | 2 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_est... | -2.810721e+09 | -3.107789e+09 | ... | -2.987513e+09 | 1.539231e+08 | 10 | -6.056477e-02 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -2.967449e+00 | -6.056027e-01 | 1.181156e+00 | . 14 0.125452 | 0.002900 | 0.004471 | 0.000239 | 3 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_est... | -3.618324e+09 | -3.441527e+09 | ... | -3.536728e+09 | 7.795196e+07 | 15 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -6.072840e+01 | -1.214568e+01 | 2.429136e+01 | . 15 0.413205 | 0.003939 | 0.013399 | 0.001358 | 3 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_est... | -2.757999e+09 | -2.851737e+09 | ... | -2.779927e+09 | 6.286611e+07 | 7 | -2.089484e+01 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -5.465556e+00 | -5.272080e+00 | 8.093117e+00 | . 16 0.155506 | 0.002193 | 0.004849 | 0.000425 | 4 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_est... | -3.134040e+09 | -3.559375e+09 | ... | -3.305171e+09 | 1.879203e+08 | 12 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 17 0.516612 | 0.001973 | 0.013149 | 0.000834 | 4 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_est... | -2.525578e+09 | -2.710011e+09 | ... | -2.601971e+09 | 1.088031e+08 | 4 | -0.000000e+00 | -1.514119e-02 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -3.028238e-03 | 6.056477e-03 | . 18 rows Ã— 23 columns . #collapse-show from sklearn.model_selection import RandomizedSearchCV from scipy.stats import randint param_distribs = { &#39;n_estimators&#39;: randint(low=1, high=200), &#39;max_features&#39;: randint(low=1, high=8), } forest_reg = RandomForestRegressor(random_state=42) rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs, n_iter=10, cv=5, scoring=&#39;neg_mean_squared_error&#39;, random_state=42) rnd_search.fit(housing_prepared, housing_labels) . . RandomizedSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_iter=10, n_jobs=None, param_distributions={&#39;n_estimators&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x127470860&gt;, &#39;max_features&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x127470828&gt;}, pre_dispatch=&#39;2*n_jobs&#39;, random_state=42, refit=True, return_train_score=&#39;warn&#39;, scoring=&#39;neg_mean_squared_error&#39;, verbose=0) . cvres = rnd_search.cv_results_ for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]): print(np.sqrt(-mean_score), params) . 49150.657232934034 {&#39;max_features&#39;: 7, &#39;n_estimators&#39;: 180} 51389.85295710133 {&#39;max_features&#39;: 5, &#39;n_estimators&#39;: 15} 50796.12045980556 {&#39;max_features&#39;: 3, &#39;n_estimators&#39;: 72} 50835.09932039744 {&#39;max_features&#39;: 5, &#39;n_estimators&#39;: 21} 49280.90117886215 {&#39;max_features&#39;: 7, &#39;n_estimators&#39;: 122} 50774.86679035961 {&#39;max_features&#39;: 3, &#39;n_estimators&#39;: 75} 50682.75001237282 {&#39;max_features&#39;: 3, &#39;n_estimators&#39;: 88} 49608.94061293652 {&#39;max_features&#39;: 5, &#39;n_estimators&#39;: 100} 50473.57642831875 {&#39;max_features&#39;: 3, &#39;n_estimators&#39;: 150} 64429.763804893395 {&#39;max_features&#39;: 5, &#39;n_estimators&#39;: 2} . feature_importances = grid_search.best_estimator_.feature_importances_ feature_importances . array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02, 1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01, 5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02, 1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03]) . extra_attribs = [&quot;rooms_per_hhold&quot;, &quot;pop_per_hhold&quot;, &quot;bedrooms_per_room&quot;] #cat_encoder = cat_pipeline.named_steps[&quot;cat_encoder&quot;] # old solution cat_encoder = full_pipeline.named_transformers_[&quot;cat&quot;] cat_one_hot_attribs = list(cat_encoder.categories_[0]) attributes = num_attribs + extra_attribs + cat_one_hot_attribs sorted(zip(feature_importances, attributes), reverse=True) . [(0.3661589806181342, &#39;median_income&#39;), (0.1647809935615905, &#39;INLAND&#39;), (0.10879295677551573, &#39;pop_per_hhold&#39;), (0.07334423551601242, &#39;longitude&#39;), (0.0629090704826203, &#39;latitude&#39;), (0.05641917918195401, &#39;rooms_per_hhold&#39;), (0.05335107734767581, &#39;bedrooms_per_room&#39;), (0.041143798478729635, &#39;housing_median_age&#39;), (0.014874280890402767, &#39;population&#39;), (0.014672685420543237, &#39;total_rooms&#39;), (0.014257599323407807, &#39;households&#39;), (0.014106483453584102, &#39;total_bedrooms&#39;), (0.010311488326303787, &#39;&lt;1H OCEAN&#39;), (0.002856474637320158, &#39;NEAR OCEAN&#39;), (0.00196041559947807, &#39;NEAR BAY&#39;), (6.028038672736599e-05, &#39;ISLAND&#39;)] . final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(&quot;median_house_value&quot;, axis=1) y_test = strat_test_set[&quot;median_house_value&quot;].copy() X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared) final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse) . final_rmse . 47730.22690385927 . We can compute a 95% confidence interval for the test RMSE: . from scipy import stats confidence = 0.95 squared_errors = (final_predictions - y_test) ** 2 np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors))) . array([45685.10470776, 49691.25001878]) . We could compute the interval manually like this: . m = len(squared_errors) mean = squared_errors.mean() tscore = stats.t.ppf((1 + confidence) / 2, df=m - 1) tmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m) np.sqrt(mean - tmargin), np.sqrt(mean + tmargin) . (45685.10470776014, 49691.25001877871) . Alternatively, we could use a z-scores rather than t-scores: . zscore = stats.norm.ppf((1 + confidence) / 2) zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m) np.sqrt(mean - zmargin), np.sqrt(mean + zmargin) . (45685.717918136594, 49690.68623889426) . Extra material . A full pipeline with both preparation and prediction . full_pipeline_with_predictor = Pipeline([ (&quot;preparation&quot;, full_pipeline), (&quot;linear&quot;, LinearRegression()) ]) full_pipeline_with_predictor.fit(housing, housing_labels) full_pipeline_with_predictor.predict(some_data) . array([210644.60459286, 317768.80697211, 210956.43331178, 59218.98886849, 189747.55849879]) . Model persistence using joblib . my_model = full_pipeline_with_predictor . import joblib joblib.dump(my_model, &quot;my_model.pkl&quot;) # DIFF #... my_model_loaded = joblib.load(&quot;my_model.pkl&quot;) # DIFF . Example SciPy distributions for RandomizedSearchCV . from scipy.stats import geom, expon geom_distrib=geom(0.5).rvs(10000, random_state=42) expon_distrib=expon(scale=1).rvs(10000, random_state=42) plt.hist(geom_distrib, bins=50) plt.show() plt.hist(expon_distrib, bins=50) plt.show() . Exercise solutions . 1. . Question: Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyperparameters such as kernel=&quot;linear&quot; (with various values for the C hyperparameter) or kernel=&quot;rbf&quot; (with various values for the C and gamma hyperparameters). Don&#39;t worry about what these hyperparameters mean for now. How does the best SVR predictor perform? . from sklearn.model_selection import GridSearchCV param_grid = [ {&#39;kernel&#39;: [&#39;linear&#39;], &#39;C&#39;: [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]}, {&#39;kernel&#39;: [&#39;rbf&#39;], &#39;C&#39;: [1.0, 3.0, 10., 30., 100., 300., 1000.0], &#39;gamma&#39;: [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]}, ] svm_reg = SVR() grid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) grid_search.fit(housing_prepared, housing_labels) . Fitting 5 folds for each of 50 candidates, totalling 250 fits [CV] C=10.0, kernel=linear ........................................... . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . [CV] ............................ C=10.0, kernel=linear, total= 5.1s [CV] C=10.0, kernel=linear ........................................... . [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 7.3s remaining: 0.0s . [CV] ............................ C=10.0, kernel=linear, total= 5.1s [CV] C=10.0, kernel=linear ........................................... [CV] ............................ C=10.0, kernel=linear, total= 5.2s [CV] C=10.0, kernel=linear ........................................... [CV] ............................ C=10.0, kernel=linear, total= 5.1s [CV] C=10.0, kernel=linear ........................................... [CV] ............................ C=10.0, kernel=linear, total= 5.1s [CV] C=30.0, kernel=linear ........................................... [CV] ............................ C=30.0, kernel=linear, total= 5.0s [CV] C=30.0, kernel=linear ........................................... [CV] ............................ C=30.0, kernel=linear, total= 5.0s [CV] C=30.0, kernel=linear ........................................... [CV] ............................ C=30.0, kernel=linear, total= 5.3s [CV] C=30.0, kernel=linear ........................................... [CV] ............................ C=30.0, kernel=linear, total= 5.1s [CV] C=30.0, kernel=linear ........................................... [CV] ............................ C=30.0, kernel=linear, total= 5.0s [CV] C=100.0, kernel=linear .......................................... [CV] ........................... C=100.0, kernel=linear, total= 5.1s [CV] C=100.0, kernel=linear .......................................... [CV] ........................... C=100.0, kernel=linear, total= 5.0s [CV] C=100.0, kernel=linear .......................................... [CV] ........................... C=100.0, kernel=linear, total= 5.1s [CV] C=100.0, kernel=linear .......................................... [CV] ........................... C=100.0, kernel=linear, total= 5.0s [CV] C=100.0, kernel=linear .......................................... [CV] ........................... C=100.0, kernel=linear, total= 5.0s [CV] C=300.0, kernel=linear .......................................... [CV] ........................... C=300.0, kernel=linear, total= 5.1s &lt;&lt;434 more lines&gt;&gt; [CV] C=1000.0, gamma=0.1, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.1, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.1, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.1, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.3, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.3, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.3, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.3, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.3, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=1.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total= 8.6s [CV] C=1000.0, gamma=1.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total= 8.7s [CV] C=1000.0, gamma=1.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total= 9.4s [CV] C=1000.0, gamma=1.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total= 9.1s [CV] C=1000.0, gamma=1.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total= 8.9s [CV] C=1000.0, gamma=3.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total= 10.9s [CV] C=1000.0, gamma=3.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total= 11.1s [CV] C=1000.0, gamma=3.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total= 11.1s [CV] C=1000.0, gamma=3.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total= 11.0s [CV] C=1000.0, gamma=3.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total= 11.1s . [Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed: 52.8min finished . GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid=[{&#39;kernel&#39;: [&#39;linear&#39;], &#39;C&#39;: [10.0, 30.0, 100.0, 300.0, 1000.0, 3000.0, 10000.0, 30000.0]}, {&#39;kernel&#39;: [&#39;rbf&#39;], &#39;C&#39;: [1.0, 3.0, 10.0, 30.0, 100.0, 300.0, 1000.0], &#39;gamma&#39;: [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) . The best model achieves the following score (evaluated using 5-fold cross validation): . negative_mse = grid_search.best_score_ rmse = np.sqrt(-negative_mse) rmse . 70363.90313964167 . That&#39;s much worse than the RandomForestRegressor. Let&#39;s check the best hyperparameters found: . grid_search.best_params_ . {&#39;C&#39;: 30000.0, &#39;kernel&#39;: &#39;linear&#39;} . The linear kernel seems better than the RBF kernel. Notice that the value of C is the maximum tested value. When this happens you definitely want to launch the grid search again with higher values for C (removing the smallest values), because it is likely that higher values of C will be better. . 2. . Question: Try replacing GridSearchCV with RandomizedSearchCV. . #collapse-show from sklearn.model_selection import RandomizedSearchCV from scipy.stats import expon, reciprocal # see https://docs.scipy.org/doc/scipy/reference/stats.html # for `expon()` and `reciprocal()` documentation and more probability distribution functions. # Note: gamma is ignored when kernel is &quot;linear&quot; param_distribs = { &#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;], &#39;C&#39;: reciprocal(20, 200000), &#39;gamma&#39;: expon(scale=1.0), } svm_reg = SVR() rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs, n_iter=50, cv=5, scoring=&#39;neg_mean_squared_error&#39;, verbose=2, random_state=42) rnd_search.fit(housing_prepared, housing_labels) . . Fitting 5 folds for each of 50 candidates, totalling 250 fits [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ...... . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear, total= 5.6s [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ...... . [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 8.0s remaining: 0.0s . [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear, total= 5.7s [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ...... [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear, total= 5.8s [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ...... [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear, total= 5.4s [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ...... [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear, total= 5.8s [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ...... [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total= 10.8s [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ...... [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total= 11.2s [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ...... [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total= 10.8s [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ...... [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total= 11.4s [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ...... [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total= 11.7s [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf ..... [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total= 9.0s [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf ..... [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total= 9.3s [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf ..... [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total= 9.0s [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf ..... [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total= 9.3s [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf ..... [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total= 8.9s [CV] C=432.37884813148855, gamma=0.15416196746656105, kernel=linear .. [CV] C=432.37884813148855, gamma=0.15416196746656105, kernel=linear, total= 5.2s &lt;&lt;434 more lines&gt;&gt; [CV] C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf ....... [CV] C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf, total= 36.9s [CV] C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf ....... [CV] C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf, total= 34.8s [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........ [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total= 9.3s [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........ [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total= 9.2s [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........ [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total= 9.4s [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........ [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total= 9.5s [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........ [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total= 9.2s [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ...... [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total= 13.4s [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ...... [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total= 13.5s [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ...... [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total= 12.2s [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ...... [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total= 13.6s [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ...... [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total= 12.7s [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear .... [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total= 34.2s [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear .... [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total= 24.6s [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear .... [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total= 38.2s [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear .... [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total= 27.7s [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear .... [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total= 21.0s . [Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed: 70.9min finished . RandomizedSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False), fit_params=None, iid=&#39;warn&#39;, n_iter=50, n_jobs=None, param_distributions={&#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;], &#39;C&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x1411fbbe0&gt;, &#39;gamma&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x1411fb780&gt;}, pre_dispatch=&#39;2*n_jobs&#39;, random_state=42, refit=True, return_train_score=&#39;warn&#39;, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) . The best model achieves the following score (evaluated using 5-fold cross validation): . negative_mse = rnd_search.best_score_ rmse = np.sqrt(-negative_mse) rmse . 54767.99053704408 . Now this is much closer to the performance of the RandomForestRegressor (but not quite there yet). Let&#39;s check the best hyperparameters found: . rnd_search.best_params_ . {&#39;C&#39;: 157055.10989448498, &#39;gamma&#39;: 0.26497040005002437, &#39;kernel&#39;: &#39;rbf&#39;} . This time the search found a good set of hyperparameters for the RBF kernel. Randomized search tends to find better hyperparameters than grid search in the same amount of time. . Let&#39;s look at the exponential distribution we used, with scale=1.0. Note that some samples are much larger or smaller than 1.0, but when you look at the log of the distribution, you can see that most values are actually concentrated roughly in the range of exp(-2) to exp(+2), which is about 0.1 to 7.4. . expon_distrib = expon(scale=1.) samples = expon_distrib.rvs(10000, random_state=42) plt.figure(figsize=(10, 4)) plt.subplot(121) plt.title(&quot;Exponential distribution (scale=1.0)&quot;) plt.hist(samples, bins=50) plt.subplot(122) plt.title(&quot;Log of this distribution&quot;) plt.hist(np.log(samples), bins=50) plt.show() . The distribution we used for C looks quite different: the scale of the samples is picked from a uniform distribution within a given range, which is why the right graph, which represents the log of the samples, looks roughly constant. This distribution is useful when you don&#39;t have a clue of what the target scale is: . reciprocal_distrib = reciprocal(20, 200000) samples = reciprocal_distrib.rvs(10000, random_state=42) plt.figure(figsize=(10, 4)) plt.subplot(121) plt.title(&quot;Reciprocal distribution (scale=1.0)&quot;) plt.hist(samples, bins=50) plt.subplot(122) plt.title(&quot;Log of this distribution&quot;) plt.hist(np.log(samples), bins=50) plt.show() . The reciprocal distribution is useful when you have no idea what the scale of the hyperparameter should be (indeed, as you can see on the figure on the right, all scales are equally likely, within the given range), whereas the exponential distribution is best when you know (more or less) what the scale of the hyperparameter should be. . 3. . Question: Try adding a transformer in the preparation pipeline to select only the most important attributes. . from sklearn.base import BaseEstimator, TransformerMixin def indices_of_top_k(arr, k): return np.sort(np.argpartition(np.array(arr), -k)[-k:]) class TopFeatureSelector(BaseEstimator, TransformerMixin): def __init__(self, feature_importances, k): self.feature_importances = feature_importances self.k = k def fit(self, X, y=None): self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k) return self def transform(self, X): return X[:, self.feature_indices_] . Note: this feature selector assumes that you have already computed the feature importances somehow (for example using a RandomForestRegressor). You may be tempted to compute them directly in the TopFeatureSelector&#39;s fit() method, however this would likely slow down grid/randomized search since the feature importances would have to be computed for every hyperparameter combination (unless you implement some sort of cache). . Let&#39;s define the number of top features we want to keep: . k = 5 . Now let&#39;s look for the indices of the top k features: . top_k_feature_indices = indices_of_top_k(feature_importances, k) top_k_feature_indices . array([ 0, 1, 7, 9, 12]) . np.array(attributes)[top_k_feature_indices] . array([&#39;longitude&#39;, &#39;latitude&#39;, &#39;median_income&#39;, &#39;pop_per_hhold&#39;, &#39;INLAND&#39;], dtype=&#39;&lt;U18&#39;) . Let&#39;s double check that these are indeed the top k features: . sorted(zip(feature_importances, attributes), reverse=True)[:k] . [(0.3661589806181342, &#39;median_income&#39;), (0.1647809935615905, &#39;INLAND&#39;), (0.10879295677551573, &#39;pop_per_hhold&#39;), (0.07334423551601242, &#39;longitude&#39;), (0.0629090704826203, &#39;latitude&#39;)] . Looking good... Now let&#39;s create a new pipeline that runs the previously defined preparation pipeline, and adds top k feature selection: . preparation_and_feature_selection_pipeline = Pipeline([ (&#39;preparation&#39;, full_pipeline), (&#39;feature_selection&#39;, TopFeatureSelector(feature_importances, k)) ]) . housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing) . Let&#39;s look at the features of the first 3 instances: . housing_prepared_top_k_features[0:3] . array([[-1.15604281, 0.77194962, -0.61493744, -0.08649871, 0. ], [-1.17602483, 0.6596948 , 1.33645936, -0.03353391, 0. ], [ 1.18684903, -1.34218285, -0.5320456 , -0.09240499, 0. ]]) . Now let&#39;s double check that these are indeed the top k features: . housing_prepared[0:3, top_k_feature_indices] . array([[-1.15604281, 0.77194962, -0.61493744, -0.08649871, 0. ], [-1.17602483, 0.6596948 , 1.33645936, -0.03353391, 0. ], [ 1.18684903, -1.34218285, -0.5320456 , -0.09240499, 0. ]]) . Works great! :) . 4. . Question: Try creating a single pipeline that does the full data preparation plus the final prediction. . prepare_select_and_predict_pipeline = Pipeline([ (&#39;preparation&#39;, full_pipeline), (&#39;feature_selection&#39;, TopFeatureSelector(feature_importances, k)), (&#39;svm_reg&#39;, SVR(**rnd_search.best_params_)) ]) . prepare_select_and_predict_pipeline.fit(housing, housing_labels) . Pipeline(memory=None, steps=[(&#39;preparation&#39;, ColumnTransformer(n_jobs=None, remainder=&#39;drop&#39;, sparse_threshold=0.3, transformer_weights=None, transformers=[(&#39;num&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbos... gamma=0.26497040005002437, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False))]) . Let&#39;s try the full pipeline on a few instances: . some_data = housing.iloc[:4] some_labels = housing_labels.iloc[:4] print(&quot;Predictions: t&quot;, prepare_select_and_predict_pipeline.predict(some_data)) print(&quot;Labels: t t&quot;, list(some_labels)) . Predictions: [203214.28978849 371846.88152572 173295.65441612 47328.3970888 ] Labels: [286600.0, 340600.0, 196900.0, 46300.0] . Well, the full pipeline seems to work fine. Of course, the predictions are not fantastic: they would be better if we used the best RandomForestRegressor that we found earlier, rather than the best SVR. . 5. . Question: Automatically explore some preparation options using GridSearchCV. . param_grid = [{ &#39;preparation__num__imputer__strategy&#39;: [&#39;mean&#39;, &#39;median&#39;, &#39;most_frequent&#39;], &#39;feature_selection__k&#39;: list(range(1, len(feature_importances) + 1)) }] grid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) grid_search_prep.fit(housing, housing_labels) . Fitting 5 folds for each of 48 candidates, totalling 240 fits [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean . [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 8.9s remaining: 0.0s . [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=median [CV] feature_selection__k=1, preparation__num__imputer__strategy=median, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=median [CV] feature_selection__k=1, preparation__num__imputer__strategy=median, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=median [CV] feature_selection__k=1, preparation__num__imputer__strategy=median, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=median [CV] feature_selection__k=1, preparation__num__imputer__strategy=median, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=median [CV] feature_selection__k=1, preparation__num__imputer__strategy=median, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total= 6.3s [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total= 6.2s [CV] feature_selection__k=2, preparation__num__imputer__strategy=mean [CV] feature_selection__k=2, preparation__num__imputer__strategy=mean, total= 6.5s &lt;&lt;414 more lines&gt;&gt; [CV] feature_selection__k=15, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=15, preparation__num__imputer__strategy=most_frequent, total= 21.7s [CV] feature_selection__k=15, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=15, preparation__num__imputer__strategy=most_frequent, total= 26.9s [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean, total= 25.4s [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean, total= 26.4s [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean, total= 24.8s [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean, total= 25.9s [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean, total= 21.8s [CV] feature_selection__k=16, preparation__num__imputer__strategy=median [CV] feature_selection__k=16, preparation__num__imputer__strategy=median, total= 22.8s [CV] feature_selection__k=16, preparation__num__imputer__strategy=median [CV] feature_selection__k=16, preparation__num__imputer__strategy=median, total= 26.1s [CV] feature_selection__k=16, preparation__num__imputer__strategy=median [CV] feature_selection__k=16, preparation__num__imputer__strategy=median, total= 24.5s [CV] feature_selection__k=16, preparation__num__imputer__strategy=median [CV] feature_selection__k=16, preparation__num__imputer__strategy=median, total= 20.1s [CV] feature_selection__k=16, preparation__num__imputer__strategy=median [CV] feature_selection__k=16, preparation__num__imputer__strategy=median, total= 25.0s [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total= 22.3s [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total= 26.0s [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total= 23.3s [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total= 23.9s [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total= 26.5s . [Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed: 74.2min finished . GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=Pipeline(memory=None, steps=[(&#39;preparation&#39;, ColumnTransformer(n_jobs=None, remainder=&#39;drop&#39;, sparse_threshold=0.3, transformer_weights=None, transformers=[(&#39;num&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbos... gamma=0.26497040005002437, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False))]), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid=[{&#39;preparation__num__imputer__strategy&#39;: [&#39;mean&#39;, &#39;median&#39;, &#39;most_frequent&#39;], &#39;feature_selection__k&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) . grid_search_prep.best_params_ . {&#39;feature_selection__k&#39;: 15, &#39;preparation__num__imputer__strategy&#39;: &#39;most_frequent&#39;} . The best imputer strategy is most_frequent and apparently almost all features are useful (15 out of 16). The last one (ISLAND) seems to just add some noise. . Congratulations! You already know quite a lot about Machine Learning. :) .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/02_end_to_end_machine_learning_project",
            "relUrl": "/02_end_to_end_machine_learning_project",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Dimensionality Reduction",
            "content": "Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;dim_reduction&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) # Ignore useless warnings (see SciPy issue #5998) import warnings warnings.filterwarnings(action=&quot;ignore&quot;, message=&quot;^internal gelsd&quot;) . . Projection methods . Build 3D dataset: . np.random.seed(4) m = 60 w1, w2 = 0.1, 0.3 noise = 0.1 angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5 X = np.empty((m, 3)) X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2 X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2 X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m) . PCA using SVD decomposition . X_centered = X - X.mean(axis=0) U, s, Vt = np.linalg.svd(X_centered) c1 = Vt.T[:, 0] c2 = Vt.T[:, 1] . m, n = X.shape S = np.zeros(X_centered.shape) S[:n, :n] = np.diag(s) . np.allclose(X_centered, U.dot(S).dot(Vt)) . True . W2 = Vt.T[:, :2] X2D = X_centered.dot(W2) . X2D_using_svd = X2D . PCA using Scikit-Learn . With Scikit-Learn, PCA is really trivial. It even takes care of mean centering for you: . from sklearn.decomposition import PCA pca = PCA(n_components = 2) X2D = pca.fit_transform(X) . X2D[:5] . array([[ 1.26203346, 0.42067648], [-0.08001485, -0.35272239], [ 1.17545763, 0.36085729], [ 0.89305601, -0.30862856], [ 0.73016287, -0.25404049]]) . X2D_using_svd[:5] . array([[-1.26203346, -0.42067648], [ 0.08001485, 0.35272239], [-1.17545763, -0.36085729], [-0.89305601, 0.30862856], [-0.73016287, 0.25404049]]) . Notice that running PCA multiple times on slightly different datasets may result in different results. In general the only difference is that some axes may be flipped. In this example, PCA using Scikit-Learn gives the same projection as the one given by the SVD approach, except both axes are flipped: . np.allclose(X2D, -X2D_using_svd) . True . Recover the 3D points projected on the plane (PCA 2D subspace). . X3D_inv = pca.inverse_transform(X2D) . Of course, there was some loss of information during the projection step, so the recovered 3D points are not exactly equal to the original 3D points: . np.allclose(X3D_inv, X) . False . We can compute the reconstruction error: . np.mean(np.sum(np.square(X3D_inv - X), axis=1)) . 0.01017033779284855 . The inverse transform in the SVD approach looks like this: . X3D_inv_using_svd = X2D_using_svd.dot(Vt[:2, :]) . The reconstructions from both methods are not identical because Scikit-Learn&#39;s PCA class automatically takes care of reversing the mean centering, but if we subtract the mean, we get the same reconstruction: . np.allclose(X3D_inv_using_svd, X3D_inv - pca.mean_) . True . The PCA object gives access to the principal components that it computed: . pca.components_ . array([[-0.93636116, -0.29854881, -0.18465208], [ 0.34027485, -0.90119108, -0.2684542 ]]) . Compare to the first two principal components computed using the SVD method: . Vt[:2] . array([[ 0.93636116, 0.29854881, 0.18465208], [-0.34027485, 0.90119108, 0.2684542 ]]) . Notice how the axes are flipped. . Now let&#39;s look at the explained variance ratio: . pca.explained_variance_ratio_ . array([0.84248607, 0.14631839]) . The first dimension explains 84.2% of the variance, while the second explains 14.6%. . By projecting down to 2D, we lost about 1.1% of the variance: . 1 - pca.explained_variance_ratio_.sum() . 0.011195535570688975 . Here is how to compute the explained variance ratio using the SVD approach (recall that s is the diagonal of the matrix S): . np.square(s) / np.square(s).sum() . array([0.84248607, 0.14631839, 0.01119554]) . Next, let&#39;s generate some nice figures! :) . Utility class to draw 3D arrows (copied from http://stackoverflow.com/questions/11140163) . from matplotlib.patches import FancyArrowPatch from mpl_toolkits.mplot3d import proj3d class Arrow3D(FancyArrowPatch): def __init__(self, xs, ys, zs, *args, **kwargs): FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs) self._verts3d = xs, ys, zs def draw(self, renderer): xs3d, ys3d, zs3d = self._verts3d xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M) self.set_positions((xs[0],ys[0]),(xs[1],ys[1])) FancyArrowPatch.draw(self, renderer) . Express the plane as a function of x and y. . axes = [-1.8, 1.8, -1.3, 1.3, -1.0, 1.0] x1s = np.linspace(axes[0], axes[1], 10) x2s = np.linspace(axes[2], axes[3], 10) x1, x2 = np.meshgrid(x1s, x2s) C = pca.components_ R = C.T.dot(C) z = (R[0, 2] * x1 + R[1, 2] * x2) / (1 - R[2, 2]) . Plot the 3D dataset, the plane and the projections on that plane. . #collapse-show from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(figsize=(6, 3.8)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) X3D_above = X[X[:, 2] &gt; X3D_inv[:, 2]] X3D_below = X[X[:, 2] &lt;= X3D_inv[:, 2]] ax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], &quot;bo&quot;, alpha=0.5) ax.plot_surface(x1, x2, z, alpha=0.2, color=&quot;k&quot;) np.linalg.norm(C, axis=0) ax.add_artist(Arrow3D([0, C[0, 0]],[0, C[0, 1]],[0, C[0, 2]], mutation_scale=15, lw=1, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;)) ax.add_artist(Arrow3D([0, C[1, 0]],[0, C[1, 1]],[0, C[1, 2]], mutation_scale=15, lw=1, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;)) ax.plot([0], [0], [0], &quot;k.&quot;) for i in range(m): if X[i, 2] &gt; X3D_inv[i, 2]: ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], &quot;k-&quot;) else: ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], &quot;k-&quot;, color=&quot;#505050&quot;) ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], &quot;k+&quot;) ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], &quot;k.&quot;) ax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], &quot;bo&quot;) ax.set_xlabel(&quot;$x_1$&quot;, fontsize=18, labelpad=10) ax.set_ylabel(&quot;$x_2$&quot;, fontsize=18, labelpad=10) ax.set_zlabel(&quot;$x_3$&quot;, fontsize=18, labelpad=10) ax.set_xlim(axes[0:2]) ax.set_ylim(axes[2:4]) ax.set_zlim(axes[4:6]) # Note: If you are using Matplotlib 3.0.0, it has a bug and does not # display 3D graphs properly. # See https://github.com/matplotlib/matplotlib/issues/12239 # You should upgrade to a later version. If you cannot, then you can # use the following workaround before displaying each 3D graph: # for spine in ax.spines.values(): # spine.set_visible(False) save_fig(&quot;dataset_3d_plot&quot;) plt.show() . . Saving figure dataset_3d_plot . fig = plt.figure() ax = fig.add_subplot(111, aspect=&#39;equal&#39;) ax.plot(X2D[:, 0], X2D[:, 1], &quot;k+&quot;) ax.plot(X2D[:, 0], X2D[:, 1], &quot;k.&quot;) ax.plot([0], [0], &quot;ko&quot;) ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc=&#39;k&#39;, ec=&#39;k&#39;) ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc=&#39;k&#39;, ec=&#39;k&#39;) ax.set_xlabel(&quot;$z_1$&quot;, fontsize=18) ax.set_ylabel(&quot;$z_2$&quot;, fontsize=18, rotation=0) ax.axis([-1.5, 1.3, -1.2, 1.2]) ax.grid(True) save_fig(&quot;dataset_2d_plot&quot;) . Saving figure dataset_2d_plot . Manifold learning . Swiss roll: . from sklearn.datasets import make_swiss_roll X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42) . #collapse-show axes = [-11.5, 14, -2, 23, -12, 15] fig = plt.figure(figsize=(6, 5)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot) ax.view_init(10, -70) ax.set_xlabel(&quot;$x_1$&quot;, fontsize=18) ax.set_ylabel(&quot;$x_2$&quot;, fontsize=18) ax.set_zlabel(&quot;$x_3$&quot;, fontsize=18) ax.set_xlim(axes[0:2]) ax.set_ylim(axes[2:4]) ax.set_zlim(axes[4:6]) save_fig(&quot;swiss_roll_plot&quot;) plt.show() . . Saving figure swiss_roll_plot . #collapse-show plt.figure(figsize=(11, 4)) plt.subplot(121) plt.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.hot) plt.axis(axes[:4]) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.ylabel(&quot;$x_2$&quot;, fontsize=18, rotation=0) plt.grid(True) plt.subplot(122) plt.scatter(t, X[:, 1], c=t, cmap=plt.cm.hot) plt.axis([4, 15, axes[2], axes[3]]) plt.xlabel(&quot;$z_1$&quot;, fontsize=18) plt.grid(True) save_fig(&quot;squished_swiss_roll_plot&quot;) plt.show() . . Saving figure squished_swiss_roll_plot . #collapse-show from matplotlib import gridspec axes = [-11.5, 14, -2, 23, -12, 15] x2s = np.linspace(axes[2], axes[3], 10) x3s = np.linspace(axes[4], axes[5], 10) x2, x3 = np.meshgrid(x2s, x3s) fig = plt.figure(figsize=(6, 5)) ax = plt.subplot(111, projection=&#39;3d&#39;) positive_class = X[:, 0] &gt; 5 X_pos = X[positive_class] X_neg = X[~positive_class] ax.view_init(10, -70) ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], &quot;y^&quot;) ax.plot_wireframe(5, x2, x3, alpha=0.5) ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], &quot;gs&quot;) ax.set_xlabel(&quot;$x_1$&quot;, fontsize=18) ax.set_ylabel(&quot;$x_2$&quot;, fontsize=18) ax.set_zlabel(&quot;$x_3$&quot;, fontsize=18) ax.set_xlim(axes[0:2]) ax.set_ylim(axes[2:4]) ax.set_zlim(axes[4:6]) save_fig(&quot;manifold_decision_boundary_plot1&quot;) plt.show() fig = plt.figure(figsize=(5, 4)) ax = plt.subplot(111) plt.plot(t[positive_class], X[positive_class, 1], &quot;gs&quot;) plt.plot(t[~positive_class], X[~positive_class, 1], &quot;y^&quot;) plt.axis([4, 15, axes[2], axes[3]]) plt.xlabel(&quot;$z_1$&quot;, fontsize=18) plt.ylabel(&quot;$z_2$&quot;, fontsize=18, rotation=0) plt.grid(True) save_fig(&quot;manifold_decision_boundary_plot2&quot;) plt.show() fig = plt.figure(figsize=(6, 5)) ax = plt.subplot(111, projection=&#39;3d&#39;) positive_class = 2 * (t[:] - 4) &gt; X[:, 1] X_pos = X[positive_class] X_neg = X[~positive_class] ax.view_init(10, -70) ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], &quot;y^&quot;) ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], &quot;gs&quot;) ax.set_xlabel(&quot;$x_1$&quot;, fontsize=18) ax.set_ylabel(&quot;$x_2$&quot;, fontsize=18) ax.set_zlabel(&quot;$x_3$&quot;, fontsize=18) ax.set_xlim(axes[0:2]) ax.set_ylim(axes[2:4]) ax.set_zlim(axes[4:6]) save_fig(&quot;manifold_decision_boundary_plot3&quot;) plt.show() fig = plt.figure(figsize=(5, 4)) ax = plt.subplot(111) plt.plot(t[positive_class], X[positive_class, 1], &quot;gs&quot;) plt.plot(t[~positive_class], X[~positive_class, 1], &quot;y^&quot;) plt.plot([4, 15], [0, 22], &quot;b-&quot;, linewidth=2) plt.axis([4, 15, axes[2], axes[3]]) plt.xlabel(&quot;$z_1$&quot;, fontsize=18) plt.ylabel(&quot;$z_2$&quot;, fontsize=18, rotation=0) plt.grid(True) save_fig(&quot;manifold_decision_boundary_plot4&quot;) plt.show() . . Saving figure manifold_decision_boundary_plot1 . Saving figure manifold_decision_boundary_plot2 . Saving figure manifold_decision_boundary_plot3 . Saving figure manifold_decision_boundary_plot4 . PCA . #collapse-show angle = np.pi / 5 stretch = 5 m = 200 np.random.seed(3) X = np.random.randn(m, 2) / 10 X = X.dot(np.array([[stretch, 0],[0, 1]])) # stretch X = X.dot([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]]) # rotate u1 = np.array([np.cos(angle), np.sin(angle)]) u2 = np.array([np.cos(angle - 2 * np.pi/6), np.sin(angle - 2 * np.pi/6)]) u3 = np.array([np.cos(angle - np.pi/2), np.sin(angle - np.pi/2)]) X_proj1 = X.dot(u1.reshape(-1, 1)) X_proj2 = X.dot(u2.reshape(-1, 1)) X_proj3 = X.dot(u3.reshape(-1, 1)) plt.figure(figsize=(8,4)) plt.subplot2grid((3,2), (0, 0), rowspan=3) plt.plot([-1.4, 1.4], [-1.4*u1[1]/u1[0], 1.4*u1[1]/u1[0]], &quot;k-&quot;, linewidth=1) plt.plot([-1.4, 1.4], [-1.4*u2[1]/u2[0], 1.4*u2[1]/u2[0]], &quot;k--&quot;, linewidth=1) plt.plot([-1.4, 1.4], [-1.4*u3[1]/u3[0], 1.4*u3[1]/u3[0]], &quot;k:&quot;, linewidth=2) plt.plot(X[:, 0], X[:, 1], &quot;bo&quot;, alpha=0.5) plt.axis([-1.4, 1.4, -1.4, 1.4]) plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc=&#39;k&#39;, ec=&#39;k&#39;) plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc=&#39;k&#39;, ec=&#39;k&#39;) plt.text(u1[0] + 0.1, u1[1] - 0.05, r&quot;$ mathbf{c_1}$&quot;, fontsize=22) plt.text(u3[0] + 0.1, u3[1], r&quot;$ mathbf{c_2}$&quot;, fontsize=22) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.ylabel(&quot;$x_2$&quot;, fontsize=18, rotation=0) plt.grid(True) plt.subplot2grid((3,2), (0, 1)) plt.plot([-2, 2], [0, 0], &quot;k-&quot;, linewidth=1) plt.plot(X_proj1[:, 0], np.zeros(m), &quot;bo&quot;, alpha=0.3) plt.gca().get_yaxis().set_ticks([]) plt.gca().get_xaxis().set_ticklabels([]) plt.axis([-2, 2, -1, 1]) plt.grid(True) plt.subplot2grid((3,2), (1, 1)) plt.plot([-2, 2], [0, 0], &quot;k--&quot;, linewidth=1) plt.plot(X_proj2[:, 0], np.zeros(m), &quot;bo&quot;, alpha=0.3) plt.gca().get_yaxis().set_ticks([]) plt.gca().get_xaxis().set_ticklabels([]) plt.axis([-2, 2, -1, 1]) plt.grid(True) plt.subplot2grid((3,2), (2, 1)) plt.plot([-2, 2], [0, 0], &quot;k:&quot;, linewidth=2) plt.plot(X_proj3[:, 0], np.zeros(m), &quot;bo&quot;, alpha=0.3) plt.gca().get_yaxis().set_ticks([]) plt.axis([-2, 2, -1, 1]) plt.xlabel(&quot;$z_1$&quot;, fontsize=18) plt.grid(True) save_fig(&quot;pca_best_projection_plot&quot;) plt.show() . . Saving figure pca_best_projection_plot . MNIST compression . from sklearn.datasets import fetch_openml mnist = fetch_openml(&#39;mnist_784&#39;, version=1) mnist.target = mnist.target.astype(np.uint8) . from sklearn.model_selection import train_test_split X = mnist[&quot;data&quot;] y = mnist[&quot;target&quot;] X_train, X_test, y_train, y_test = train_test_split(X, y) . pca = PCA() pca.fit(X_train) cumsum = np.cumsum(pca.explained_variance_ratio_) d = np.argmax(cumsum &gt;= 0.95) + 1 . d . 154 . plt.figure(figsize=(6,4)) plt.plot(cumsum, linewidth=3) plt.axis([0, 400, 0, 1]) plt.xlabel(&quot;Dimensions&quot;) plt.ylabel(&quot;Explained Variance&quot;) plt.plot([d, d], [0, 0.95], &quot;k:&quot;) plt.plot([0, d], [0.95, 0.95], &quot;k:&quot;) plt.plot(d, 0.95, &quot;ko&quot;) plt.annotate(&quot;Elbow&quot;, xy=(65, 0.85), xytext=(70, 0.7), arrowprops=dict(arrowstyle=&quot;-&gt;&quot;), fontsize=16) plt.grid(True) save_fig(&quot;explained_variance_plot&quot;) plt.show() . Saving figure explained_variance_plot . pca = PCA(n_components=0.95) X_reduced = pca.fit_transform(X_train) . pca.n_components_ . 154 . np.sum(pca.explained_variance_ratio_) . 0.9504334914295708 . pca = PCA(n_components = 154) X_reduced = pca.fit_transform(X_train) X_recovered = pca.inverse_transform(X_reduced) . #collapse-show def plot_digits(instances, images_per_row=5, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = [instance.reshape(size,size) for instance in instances] n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = mpl.cm.binary, **options) plt.axis(&quot;off&quot;) . . #collapse-show plt.figure(figsize=(7, 4)) plt.subplot(121) plot_digits(X_train[::2100]) plt.title(&quot;Original&quot;, fontsize=16) plt.subplot(122) plot_digits(X_recovered[::2100]) plt.title(&quot;Compressed&quot;, fontsize=16) save_fig(&quot;mnist_compression_plot&quot;) . . Saving figure mnist_compression_plot . X_reduced_pca = X_reduced . Incremental PCA . from sklearn.decomposition import IncrementalPCA n_batches = 100 inc_pca = IncrementalPCA(n_components=154) for X_batch in np.array_split(X_train, n_batches): print(&quot;.&quot;, end=&quot;&quot;) # not shown in the book inc_pca.partial_fit(X_batch) X_reduced = inc_pca.transform(X_train) . .................................................................................................... . X_recovered_inc_pca = inc_pca.inverse_transform(X_reduced) . plt.figure(figsize=(7, 4)) plt.subplot(121) plot_digits(X_train[::2100]) plt.subplot(122) plot_digits(X_recovered_inc_pca[::2100]) plt.tight_layout() . X_reduced_inc_pca = X_reduced . Let&#39;s compare the results of transforming MNIST using regular PCA and incremental PCA. First, the means are equal: . np.allclose(pca.mean_, inc_pca.mean_) . True . But the results are not exactly identical. Incremental PCA gives a very good approximate solution, but it&#39;s not perfect: . np.allclose(X_reduced_pca, X_reduced_inc_pca) . False . Using memmap() . Let&#39;s create the memmap() structure and copy the MNIST data into it. This would typically be done by a first program: . filename = &quot;my_mnist.data&quot; m, n = X_train.shape X_mm = np.memmap(filename, dtype=&#39;float32&#39;, mode=&#39;write&#39;, shape=(m, n)) X_mm[:] = X_train . Now deleting the memmap() object will trigger its Python finalizer, which ensures that the data is saved to disk. . del X_mm . Next, another program would load the data and use it for training: . X_mm = np.memmap(filename, dtype=&quot;float32&quot;, mode=&quot;readonly&quot;, shape=(m, n)) batch_size = m // n_batches inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size) inc_pca.fit(X_mm) . IncrementalPCA(batch_size=525, copy=True, n_components=154, whiten=False) . rnd_pca = PCA(n_components=154, svd_solver=&quot;randomized&quot;, random_state=42) X_reduced = rnd_pca.fit_transform(X_train) . Time complexity . Let&#39;s time regular PCA against Incremental PCA and Randomized PCA, for various number of principal components: . import time for n_components in (2, 10, 154): print(&quot;n_components =&quot;, n_components) regular_pca = PCA(n_components=n_components) inc_pca = IncrementalPCA(n_components=n_components, batch_size=500) rnd_pca = PCA(n_components=n_components, random_state=42, svd_solver=&quot;randomized&quot;) for pca in (regular_pca, inc_pca, rnd_pca): t1 = time.time() pca.fit(X_train) t2 = time.time() print(&quot; {}: {:.1f} seconds&quot;.format(pca.__class__.__name__, t2 - t1)) . n_components = 2 PCA: 2.3 seconds IncrementalPCA: 16.2 seconds PCA: 2.0 seconds n_components = 10 PCA: 2.7 seconds IncrementalPCA: 16.8 seconds PCA: 2.9 seconds n_components = 154 PCA: 5.2 seconds IncrementalPCA: 23.5 seconds PCA: 5.1 seconds . Now let&#39;s compare PCA and Randomized PCA for datasets of different sizes (number of instances): . #collapse-show times_rpca = [] times_pca = [] sizes = [1000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 200000, 500000] for n_samples in sizes: X = np.random.randn(n_samples, 5) pca = PCA(n_components = 2, svd_solver=&quot;randomized&quot;, random_state=42) t1 = time.time() pca.fit(X) t2 = time.time() times_rpca.append(t2 - t1) pca = PCA(n_components = 2) t1 = time.time() pca.fit(X) t2 = time.time() times_pca.append(t2 - t1) plt.plot(sizes, times_rpca, &quot;b-o&quot;, label=&quot;RPCA&quot;) plt.plot(sizes, times_pca, &quot;r-s&quot;, label=&quot;PCA&quot;) plt.xlabel(&quot;n_samples&quot;) plt.ylabel(&quot;Training time&quot;) plt.legend(loc=&quot;upper left&quot;) plt.title(&quot;PCA and Randomized PCA time complexity &quot;) . . Text(0.5, 1.0, &#39;PCA and Randomized PCA time complexity &#39;) . And now let&#39;s compare their performance on datasets of 2,000 instances with various numbers of features: . #collapse-show times_rpca = [] times_pca = [] sizes = [1000, 2000, 3000, 4000, 5000, 6000] for n_features in sizes: X = np.random.randn(2000, n_features) pca = PCA(n_components = 2, random_state=42, svd_solver=&quot;randomized&quot;) t1 = time.time() pca.fit(X) t2 = time.time() times_rpca.append(t2 - t1) pca = PCA(n_components = 2) t1 = time.time() pca.fit(X) t2 = time.time() times_pca.append(t2 - t1) plt.plot(sizes, times_rpca, &quot;b-o&quot;, label=&quot;RPCA&quot;) plt.plot(sizes, times_pca, &quot;r-s&quot;, label=&quot;PCA&quot;) plt.xlabel(&quot;n_features&quot;) plt.ylabel(&quot;Training time&quot;) plt.legend(loc=&quot;upper left&quot;) plt.title(&quot;PCA and Randomized PCA time complexity &quot;) . . Text(0.5, 1.0, &#39;PCA and Randomized PCA time complexity &#39;) . Kernel PCA . X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42) . from sklearn.decomposition import KernelPCA rbf_pca = KernelPCA(n_components = 2, kernel=&quot;rbf&quot;, gamma=0.04) X_reduced = rbf_pca.fit_transform(X) . #collapse-show from sklearn.decomposition import KernelPCA lin_pca = KernelPCA(n_components = 2, kernel=&quot;linear&quot;, fit_inverse_transform=True) rbf_pca = KernelPCA(n_components = 2, kernel=&quot;rbf&quot;, gamma=0.0433, fit_inverse_transform=True) sig_pca = KernelPCA(n_components = 2, kernel=&quot;sigmoid&quot;, gamma=0.001, coef0=1, fit_inverse_transform=True) y = t &gt; 6.9 plt.figure(figsize=(11, 4)) for subplot, pca, title in ((131, lin_pca, &quot;Linear kernel&quot;), (132, rbf_pca, &quot;RBF kernel, $ gamma=0.04$&quot;), (133, sig_pca, &quot;Sigmoid kernel, $ gamma=10^{-3}, r=1$&quot;)): X_reduced = pca.fit_transform(X) if subplot == 132: X_reduced_rbf = X_reduced plt.subplot(subplot) #plt.plot(X_reduced[y, 0], X_reduced[y, 1], &quot;gs&quot;) #plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], &quot;y^&quot;) plt.title(title, fontsize=14) plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot) plt.xlabel(&quot;$z_1$&quot;, fontsize=18) if subplot == 131: plt.ylabel(&quot;$z_2$&quot;, fontsize=18, rotation=0) plt.grid(True) save_fig(&quot;kernel_pca_plot&quot;) plt.show() . . Saving figure kernel_pca_plot . plt.figure(figsize=(6, 5)) X_inverse = rbf_pca.inverse_transform(X_reduced_rbf) ax = plt.subplot(111, projection=&#39;3d&#39;) ax.view_init(10, -70) ax.scatter(X_inverse[:, 0], X_inverse[:, 1], X_inverse[:, 2], c=t, cmap=plt.cm.hot, marker=&quot;x&quot;) ax.set_xlabel(&quot;&quot;) ax.set_ylabel(&quot;&quot;) ax.set_zlabel(&quot;&quot;) ax.set_xticklabels([]) ax.set_yticklabels([]) ax.set_zticklabels([]) save_fig(&quot;preimage_plot&quot;, tight_layout=False) plt.show() . Saving figure preimage_plot . X_reduced = rbf_pca.fit_transform(X) plt.figure(figsize=(11, 4)) plt.subplot(132) plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot, marker=&quot;x&quot;) plt.xlabel(&quot;$z_1$&quot;, fontsize=18) plt.ylabel(&quot;$z_2$&quot;, fontsize=18, rotation=0) plt.grid(True) . from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline clf = Pipeline([ (&quot;kpca&quot;, KernelPCA(n_components=2)), (&quot;log_reg&quot;, LogisticRegression(solver=&quot;lbfgs&quot;)) ]) param_grid = [{ &quot;kpca__gamma&quot;: np.linspace(0.03, 0.05, 10), &quot;kpca__kernel&quot;: [&quot;rbf&quot;, &quot;sigmoid&quot;] }] grid_search = GridSearchCV(clf, param_grid, cv=3) grid_search.fit(X, y) . GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=Pipeline(memory=None, steps=[(&#39;kpca&#39;, KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver=&#39;auto&#39;, fit_inverse_transform=False, gamma=None, kernel=&#39;linear&#39;, kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)), (&#39;log_reg&#39;, LogisticRe...enalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False))]), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid=[{&#39;kpca__gamma&#39;: array([0.03 , 0.03222, 0.03444, 0.03667, 0.03889, 0.04111, 0.04333, 0.04556, 0.04778, 0.05 ]), &#39;kpca__kernel&#39;: [&#39;rbf&#39;, &#39;sigmoid&#39;]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=0) . print(grid_search.best_params_) . {&#39;kpca__gamma&#39;: 0.043333333333333335, &#39;kpca__kernel&#39;: &#39;rbf&#39;} . rbf_pca = KernelPCA(n_components = 2, kernel=&quot;rbf&quot;, gamma=0.0433, fit_inverse_transform=True) X_reduced = rbf_pca.fit_transform(X) X_preimage = rbf_pca.inverse_transform(X_reduced) . from sklearn.metrics import mean_squared_error mean_squared_error(X, X_preimage) . 32.78630879576612 . LLE . X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=41) . from sklearn.manifold import LocallyLinearEmbedding lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42) X_reduced = lle.fit_transform(X) . plt.title(&quot;Unrolled swiss roll using LLE&quot;, fontsize=14) plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot) plt.xlabel(&quot;$z_1$&quot;, fontsize=18) plt.ylabel(&quot;$z_2$&quot;, fontsize=18) plt.axis([-0.065, 0.055, -0.1, 0.12]) plt.grid(True) save_fig(&quot;lle_unrolling_plot&quot;) plt.show() . Saving figure lle_unrolling_plot . MDS, Isomap and t-SNE . from sklearn.manifold import MDS mds = MDS(n_components=2, random_state=42) X_reduced_mds = mds.fit_transform(X) . from sklearn.manifold import Isomap isomap = Isomap(n_components=2) X_reduced_isomap = isomap.fit_transform(X) . from sklearn.manifold import TSNE tsne = TSNE(n_components=2, random_state=42) X_reduced_tsne = tsne.fit_transform(X) . from sklearn.discriminant_analysis import LinearDiscriminantAnalysis lda = LinearDiscriminantAnalysis(n_components=2) X_mnist = mnist[&quot;data&quot;] y_mnist = mnist[&quot;target&quot;] lda.fit(X_mnist, y_mnist) X_reduced_lda = lda.transform(X_mnist) . /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear. warnings.warn(&#34;Variables are collinear.&#34;) . titles = [&quot;MDS&quot;, &quot;Isomap&quot;, &quot;t-SNE&quot;] plt.figure(figsize=(11,4)) for subplot, title, X_reduced in zip((131, 132, 133), titles, (X_reduced_mds, X_reduced_isomap, X_reduced_tsne)): plt.subplot(subplot) plt.title(title, fontsize=14) plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot) plt.xlabel(&quot;$z_1$&quot;, fontsize=18) if subplot == 131: plt.ylabel(&quot;$z_2$&quot;, fontsize=18, rotation=0) plt.grid(True) save_fig(&quot;other_dim_reduction_plot&quot;) plt.show() . Saving figure other_dim_reduction_plot . Exercise solutions . 1. to 8. . See appendix A. . 9. . Exercise: Load the MNIST dataset (introduced in chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). . The MNIST dataset was loaded earlier. . X_train = mnist[&#39;data&#39;][:60000] y_train = mnist[&#39;target&#39;][:60000] X_test = mnist[&#39;data&#39;][60000:] y_test = mnist[&#39;target&#39;][60000:] . Exercise: Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. . from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42) . import time t0 = time.time() rnd_clf.fit(X_train, y_train) t1 = time.time() . print(&quot;Training took {:.2f}s&quot;.format(t1 - t0)) . Training took 3.98s . from sklearn.metrics import accuracy_score y_pred = rnd_clf.predict(X_test) accuracy_score(y_test, y_pred) . 0.9492 . Exercise: Next, use PCA to reduce the dataset&#39;s dimensionality, with an explained variance ratio of 95%. . from sklearn.decomposition import PCA pca = PCA(n_components=0.95) X_train_reduced = pca.fit_transform(X_train) . Exercise: Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? . rnd_clf2 = RandomForestClassifier(n_estimators=100, random_state=42) t0 = time.time() rnd_clf2.fit(X_train_reduced, y_train) t1 = time.time() . print(&quot;Training took {:.2f}s&quot;.format(t1 - t0)) . Training took 10.29s . Oh no! Training is actually more than twice slower now! How can that be? Well, as we saw in this chapter, dimensionality reduction does not always lead to faster training time: it depends on the dataset, the model and the training algorithm. See figure 8-6 (the manifold_decision_boundary_plot* plots above). If you try a softmax classifier instead of a random forest classifier, you will find that training time is reduced by a factor of 3 when using PCA. Actually, we will do this in a second, but first let&#39;s check the precision of the new random forest classifier. . Exercise: Next evaluate the classifier on the test set: how does it compare to the previous classifier? . X_test_reduced = pca.transform(X_test) y_pred = rnd_clf2.predict(X_test_reduced) accuracy_score(y_test, y_pred) . 0.9009 . It is common for performance to drop slightly when reducing dimensionality, because we do lose some useful signal in the process. However, the performance drop is rather severe in this case. So PCA really did not help: it slowed down training and reduced performance. :( . Let&#39;s see if it helps when using softmax regression: . from sklearn.linear_model import LogisticRegression log_clf = LogisticRegression(multi_class=&quot;multinomial&quot;, solver=&quot;lbfgs&quot;, random_state=42) t0 = time.time() log_clf.fit(X_train, y_train) t1 = time.time() . /Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations. &#34;of iterations.&#34;, ConvergenceWarning) . print(&quot;Training took {:.2f}s&quot;.format(t1 - t0)) . Training took 17.35s . y_pred = log_clf.predict(X_test) accuracy_score(y_test, y_pred) . 0.9255 . Okay, so softmax regression takes much longer to train on this dataset than the random forest classifier, plus it performs worse on the test set. But that&#39;s not what we are interested in right now, we want to see how much PCA can help softmax regression. Let&#39;s train the softmax regression model using the reduced dataset: . log_clf2 = LogisticRegression(multi_class=&quot;multinomial&quot;, solver=&quot;lbfgs&quot;, random_state=42) t0 = time.time() log_clf2.fit(X_train_reduced, y_train) t1 = time.time() . /Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations. &#34;of iterations.&#34;, ConvergenceWarning) . print(&quot;Training took {:.2f}s&quot;.format(t1 - t0)) . Training took 4.87s . Nice! Reducing dimensionality led to a 4Ã— speedup. :) Let&#39;s check the model&#39;s accuracy: . y_pred = log_clf2.predict(X_test_reduced) accuracy_score(y_test, y_pred) . 0.9201 . A very slight drop in performance, which might be a reasonable price to pay for a 4Ã— speedup, depending on the application. . So there you have it: PCA can give you a formidable speedup... but not always! . 10. . Exercise: Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image&#39;s target class. . The MNIST dataset was loaded above. . Dimensionality reduction on the full 60,000 images takes a very long time, so let&#39;s only do this on a random subset of 10,000 images: . np.random.seed(42) m = 10000 idx = np.random.permutation(60000)[:m] X = mnist[&#39;data&#39;][idx] y = mnist[&#39;target&#39;][idx] . Now let&#39;s use t-SNE to reduce dimensionality down to 2D so we can plot the dataset: . from sklearn.manifold import TSNE tsne = TSNE(n_components=2, random_state=42) X_reduced = tsne.fit_transform(X) . Now let&#39;s use Matplotlib&#39;s scatter() function to plot a scatterplot, using a different color for each digit: . plt.figure(figsize=(13,10)) plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=&quot;jet&quot;) plt.axis(&#39;off&#39;) plt.colorbar() plt.show() . Isn&#39;t this just beautiful? :) This plot tells us which numbers are easily distinguishable from the others (e.g., 0s, 6s, and most 8s are rather well separated clusters), and it also tells us which numbers are often hard to distinguish (e.g., 4s and 9s, 5s and 3s, and so on). . Let&#39;s focus on digits 3 and 5, which seem to overlap a lot. . plt.figure(figsize=(9,9)) cmap = mpl.cm.get_cmap(&quot;jet&quot;) for digit in (2, 3, 5): plt.scatter(X_reduced[y == digit, 0], X_reduced[y == digit, 1], c=[cmap(digit / 9)]) plt.axis(&#39;off&#39;) plt.show() . Let&#39;s see if we can produce a nicer image by running t-SNE on these 3 digits: . idx = (y == 2) | (y == 3) | (y == 5) X_subset = X[idx] y_subset = y[idx] tsne_subset = TSNE(n_components=2, random_state=42) X_subset_reduced = tsne_subset.fit_transform(X_subset) . plt.figure(figsize=(9,9)) for digit in (2, 3, 5): plt.scatter(X_subset_reduced[y_subset == digit, 0], X_subset_reduced[y_subset == digit, 1], c=[cmap(digit / 9)]) plt.axis(&#39;off&#39;) plt.show() . Much better, now the clusters have far less overlap. But some 3s are all over the place. Plus, there are two distinct clusters of 2s, and also two distinct clusters of 5s. It would be nice if we could visualize a few digits from each cluster, to understand why this is the case. Let&#39;s do that now. . Exercise: Alternatively, you can write colored digits at the location of each instance, or even plot scaled-down versions of the digit images themselves (if you plot all digits, the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits. . Let&#39;s create a plot_digits() function that will draw a scatterplot (similar to the above scatterplots) plus write colored digits, with a minimum distance guaranteed between these digits. If the digit images are provided, they are plotted instead. This implementation was inspired from one of Scikit-Learn&#39;s excellent examples (plot_lle_digits, based on a different digit dataset). . #collapse-show from sklearn.preprocessing import MinMaxScaler from matplotlib.offsetbox import AnnotationBbox, OffsetImage def plot_digits(X, y, min_distance=0.05, images=None, figsize=(13, 10)): # Let&#39;s scale the input features so that they range from 0 to 1 X_normalized = MinMaxScaler().fit_transform(X) # Now we create the list of coordinates of the digits plotted so far. # We pretend that one is already plotted far away at the start, to # avoid `if` statements in the loop below neighbors = np.array([[10., 10.]]) # The rest should be self-explanatory plt.figure(figsize=figsize) cmap = mpl.cm.get_cmap(&quot;jet&quot;) digits = np.unique(y) for digit in digits: plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1], c=[cmap(digit / 9)]) plt.axis(&quot;off&quot;) ax = plt.gcf().gca() # get current axes in current figure for index, image_coord in enumerate(X_normalized): closest_distance = np.linalg.norm(np.array(neighbors) - image_coord, axis=1).min() if closest_distance &gt; min_distance: neighbors = np.r_[neighbors, [image_coord]] if images is None: plt.text(image_coord[0], image_coord[1], str(int(y[index])), color=cmap(y[index] / 9), fontdict={&quot;weight&quot;: &quot;bold&quot;, &quot;size&quot;: 16}) else: image = images[index].reshape(28, 28) imagebox = AnnotationBbox(OffsetImage(image, cmap=&quot;binary&quot;), image_coord) ax.add_artist(imagebox) . . Let&#39;s try it! First let&#39;s just write colored digits: . plot_digits(X_reduced, y) . Well that&#39;s okay, but not that beautiful. Let&#39;s try with the digit images: . plot_digits(X_reduced, y, images=X, figsize=(35, 25)) . plot_digits(X_subset_reduced, y_subset, images=X_subset, figsize=(22, 22)) . Exercise: Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations. . Let&#39;s start with PCA. We will also time how long it takes: . from sklearn.decomposition import PCA import time t0 = time.time() X_pca_reduced = PCA(n_components=2, random_state=42).fit_transform(X) t1 = time.time() print(&quot;PCA took {:.1f}s.&quot;.format(t1 - t0)) plot_digits(X_pca_reduced, y) plt.show() . PCA took 0.3s. . Wow, PCA is blazingly fast! But although we do see a few clusters, there&#39;s way too much overlap. Let&#39;s try LLE: . from sklearn.manifold import LocallyLinearEmbedding t0 = time.time() X_lle_reduced = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X) t1 = time.time() print(&quot;LLE took {:.1f}s.&quot;.format(t1 - t0)) plot_digits(X_lle_reduced, y) plt.show() . LLE took 118.6s. . That took a while, and the result does not look too good. Let&#39;s see what happens if we apply PCA first, preserving 95% of the variance: . from sklearn.pipeline import Pipeline pca_lle = Pipeline([ (&quot;pca&quot;, PCA(n_components=0.95, random_state=42)), (&quot;lle&quot;, LocallyLinearEmbedding(n_components=2, random_state=42)), ]) t0 = time.time() X_pca_lle_reduced = pca_lle.fit_transform(X) t1 = time.time() print(&quot;PCA+LLE took {:.1f}s.&quot;.format(t1 - t0)) plot_digits(X_pca_lle_reduced, y) plt.show() . PCA+LLE took 38.1s. . The result is more or less the same, but this time it was almost 4Ã— faster. . Let&#39;s try MDS. It&#39;s much too long if we run it on 10,000 instances, so let&#39;s just try 2,000 for now: . from sklearn.manifold import MDS m = 2000 t0 = time.time() X_mds_reduced = MDS(n_components=2, random_state=42).fit_transform(X[:m]) t1 = time.time() print(&quot;MDS took {:.1f}s (on just 2,000 MNIST images instead of 10,000).&quot;.format(t1 - t0)) plot_digits(X_mds_reduced, y[:m]) plt.show() . MDS took 119.8s (on just 2,000 MNIST images instead of 10,000). . Meh. This does not look great, all clusters overlap too much. Let&#39;s try with PCA first, perhaps it will be faster? . from sklearn.pipeline import Pipeline pca_mds = Pipeline([ (&quot;pca&quot;, PCA(n_components=0.95, random_state=42)), (&quot;mds&quot;, MDS(n_components=2, random_state=42)), ]) t0 = time.time() X_pca_mds_reduced = pca_mds.fit_transform(X[:2000]) t1 = time.time() print(&quot;PCA+MDS took {:.1f}s (on 2,000 MNIST images).&quot;.format(t1 - t0)) plot_digits(X_pca_mds_reduced, y[:2000]) plt.show() . PCA+MDS took 121.2s (on 2,000 MNIST images). . Same result, and no speedup: PCA did not help (or hurt). . Let&#39;s try LDA: . from sklearn.discriminant_analysis import LinearDiscriminantAnalysis t0 = time.time() X_lda_reduced = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y) t1 = time.time() print(&quot;LDA took {:.1f}s.&quot;.format(t1 - t0)) plot_digits(X_lda_reduced, y, figsize=(12,12)) plt.show() . /Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear. warnings.warn(&#34;Variables are collinear.&#34;) . LDA took 1.9s. . This one is very fast, and it looks nice at first, until you realize that several clusters overlap severely. . Well, it&#39;s pretty clear that t-SNE won this little competition, wouldn&#39;t you agree? We did not time it, so let&#39;s do that now: . from sklearn.manifold import TSNE t0 = time.time() X_tsne_reduced = TSNE(n_components=2, random_state=42).fit_transform(X) t1 = time.time() print(&quot;t-SNE took {:.1f}s.&quot;.format(t1 - t0)) plot_digits(X_tsne_reduced, y) plt.show() . t-SNE took 199.5s. . It&#39;s twice slower than LLE, but still much faster than MDS, and the result looks great. Let&#39;s see if a bit of PCA can speed it up: . pca_tsne = Pipeline([ (&quot;pca&quot;, PCA(n_components=0.95, random_state=42)), (&quot;tsne&quot;, TSNE(n_components=2, random_state=42)), ]) t0 = time.time() X_pca_tsne_reduced = pca_tsne.fit_transform(X) t1 = time.time() print(&quot;PCA+t-SNE took {:.1f}s.&quot;.format(t1 - t0)) plot_digits(X_pca_tsne_reduced, y) plt.show() . PCA+t-SNE took 124.8s. . Yes, PCA roughly gave us a 25% speedup, without damaging the result. We have a winner! .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/08_dimensionality_reduction",
            "relUrl": "/08_dimensionality_reduction",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Title",
            "content": "Title: Deep Computer Vision Using Convolutional Neural Networks . Chapter 14 . permalink: /14_deep_computer_vision_with_cnns | . This notebook contains all the sample code in chapter 14. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20 and TensorFlow â‰¥2.0. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x IS_COLAB = True except Exception: IS_COLAB = False # TensorFlow â‰¥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; if not tf.test.is_gpu_available(): print(&quot;No GPU was detected. CNNs can be very slow without a GPU.&quot;) if IS_COLAB: print(&quot;Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.&quot;) # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) tf.random.set_seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;cnn&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . . A couple utility functions to plot grayscale and RGB images: . def plot_image(image): plt.imshow(image, cmap=&quot;gray&quot;, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;) def plot_color_image(image): plt.imshow(image, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;) . What is a Convolution? . import numpy as np from sklearn.datasets import load_sample_image # Load sample images china = load_sample_image(&quot;china.jpg&quot;) / 255 flower = load_sample_image(&quot;flower.jpg&quot;) / 255 images = np.array([china, flower]) batch_size, height, width, channels = images.shape # Create 2 filters filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32) filters[:, 3, :, 0] = 1 # vertical line filters[3, :, :, 1] = 1 # horizontal line outputs = tf.nn.conv2d(images, filters, strides=1, padding=&quot;SAME&quot;) plt.imshow(outputs[0, :, :, 1], cmap=&quot;gray&quot;) # plot 1st image&#39;s 2nd feature map plt.axis(&quot;off&quot;) # Not shown in the book plt.show() . for image_index in (0, 1): for feature_map_index in (0, 1): plt.subplot(2, 2, image_index * 2 + feature_map_index + 1) plot_image(outputs[image_index, :, :, feature_map_index]) plt.show() . def crop(images): return images[150:220, 130:250] . plot_image(crop(images[0, :, :, 0])) save_fig(&quot;china_original&quot;, tight_layout=False) plt.show() for feature_map_index, filename in enumerate([&quot;china_vertical&quot;, &quot;china_horizontal&quot;]): plot_image(crop(outputs[0, :, :, feature_map_index])) save_fig(filename, tight_layout=False) plt.show() . Saving figure china_original . Saving figure china_vertical . Saving figure china_horizontal . plot_image(filters[:, :, 0, 0]) plt.show() plot_image(filters[:, :, 0, 1]) plt.show() . Convolutional Layer . Using keras.layers.Conv2D(): . conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=&quot;SAME&quot;, activation=&quot;relu&quot;) . plot_image(crop(outputs[0, :, :, 0])) plt.show() . VALID vs SAME padding . def feature_map_size(input_size, kernel_size, strides=1, padding=&quot;SAME&quot;): if padding == &quot;SAME&quot;: return (input_size - 1) // strides + 1 else: return (input_size - kernel_size) // strides + 1 . def pad_before_and_padded_size(input_size, kernel_size, strides=1): fmap_size = feature_map_size(input_size, kernel_size, strides) padded_size = max((fmap_size - 1) * strides + kernel_size, input_size) pad_before = (padded_size - input_size) // 2 return pad_before, padded_size . def manual_same_padding(images, kernel_size, strides=1): if kernel_size == 1: return images.astype(np.float32) batch_size, height, width, channels = images.shape top_pad, padded_height = pad_before_and_padded_size(height, kernel_size, strides) left_pad, padded_width = pad_before_and_padded_size(width, kernel_size, strides) padded_shape = [batch_size, padded_height, padded_width, channels] padded_images = np.zeros(padded_shape, dtype=np.float32) padded_images[:, top_pad:height+top_pad, left_pad:width+left_pad, :] = images return padded_images . Using &quot;SAME&quot; padding is equivalent to padding manually using manual_same_padding() then using &quot;VALID&quot; padding (confusingly, &quot;VALID&quot; padding means no padding at all): . kernel_size = 7 strides = 2 conv_valid = keras.layers.Conv2D(filters=1, kernel_size=kernel_size, strides=strides, padding=&quot;VALID&quot;) conv_same = keras.layers.Conv2D(filters=1, kernel_size=kernel_size, strides=strides, padding=&quot;SAME&quot;) valid_output = conv_valid(manual_same_padding(images, kernel_size, strides)) # Need to call build() so conv_same&#39;s weights get created conv_same.build(tf.TensorShape(images.shape)) # Copy the weights from conv_valid to conv_same conv_same.set_weights(conv_valid.get_weights()) same_output = conv_same(images.astype(np.float32)) assert np.allclose(valid_output.numpy(), same_output.numpy()) . Pooling layer . Max pooling . max_pool = keras.layers.MaxPool2D(pool_size=2) . cropped_images = np.array([crop(image) for image in images]) output = max_pool(cropped_images) . fig = plt.figure(figsize=(12, 8)) gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1]) ax1 = fig.add_subplot(gs[0, 0]) ax1.set_title(&quot;Input&quot;, fontsize=14) ax1.imshow(cropped_images[0]) # plot the 1st image ax1.axis(&quot;off&quot;) ax2 = fig.add_subplot(gs[0, 1]) ax2.set_title(&quot;Output&quot;, fontsize=14) ax2.imshow(output[0]) # plot the output for the 1st image ax2.axis(&quot;off&quot;) save_fig(&quot;china_max_pooling&quot;) plt.show() . Saving figure china_max_pooling . Depth-wise pooling . class DepthMaxPool(keras.layers.Layer): def __init__(self, pool_size, strides=None, padding=&quot;VALID&quot;, **kwargs): super().__init__(**kwargs) if strides is None: strides = pool_size self.pool_size = pool_size self.strides = strides self.padding = padding def call(self, inputs): return tf.nn.max_pool(inputs, ksize=(1, 1, 1, self.pool_size), strides=(1, 1, 1, self.pool_size), padding=self.padding) . depth_pool = DepthMaxPool(3) with tf.device(&quot;/cpu:0&quot;): # there is no GPU-kernel yet depth_output = depth_pool(cropped_images) depth_output.shape . TensorShape([2, 70, 120, 1]) . Or just use a Lambda layer: . depth_pool = keras.layers.Lambda(lambda X: tf.nn.max_pool( X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=&quot;VALID&quot;)) with tf.device(&quot;/cpu:0&quot;): # there is no GPU-kernel yet depth_output = depth_pool(cropped_images) depth_output.shape . TensorShape([2, 70, 120, 1]) . plt.figure(figsize=(12, 8)) plt.subplot(1, 2, 1) plt.title(&quot;Input&quot;, fontsize=14) plot_color_image(cropped_images[0]) # plot the 1st image plt.subplot(1, 2, 2) plt.title(&quot;Output&quot;, fontsize=14) plot_image(depth_output[0, ..., 0]) # plot the output for the 1st image plt.axis(&quot;off&quot;) plt.show() . Average pooling . avg_pool = keras.layers.AvgPool2D(pool_size=2) . output_avg = avg_pool(cropped_images) . fig = plt.figure(figsize=(12, 8)) gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1]) ax1 = fig.add_subplot(gs[0, 0]) ax1.set_title(&quot;Input&quot;, fontsize=14) ax1.imshow(cropped_images[0]) # plot the 1st image ax1.axis(&quot;off&quot;) ax2 = fig.add_subplot(gs[0, 1]) ax2.set_title(&quot;Output&quot;, fontsize=14) ax2.imshow(output_avg[0]) # plot the output for the 1st image ax2.axis(&quot;off&quot;) plt.show() . Global Average Pooling . global_avg_pool = keras.layers.GlobalAvgPool2D() global_avg_pool(cropped_images) . &lt;tf.Tensor: id=151, shape=(2, 3), dtype=float64, numpy= array([[0.27887768, 0.2250719 , 0.20967274], [0.51288515, 0.45951634, 0.33423483]])&gt; . output_global_avg2 = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2])) output_global_avg2(cropped_images) . &lt;tf.Tensor: id=155, shape=(2, 3), dtype=float64, numpy= array([[0.27887768, 0.2250719 , 0.20967274], [0.51288515, 0.45951634, 0.33423483]])&gt; . Tackling Fashion MNIST With a CNN . (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:] y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:] X_mean = X_train.mean(axis=0, keepdims=True) X_std = X_train.std(axis=0, keepdims=True) + 1e-7 X_train = (X_train - X_mean) / X_std X_valid = (X_valid - X_mean) / X_std X_test = (X_test - X_mean) / X_std X_train = X_train[..., np.newaxis] X_valid = X_valid[..., np.newaxis] X_test = X_test[..., np.newaxis] . from functools import partial DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, activation=&#39;relu&#39;, padding=&quot;SAME&quot;) model = keras.models.Sequential([ DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]), keras.layers.MaxPooling2D(pool_size=2), DefaultConv2D(filters=128), DefaultConv2D(filters=128), keras.layers.MaxPooling2D(pool_size=2), DefaultConv2D(filters=256), DefaultConv2D(filters=256), keras.layers.MaxPooling2D(pool_size=2), keras.layers.Flatten(), keras.layers.Dense(units=128, activation=&#39;relu&#39;), keras.layers.Dropout(0.5), keras.layers.Dense(units=64, activation=&#39;relu&#39;), keras.layers.Dropout(0.5), keras.layers.Dense(units=10, activation=&#39;softmax&#39;), ]) . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;, metrics=[&quot;accuracy&quot;]) history = model.fit(X_train, y_train, epochs=10, validation_data=[X_valid, y_valid]) score = model.evaluate(X_test, y_test) X_new = X_test[:10] # pretend we have new images y_pred = model.predict(X_new) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 51s 923us/sample - loss: 0.7183 - accuracy: 0.7529 - val_loss: 0.4029 - val_accuracy: 0.8510 Epoch 2/10 55000/55000 [==============================] - 47s 863us/sample - loss: 0.4185 - accuracy: 0.8592 - val_loss: 0.3285 - val_accuracy: 0.8854 Epoch 3/10 55000/55000 [==============================] - 46s 836us/sample - loss: 0.3691 - accuracy: 0.8765 - val_loss: 0.2905 - val_accuracy: 0.8936 Epoch 4/10 55000/55000 [==============================] - 46s 832us/sample - loss: 0.3324 - accuracy: 0.8879 - val_loss: 0.2794 - val_accuracy: 0.8970 Epoch 5/10 55000/55000 [==============================] - 48s 880us/sample - loss: 0.3100 - accuracy: 0.8960 - val_loss: 0.2872 - val_accuracy: 0.8942 Epoch 6/10 55000/55000 [==============================] - 51s 921us/sample - loss: 0.2930 - accuracy: 0.9008 - val_loss: 0.2863 - val_accuracy: 0.8980 Epoch 7/10 55000/55000 [==============================] - 50s 918us/sample - loss: 0.2847 - accuracy: 0.9030 - val_loss: 0.2825 - val_accuracy: 0.8972 Epoch 8/10 55000/55000 [==============================] - 50s 915us/sample - loss: 0.2728 - accuracy: 0.9080 - val_loss: 0.2734 - val_accuracy: 0.8990 Epoch 9/10 55000/55000 [==============================] - 50s 913us/sample - loss: 0.2558 - accuracy: 0.9139 - val_loss: 0.2775 - val_accuracy: 0.9056 Epoch 10/10 55000/55000 [==============================] - 50s 911us/sample - loss: 0.2561 - accuracy: 0.9145 - val_loss: 0.2891 - val_accuracy: 0.9036 10000/10000 [==============================] - 2s 239us/sample - loss: 0.2972 - accuracy: 0.8983 . ResNet-34 . DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1, padding=&quot;SAME&quot;, use_bias=False) class ResidualUnit(keras.layers.Layer): def __init__(self, filters, strides=1, activation=&quot;relu&quot;, **kwargs): super().__init__(**kwargs) self.activation = keras.activations.get(activation) self.main_layers = [ DefaultConv2D(filters, strides=strides), keras.layers.BatchNormalization(), self.activation, DefaultConv2D(filters), keras.layers.BatchNormalization()] self.skip_layers = [] if strides &gt; 1: self.skip_layers = [ DefaultConv2D(filters, kernel_size=1, strides=strides), keras.layers.BatchNormalization()] def call(self, inputs): Z = inputs for layer in self.main_layers: Z = layer(Z) skip_Z = inputs for layer in self.skip_layers: skip_Z = layer(skip_Z) return self.activation(Z + skip_Z) . model = keras.models.Sequential() model.add(DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3])) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.Activation(&quot;relu&quot;)) model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=&quot;SAME&quot;)) prev_filters = 64 for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3: strides = 1 if filters == prev_filters else 2 model.add(ResidualUnit(filters, strides=strides)) prev_filters = filters model.add(keras.layers.GlobalAvgPool2D()) model.add(keras.layers.Flatten()) model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;)) . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 112, 112, 64) 9408 _________________________________________________________________ batch_normalization (BatchNo (None, 112, 112, 64) 256 _________________________________________________________________ activation (Activation) (None, 112, 112, 64) 0 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 56, 56, 64) 0 _________________________________________________________________ residual_unit (ResidualUnit) (None, 56, 56, 64) 74240 _________________________________________________________________ residual_unit_1 (ResidualUni (None, 56, 56, 64) 74240 _________________________________________________________________ residual_unit_2 (ResidualUni (None, 56, 56, 64) 74240 _________________________________________________________________ residual_unit_3 (ResidualUni (None, 28, 28, 128) 230912 _________________________________________________________________ residual_unit_4 (ResidualUni (None, 28, 28, 128) 295936 _________________________________________________________________ residual_unit_5 (ResidualUni (None, 28, 28, 128) 295936 _________________________________________________________________ residual_unit_6 (ResidualUni (None, 28, 28, 128) 295936 _________________________________________________________________ residual_unit_7 (ResidualUni (None, 14, 14, 256) 920576 _________________________________________________________________ residual_unit_8 (ResidualUni (None, 14, 14, 256) 1181696 _________________________________________________________________ residual_unit_9 (ResidualUni (None, 14, 14, 256) 1181696 _________________________________________________________________ residual_unit_10 (ResidualUn (None, 14, 14, 256) 1181696 _________________________________________________________________ residual_unit_11 (ResidualUn (None, 14, 14, 256) 1181696 _________________________________________________________________ residual_unit_12 (ResidualUn (None, 14, 14, 256) 1181696 _________________________________________________________________ residual_unit_13 (ResidualUn (None, 7, 7, 512) 3676160 _________________________________________________________________ residual_unit_14 (ResidualUn (None, 7, 7, 512) 4722688 _________________________________________________________________ residual_unit_15 (ResidualUn (None, 7, 7, 512) 4722688 _________________________________________________________________ global_average_pooling2d (Gl (None, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 512) 0 _________________________________________________________________ dense (Dense) (None, 10) 5130 ================================================================= Total params: 21,306,826 Trainable params: 21,289,802 Non-trainable params: 17,024 _________________________________________________________________ . Using a Pretrained Model . model = keras.applications.resnet50.ResNet50(weights=&quot;imagenet&quot;) . images_resized = tf.image.resize(images, [224, 224]) plot_color_image(images_resized[0]) plt.show() . images_resized = tf.image.resize_with_pad(images, 224, 224, antialias=True) plot_color_image(images_resized[0]) . WARNING: Logging before flag parsing goes to stderr. W0323 19:04:32.720291 140735783818112 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . images_resized = tf.image.resize_with_crop_or_pad(images, 224, 224) plot_color_image(images_resized[0]) plt.show() . china_box = [0, 0.03, 1, 0.68] flower_box = [0.19, 0.26, 0.86, 0.7] images_resized = tf.image.crop_and_resize(images, [china_box, flower_box], [0, 1], [224, 224]) plot_color_image(images_resized[0]) plt.show() plot_color_image(images_resized[1]) plt.show() . inputs = keras.applications.resnet50.preprocess_input(images_resized * 255) Y_proba = model.predict(inputs) . Y_proba.shape . (2, 1000) . top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3) for image_index in range(len(images)): print(&quot;Image #{}&quot;.format(image_index)) for class_id, name, y_proba in top_K[image_index]: print(&quot; {} - {:12s} {:.2f}%&quot;.format(class_id, name, y_proba * 100)) print() . Image #0 n03877845 - palace 42.87% n02825657 - bell_cote 40.57% n03781244 - monastery 14.56% Image #1 n04522168 - vase 46.83% n07930864 - cup 7.78% n11939491 - daisy 4.87% . Pretrained Models for Transfer Learning . import tensorflow_datasets as tfds dataset, info = tfds.load(&quot;tf_flowers&quot;, as_supervised=True, with_info=True) . info.splits . {&#39;train&#39;: &lt;tfds.core.SplitInfo num_examples=3670&gt;} . info.splits[&quot;train&quot;] . &lt;tfds.core.SplitInfo num_examples=3670&gt; . class_names = info.features[&quot;label&quot;].names class_names . [&#39;dandelion&#39;, &#39;daisy&#39;, &#39;tulips&#39;, &#39;sunflowers&#39;, &#39;roses&#39;] . n_classes = info.features[&quot;label&quot;].num_classes . dataset_size = info.splits[&quot;train&quot;].num_examples dataset_size . 3670 . test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75]) test_set_raw = tfds.load(&quot;tf_flowers&quot;, split=test_split, as_supervised=True) valid_set_raw = tfds.load(&quot;tf_flowers&quot;, split=valid_split, as_supervised=True) train_set_raw = tfds.load(&quot;tf_flowers&quot;, split=train_split, as_supervised=True) . plt.figure(figsize=(12, 10)) index = 0 for image, label in train_set_raw.take(9): index += 1 plt.subplot(3, 3, index) plt.imshow(image) plt.title(&quot;Class: {}&quot;.format(class_names[label])) plt.axis(&quot;off&quot;) plt.show() . Basic preprocessing: . def preprocess(image, label): resized_image = tf.image.resize(image, [224, 224]) final_image = keras.applications.xception.preprocess_input(resized_image) return final_image, label . Slightly fancier preprocessing (but you could add much more data augmentation): . def central_crop(image): shape = tf.shape(image) min_dim = tf.reduce_min([shape[0], shape[1]]) top_crop = (shape[0] - min_dim) // 4 bottom_crop = shape[0] - top_crop left_crop = (shape[1] - min_dim) // 4 right_crop = shape[1] - left_crop return image[top_crop:bottom_crop, left_crop:right_crop] def random_crop(image): shape = tf.shape(image) min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100 return tf.image.random_crop(image, [min_dim, min_dim, 3]) def preprocess(image, label, randomize=False): if randomize: cropped_image = random_crop(image) cropped_image = tf.image.random_flip_left_right(cropped_image) else: cropped_image = central_crop(image) resized_image = tf.image.resize(cropped_image, [224, 224]) final_image = keras.applications.xception.preprocess_input(resized_image) return final_image, label batch_size = 32 train_set = train_set_raw.shuffle(1000).repeat() train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1) valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1) test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1) . plt.figure(figsize=(12, 12)) for X_batch, y_batch in train_set.take(1): for index in range(9): plt.subplot(3, 3, index + 1) plt.imshow(X_batch[index] / 2 + 0.5) plt.title(&quot;Class: {}&quot;.format(class_names[y_batch[index]])) plt.axis(&quot;off&quot;) plt.show() . plt.figure(figsize=(12, 12)) for X_batch, y_batch in test_set.take(1): for index in range(9): plt.subplot(3, 3, index + 1) plt.imshow(X_batch[index] / 2 + 0.5) plt.title(&quot;Class: {}&quot;.format(class_names[y_batch[index]])) plt.axis(&quot;off&quot;) plt.show() . base_model = keras.applications.xception.Xception(weights=&quot;imagenet&quot;, include_top=False) avg = keras.layers.GlobalAveragePooling2D()(base_model.output) output = keras.layers.Dense(n_classes, activation=&quot;softmax&quot;)(avg) model = keras.models.Model(inputs=base_model.input, outputs=output) . for index, layer in enumerate(base_model.layers): print(index, layer.name) . 0 input_2 1 block1_conv1 2 block1_conv1_bn 3 block1_conv1_act 4 block1_conv2 5 block1_conv2_bn 6 block1_conv2_act 7 block2_sepconv1 8 block2_sepconv1_bn 9 block2_sepconv2_act 10 block2_sepconv2 11 block2_sepconv2_bn 12 conv2d_44 13 block2_pool 14 batch_normalization_35 15 add_16 16 block3_sepconv1_act 17 block3_sepconv1 18 block3_sepconv1_bn 19 block3_sepconv2_act 20 block3_sepconv2 21 block3_sepconv2_bn 22 conv2d_45 23 block3_pool 24 batch_normalization_36 25 add_17 26 block4_sepconv1_act 27 block4_sepconv1 28 block4_sepconv1_bn 29 block4_sepconv2_act 30 block4_sepconv2 31 block4_sepconv2_bn 32 conv2d_46 33 block4_pool 34 batch_normalization_37 &lt;&lt;62 more lines&gt;&gt; 97 block11_sepconv1 98 block11_sepconv1_bn 99 block11_sepconv2_act 100 block11_sepconv2 101 block11_sepconv2_bn 102 block11_sepconv3_act 103 block11_sepconv3 104 block11_sepconv3_bn 105 add_25 106 block12_sepconv1_act 107 block12_sepconv1 108 block12_sepconv1_bn 109 block12_sepconv2_act 110 block12_sepconv2 111 block12_sepconv2_bn 112 block12_sepconv3_act 113 block12_sepconv3 114 block12_sepconv3_bn 115 add_26 116 block13_sepconv1_act 117 block13_sepconv1 118 block13_sepconv1_bn 119 block13_sepconv2_act 120 block13_sepconv2 121 block13_sepconv2_bn 122 conv2d_47 123 block13_pool 124 batch_normalization_38 125 add_27 126 block14_sepconv1 127 block14_sepconv1_bn 128 block14_sepconv1_act 129 block14_sepconv2 130 block14_sepconv2_bn 131 block14_sepconv2_act . for layer in base_model.layers: layer.trainable = False optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=optimizer, metrics=[&quot;accuracy&quot;]) history = model.fit(train_set, steps_per_epoch=int(0.75 * dataset_size / batch_size), validation_data=valid_set, validation_steps=int(0.15 * dataset_size / batch_size), epochs=5) . Epoch 1/5 86/86 [==============================] - 46s 532ms/step - loss: 0.6858 - accuracy: 0.7758 - val_loss: 1.7375 - val_accuracy: 0.7335 Epoch 2/5 86/86 [==============================] - 39s 450ms/step - loss: 0.3833 - accuracy: 0.8765 - val_loss: 1.2491 - val_accuracy: 0.7592 Epoch 3/5 86/86 [==============================] - 39s 450ms/step - loss: 0.3270 - accuracy: 0.8903 - val_loss: 1.2740 - val_accuracy: 0.7647 Epoch 4/5 86/86 [==============================] - 39s 451ms/step - loss: 0.2821 - accuracy: 0.9113 - val_loss: 1.1322 - val_accuracy: 0.7757 Epoch 5/5 86/86 [==============================] - 39s 452ms/step - loss: 0.2430 - accuracy: 0.9121 - val_loss: 1.5182 - val_accuracy: 0.7426 . for layer in base_model.layers: layer.trainable = True optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True, decay=0.001) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=optimizer, metrics=[&quot;accuracy&quot;]) history = model.fit(train_set, steps_per_epoch=int(0.75 * dataset_size / batch_size), validation_data=valid_set, validation_steps=int(0.15 * dataset_size / batch_size), epochs=40) . Epoch 1/40 86/86 [==============================] - 172s 2s/step - loss: 0.2257 - accuracy: 0.9288 - val_loss: 0.6762 - val_accuracy: 0.8346 Epoch 2/40 86/86 [==============================] - 128s 1s/step - loss: 0.1124 - accuracy: 0.9640 - val_loss: 0.3932 - val_accuracy: 0.9154 Epoch 3/40 86/86 [==============================] - 129s 1s/step - loss: 0.0497 - accuracy: 0.9829 - val_loss: 0.2618 - val_accuracy: 0.9246 Epoch 4/40 86/86 [==============================] - 128s 1s/step - loss: 0.0425 - accuracy: 0.9836 - val_loss: 0.3446 - val_accuracy: 0.9136 Epoch 5/40 86/86 [==============================] - 128s 1s/step - loss: 0.0251 - accuracy: 0.9909 - val_loss: 0.2486 - val_accuracy: 0.9338 Epoch 6/40 86/86 [==============================] - 127s 1s/step - loss: 0.0142 - accuracy: 0.9949 - val_loss: 0.2324 - val_accuracy: 0.9430 Epoch 7/40 86/86 [==============================] - 128s 1s/step - loss: 0.0195 - accuracy: 0.9945 - val_loss: 0.2785 - val_accuracy: 0.9357 Epoch 8/40 86/86 [==============================] - 128s 1s/step - loss: 0.0154 - accuracy: 0.9956 - val_loss: 0.3262 - val_accuracy: 0.9173 Epoch 9/40 86/86 [==============================] - 128s 1s/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.2554 - val_accuracy: 0.9357 Epoch 10/40 86/86 [==============================] - 128s 1s/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.2573 - val_accuracy: 0.9357 Epoch 11/40 86/86 [==============================] - 128s 1s/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 0.2744 - val_accuracy: 0.9430 Epoch 12/40 86/86 [==============================] - 128s 1s/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.2788 - val_accuracy: 0.9301 Epoch 13/40 86/86 [==============================] - 128s 1s/step - loss: 0.0089 - accuracy: 0.9975 - val_loss: 0.2488 - val_accuracy: 0.9393 Epoch 14/40 86/86 [==============================] - 128s 1s/step - loss: 0.0113 - accuracy: 0.9956 - val_loss: 0.2703 - val_accuracy: 0.9430 Epoch 15/40 86/86 [==============================] - 129s 1s/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.2452 - val_accuracy: 0.9504 Epoch 16/40 86/86 [==============================] - 127s 1s/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.2327 - val_accuracy: 0.9522 Epoch 17/40 86/86 [==============================] - 127s 1s/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.2194 - val_accuracy: 0.9540 Epoch 18/40 86/86 [==============================] - 129s 1s/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.2283 - val_accuracy: 0.9540 Epoch 19/40 86/86 [==============================] - 128s 1s/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.2384 - val_accuracy: 0.9504 Epoch 20/40 86/86 [==============================] - 128s 1s/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.2610 - val_accuracy: 0.9467 Epoch 21/40 86/86 [==============================] - 128s 1s/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.2403 - val_accuracy: 0.9522 Epoch 22/40 86/86 [==============================] - 128s 1s/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.2339 - val_accuracy: 0.9485 Epoch 23/40 86/86 [==============================] - 128s 1s/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.2295 - val_accuracy: 0.9467 Epoch 24/40 86/86 [==============================] - 128s 1s/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.2327 - val_accuracy: 0.9485 Epoch 25/40 86/86 [==============================] - 128s 1s/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.2381 - val_accuracy: 0.9522 Epoch 26/40 86/86 [==============================] - 127s 1s/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2348 - val_accuracy: 0.9467 Epoch 27/40 86/86 [==============================] - 127s 1s/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.2365 - val_accuracy: 0.9504 Epoch 28/40 86/86 [==============================] - 128s 1s/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.2526 - val_accuracy: 0.9467 Epoch 29/40 86/86 [==============================] - 127s 1s/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2445 - val_accuracy: 0.9485 Epoch 30/40 86/86 [==============================] - 128s 1s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2423 - val_accuracy: 0.9485 Epoch 31/40 86/86 [==============================] - 127s 1s/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.2378 - val_accuracy: 0.9485 Epoch 32/40 86/86 [==============================] - 128s 1s/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.2440 - val_accuracy: 0.9504 Epoch 33/40 86/86 [==============================] - 128s 1s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9485 Epoch 34/40 86/86 [==============================] - 127s 1s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2450 - val_accuracy: 0.9504 Epoch 35/40 86/86 [==============================] - 127s 1s/step - loss: 6.0323e-04 - accuracy: 1.0000 - val_loss: 0.2460 - val_accuracy: 0.9522 Epoch 36/40 86/86 [==============================] - 126s 1s/step - loss: 0.0039 - accuracy: 0.9982 - val_loss: 0.2317 - val_accuracy: 0.9577 Epoch 37/40 86/86 [==============================] - 127s 1s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2332 - val_accuracy: 0.9559 Epoch 38/40 86/86 [==============================] - 126s 1s/step - loss: 9.6522e-04 - accuracy: 0.9996 - val_loss: 0.2356 - val_accuracy: 0.9522 Epoch 39/40 86/86 [==============================] - 127s 1s/step - loss: 0.0032 - accuracy: 0.9985 - val_loss: 0.2409 - val_accuracy: 0.9522 Epoch 40/40 86/86 [==============================] - 127s 1s/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2386 - val_accuracy: 0.9540 . Classification and Localization . base_model = keras.applications.xception.Xception(weights=&quot;imagenet&quot;, include_top=False) avg = keras.layers.GlobalAveragePooling2D()(base_model.output) class_output = keras.layers.Dense(n_classes, activation=&quot;softmax&quot;)(avg) loc_output = keras.layers.Dense(4)(avg) model = keras.models.Model(inputs=base_model.input, outputs=[class_output, loc_output]) model.compile(loss=[&quot;sparse_categorical_crossentropy&quot;, &quot;mse&quot;], loss_weights=[0.8, 0.2], # depends on what you care most about optimizer=optimizer, metrics=[&quot;accuracy&quot;]) . def add_random_bounding_boxes(images, labels): fake_bboxes = tf.random.uniform([tf.shape(images)[0], 4]) return images, (labels, fake_bboxes) fake_train_set = train_set.take(5).repeat(2).map(add_random_bounding_boxes) . model.fit(fake_train_set, steps_per_epoch=5, epochs=2) . Epoch 1/2 5/5 [==============================] - 134s 27s/step - loss: 1.3279 - dense_5_loss: 1.5696 - dense_6_loss: 0.3612 - dense_5_accuracy: 0.2625 - dense_6_accuracy: 0.2562 Epoch 2/2 5/5 [==============================] - 93s 19s/step - loss: 1.0533 - dense_5_loss: 1.2658 - dense_6_loss: 0.2035 - dense_5_accuracy: 0.5938 - dense_6_accuracy: 0.2000 . &lt;tensorflow.python.keras.callbacks.History at 0x152562d68&gt; . Mean Average Precision (mAP) . def maximum_precisions(precisions): return np.flip(np.maximum.accumulate(np.flip(precisions))) . recalls = np.linspace(0, 1, 11) precisions = [0.91, 0.94, 0.96, 0.94, 0.95, 0.92, 0.80, 0.60, 0.45, 0.20, 0.10] max_precisions = maximum_precisions(precisions) mAP = max_precisions.mean() plt.plot(recalls, precisions, &quot;ro--&quot;, label=&quot;Precision&quot;) plt.plot(recalls, max_precisions, &quot;bo-&quot;, label=&quot;Max Precision&quot;) plt.xlabel(&quot;Recall&quot;) plt.ylabel(&quot;Precision&quot;) plt.plot([0, 1], [mAP, mAP], &quot;g:&quot;, linewidth=3, label=&quot;mAP&quot;) plt.grid(True) plt.axis([0, 1, 0, 1]) plt.legend(loc=&quot;lower center&quot;, fontsize=14) plt.show() . Transpose convolutions: . tf.random.set_seed(42) X = images_resized.numpy() conv_transpose = keras.layers.Conv2DTranspose(filters=5, kernel_size=3, strides=2, padding=&quot;VALID&quot;) output = conv_transpose(X) output.shape . TensorShape([2, 449, 449, 5]) . def normalize(X): return (X - tf.reduce_min(X)) / (tf.reduce_max(X) - tf.reduce_min(X)) fig = plt.figure(figsize=(12, 8)) gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[1, 2]) ax1 = fig.add_subplot(gs[0, 0]) ax1.set_title(&quot;Input&quot;, fontsize=14) ax1.imshow(X[0]) # plot the 1st image ax1.axis(&quot;off&quot;) ax2 = fig.add_subplot(gs[0, 1]) ax2.set_title(&quot;Output&quot;, fontsize=14) ax2.imshow(normalize(output[0, ..., :3]), interpolation=&quot;bicubic&quot;) # plot the output for the 1st image ax2.axis(&quot;off&quot;) plt.show() . def upscale_images(images, stride, kernel_size): batch_size, height, width, channels = images.shape upscaled = np.zeros((batch_size, (height - 1) * stride + 2 * kernel_size - 1, (width - 1) * stride + 2 * kernel_size - 1, channels)) upscaled[:, kernel_size - 1:(height - 1) * stride + kernel_size:stride, kernel_size - 1:(width - 1) * stride + kernel_size:stride, :] = images return upscaled . upscaled = upscale_images(X, stride=2, kernel_size=3) weights, biases = conv_transpose.weights reversed_filters = np.flip(weights.numpy(), axis=[0, 1]) reversed_filters = np.transpose(reversed_filters, [0, 1, 3, 2]) manual_output = tf.nn.conv2d(upscaled, reversed_filters, strides=1, padding=&quot;VALID&quot;) . def normalize(X): return (X - tf.reduce_min(X)) / (tf.reduce_max(X) - tf.reduce_min(X)) fig = plt.figure(figsize=(12, 8)) gs = mpl.gridspec.GridSpec(nrows=1, ncols=3, width_ratios=[1, 2, 2]) ax1 = fig.add_subplot(gs[0, 0]) ax1.set_title(&quot;Input&quot;, fontsize=14) ax1.imshow(X[0]) # plot the 1st image ax1.axis(&quot;off&quot;) ax2 = fig.add_subplot(gs[0, 1]) ax2.set_title(&quot;Upscaled&quot;, fontsize=14) ax2.imshow(upscaled[0], interpolation=&quot;bicubic&quot;) ax2.axis(&quot;off&quot;) ax3 = fig.add_subplot(gs[0, 2]) ax3.set_title(&quot;Output&quot;, fontsize=14) ax3.imshow(normalize(manual_output[0, ..., :3]), interpolation=&quot;bicubic&quot;) # plot the output for the 1st image ax3.axis(&quot;off&quot;) plt.show() . np.allclose(output, manual_output.numpy(), atol=1e-7) . True . Exercises . 1. to 8. . See appendix A. . 9. High Accuracy CNN for MNIST . Exercise: Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST. . 10. Use transfer learning for large image classification . 10.1) . Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can just use an existing dataset (e.g., from TensorFlow Datasets). . 10.2) . Split it into a training set, a validation set and a test set. . 10.3) . Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation. . 10.4) . Fine-tune a pretrained model on this dataset. . 11. . Exercise: Go through TensorFlow&#39;s DeepDream tutorial. It is a fun way to familiarize yourself with various ways of visualizing the patterns learned by a CNN, and to generate art using Deep Learning. . Simply download the notebook and follow its instructions. For extra fun, you can produce a series of images, by repeatedly zooming in and running the DeepDream algorithm: using a tool such as ffmpeg you can then create a video from these images. For example, here is a DeepDream video I made... as you will see, it quickly turns into a nightmare. ;-) You can find hundreds of similar videos (often much more artistic) on the web. .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/_deep_computer_vision_with_cnns.html",
            "relUrl": "/2020/03/09/_deep_computer_vision_with_cnns.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Decision Trees",
            "content": "This notebook contains all the sample code and solutions to the exercises in chapter 6. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;decision_trees&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . . Training and visualizing . #collapse-show from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42) tree_clf.fit(X, y) . . DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . from graphviz import Source from sklearn.tree import export_graphviz export_graphviz( tree_clf, out_file=os.path.join(IMAGES_PATH, &quot;iris_tree.dot&quot;), feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True ) Source.from_file(os.path.join(IMAGES_PATH, &quot;iris_tree.dot&quot;)) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 petal length (cm) &lt;= 2.45 gini = 0.667 samples = 150 value = [50, 50, 50] class = setosa 1 gini = 0.0 samples = 50 value = [50, 0, 0] class = setosa 0&#45;&gt;1 True 2 petal width (cm) &lt;= 1.75 gini = 0.5 samples = 100 value = [0, 50, 50] class = versicolor 0&#45;&gt;2 False 3 gini = 0.168 samples = 54 value = [0, 49, 5] class = versicolor 2&#45;&gt;3 4 gini = 0.043 samples = 46 value = [0, 1, 45] class = virginica 2&#45;&gt;4 #collapse-show from matplotlib.colors import ListedColormap def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True): x1s = np.linspace(axes[0], axes[1], 100) x2s = np.linspace(axes[2], axes[3], 100) x1, x2 = np.meshgrid(x1s, x2s) X_new = np.c_[x1.ravel(), x2.ravel()] y_pred = clf.predict(X_new).reshape(x1.shape) custom_cmap = ListedColormap([&#39;#fafab0&#39;,&#39;#9898ff&#39;,&#39;#a0faa0&#39;]) plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap) if not iris: custom_cmap2 = ListedColormap([&#39;#7d7d58&#39;,&#39;#4c4c7f&#39;,&#39;#507d50&#39;]) plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8) if plot_training: plt.plot(X[:, 0][y==0], X[:, 1][y==0], &quot;yo&quot;, label=&quot;Iris setosa&quot;) plt.plot(X[:, 0][y==1], X[:, 1][y==1], &quot;bs&quot;, label=&quot;Iris versicolor&quot;) plt.plot(X[:, 0][y==2], X[:, 1][y==2], &quot;g^&quot;, label=&quot;Iris virginica&quot;) plt.axis(axes) if iris: plt.xlabel(&quot;Petal length&quot;, fontsize=14) plt.ylabel(&quot;Petal width&quot;, fontsize=14) else: plt.xlabel(r&quot;$x_1$&quot;, fontsize=18) plt.ylabel(r&quot;$x_2$&quot;, fontsize=18, rotation=0) if legend: plt.legend(loc=&quot;lower right&quot;, fontsize=14) plt.figure(figsize=(8, 4)) plot_decision_boundary(tree_clf, X, y) plt.plot([2.45, 2.45], [0, 3], &quot;k-&quot;, linewidth=2) plt.plot([2.45, 7.5], [1.75, 1.75], &quot;k--&quot;, linewidth=2) plt.plot([4.95, 4.95], [0, 1.75], &quot;k:&quot;, linewidth=2) plt.plot([4.85, 4.85], [1.75, 3], &quot;k:&quot;, linewidth=2) plt.text(1.40, 1.0, &quot;Depth=0&quot;, fontsize=15) plt.text(3.2, 1.80, &quot;Depth=1&quot;, fontsize=13) plt.text(4.05, 0.5, &quot;(Depth=2)&quot;, fontsize=11) save_fig(&quot;decision_tree_decision_boundaries_plot&quot;) plt.show() . . Saving figure decision_tree_decision_boundaries_plot . Predicting classes and class probabilities . tree_clf.predict_proba([[5, 1.5]]) . array([[0. , 0.90740741, 0.09259259]]) . tree_clf.predict([[5, 1.5]]) . array([1]) . Sensitivity to training set details . X[(X[:, 1]==X[:, 1][y==1].max()) &amp; (y==1)] # widest Iris versicolor flower . array([[4.8, 1.8]]) . not_widest_versicolor = (X[:, 1]!=1.8) | (y==2) X_tweaked = X[not_widest_versicolor] y_tweaked = y[not_widest_versicolor] tree_clf_tweaked = DecisionTreeClassifier(max_depth=2, random_state=40) tree_clf_tweaked.fit(X_tweaked, y_tweaked) . DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=40, splitter=&#39;best&#39;) . #collapse-show plt.figure(figsize=(8, 4)) plot_decision_boundary(tree_clf_tweaked, X_tweaked, y_tweaked, legend=False) plt.plot([0, 7.5], [0.8, 0.8], &quot;k-&quot;, linewidth=2) plt.plot([0, 7.5], [1.75, 1.75], &quot;k--&quot;, linewidth=2) plt.text(1.0, 0.9, &quot;Depth=0&quot;, fontsize=15) plt.text(1.0, 1.80, &quot;Depth=1&quot;, fontsize=13) save_fig(&quot;decision_tree_instability_plot&quot;) plt.show() . . Saving figure decision_tree_instability_plot . #collapse-show from sklearn.datasets import make_moons Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53) deep_tree_clf1 = DecisionTreeClassifier(random_state=42) deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42) deep_tree_clf1.fit(Xm, ym) deep_tree_clf2.fit(Xm, ym) fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True) plt.sca(axes[0]) plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False) plt.title(&quot;No restrictions&quot;, fontsize=16) plt.sca(axes[1]) plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False) plt.title(&quot;min_samples_leaf = {}&quot;.format(deep_tree_clf2.min_samples_leaf), fontsize=14) plt.ylabel(&quot;&quot;) save_fig(&quot;min_samples_leaf_plot&quot;) plt.show() . . Saving figure min_samples_leaf_plot . #collapse-show angle = np.pi / 180 * 20 rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]]) Xr = X.dot(rotation_matrix) tree_clf_r = DecisionTreeClassifier(random_state=42) tree_clf_r.fit(Xr, y) plt.figure(figsize=(8, 3)) plot_decision_boundary(tree_clf_r, Xr, y, axes=[0.5, 7.5, -1.0, 1], iris=False) plt.show() . . #collapse-show np.random.seed(6) Xs = np.random.rand(100, 2) - 0.5 ys = (Xs[:, 0] &gt; 0).astype(np.float32) * 2 angle = np.pi / 4 rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]]) Xsr = Xs.dot(rotation_matrix) tree_clf_s = DecisionTreeClassifier(random_state=42) tree_clf_s.fit(Xs, ys) tree_clf_sr = DecisionTreeClassifier(random_state=42) tree_clf_sr.fit(Xsr, ys) fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True) plt.sca(axes[0]) plot_decision_boundary(tree_clf_s, Xs, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False) plt.sca(axes[1]) plot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False) plt.ylabel(&quot;&quot;) save_fig(&quot;sensitivity_to_rotation_plot&quot;) plt.show() . . Saving figure sensitivity_to_rotation_plot . Regression trees . # Quadratic training set + noise np.random.seed(42) m = 200 X = np.random.rand(m, 1) y = 4 * (X - 0.5) ** 2 y = y + np.random.randn(m, 1) / 10 . from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42) tree_reg.fit(X, y) . DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . #collapse-show from sklearn.tree import DecisionTreeRegressor tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2) tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3) tree_reg1.fit(X, y) tree_reg2.fit(X, y) def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=&quot;$y$&quot;): x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1) y_pred = tree_reg.predict(x1) plt.axis(axes) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) if ylabel: plt.ylabel(ylabel, fontsize=18, rotation=0) plt.plot(X, y, &quot;b.&quot;) plt.plot(x1, y_pred, &quot;r.-&quot;, linewidth=2, label=r&quot;$ hat{y}$&quot;) fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True) plt.sca(axes[0]) plot_regression_predictions(tree_reg1, X, y) for split, style in ((0.1973, &quot;k-&quot;), (0.0917, &quot;k--&quot;), (0.7718, &quot;k--&quot;)): plt.plot([split, split], [-0.2, 1], style, linewidth=2) plt.text(0.21, 0.65, &quot;Depth=0&quot;, fontsize=15) plt.text(0.01, 0.2, &quot;Depth=1&quot;, fontsize=13) plt.text(0.65, 0.8, &quot;Depth=1&quot;, fontsize=13) plt.legend(loc=&quot;upper center&quot;, fontsize=18) plt.title(&quot;max_depth=2&quot;, fontsize=14) plt.sca(axes[1]) plot_regression_predictions(tree_reg2, X, y, ylabel=None) for split, style in ((0.1973, &quot;k-&quot;), (0.0917, &quot;k--&quot;), (0.7718, &quot;k--&quot;)): plt.plot([split, split], [-0.2, 1], style, linewidth=2) for split in (0.0458, 0.1298, 0.2873, 0.9040): plt.plot([split, split], [-0.2, 1], &quot;k:&quot;, linewidth=1) plt.text(0.3, 0.5, &quot;Depth=2&quot;, fontsize=13) plt.title(&quot;max_depth=3&quot;, fontsize=14) save_fig(&quot;tree_regression_plot&quot;) plt.show() . . Saving figure tree_regression_plot . export_graphviz( tree_reg1, out_file=os.path.join(IMAGES_PATH, &quot;regression_tree.dot&quot;), feature_names=[&quot;x1&quot;], rounded=True, filled=True ) . Source.from_file(os.path.join(IMAGES_PATH, &quot;regression_tree.dot&quot;)) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 x1 &lt;= 0.197 mse = 0.098 samples = 200 value = 0.354 1 x1 &lt;= 0.092 mse = 0.038 samples = 44 value = 0.689 0&#45;&gt;1 True 4 x1 &lt;= 0.772 mse = 0.074 samples = 156 value = 0.259 0&#45;&gt;4 False 2 mse = 0.018 samples = 20 value = 0.854 1&#45;&gt;2 3 mse = 0.013 samples = 24 value = 0.552 1&#45;&gt;3 5 mse = 0.015 samples = 110 value = 0.111 4&#45;&gt;5 6 mse = 0.036 samples = 46 value = 0.615 4&#45;&gt;6 #collapse-show tree_reg1 = DecisionTreeRegressor(random_state=42) tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10) tree_reg1.fit(X, y) tree_reg2.fit(X, y) x1 = np.linspace(0, 1, 500).reshape(-1, 1) y_pred1 = tree_reg1.predict(x1) y_pred2 = tree_reg2.predict(x1) fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True) plt.sca(axes[0]) plt.plot(X, y, &quot;b.&quot;) plt.plot(x1, y_pred1, &quot;r.-&quot;, linewidth=2, label=r&quot;$ hat{y}$&quot;) plt.axis([0, 1, -0.2, 1.1]) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.ylabel(&quot;$y$&quot;, fontsize=18, rotation=0) plt.legend(loc=&quot;upper center&quot;, fontsize=18) plt.title(&quot;No restrictions&quot;, fontsize=14) plt.sca(axes[1]) plt.plot(X, y, &quot;b.&quot;) plt.plot(x1, y_pred2, &quot;r.-&quot;, linewidth=2, label=r&quot;$ hat{y}$&quot;) plt.axis([0, 1, -0.2, 1.1]) plt.xlabel(&quot;$x_1$&quot;, fontsize=18) plt.title(&quot;min_samples_leaf={}&quot;.format(tree_reg2.min_samples_leaf), fontsize=14) save_fig(&quot;tree_regression_regularization_plot&quot;) plt.show() . . Saving figure tree_regression_regularization_plot . Exercise solutions . 1. to 6. . See appendix A. . 7. . Exercise: train and fine-tune a Decision Tree for the moons dataset. . a. Generate a moons dataset using make_moons(n_samples=10000, noise=0.4). . Adding random_state=42 to make this notebook&#39;s output constant: . from sklearn.datasets import make_moons X, y = make_moons(n_samples=10000, noise=0.4, random_state=42) . b. Split it into a training set and a test set using train_test_split(). . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . c. Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes. . from sklearn.model_selection import GridSearchCV params = {&#39;max_leaf_nodes&#39;: list(range(2, 100)), &#39;min_samples_split&#39;: [2, 3, 4]} grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3) grid_search_cv.fit(X_train, y_train) . Fitting 3 folds for each of 294 candidates, totalling 882 fits . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 882 out of 882 | elapsed: 6.8s finished . GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid={&#39;max_leaf_nodes&#39;: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], &#39;min_samples_split&#39;: [2, 3, 4]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=1) . grid_search_cv.best_estimator_ . DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=17, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . d. Train it on the full training set using these hyperparameters, and measure your model&#39;s performance on the test set. You should get roughly 85% to 87% accuracy. . By default, GridSearchCV trains the best model found on the whole training set (you can change this by setting refit=False), so we don&#39;t need to do it again. We can simply evaluate the model&#39;s accuracy: . from sklearn.metrics import accuracy_score y_pred = grid_search_cv.predict(X_test) accuracy_score(y_test, y_pred) . 0.8695 . 8. . Exercise: Grow a forest. . a. Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn&#39;s ShuffleSplit class for this. . from sklearn.model_selection import ShuffleSplit n_trees = 1000 n_instances = 100 mini_sets = [] rs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42) for mini_train_index, mini_test_index in rs.split(X_train): X_mini_train = X_train[mini_train_index] y_mini_train = y_train[mini_train_index] mini_sets.append((X_mini_train, y_mini_train)) . b. Train one Decision Tree on each subset, using the best hyperparameter values found above. Evaluate these 1,000 Decision Trees on the test set. Since they were trained on smaller sets, these Decision Trees will likely perform worse than the first Decision Tree, achieving only about 80% accuracy. . from sklearn.base import clone forest = [clone(grid_search_cv.best_estimator_) for _ in range(n_trees)] accuracy_scores = [] for tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets): tree.fit(X_mini_train, y_mini_train) y_pred = tree.predict(X_test) accuracy_scores.append(accuracy_score(y_test, y_pred)) np.mean(accuracy_scores) . 0.8054499999999999 . c. Now comes the magic. For each test set instance, generate the predictions of the 1,000 Decision Trees, and keep only the most frequent prediction (you can use SciPy&#39;s mode() function for this). This gives you majority-vote predictions over the test set. . Y_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8) for tree_index, tree in enumerate(forest): Y_pred[tree_index] = tree.predict(X_test) . from scipy.stats import mode y_pred_majority_votes, n_votes = mode(Y_pred, axis=0) . d. Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a Random Forest classifier! . accuracy_score(y_test, y_pred_majority_votes.reshape([-1])) . 0.872 .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/06_decision_trees",
            "relUrl": "/06_decision_trees",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Custom Models and Training with TensorFlow",
            "content": "This notebook contains all the sample code in chapter 12. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20 and TensorFlow â‰¥2.0. . #collapse-show # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass # TensorFlow â‰¥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;deep&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . . Tensors and operations . Tensors . tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix . &lt;tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy= array([[1., 2., 3.], [4., 5., 6.]], dtype=float32)&gt; . tf.constant(42) # scalar . &lt;tf.Tensor: id=1, shape=(), dtype=int32, numpy=42&gt; . t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) t . &lt;tf.Tensor: id=2, shape=(2, 3), dtype=float32, numpy= array([[1., 2., 3.], [4., 5., 6.]], dtype=float32)&gt; . t.shape . TensorShape([2, 3]) . t.dtype . tf.float32 . Indexing . t[:, 1:] . &lt;tf.Tensor: id=6, shape=(2, 2), dtype=float32, numpy= array([[2., 3.], [5., 6.]], dtype=float32)&gt; . t[..., 1, tf.newaxis] . &lt;tf.Tensor: id=10, shape=(2, 1), dtype=float32, numpy= array([[2.], [5.]], dtype=float32)&gt; . Ops . t + 10 . &lt;tf.Tensor: id=12, shape=(2, 3), dtype=float32, numpy= array([[11., 12., 13.], [14., 15., 16.]], dtype=float32)&gt; . tf.square(t) . &lt;tf.Tensor: id=13, shape=(2, 3), dtype=float32, numpy= array([[ 1., 4., 9.], [16., 25., 36.]], dtype=float32)&gt; . t @ tf.transpose(t) . &lt;tf.Tensor: id=16, shape=(2, 2), dtype=float32, numpy= array([[14., 32.], [32., 77.]], dtype=float32)&gt; . Using keras.backend . from tensorflow import keras K = keras.backend K.square(K.transpose(t)) + 10 . &lt;tf.Tensor: id=21, shape=(3, 2), dtype=float32, numpy= array([[11., 26.], [14., 35.], [19., 46.]], dtype=float32)&gt; . From/To NumPy . a = np.array([2., 4., 5.]) tf.constant(a) . &lt;tf.Tensor: id=22, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])&gt; . t.numpy() . array([[1., 2., 3.], [4., 5., 6.]], dtype=float32) . np.array(t) . array([[1., 2., 3.], [4., 5., 6.]], dtype=float32) . tf.square(a) . &lt;tf.Tensor: id=24, shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])&gt; . np.square(t) . array([[ 1., 4., 9.], [16., 25., 36.]], dtype=float32) . Conflicting Types . try: tf.constant(2.0) + tf.constant(40) except tf.errors.InvalidArgumentError as ex: print(ex) . cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: add/ . try: tf.constant(2.0) + tf.constant(40., dtype=tf.float64) except tf.errors.InvalidArgumentError as ex: print(ex) . cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] name: add/ . t2 = tf.constant(40., dtype=tf.float64) tf.constant(2.0) + tf.cast(t2, tf.float32) . &lt;tf.Tensor: id=32, shape=(), dtype=float32, numpy=42.0&gt; . Strings . tf.constant(b&quot;hello world&quot;) . &lt;tf.Tensor: id=33, shape=(), dtype=string, numpy=b&#39;hello world&#39;&gt; . tf.constant(&quot;cafÃ©&quot;) . &lt;tf.Tensor: id=34, shape=(), dtype=string, numpy=b&#39;caf xc3 xa9&#39;&gt; . u = tf.constant([ord(c) for c in &quot;cafÃ©&quot;]) u . &lt;tf.Tensor: id=35, shape=(4,), dtype=int32, numpy=array([ 99, 97, 102, 233], dtype=int32)&gt; . b = tf.strings.unicode_encode(u, &quot;UTF-8&quot;) tf.strings.length(b, unit=&quot;UTF8_CHAR&quot;) . &lt;tf.Tensor: id=46, shape=(), dtype=int32, numpy=4&gt; . tf.strings.unicode_decode(b, &quot;UTF-8&quot;) . &lt;tf.Tensor: id=50, shape=(4,), dtype=int32, numpy=array([ 99, 97, 102, 233], dtype=int32)&gt; . String arrays . p = tf.constant([&quot;CafÃ©&quot;, &quot;Coffee&quot;, &quot;caffÃ¨&quot;, &quot;å’–å•¡&quot;]) . tf.strings.length(p, unit=&quot;UTF8_CHAR&quot;) . &lt;tf.Tensor: id=52, shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2], dtype=int32)&gt; . r = tf.strings.unicode_decode(p, &quot;UTF8&quot;) r . &lt;tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]&gt; . print(r) . &lt;tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]&gt; . Ragged tensors . print(r[1]) . tf.Tensor([ 67 111 102 102 101 101], shape=(6,), dtype=int32) . print(r[1:3]) . &lt;tf.RaggedTensor [[67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232]]&gt; . r2 = tf.ragged.constant([[65, 66], [], [67]]) print(tf.concat([r, r2], axis=0)) . &lt;tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857], [65, 66], [], [67]]&gt; . r3 = tf.ragged.constant([[68, 69, 70], [71], [], [72, 73]]) print(tf.concat([r, r3], axis=1)) . &lt;tf.RaggedTensor [[67, 97, 102, 233, 68, 69, 70], [67, 111, 102, 102, 101, 101, 71], [99, 97, 102, 102, 232], [21654, 21857, 72, 73]]&gt; . tf.strings.unicode_encode(r3, &quot;UTF-8&quot;) . &lt;tf.Tensor: id=202, shape=(4,), dtype=string, numpy=array([b&#39;DEF&#39;, b&#39;G&#39;, b&#39;&#39;, b&#39;HI&#39;], dtype=object)&gt; . r.to_tensor() . &lt;tf.Tensor: id=267, shape=(4, 6), dtype=int32, numpy= array([[ 67, 97, 102, 233, 0, 0], [ 67, 111, 102, 102, 101, 101], [ 99, 97, 102, 102, 232, 0], [21654, 21857, 0, 0, 0, 0]], dtype=int32)&gt; . Sparse tensors . s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]], values=[1., 2., 3.], dense_shape=[3, 4]) . print(s) . SparseTensor(indices=tf.Tensor( [[0 1] [1 0] [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) . tf.sparse.to_dense(s) . &lt;tf.Tensor: id=272, shape=(3, 4), dtype=float32, numpy= array([[0., 1., 0., 0.], [2., 0., 0., 0.], [0., 0., 0., 3.]], dtype=float32)&gt; . s2 = s * 2.0 . try: s3 = s + 1. except TypeError as ex: print(ex) . unsupported operand type(s) for +: &#39;SparseTensor&#39; and &#39;float&#39; . s4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]]) tf.sparse.sparse_dense_matmul(s, s4) . &lt;tf.Tensor: id=276, shape=(3, 2), dtype=float32, numpy= array([[ 30., 40.], [ 20., 40.], [210., 240.]], dtype=float32)&gt; . s5 = tf.SparseTensor(indices=[[0, 2], [0, 1]], values=[1., 2.], dense_shape=[3, 4]) print(s5) . SparseTensor(indices=tf.Tensor( [[0 2] [0 1]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) . try: tf.sparse.to_dense(s5) except tf.errors.InvalidArgumentError as ex: print(ex) . indices[1] = [0,1] is out of order [Op:SparseToDense] . s6 = tf.sparse.reorder(s5) tf.sparse.to_dense(s6) . &lt;tf.Tensor: id=285, shape=(3, 4), dtype=float32, numpy= array([[0., 2., 1., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], dtype=float32)&gt; . Sets . set1 = tf.constant([[2, 3, 5, 7], [7, 9, 0, 0]]) set2 = tf.constant([[4, 5, 6], [9, 10, 0]]) tf.sparse.to_dense(tf.sets.union(set1, set2)) . &lt;tf.Tensor: id=292, shape=(2, 6), dtype=int32, numpy= array([[ 2, 3, 4, 5, 6, 7], [ 0, 7, 9, 10, 0, 0]], dtype=int32)&gt; . tf.sparse.to_dense(tf.sets.difference(set1, set2)) . &lt;tf.Tensor: id=297, shape=(2, 3), dtype=int32, numpy= array([[2, 3, 7], [7, 0, 0]], dtype=int32)&gt; . tf.sparse.to_dense(tf.sets.intersection(set1, set2)) . &lt;tf.Tensor: id=302, shape=(2, 2), dtype=int32, numpy= array([[5, 0], [0, 9]], dtype=int32)&gt; . Variables . v = tf.Variable([[1., 2., 3.], [4., 5., 6.]]) . v.assign(2 * v) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2, 3) dtype=float32, numpy= array([[ 2., 4., 6.], [ 8., 10., 12.]], dtype=float32)&gt; . v[0, 1].assign(42) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2, 3) dtype=float32, numpy= array([[ 2., 42., 6.], [ 8., 10., 12.]], dtype=float32)&gt; . v[:, 2].assign([0., 1.]) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2, 3) dtype=float32, numpy= array([[ 2., 42., 0.], [ 8., 10., 1.]], dtype=float32)&gt; . try: v[1] = [7., 8., 9.] except TypeError as ex: print(ex) . &#39;ResourceVariable&#39; object does not support item assignment . v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.]) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2, 3) dtype=float32, numpy= array([[100., 42., 0.], [ 8., 10., 200.]], dtype=float32)&gt; . sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]], indices=[1, 0]) v.scatter_update(sparse_delta) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2, 3) dtype=float32, numpy= array([[4., 5., 6.], [1., 2., 3.]], dtype=float32)&gt; . Tensor Arrays . array = tf.TensorArray(dtype=tf.float32, size=3) array = array.write(0, tf.constant([1., 2.])) array = array.write(1, tf.constant([3., 10.])) array = array.write(2, tf.constant([5., 7.])) . array.read(1) . &lt;tf.Tensor: id=337, shape=(2,), dtype=float32, numpy=array([ 3., 10.], dtype=float32)&gt; . array.stack() . &lt;tf.Tensor: id=342, shape=(3, 2), dtype=float32, numpy= array([[1., 2.], [0., 0.], [5., 7.]], dtype=float32)&gt; . mean, variance = tf.nn.moments(array.stack(), axes=0) mean . &lt;tf.Tensor: id=350, shape=(2,), dtype=float32, numpy=array([2., 3.], dtype=float32)&gt; . variance . &lt;tf.Tensor: id=351, shape=(2,), dtype=float32, numpy=array([4.6666665, 8.666667 ], dtype=float32)&gt; . Custom loss function . Let&#39;s start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it: . from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler housing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split( housing.data, housing.target.reshape(-1, 1), random_state=42) X_train, X_valid, y_train, y_valid = train_test_split( X_train_full, y_train_full, random_state=42) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_valid_scaled = scaler.transform(X_valid) X_test_scaled = scaler.transform(X_test) . def huber_fn(y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) &lt; 1 squared_loss = tf.square(error) / 2 linear_loss = tf.abs(error) - 0.5 return tf.where(is_small_error, squared_loss, linear_loss) . plt.figure(figsize=(8, 3.5)) z = np.linspace(-4, 4, 200) plt.plot(z, huber_fn(0, z), &quot;b-&quot;, linewidth=2, label=&quot;huber($z$)&quot;) plt.plot(z, z**2 / 2, &quot;b:&quot;, linewidth=1, label=r&quot;$ frac{1}{2}z^2$&quot;) plt.plot([-1, -1], [0, huber_fn(0., -1.)], &quot;r--&quot;) plt.plot([1, 1], [0, huber_fn(0., 1.)], &quot;r--&quot;) plt.gca().axhline(y=0, color=&#39;k&#39;) plt.gca().axvline(x=0, color=&#39;k&#39;) plt.axis([-4, 4, 0, 4]) plt.grid(True) plt.xlabel(&quot;$z$&quot;) plt.legend(fontsize=14) plt.title(&quot;Huber loss&quot;, fontsize=14) plt.show() . input_shape = X_train.shape[1:] model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;, input_shape=input_shape), keras.layers.Dense(1), ]) . model.compile(loss=huber_fn, optimizer=&quot;nadam&quot;, metrics=[&quot;mae&quot;]) . model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) . Train on 11610 samples, validate on 3870 samples Epoch 1/2 11610/11610 [==============================] - 1s 98us/sample - loss: 0.5815 - mae: 0.9377 - val_loss: 0.2677 - val_mae: 0.5595 Epoch 2/2 11610/11610 [==============================] - 1s 50us/sample - loss: 0.2153 - mae: 0.5097 - val_loss: 0.2201 - val_mae: 0.5094 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdf205e3c50&gt; . Saving/Loading Models with Custom Objects . model.save(&quot;my_model_with_a_custom_loss.h5&quot;) . model = keras.models.load_model(&quot;my_model_with_a_custom_loss.h5&quot;, custom_objects={&quot;huber_fn&quot;: huber_fn}) . model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) . Train on 11610 samples, validate on 3870 samples Epoch 1/2 11610/11610 [==============================] - 1s 75us/sample - loss: 0.2045 - mae: 0.4953 - val_loss: 0.1935 - val_mae: 0.4800 Epoch 2/2 11610/11610 [==============================] - 1s 48us/sample - loss: 0.1995 - mae: 0.4896 - val_loss: 0.1863 - val_mae: 0.4690 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdf4434b588&gt; . def create_huber(threshold=1.0): def huber_fn(y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) &lt; threshold squared_loss = tf.square(error) / 2 linear_loss = threshold * tf.abs(error) - threshold**2 / 2 return tf.where(is_small_error, squared_loss, linear_loss) return huber_fn . model.compile(loss=create_huber(2.0), optimizer=&quot;nadam&quot;, metrics=[&quot;mae&quot;]) . model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) . Train on 11610 samples, validate on 3870 samples Epoch 1/2 11610/11610 [==============================] - 1s 90us/sample - loss: 0.2210 - mae: 0.4890 - val_loss: 0.2199 - val_mae: 0.4771 Epoch 2/2 11610/11610 [==============================] - 1s 52us/sample - loss: 0.2180 - mae: 0.4859 - val_loss: 0.2402 - val_mae: 0.4825 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdef8480e80&gt; . model.save(&quot;my_model_with_a_custom_loss_threshold_2.h5&quot;) . model = keras.models.load_model(&quot;my_model_with_a_custom_loss_threshold_2.h5&quot;, custom_objects={&quot;huber_fn&quot;: create_huber(2.0)}) . model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) . Train on 11610 samples, validate on 3870 samples Epoch 1/2 11610/11610 [==============================] - 1s 80us/sample - loss: 0.2155 - mae: 0.4825 - val_loss: 0.2188 - val_mae: 0.4786 Epoch 2/2 11610/11610 [==============================] - 1s 48us/sample - loss: 0.2122 - mae: 0.4790 - val_loss: 0.2077 - val_mae: 0.4626 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdef38425f8&gt; . class HuberLoss(keras.losses.Loss): def __init__(self, threshold=1.0, **kwargs): self.threshold = threshold super().__init__(**kwargs) def call(self, y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) &lt; self.threshold squared_loss = tf.square(error) / 2 linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2 return tf.where(is_small_error, squared_loss, linear_loss) def get_config(self): base_config = super().get_config() return {**base_config, &quot;threshold&quot;: self.threshold} . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;, input_shape=input_shape), keras.layers.Dense(1), ]) . model.compile(loss=HuberLoss(2.), optimizer=&quot;nadam&quot;, metrics=[&quot;mae&quot;]) . model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) . Train on 11610 samples, validate on 3870 samples Epoch 1/2 11610/11610 [==============================] - 1s 103us/sample - loss: 0.8104 - mae: 0.9605 - val_loss: 0.2599 - val_mae: 0.5205 Epoch 2/2 11610/11610 [==============================] - 1s 49us/sample - loss: 0.2347 - mae: 0.5081 - val_loss: 0.2270 - val_mae: 0.4921 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdef062fa58&gt; . model.save(&quot;my_model_with_a_custom_loss_class.h5&quot;) . #model = keras.models.load_model(&quot;my_model_with_a_custom_loss_class.h5&quot;, # TODO: check PR #25956 # custom_objects={&quot;HuberLoss&quot;: HuberLoss}) . model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) . Train on 11610 samples, validate on 3870 samples Epoch 1/2 11610/11610 [==============================] - 1s 53us/sample - loss: 0.2235 - mae: 0.4955 - val_loss: 0.2241 - val_mae: 0.4810 Epoch 2/2 11610/11610 [==============================] - 1s 51us/sample - loss: 0.2187 - mae: 0.4883 - val_loss: 0.2121 - val_mae: 0.4767 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdeced2d320&gt; . #model = keras.models.load_model(&quot;my_model_with_a_custom_loss_class.h5&quot;, # TODO: check PR #25956 # custom_objects={&quot;HuberLoss&quot;: HuberLoss}) . model.loss.threshold . 2.0 . Other Custom Functions . def my_softplus(z): # return value is just tf.nn.softplus(z) return tf.math.log(tf.exp(z) + 1.0) def my_glorot_initializer(shape, dtype=tf.float32): stddev = tf.sqrt(2. / (shape[0] + shape[1])) return tf.random.normal(shape, stddev=stddev, dtype=dtype) def my_l1_regularizer(weights): return tf.reduce_sum(tf.abs(0.01 * weights)) def my_positive_weights(weights): # return value is just tf.nn.relu(weights) return tf.where(weights &lt; 0., tf.zeros_like(weights), weights) . layer = keras.layers.Dense(1, activation=my_softplus, kernel_initializer=my_glorot_initializer, kernel_regularizer=my_l1_regularizer, kernel_constraint=my_positive_weights) . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;, input_shape=input_shape), keras.layers.Dense(1, activation=my_softplus, kernel_regularizer=my_l1_regularizer, kernel_constraint=my_positive_weights, kernel_initializer=my_glorot_initializer), ]) . model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;, metrics=[&quot;mae&quot;]) . model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) . Train on 11610 samples, validate on 3870 samples Epoch 1/2 11610/11610 [==============================] - 1s 88us/sample - loss: 2.0418 - mae: 0.8980 - val_loss: inf - val_mae: inf Epoch 2/2 11610/11610 [==============================] - 1s 49us/sample - loss: 0.8818 - mae: 0.5398 - val_loss: inf - val_mae: inf . &lt;tensorflow.python.keras.callbacks.History at 0x7fdece3b2cc0&gt; . model.save(&quot;my_model_with_many_custom_parts.h5&quot;) . model = keras.models.load_model( &quot;my_model_with_many_custom_parts.h5&quot;, custom_objects={ &quot;my_l1_regularizer&quot;: my_l1_regularizer, &quot;my_positive_weights&quot;: lambda: my_positive_weights, &quot;my_glorot_initializer&quot;: my_glorot_initializer, &quot;my_softplus&quot;: my_softplus, }) . class MyL1Regularizer(keras.regularizers.Regularizer): def __init__(self, factor): self.factor = factor def __call__(self, weights): return tf.reduce_sum(tf.abs(self.factor * weights)) def get_config(self): return {&quot;factor&quot;: self.factor} . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;, input_shape=input_shape), keras.layers.Dense(1, activation=my_softplus, kernel_regularizer=MyL1Regularizer(0.01), kernel_constraint=my_positive_weights, kernel_initializer=my_glorot_initializer), ]) . model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;, metrics=[&quot;mae&quot;]) . model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) . Train on 11610 samples, validate on 3870 samples Epoch 1/2 11610/11610 [==============================] - 1s 86us/sample - loss: 1.6833 - mae: 0.8724 - val_loss: inf - val_mae: inf Epoch 2/2 11610/11610 [==============================] - 1s 47us/sample - loss: 0.6906 - mae: 0.5411 - val_loss: inf - val_mae: inf . &lt;tensorflow.python.keras.callbacks.History at 0x7fdecd41ae10&gt; . model.save(&quot;my_model_with_many_custom_parts.h5&quot;) . model = keras.models.load_model( &quot;my_model_with_many_custom_parts.h5&quot;, custom_objects={ &quot;MyL1Regularizer&quot;: MyL1Regularizer, &quot;my_positive_weights&quot;: lambda: my_positive_weights, &quot;my_glorot_initializer&quot;: my_glorot_initializer, &quot;my_softplus&quot;: my_softplus, }) . Custom Metrics . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;, input_shape=input_shape), keras.layers.Dense(1), ]) . model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;, metrics=[create_huber(2.0)]) . model.fit(X_train_scaled, y_train, epochs=2) . Train on 11610 samples Epoch 1/2 11610/11610 [==============================] - 1s 84us/sample - loss: 1.9532 - huber_fn: 0.8643 Epoch 2/2 11610/11610 [==============================] - 0s 40us/sample - loss: 0.4976 - huber_fn: 0.2412 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdec63f3ba8&gt; . Warning: if you use the same function as the loss and a metric, you may be surprised to see different results. This is generally just due to floating point precision errors: even though the mathematical equations are equivalent, the operations are not run in the same order, which can lead to small differences. Moreover, when using sample weights, there&#39;s more than just precision errors: . the loss since the start of the epoch is the mean of all batch losses seen so far. Each batch loss is the sum of the weighted instance losses divided by the batch size (not the sum of weights, so the batch loss is not the weighted mean of the losses). | the metric since the start of the epoch is equal to the sum of weighted instance losses divided by sum of all weights seen so far. In other words, it is the weighted mean of all the instance losses. Not the same thing. | . If you do the math, you will find that loss = metric * mean of sample weights (plus some floating point precision error). . model.compile(loss=create_huber(2.0), optimizer=&quot;nadam&quot;, metrics=[create_huber(2.0)]) . sample_weight = np.random.rand(len(y_train)) history = model.fit(X_train_scaled, y_train, epochs=2, sample_weight=sample_weight) . Train on 11610 samples Epoch 1/2 11610/11610 [==============================] - 1s 77us/sample - loss: 0.1146 - huber_fn: 0.2319 Epoch 2/2 11610/11610 [==============================] - 0s 40us/sample - loss: 0.1109 - huber_fn: 0.2245 . history.history[&quot;loss&quot;][0], history.history[&quot;huber_fn&quot;][0] * sample_weight.mean() . (0.11463345723687114, 0.11505695444753776) . Streaming metrics . precision = keras.metrics.Precision() precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1]) . &lt;tf.Tensor: id=40989, shape=(), dtype=float32, numpy=0.8&gt; . precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0]) . &lt;tf.Tensor: id=41036, shape=(), dtype=float32, numpy=0.5&gt; . precision.result() . &lt;tf.Tensor: id=41045, shape=(), dtype=float32, numpy=0.5&gt; . precision.variables . [&lt;tf.Variable &#39;true_positives:0&#39; shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)&gt;, &lt;tf.Variable &#39;false_positives:0&#39; shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)&gt;] . precision.reset_states() . Creating a streaming metric: . class HuberMetric(keras.metrics.Metric): def __init__(self, threshold=1.0, **kwargs): super().__init__(**kwargs) # handles base args (e.g., dtype) self.threshold = threshold #self.huber_fn = create_huber(threshold) # TODO: investigate why this fails self.total = self.add_weight(&quot;total&quot;, initializer=&quot;zeros&quot;) self.count = self.add_weight(&quot;count&quot;, initializer=&quot;zeros&quot;) def huber_fn(self, y_true, y_pred): # workaround error = y_true - y_pred is_small_error = tf.abs(error) &lt; self.threshold squared_loss = tf.square(error) / 2 linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2 return tf.where(is_small_error, squared_loss, linear_loss) def update_state(self, y_true, y_pred, sample_weight=None): metric = self.huber_fn(y_true, y_pred) self.total.assign_add(tf.reduce_sum(metric)) self.count.assign_add(tf.cast(tf.size(y_true), tf.float32)) def result(self): return self.total / self.count def get_config(self): base_config = super().get_config() return {**base_config, &quot;threshold&quot;: self.threshold} . m = HuberMetric(2.) # total = 2 * |10 - 2| - 2Â²/2 = 14 # count = 1 # result = 14 / 1 = 14 m(tf.constant([[2.]]), tf.constant([[10.]])) . &lt;tf.Tensor: id=41092, shape=(), dtype=float32, numpy=14.0&gt; . # total = total + (|1 - 0|Â² / 2) + (2 * |9.25 - 5| - 2Â² / 2) = 14 + 7 = 21 # count = count + 2 = 3 # result = total / count = 21 / 3 = 7 m(tf.constant([[0.], [5.]]), tf.constant([[1.], [9.25]])) m.result() . &lt;tf.Tensor: id=41123, shape=(), dtype=float32, numpy=7.0&gt; . m.variables . [&lt;tf.Variable &#39;total:0&#39; shape=() dtype=float32, numpy=21.0&gt;, &lt;tf.Variable &#39;count:0&#39; shape=() dtype=float32, numpy=3.0&gt;] . m.reset_states() m.variables . [&lt;tf.Variable &#39;total:0&#39; shape=() dtype=float32, numpy=0.0&gt;, &lt;tf.Variable &#39;count:0&#39; shape=() dtype=float32, numpy=0.0&gt;] . Let&#39;s check that the HuberMetric class works well: . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;, input_shape=input_shape), keras.layers.Dense(1), ]) . model.compile(loss=create_huber(2.0), optimizer=&quot;nadam&quot;, metrics=[HuberMetric(2.0)]) . model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2) . Train on 11610 samples Epoch 1/2 11610/11610 [==============================] - 1s 70us/sample - loss: 0.8064 - huber_metric_1: 0.8064 Epoch 2/2 11610/11610 [==============================] - 0s 37us/sample - loss: 0.2533 - huber_metric_1: 0.2533 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdec5c159e8&gt; . model.save(&quot;my_model_with_a_custom_metric.h5&quot;) . #model = keras.models.load_model(&quot;my_model_with_a_custom_metric.h5&quot;, # TODO: check PR #25956 # custom_objects={&quot;huber_fn&quot;: create_huber(2.0), # &quot;HuberMetric&quot;: HuberMetric}) . model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2) . Train on 11610 samples Epoch 1/2 11610/11610 [==============================] - 0s 38us/sample - loss: 0.2345 - huber_metric_1: 0.2345 Epoch 2/2 11610/11610 [==============================] - 0s 37us/sample - loss: 0.2278 - huber_metric_1: 0.2278 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdec35f9f98&gt; . model.metrics[0].threshold . 2.0 . Looks like it works fine! More simply, we could have created the class like this: . class HuberMetric(keras.metrics.Mean): def __init__(self, threshold=1.0, name=&#39;HuberMetric&#39;, dtype=None): self.threshold = threshold self.huber_fn = create_huber(threshold) super().__init__(name=name, dtype=dtype) def update_state(self, y_true, y_pred, sample_weight=None): metric = self.huber_fn(y_true, y_pred) super(HuberMetric, self).update_state(metric, sample_weight) def get_config(self): base_config = super().get_config() return {**base_config, &quot;threshold&quot;: self.threshold} . This class handles shapes better, and it also supports sample weights. . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;, input_shape=input_shape), keras.layers.Dense(1), ]) . model.compile(loss=keras.losses.Huber(2.0), optimizer=&quot;nadam&quot;, weighted_metrics=[HuberMetric(2.0)]) . sample_weight = np.random.rand(len(y_train)) history = model.fit(X_train_scaled, y_train, epochs=2, sample_weight=sample_weight) . Train on 11610 samples Epoch 1/2 11610/11610 [==============================] - 0s 41us/sample - loss: 0.3616 - HuberMetric: 0.7141 Epoch 2/2 11610/11610 [==============================] - 0s 32us/sample - loss: 0.1228 - HuberMetric: 0.2424 . history.history[&quot;loss&quot;][0], history.history[&quot;HuberMetric&quot;][0] * sample_weight.mean() . (0.36160108902965715, 0.36160092789449944) . model.save(&quot;my_model_with_a_custom_metric_v2.h5&quot;) . #model = keras.models.load_model(&quot;my_model_with_a_custom_metric_v2.h5&quot;, # TODO: check PR #25956 # custom_objects={&quot;HuberMetric&quot;: HuberMetric}) . model.fit(X_train_scaled, y_train, epochs=2) . Train on 11610 samples Epoch 1/2 11610/11610 [==============================] - 0s 37us/sample - loss: 0.2290 - HuberMetric: 0.2290 Epoch 2/2 11610/11610 [==============================] - 0s 29us/sample - loss: 0.2185 - HuberMetric: 0.2185 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdec244fa20&gt; . model.metrics[0].threshold . 2.0 . Custom Layers . exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x)) . exponential_layer([-1., 0., 1.]) . &lt;tf.Tensor: id=56424, shape=(3,), dtype=float32, numpy=array([0.36787945, 1. , 2.7182817 ], dtype=float32)&gt; . Adding an exponential layer at the output of a regression model can be useful if the values to predict are positive and with very different scales (e.g., 0.001, 10., 10000): . model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;relu&quot;, input_shape=input_shape), keras.layers.Dense(1), exponential_layer ]) model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;) model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid)) model.evaluate(X_test_scaled, y_test) . Train on 11610 samples, validate on 3870 samples Epoch 1/5 11610/11610 [==============================] - 1s 81us/sample - loss: 0.8810 - val_loss: 0.5956 Epoch 2/5 11610/11610 [==============================] - 1s 49us/sample - loss: 0.4667 - val_loss: 0.4028 Epoch 3/5 11610/11610 [==============================] - 1s 50us/sample - loss: 0.4125 - val_loss: 0.3788 Epoch 4/5 11610/11610 [==============================] - 1s 52us/sample - loss: 0.3934 - val_loss: 0.3595 Epoch 5/5 11610/11610 [==============================] - 1s 52us/sample - loss: 0.3883 - val_loss: 6.8527 5160/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 23us/sample - loss: 0.2731 . 0.36313450114671575 . #collapse-show class MyDense(keras.layers.Layer): def __init__(self, units, activation=None, **kwargs): super().__init__(**kwargs) self.units = units self.activation = keras.activations.get(activation) def build(self, batch_input_shape): self.kernel = self.add_weight( name=&quot;kernel&quot;, shape=[batch_input_shape[-1], self.units], initializer=&quot;glorot_normal&quot;) self.bias = self.add_weight( name=&quot;bias&quot;, shape=[self.units], initializer=&quot;zeros&quot;) super().build(batch_input_shape) # must be at the end def call(self, X): return self.activation(X @ self.kernel + self.bias) def compute_output_shape(self, batch_input_shape): return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units]) def get_config(self): base_config = super().get_config() return {**base_config, &quot;units&quot;: self.units, &quot;activation&quot;: keras.activations.serialize(self.activation)} . . model = keras.models.Sequential([ MyDense(30, activation=&quot;relu&quot;, input_shape=input_shape), MyDense(1) ]) . model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;) model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) model.evaluate(X_test_scaled, y_test) . Train on 11610 samples, validate on 3870 samples Epoch 1/2 11610/11610 [==============================] - 1s 79us/sample - loss: 1.9793 - val_loss: 1.3162 Epoch 2/2 11610/11610 [==============================] - 1s 46us/sample - loss: 0.5866 - val_loss: 0.4919 5160/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 20us/sample - loss: 0.4977 . 0.4982084909150767 . model.save(&quot;my_model_with_a_custom_layer.h5&quot;) . model = keras.models.load_model(&quot;my_model_with_a_custom_layer.h5&quot;, custom_objects={&quot;MyDense&quot;: MyDense}) . class MyMultiLayer(keras.layers.Layer): def call(self, X): X1, X2 = X return X1 + X2, X1 * X2 def compute_output_shape(self, batch_input_shape): batch_input_shape1, batch_input_shape2 = batch_input_shape return [batch_input_shape1, batch_input_shape2] . inputs1 = keras.layers.Input(shape=[2]) inputs2 = keras.layers.Input(shape=[2]) outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2)) . Let&#39;s create a layer with a different behavior during training and testing: . class AddGaussianNoise(keras.layers.Layer): def __init__(self, stddev, **kwargs): super().__init__(**kwargs) self.stddev = stddev def call(self, X, training=None): if training: noise = tf.random.normal(tf.shape(X), stddev=self.stddev) return X + noise else: return X def compute_output_shape(self, batch_input_shape): return batch_input_shape . model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;) model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) model.evaluate(X_test_scaled, y_test) . Train on 11610 samples, validate on 3870 samples Epoch 1/2 11610/11610 [==============================] - 1s 97us/sample - loss: 0.4592 - val_loss: 0.4268 Epoch 2/2 11610/11610 [==============================] - 1s 48us/sample - loss: 0.4106 - val_loss: 0.4150 5160/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 22us/sample - loss: 0.3190 . 0.38935703281746353 . Custom Models . X_new_scaled = X_test_scaled . class ResidualBlock(keras.layers.Layer): def __init__(self, n_layers, n_neurons, **kwargs): super().__init__(**kwargs) self.n_layers = n_layers # not shown in the book self.n_neurons = n_neurons # not shown self.hidden = [keras.layers.Dense(n_neurons, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;) for _ in range(n_layers)] def call(self, inputs): Z = inputs for layer in self.hidden: Z = layer(Z) return inputs + Z def get_config(self): # not shown base_config = super().get_config() # not shown return {**base_config, # not shown &quot;n_layers&quot;: self.n_layers, &quot;n_neurons&quot;: self.n_neurons} # not shown . class ResidualRegressor(keras.models.Model): def __init__(self, output_dim, **kwargs): super().__init__(**kwargs) self.output_dim = output_dim # not shown in the book self.hidden1 = keras.layers.Dense(30, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;) self.block1 = ResidualBlock(2, 30) self.block2 = ResidualBlock(2, 30) self.out = keras.layers.Dense(output_dim) def call(self, inputs): Z = self.hidden1(inputs) for _ in range(1 + 3): Z = self.block1(Z) Z = self.block2(Z) return self.out(Z) def get_config(self): # not shown base_config = super().get_config() # not shown return {**base_config, # not shown &quot;output_dim&quot;: self.output_dim} # not shown . model = ResidualRegressor(1) model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;) history = model.fit(X_train_scaled, y_train, epochs=5) score = model.evaluate(X_test_scaled, y_test) y_pred = model.predict(X_new_scaled) . Train on 11610 samples Epoch 1/5 11610/11610 [==============================] - 1s 115us/sample - loss: 7.5069 Epoch 2/5 11610/11610 [==============================] - 1s 50us/sample - loss: 1.3145 Epoch 3/5 11610/11610 [==============================] - 1s 48us/sample - loss: 1.3108 Epoch 4/5 11610/11610 [==============================] - 1s 46us/sample - loss: 0.9334 Epoch 5/5 11610/11610 [==============================] - 1s 49us/sample - loss: 0.7817 5160/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 41us/sample - loss: 0.4245 . model.save(&quot;my_custom_model.ckpt&quot;) . WARNING:tensorflow:From /opt/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. INFO:tensorflow:Assets written to: my_custom_model.ckpt/assets . model = keras.models.load_model(&quot;my_custom_model.ckpt&quot;) . history = model.fit(X_train_scaled, y_train, epochs=5) . Train on 11610 samples Epoch 1/5 11610/11610 [==============================] - 2s 135us/sample - loss: 0.6609 Epoch 2/5 11610/11610 [==============================] - 1s 50us/sample - loss: 2.0047 Epoch 3/5 11610/11610 [==============================] - 1s 51us/sample - loss: 0.9564 Epoch 4/5 11610/11610 [==============================] - 1s 49us/sample - loss: 0.7532 Epoch 5/5 11610/11610 [==============================] - 1s 49us/sample - loss: 1.1412 . We could have defined the model using the sequential API instead: . block1 = ResidualBlock(2, 30) model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;), block1, block1, block1, block1, ResidualBlock(2, 30), keras.layers.Dense(1) ]) . model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;) history = model.fit(X_train_scaled, y_train, epochs=5) score = model.evaluate(X_test_scaled, y_test) y_pred = model.predict(X_new_scaled) . Train on 11610 samples Epoch 1/5 11610/11610 [==============================] - 1s 106us/sample - loss: 1.1764 Epoch 2/5 11610/11610 [==============================] - 1s 47us/sample - loss: 0.4245 Epoch 3/5 11610/11610 [==============================] - 1s 49us/sample - loss: 0.4814 Epoch 4/5 11610/11610 [==============================] - 1s 48us/sample - loss: 0.3887 Epoch 5/5 11610/11610 [==============================] - 1s 49us/sample - loss: 0.6337 5160/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 35us/sample - loss: 0.2956 . Losses and Metrics Based on Model Internals . class ReconstructingRegressor(keras.models.Model): def __init__(self, output_dim, **kwargs): super().__init__(**kwargs) self.hidden = [keras.layers.Dense(30, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;) for _ in range(5)] self.out = keras.layers.Dense(output_dim) # TODO: check https://github.com/tensorflow/tensorflow/issues/26260 #self.reconstruction_mean = keras.metrics.Mean(name=&quot;reconstruction_error&quot;) def build(self, batch_input_shape): n_inputs = batch_input_shape[-1] self.reconstruct = keras.layers.Dense(n_inputs) super().build(batch_input_shape) def call(self, inputs, training=None): Z = inputs for layer in self.hidden: Z = layer(Z) reconstruction = self.reconstruct(Z) recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs)) self.add_loss(0.05 * recon_loss) #if training: # result = self.reconstruction_mean(recon_loss) # self.add_metric(result) return self.out(Z) . model = ReconstructingRegressor(1) model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;) history = model.fit(X_train_scaled, y_train, epochs=2) y_pred = model.predict(X_test_scaled) . Train on 11610 samples Epoch 1/2 11610/11610 [==============================] - 2s 167us/sample - loss: 0.8176 Epoch 2/2 11610/11610 [==============================] - 1s 56us/sample - loss: 0.4264 . Computing Gradients with Autodiff . def f(w1, w2): return 3 * w1 ** 2 + 2 * w1 * w2 . w1, w2 = 5, 3 eps = 1e-6 (f(w1 + eps, w2) - f(w1, w2)) / eps . 36.000003007075065 . (f(w1, w2 + eps) - f(w1, w2)) / eps . 10.000000003174137 . w1, w2 = tf.Variable(5.), tf.Variable(3.) with tf.GradientTape() as tape: z = f(w1, w2) gradients = tape.gradient(z, [w1, w2]) . gradients . [&lt;tf.Tensor: id=93635, shape=(), dtype=float32, numpy=36.0&gt;, &lt;tf.Tensor: id=93627, shape=(), dtype=float32, numpy=10.0&gt;] . with tf.GradientTape() as tape: z = f(w1, w2) dz_dw1 = tape.gradient(z, w1) try: dz_dw2 = tape.gradient(z, w2) except RuntimeError as ex: print(ex) . GradientTape.gradient can only be called once on non-persistent tapes. . with tf.GradientTape(persistent=True) as tape: z = f(w1, w2) dz_dw1 = tape.gradient(z, w1) dz_dw2 = tape.gradient(z, w2) # works now! del tape . dz_dw1, dz_dw2 . (&lt;tf.Tensor: id=93683, shape=(), dtype=float32, numpy=36.0&gt;, &lt;tf.Tensor: id=93688, shape=(), dtype=float32, numpy=10.0&gt;) . c1, c2 = tf.constant(5.), tf.constant(3.) with tf.GradientTape() as tape: z = f(c1, c2) gradients = tape.gradient(z, [c1, c2]) . gradients . [None, None] . with tf.GradientTape() as tape: tape.watch(c1) tape.watch(c2) z = f(c1, c2) gradients = tape.gradient(z, [c1, c2]) . gradients . [&lt;tf.Tensor: id=93726, shape=(), dtype=float32, numpy=36.0&gt;, &lt;tf.Tensor: id=93718, shape=(), dtype=float32, numpy=10.0&gt;] . with tf.GradientTape() as tape: z1 = f(w1, w2 + 2.) z2 = f(w1, w2 + 5.) z3 = f(w1, w2 + 7.) tape.gradient([z1, z2, z3], [w1, w2]) . [&lt;tf.Tensor: id=93802, shape=(), dtype=float32, numpy=136.0&gt;, &lt;tf.Tensor: id=93803, shape=(), dtype=float32, numpy=30.0&gt;] . with tf.GradientTape(persistent=True) as tape: z1 = f(w1, w2 + 2.) z2 = f(w1, w2 + 5.) z3 = f(w1, w2 + 7.) tf.reduce_sum(tf.stack([tape.gradient(z, [w1, w2]) for z in (z1, z2, z3)]), axis=0) del tape . with tf.GradientTape(persistent=True) as hessian_tape: with tf.GradientTape() as jacobian_tape: z = f(w1, w2) jacobians = jacobian_tape.gradient(z, [w1, w2]) hessians = [hessian_tape.gradient(jacobian, [w1, w2]) for jacobian in jacobians] del hessian_tape . jacobians . [&lt;tf.Tensor: id=93911, shape=(), dtype=float32, numpy=36.0&gt;, &lt;tf.Tensor: id=93903, shape=(), dtype=float32, numpy=10.0&gt;] . hessians . [[&lt;tf.Tensor: id=93920, shape=(), dtype=float32, numpy=6.0&gt;, &lt;tf.Tensor: id=93922, shape=(), dtype=float32, numpy=2.0&gt;], [&lt;tf.Tensor: id=93927, shape=(), dtype=float32, numpy=2.0&gt;, None]] . def f(w1, w2): return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2) with tf.GradientTape() as tape: z = f(w1, w2) tape.gradient(z, [w1, w2]) . [&lt;tf.Tensor: id=93947, shape=(), dtype=float32, numpy=30.0&gt;, None] . x = tf.Variable(100.) with tf.GradientTape() as tape: z = my_softplus(x) tape.gradient(z, [x]) . [&lt;tf.Tensor: id=93963, shape=(), dtype=float32, numpy=nan&gt;] . tf.math.log(tf.exp(tf.constant(30., dtype=tf.float32)) + 1.) . &lt;tf.Tensor: id=93968, shape=(), dtype=float32, numpy=30.0&gt; . x = tf.Variable([100.]) with tf.GradientTape() as tape: z = my_softplus(x) tape.gradient(z, [x]) . [&lt;tf.Tensor: id=93986, shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)&gt;] . @tf.custom_gradient def my_better_softplus(z): exp = tf.exp(z) def my_softplus_gradients(grad): return grad / (1 + 1 / exp) return tf.math.log(exp + 1), my_softplus_gradients . def my_better_softplus(z): return tf.where(z &gt; 30., z, tf.math.log(tf.exp(z) + 1.)) . x = tf.Variable([1000.]) with tf.GradientTape() as tape: z = my_better_softplus(x) z, tape.gradient(z, [x]) . (&lt;tf.Tensor: id=94003, shape=(1,), dtype=float32, numpy=array([1000.], dtype=float32)&gt;, [&lt;tf.Tensor: id=94024, shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)&gt;]) . Computing Gradients Using Autodiff . l2_reg = keras.regularizers.l2(0.05) model = keras.models.Sequential([ keras.layers.Dense(30, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;, kernel_regularizer=l2_reg), keras.layers.Dense(1, kernel_regularizer=l2_reg) ]) . def random_batch(X, y, batch_size=32): idx = np.random.randint(len(X), size=batch_size) return X[idx], y[idx] . def print_status_bar(iteration, total, loss, metrics=None): metrics = &quot; - &quot;.join([&quot;{}: {:.4f}&quot;.format(m.name, m.result()) for m in [loss] + (metrics or [])]) end = &quot;&quot; if iteration &lt; total else &quot; n&quot; print(&quot; r{}/{} - &quot;.format(iteration, total) + metrics, end=end) . import time mean_loss = keras.metrics.Mean(name=&quot;loss&quot;) mean_square = keras.metrics.Mean(name=&quot;mean_square&quot;) for i in range(1, 50 + 1): loss = 1 / i mean_loss(loss) mean_square(i ** 2) print_status_bar(i, 50, mean_loss, [mean_square]) time.sleep(0.05) . 50/50 - loss: 0.0900 - mean_square: 858.5000 . A fancier version with a progress bar: . def progress_bar(iteration, total, size=30): running = iteration &lt; total c = &quot;&gt;&quot; if running else &quot;=&quot; p = (size - 1) * iteration // total fmt = &quot;{{:-{}d}}/{{}} [{{}}]&quot;.format(len(str(total))) params = [iteration, total, &quot;=&quot; * p + c + &quot;.&quot; * (size - p - 1)] return fmt.format(*params) . progress_bar(3500, 10000, size=6) . &#39; 3500/10000 [=&gt;....]&#39; . def print_status_bar(iteration, total, loss, metrics=None, size=30): metrics = &quot; - &quot;.join([&quot;{}: {:.4f}&quot;.format(m.name, m.result()) for m in [loss] + (metrics or [])]) end = &quot;&quot; if iteration &lt; total else &quot; n&quot; print(&quot; r{} - {}&quot;.format(progress_bar(iteration, total), metrics), end=end) . mean_loss = keras.metrics.Mean(name=&quot;loss&quot;) mean_square = keras.metrics.Mean(name=&quot;mean_square&quot;) for i in range(1, 50 + 1): loss = 1 / i mean_loss(loss) mean_square(i ** 2) print_status_bar(i, 50, mean_loss, [mean_square]) time.sleep(0.05) . 50/50 [==============================] - loss: 0.0900 - mean_square: 858.5000 . n_epochs = 5 batch_size = 32 n_steps = len(X_train) // batch_size optimizer = keras.optimizers.Nadam(lr=0.01) loss_fn = keras.losses.mean_squared_error mean_loss = keras.metrics.Mean() metrics = [keras.metrics.MeanAbsoluteError()] . for epoch in range(1, n_epochs + 1): print(&quot;Epoch {}/{}&quot;.format(epoch, n_epochs)) for step in range(1, n_steps + 1): X_batch, y_batch = random_batch(X_train_scaled, y_train) with tf.GradientTape() as tape: y_pred = model(X_batch) main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) loss = tf.add_n([main_loss] + model.losses) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) for variable in model.variables: if variable.constraint is not None: variable.assign(variable.constraint(variable)) mean_loss(loss) for metric in metrics: metric(y_batch, y_pred) print_status_bar(step * batch_size, len(y_train), mean_loss, metrics) print_status_bar(len(y_train), len(y_train), mean_loss, metrics) for metric in [mean_loss] + metrics: metric.reset_states() . Epoch 1/5 WARNING:tensorflow:Layer sequential_10 is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2. The layer has dtype float32 because it&#39;s dtype defaults to floatx. If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2. To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(&#39;float64&#39;)`. To change just this layer, pass dtype=&#39;float64&#39; to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor. 11610/11610 [==============================] - mean: 1.5259 - mean_absolute_error: 0.57185 Epoch 2/5 11610/11610 [==============================] - mean: 0.7196 - mean_absolute_error: 0.5184 Epoch 3/5 11610/11610 [==============================] - mean: 0.6955 - mean_absolute_error: 0.5289 Epoch 4/5 11610/11610 [==============================] - mean: 0.6743 - mean_absolute_error: 0.5327 Epoch 5/5 11610/11610 [==============================] - mean: 0.6476 - mean_absolute_error: 0.5184 . try: from tqdm import tnrange from collections import OrderedDict with tnrange(1, n_epochs + 1, desc=&quot;All epochs&quot;) as epochs: for epoch in epochs: with tnrange(1, n_steps + 1, desc=&quot;Epoch {}/{}&quot;.format(epoch, n_epochs)) as steps: for step in steps: X_batch, y_batch = random_batch(X_train_scaled, y_train) with tf.GradientTape() as tape: y_pred = model(X_batch) main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) loss = tf.add_n([main_loss] + model.losses) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) for variable in model.variables: if variable.constraint is not None: variable.assign(variable.constraint(variable)) status = OrderedDict() mean_loss(loss) status[&quot;loss&quot;] = mean_loss.result().numpy() for metric in metrics: metric(y_batch, y_pred) status[metric.name] = metric.result().numpy() steps.set_postfix(status) for metric in [mean_loss] + metrics: metric.reset_states() except ImportError as ex: print(&quot;To run this cell, please install tqdm, ipywidgets and restart Jupyter&quot;) . . TensorFlow Functions . def cube(x): return x ** 3 . cube(2) . 8 . cube(tf.constant(2.0)) . &lt;tf.Tensor: id=987160, shape=(), dtype=float32, numpy=8.0&gt; . tf_cube = tf.function(cube) tf_cube . &lt;tensorflow.python.eager.def_function.Function at 0x7fdec03ee898&gt; . tf_cube(2) . &lt;tf.Tensor: id=987166, shape=(), dtype=int32, numpy=8&gt; . tf_cube(tf.constant(2.0)) . &lt;tf.Tensor: id=987174, shape=(), dtype=float32, numpy=8.0&gt; . TF Functions and Concrete Functions . concrete_function = tf_cube.get_concrete_function(tf.constant(2.0)) concrete_function.graph . &lt;tensorflow.python.framework.func_graph.FuncGraph at 0x7fdec687dc50&gt; . concrete_function(tf.constant(2.0)) . &lt;tf.Tensor: id=987177, shape=(), dtype=float32, numpy=8.0&gt; . concrete_function is tf_cube.get_concrete_function(tf.constant(2.0)) . True . Exploring Function Definitions and Graphs . concrete_function.graph . &lt;tensorflow.python.framework.func_graph.FuncGraph at 0x7fdec687dc50&gt; . ops = concrete_function.graph.get_operations() ops . [&lt;tf.Operation &#39;x&#39; type=Placeholder&gt;, &lt;tf.Operation &#39;pow/y&#39; type=Const&gt;, &lt;tf.Operation &#39;pow&#39; type=Pow&gt;, &lt;tf.Operation &#39;Identity&#39; type=Identity&gt;] . pow_op = ops[2] list(pow_op.inputs) . [&lt;tf.Tensor &#39;x:0&#39; shape=() dtype=float32&gt;, &lt;tf.Tensor &#39;pow/y:0&#39; shape=() dtype=float32&gt;] . pow_op.outputs . [&lt;tf.Tensor &#39;pow:0&#39; shape=() dtype=float32&gt;] . concrete_function.graph.get_operation_by_name(&#39;x&#39;) . &lt;tf.Operation &#39;x&#39; type=Placeholder&gt; . concrete_function.graph.get_tensor_by_name(&#39;Identity:0&#39;) . &lt;tf.Tensor &#39;Identity:0&#39; shape=() dtype=float32&gt; . concrete_function.function_def.signature . name: &#34;__inference_cube_987173&#34; input_arg { name: &#34;x&#34; type: DT_FLOAT } output_arg { name: &#34;identity&#34; type: DT_FLOAT } . How TF Functions Trace Python Functions to Extract Their Computation Graphs . @tf.function def tf_cube(x): print(&quot;print:&quot;, x) return x ** 3 . result = tf_cube(tf.constant(2.0)) . print: Tensor(&#34;x:0&#34;, shape=(), dtype=float32) . result . &lt;tf.Tensor: id=987187, shape=(), dtype=float32, numpy=8.0&gt; . result = tf_cube(2) result = tf_cube(3) result = tf_cube(tf.constant([[1., 2.]])) # New shape: trace! result = tf_cube(tf.constant([[3., 4.], [5., 6.]])) # New shape: trace! result = tf_cube(tf.constant([[7., 8.], [9., 10.], [11., 12.]])) # no trace . print: 2 print: 3 print: Tensor(&#34;x:0&#34;, shape=(1, 2), dtype=float32) print: Tensor(&#34;x:0&#34;, shape=(2, 2), dtype=float32) WARNING:tensorflow:5 out of the last 5 calls to &lt;function tf_cube at 0x7fdf44beec80&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. print: Tensor(&#34;x:0&#34;, shape=(3, 2), dtype=float32) WARNING:tensorflow:6 out of the last 6 calls to &lt;function tf_cube at 0x7fdf44beec80&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. . It is also possible to specify a particular input signature: . @tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)]) def shrink(images): print(&quot;Tracing&quot;, images) return images[:, ::2, ::2] # drop half the rows and columns . img_batch_1 = tf.random.uniform(shape=[100, 28, 28]) img_batch_2 = tf.random.uniform(shape=[50, 28, 28]) preprocessed_images = shrink(img_batch_1) # Traces the function. preprocessed_images = shrink(img_batch_2) # Reuses the same concrete function. . Tracing Tensor(&#34;images:0&#34;, shape=(None, 28, 28), dtype=float32) . img_batch_3 = tf.random.uniform(shape=[2, 2, 2]) try: preprocessed_images = shrink(img_batch_3) # rejects unexpected types or shapes except ValueError as ex: print(ex) . Python inputs incompatible with input_signature: inputs: ( tf.Tensor( [[[0.0453676 0.4961226 ] [0.38450027 0.06386387]] [[0.5941864 0.49307775] [0.4685067 0.9383416 ]]], shape=(2, 2, 2), dtype=float32)) input_signature: ( TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=None)) . Using Autograph To Capture Control Flow . A &quot;static&quot; for loop using range(): . @tf.function def add_10(x): for i in range(10): x += 1 return x . add_10(tf.constant(5)) . &lt;tf.Tensor: id=987280, shape=(), dtype=int32, numpy=15&gt; . add_10.get_concrete_function(tf.constant(5)).graph.get_operations() . [&lt;tf.Operation &#39;x&#39; type=Placeholder&gt;, &lt;tf.Operation &#39;add/y&#39; type=Const&gt;, &lt;tf.Operation &#39;add&#39; type=AddV2&gt;, &lt;tf.Operation &#39;add_1/y&#39; type=Const&gt;, &lt;tf.Operation &#39;add_1&#39; type=AddV2&gt;, &lt;tf.Operation &#39;add_2/y&#39; type=Const&gt;, &lt;tf.Operation &#39;add_2&#39; type=AddV2&gt;, &lt;tf.Operation &#39;add_3/y&#39; type=Const&gt;, &lt;tf.Operation &#39;add_3&#39; type=AddV2&gt;, &lt;tf.Operation &#39;add_4/y&#39; type=Const&gt;, &lt;tf.Operation &#39;add_4&#39; type=AddV2&gt;, &lt;tf.Operation &#39;add_5/y&#39; type=Const&gt;, &lt;tf.Operation &#39;add_5&#39; type=AddV2&gt;, &lt;tf.Operation &#39;add_6/y&#39; type=Const&gt;, &lt;tf.Operation &#39;add_6&#39; type=AddV2&gt;, &lt;tf.Operation &#39;add_7/y&#39; type=Const&gt;, &lt;tf.Operation &#39;add_7&#39; type=AddV2&gt;, &lt;tf.Operation &#39;add_8/y&#39; type=Const&gt;, &lt;tf.Operation &#39;add_8&#39; type=AddV2&gt;, &lt;tf.Operation &#39;add_9/y&#39; type=Const&gt;, &lt;tf.Operation &#39;add_9&#39; type=AddV2&gt;, &lt;tf.Operation &#39;Identity&#39; type=Identity&gt;] . A &quot;dynamic&quot; loop using tf.while_loop(): . @tf.function def add_10(x): condition = lambda i, x: tf.less(i, 10) body = lambda i, x: (tf.add(i, 1), tf.add(x, 1)) final_i, final_x = tf.while_loop(condition, body, [tf.constant(0), x]) return final_x . add_10(tf.constant(5)) . &lt;tf.Tensor: id=987324, shape=(), dtype=int32, numpy=15&gt; . add_10.get_concrete_function(tf.constant(5)).graph.get_operations() . [&lt;tf.Operation &#39;x&#39; type=Placeholder&gt;, &lt;tf.Operation &#39;Const&#39; type=Const&gt;, &lt;tf.Operation &#39;while/maximum_iterations&#39; type=Const&gt;, &lt;tf.Operation &#39;while/loop_counter&#39; type=Const&gt;, &lt;tf.Operation &#39;while&#39; type=While&gt;, &lt;tf.Operation &#39;while/Identity&#39; type=Identity&gt;, &lt;tf.Operation &#39;while/Identity_1&#39; type=Identity&gt;, &lt;tf.Operation &#39;while/Identity_2&#39; type=Identity&gt;, &lt;tf.Operation &#39;while/Identity_3&#39; type=Identity&gt;, &lt;tf.Operation &#39;Identity&#39; type=Identity&gt;] . A &quot;dynamic&quot; for loop using tf.range() (captured by autograph): . @tf.function def add_10(x): for i in tf.range(10): x = x + 1 return x . add_10.get_concrete_function(tf.constant(0)).graph.get_operations() . [&lt;tf.Operation &#39;x&#39; type=Placeholder&gt;, &lt;tf.Operation &#39;range/start&#39; type=Const&gt;, &lt;tf.Operation &#39;range/limit&#39; type=Const&gt;, &lt;tf.Operation &#39;range/delta&#39; type=Const&gt;, &lt;tf.Operation &#39;range&#39; type=Range&gt;, &lt;tf.Operation &#39;while/maximum_iterations&#39; type=Const&gt;, &lt;tf.Operation &#39;while/loop_counter&#39; type=Const&gt;, &lt;tf.Operation &#39;while&#39; type=While&gt;, &lt;tf.Operation &#39;while/Identity&#39; type=Identity&gt;, &lt;tf.Operation &#39;while/Identity_1&#39; type=Identity&gt;, &lt;tf.Operation &#39;while/Identity_2&#39; type=Identity&gt;, &lt;tf.Operation &#39;while/Identity_3&#39; type=Identity&gt;, &lt;tf.Operation &#39;while/Identity_4&#39; type=Identity&gt;, &lt;tf.Operation &#39;while/Identity_5&#39; type=Identity&gt;, &lt;tf.Operation &#39;Identity&#39; type=Identity&gt;] . Handling Variables and Other Resources in TF Functions . counter = tf.Variable(0) @tf.function def increment(counter, c=1): return counter.assign_add(c) . increment(counter) increment(counter) . &lt;tf.Tensor: id=987394, shape=(), dtype=int32, numpy=2&gt; . function_def = increment.get_concrete_function(counter).function_def function_def.signature.input_arg[0] . name: &#34;counter&#34; type: DT_RESOURCE . counter = tf.Variable(0) @tf.function def increment(c=1): return counter.assign_add(c) . increment() increment() . &lt;tf.Tensor: id=987410, shape=(), dtype=int32, numpy=2&gt; . function_def = increment.get_concrete_function().function_def function_def.signature.input_arg[0] . name: &#34;assignaddvariableop_resource&#34; type: DT_RESOURCE . class Counter: def __init__(self): self.counter = tf.Variable(0) @tf.function def increment(self, c=1): return self.counter.assign_add(c) . c = Counter() c.increment() c.increment() . &lt;tf.Tensor: id=987426, shape=(), dtype=int32, numpy=2&gt; . @tf.function def add_10(x): for i in tf.range(10): x += 1 return x tf.autograph.to_code(add_10.python_function) . &#34;def tf__add_10(x): n do_return = False n retval_ = ag__.UndefinedReturnValue() n with ag__.FunctionScope(&#39;add_10&#39;, &#39;add_10_scope&#39;, ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as add_10_scope: n n def get_state(): n return () n n def set_state(_): n pass n n def loop_body(iterates, x): n i = iterates n x += 1 n return x, n x, = ag__.for_stmt(ag__.converted_call(tf.range, add_10_scope.callopts, (10,), None, add_10_scope), None, loop_body, get_state, set_state, (x,), (&#39;x&#39;,), ()) n do_return = True n retval_ = add_10_scope.mark_return_value(x) n do_return, n return ag__.retval(retval_) n&#34; . def display_tf_code(func): from IPython.display import display, Markdown if hasattr(func, &quot;python_function&quot;): func = func.python_function code = tf.autograph.to_code(func) display(Markdown(&#39;python n{} n&#39;.format(code))) . display_tf_code(add_10) . def tf__add_10(x): do_return = False retval_ = ag__.UndefinedReturnValue() with ag__.FunctionScope(&#39;add_10&#39;, &#39;add_10_scope&#39;, ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as add_10_scope: def get_state(): return () def set_state(_): pass def loop_body(iterates, x): i = iterates x += 1 return x, x, = ag__.for_stmt(ag__.converted_call(tf.range, add_10_scope.callopts, (10,), None, add_10_scope), None, loop_body, get_state, set_state, (x,), (&#39;x&#39;,), ()) do_return = True retval_ = add_10_scope.mark_return_value(x) do_return, return ag__.retval(retval_) . Using TF Functions with tf.keras (or Not) . By default, tf.keras will automatically convert your custom code into TF Functions, no need to use tf.function(): . # Custom loss function def my_mse(y_true, y_pred): print(&quot;Tracing loss my_mse()&quot;) return tf.reduce_mean(tf.square(y_pred - y_true)) . # Custom metric function def my_mae(y_true, y_pred): print(&quot;Tracing metric my_mae()&quot;) return tf.reduce_mean(tf.abs(y_pred - y_true)) . # Custom layer class MyDense(keras.layers.Layer): def __init__(self, units, activation=None, **kwargs): super().__init__(**kwargs) self.units = units self.activation = keras.activations.get(activation) def build(self, input_shape): self.kernel = self.add_weight(name=&#39;kernel&#39;, shape=(input_shape[1], self.units), initializer=&#39;uniform&#39;, trainable=True) self.biases = self.add_weight(name=&#39;bias&#39;, shape=(self.units,), initializer=&#39;zeros&#39;, trainable=True) super().build(input_shape) def call(self, X): print(&quot;Tracing MyDense.call()&quot;) return self.activation(X @ self.kernel + self.biases) . # Custom model class MyModel(keras.models.Model): def __init__(self, **kwargs): super().__init__(**kwargs) self.hidden1 = MyDense(30, activation=&quot;relu&quot;) self.hidden2 = MyDense(30, activation=&quot;relu&quot;) self.output_ = MyDense(1) def call(self, input): print(&quot;Tracing MyModel.call()&quot;) hidden1 = self.hidden1(input) hidden2 = self.hidden2(hidden1) concat = keras.layers.concatenate([input, hidden2]) output = self.output_(concat) return output model = MyModel() . model.compile(loss=my_mse, optimizer=&quot;nadam&quot;, metrics=[my_mae]) . model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid)) model.evaluate(X_test_scaled, y_test) . Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing metric my_mae() Tracing loss my_mse() Train on 11610 samples, validate on 3870 samples Epoch 1/2 Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() 11008/11610 [===========================&gt;..] - ETA: 0s - loss: 1.4198 - my_mae: 0.8256Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() 11610/11610 [==============================] - 1s 94us/sample - loss: 1.3717 - my_mae: 0.8089 - val_loss: 0.5179 - val_my_mae: 0.4914 Epoch 2/2 11610/11610 [==============================] - 1s 52us/sample - loss: 0.4480 - my_mae: 0.4798 - val_loss: 0.5400 - val_my_mae: 0.4560 5160/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 22us/sample - loss: 0.3452 - my_mae: 0.4607 . [0.41599989205367804, 0.4606665] . You can turn this off by creating the model with dynamic=True (or calling super().__init__(dynamic=True, **kwargs) in the model&#39;s constructor): . model = MyModel(dynamic=True) . model.compile(loss=my_mse, optimizer=&quot;nadam&quot;, metrics=[my_mae]) . Not the custom code will be called at each iteration. Let&#39;s fit, validate and evaluate with tiny datasets to avoid getting too much output: . model.fit(X_train_scaled[:64], y_train[:64], epochs=1, validation_data=(X_valid_scaled[:64], y_valid[:64]), verbose=0) model.evaluate(X_test_scaled[:64], y_test[:64], verbose=0) . Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() . [5.630997180938721, 2.07411] . Alternatively, you can compile a model with run_eagerly=True: . model = MyModel() . model.compile(loss=my_mse, optimizer=&quot;nadam&quot;, metrics=[my_mae], run_eagerly=True) . model.fit(X_train_scaled[:64], y_train[:64], epochs=1, validation_data=(X_valid_scaled[:64], y_valid[:64]), verbose=0) model.evaluate(X_test_scaled[:64], y_test[:64], verbose=0) . Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() Tracing MyModel.call() Tracing MyDense.call() Tracing MyDense.call() Tracing MyDense.call() Tracing loss my_mse() Tracing metric my_mae() . [5.651222229003906, 2.071751] . Custom Optimizers . Defining custom optimizers is not very common, but in case you are one of the happy few who gets to write one, here is an example: . class MyMomentumOptimizer(keras.optimizers.Optimizer): def __init__(self, learning_rate=0.001, momentum=0.9, name=&quot;MyMomentumOptimizer&quot;, **kwargs): &quot;&quot;&quot;Call super().__init__() and use _set_hyper() to store hyperparameters&quot;&quot;&quot; super().__init__(name, **kwargs) self._set_hyper(&quot;learning_rate&quot;, kwargs.get(&quot;lr&quot;, learning_rate)) # handle lr=learning_rate self._set_hyper(&quot;decay&quot;, self._initial_decay) # self._set_hyper(&quot;momentum&quot;, momentum) def _create_slots(self, var_list): &quot;&quot;&quot;For each model variable, create the optimizer variable associated with it. TensorFlow calls these optimizer variables &quot;slots&quot;. For momentum optimization, we need one momentum slot per model variable. &quot;&quot;&quot; for var in var_list: self.add_slot(var, &quot;momentum&quot;) @tf.function def _resource_apply_dense(self, grad, var): &quot;&quot;&quot;Update the slots and perform one optimization step for one model variable &quot;&quot;&quot; var_dtype = var.dtype.base_dtype lr_t = self._decayed_lr(var_dtype) # handle learning rate decay momentum_var = self.get_slot(var, &quot;momentum&quot;) momentum_hyper = self._get_hyper(&quot;momentum&quot;, var_dtype) momentum_var.assign(momentum_var * momentum_hyper - (1. - momentum_hyper)* grad) var.assign_add(momentum_var * lr_t) def _resource_apply_sparse(self, grad, var): raise NotImplementedError def get_config(self): base_config = super().get_config() return { **base_config, &quot;learning_rate&quot;: self._serialize_hyperparameter(&quot;learning_rate&quot;), &quot;decay&quot;: self._serialize_hyperparameter(&quot;decay&quot;), &quot;momentum&quot;: self._serialize_hyperparameter(&quot;momentum&quot;), } . model = keras.models.Sequential([keras.layers.Dense(1, input_shape=[8])]) model.compile(loss=&quot;mse&quot;, optimizer=MyMomentumOptimizer()) model.fit(X_train_scaled, y_train, epochs=5) . Train on 11610 samples Epoch 1/5 11610/11610 [==============================] - 1s 53us/sample - loss: 5.0587 Epoch 2/5 11610/11610 [==============================] - 0s 30us/sample - loss: 1.4962 Epoch 3/5 11610/11610 [==============================] - 0s 31us/sample - loss: 0.7944 Epoch 4/5 11610/11610 [==============================] - 0s 32us/sample - loss: 0.6295 Epoch 5/5 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5872 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdec08458d0&gt; .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/12_custom_models_and_training_with_tensorflow",
            "relUrl": "/12_custom_models_and_training_with_tensorflow",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Title",
            "content": "Chapter 3 â€“ Classification . This notebook contains all the sample code and solutions to the exercises in chapter 3. . Run in Google Colab | Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20. . # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;classification&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . MNIST . from sklearn.datasets import fetch_openml mnist = fetch_openml(&#39;mnist_784&#39;, version=1) mnist.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;feature_names&#39;, &#39;DESCR&#39;, &#39;details&#39;, &#39;categories&#39;, &#39;url&#39;]) . X, y = mnist[&quot;data&quot;], mnist[&quot;target&quot;] X.shape . (70000, 784) . y.shape . (70000,) . 28 * 28 . 784 . %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt some_digit = X[0] some_digit_image = some_digit.reshape(28, 28) plt.imshow(some_digit_image, cmap=mpl.cm.binary) plt.axis(&quot;off&quot;) save_fig(&quot;some_digit_plot&quot;) plt.show() . Saving figure some_digit_plot . y[0] . &#39;5&#39; . y = y.astype(np.uint8) . def plot_digit(data): image = data.reshape(28, 28) plt.imshow(image, cmap = mpl.cm.binary, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;) . # EXTRA def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = [instance.reshape(size,size) for instance in instances] n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = mpl.cm.binary, **options) plt.axis(&quot;off&quot;) . plt.figure(figsize=(9,9)) example_images = X[:100] plot_digits(example_images, images_per_row=10) save_fig(&quot;more_digits_plot&quot;) plt.show() . Saving figure more_digits_plot . y[0] . 5 . X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] . Binary classifier . y_train_5 = (y_train == 5) y_test_5 = (y_test == 5) . Note: some hyperparameters will have a different defaut value in future versions of Scikit-Learn, such as max_iter and tol. To be future-proof, we explicitly set these hyperparameters to their future default values. For simplicity, this is not shown in the book. . from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) sgd_clf.fit(X_train, y_train_5) . SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=1000, n_iter=None, n_iter_no_change=5, n_jobs=None, penalty=&#39;l2&#39;, power_t=0.5, random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) . sgd_clf.predict([some_digit]) . array([ True]) . from sklearn.model_selection import cross_val_score cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=&quot;accuracy&quot;) . array([0.96355, 0.93795, 0.95615]) . from sklearn.model_selection import StratifiedKFold from sklearn.base import clone skfolds = StratifiedKFold(n_splits=3, random_state=42) for train_index, test_index in skfolds.split(X_train, y_train_5): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_5[train_index] X_test_fold = X_train[test_index] y_test_fold = y_train_5[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred = clone_clf.predict(X_test_fold) n_correct = sum(y_pred == y_test_fold) print(n_correct / len(y_pred)) . 0.96355 0.93795 0.95615 . from sklearn.base import BaseEstimator class Never5Classifier(BaseEstimator): def fit(self, X, y=None): pass def predict(self, X): return np.zeros((len(X), 1), dtype=bool) . never_5_clf = Never5Classifier() cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=&quot;accuracy&quot;) . array([0.91125, 0.90855, 0.90915]) . from sklearn.model_selection import cross_val_predict y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3) . from sklearn.metrics import confusion_matrix confusion_matrix(y_train_5, y_train_pred) . array([[53057, 1522], [ 1325, 4096]]) . y_train_perfect_predictions = y_train_5 # pretend we reached perfection confusion_matrix(y_train_5, y_train_perfect_predictions) . array([[54579, 0], [ 0, 5421]]) . from sklearn.metrics import precision_score, recall_score precision_score(y_train_5, y_train_pred) . 0.7290850836596654 . 4096 / (4096 + 1522) . 0.7290850836596654 . recall_score(y_train_5, y_train_pred) . 0.7555801512636044 . 4096 / (4096 + 1325) . 0.7555801512636044 . from sklearn.metrics import f1_score f1_score(y_train_5, y_train_pred) . 0.7420962043663375 . 4096 / (4096 + (1522 + 1325) / 2) . 0.7420962043663375 . y_scores = sgd_clf.decision_function([some_digit]) y_scores . array([2412.53175101]) . threshold = 0 y_some_digit_pred = (y_scores &gt; threshold) . y_some_digit_pred . array([ True]) . threshold = 8000 y_some_digit_pred = (y_scores &gt; threshold) y_some_digit_pred . array([False]) . y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=&quot;decision_function&quot;) . from sklearn.metrics import precision_recall_curve precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores) . def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;, linewidth=2) plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label=&quot;Recall&quot;, linewidth=2) plt.legend(loc=&quot;center right&quot;, fontsize=16) # Not shown in the book plt.xlabel(&quot;Threshold&quot;, fontsize=16) # Not shown plt.grid(True) # Not shown plt.axis([-50000, 50000, 0, 1]) # Not shown recall_90_precision = recalls[np.argmax(precisions &gt;= 0.90)] threshold_90_precision = thresholds[np.argmax(precisions &gt;= 0.90)] plt.figure(figsize=(8, 4)) # Not shown plot_precision_recall_vs_threshold(precisions, recalls, thresholds) plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], &quot;r:&quot;) # Not shown plt.plot([-50000, threshold_90_precision], [0.9, 0.9], &quot;r:&quot;) # Not shown plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], &quot;r:&quot;)# Not shown plt.plot([threshold_90_precision], [0.9], &quot;ro&quot;) # Not shown plt.plot([threshold_90_precision], [recall_90_precision], &quot;ro&quot;) # Not shown save_fig(&quot;precision_recall_vs_threshold_plot&quot;) # Not shown plt.show() . Saving figure precision_recall_vs_threshold_plot . (y_train_pred == (y_scores &gt; 0)).all() . True . def plot_precision_vs_recall(precisions, recalls): plt.plot(recalls, precisions, &quot;b-&quot;, linewidth=2) plt.xlabel(&quot;Recall&quot;, fontsize=16) plt.ylabel(&quot;Precision&quot;, fontsize=16) plt.axis([0, 1, 0, 1]) plt.grid(True) plt.figure(figsize=(8, 6)) plot_precision_vs_recall(precisions, recalls) plt.plot([0.4368, 0.4368], [0., 0.9], &quot;r:&quot;) plt.plot([0.0, 0.4368], [0.9, 0.9], &quot;r:&quot;) plt.plot([0.4368], [0.9], &quot;ro&quot;) save_fig(&quot;precision_vs_recall_plot&quot;) plt.show() . Saving figure precision_vs_recall_plot . threshold_90_precision = thresholds[np.argmax(precisions &gt;= 0.90)] . threshold_90_precision . 7816.1555236825225 . y_train_pred_90 = (y_scores &gt;= threshold_90_precision) . precision_score(y_train_5, y_train_pred_90) . 0.9000380083618396 . recall_score(y_train_5, y_train_pred_90) . 0.4368197749492714 . ROC curves . from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_train_5, y_scores) . def plot_roc_curve(fpr, tpr, label=None): plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], &#39;k--&#39;) # dashed diagonal plt.axis([0, 1, 0, 1]) # Not shown in the book plt.xlabel(&#39;False Positive Rate (Fall-Out)&#39;, fontsize=16) # Not shown plt.ylabel(&#39;True Positive Rate (Recall)&#39;, fontsize=16) # Not shown plt.grid(True) # Not shown plt.figure(figsize=(8, 6)) # Not shown plot_roc_curve(fpr, tpr) plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], &quot;r:&quot;) # Not shown plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], &quot;r:&quot;) # Not shown plt.plot([4.837e-3], [0.4368], &quot;ro&quot;) # Not shown save_fig(&quot;roc_curve_plot&quot;) # Not shown plt.show() . Saving figure roc_curve_plot . from sklearn.metrics import roc_auc_score roc_auc_score(y_train_5, y_scores) . 0.9611778893101814 . Note: we set n_estimators=100 to be future-proof since this will be the default value in Scikit-Learn 0.22. . from sklearn.ensemble import RandomForestClassifier forest_clf = RandomForestClassifier(n_estimators=100, random_state=42) y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method=&quot;predict_proba&quot;) . y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest) . plt.figure(figsize=(8, 6)) plt.plot(fpr, tpr, &quot;b:&quot;, linewidth=2, label=&quot;SGD&quot;) plot_roc_curve(fpr_forest, tpr_forest, &quot;Random Forest&quot;) plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], &quot;r:&quot;) plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], &quot;r:&quot;) plt.plot([4.837e-3], [0.4368], &quot;ro&quot;) plt.plot([4.837e-3, 4.837e-3], [0., 0.9487], &quot;r:&quot;) plt.plot([4.837e-3], [0.9487], &quot;ro&quot;) plt.grid(True) plt.legend(loc=&quot;lower right&quot;, fontsize=16) save_fig(&quot;roc_curve_comparison_plot&quot;) plt.show() . Saving figure roc_curve_comparison_plot . roc_auc_score(y_train_5, y_scores_forest) . 0.9983436731328145 . y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3) precision_score(y_train_5, y_train_pred_forest) . 0.9905083315756169 . recall_score(y_train_5, y_train_pred_forest) . 0.8662608374838591 . Multiclass classification . from sklearn.svm import SVC svm_clf = SVC(gamma=&quot;auto&quot;, random_state=42) svm_clf.fit(X_train[:1000], y_train[:1000]) # y_train, not y_train_5 svm_clf.predict([some_digit]) . array([5], dtype=uint8) . some_digit_scores = svm_clf.decision_function([some_digit]) some_digit_scores . array([[ 2.92492871, 7.02307409, 3.93648529, 0.90117363, 5.96945908, 9.5 , 1.90718593, 8.02755089, -0.13202708, 4.94216947]]) . np.argmax(some_digit_scores) . 5 . svm_clf.classes_ . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8) . svm_clf.classes_[5] . 5 . from sklearn.multiclass import OneVsRestClassifier ovr_clf = OneVsRestClassifier(SVC(gamma=&quot;auto&quot;, random_state=42)) ovr_clf.fit(X_train[:1000], y_train[:1000]) ovr_clf.predict([some_digit]) . array([5], dtype=uint8) . len(ovr_clf.estimators_) . 10 . sgd_clf.fit(X_train, y_train) sgd_clf.predict([some_digit]) . array([5], dtype=uint8) . sgd_clf.decision_function([some_digit]) . array([[-15955.22627845, -38080.96296175, -13326.66694897, 573.52692379, -17680.6846644 , 2412.53175101, -25526.86498156, -12290.15704709, -7946.05205023, -10631.35888549]]) . cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=&quot;accuracy&quot;) . array([0.8489802 , 0.87129356, 0.86988048]) . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train.astype(np.float64)) cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=&quot;accuracy&quot;) . array([0.89707059, 0.8960948 , 0.90693604]) . y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) conf_mx = confusion_matrix(y_train, y_train_pred) conf_mx . array([[5578, 0, 22, 7, 8, 45, 35, 5, 222, 1], [ 0, 6410, 35, 26, 4, 44, 4, 8, 198, 13], [ 28, 27, 5232, 100, 74, 27, 68, 37, 354, 11], [ 23, 18, 115, 5254, 2, 209, 26, 38, 373, 73], [ 11, 14, 45, 12, 5219, 11, 33, 26, 299, 172], [ 26, 16, 31, 173, 54, 4484, 76, 14, 482, 65], [ 31, 17, 45, 2, 42, 98, 5556, 3, 123, 1], [ 20, 10, 53, 27, 50, 13, 3, 5696, 173, 220], [ 17, 64, 47, 91, 3, 125, 24, 11, 5421, 48], [ 24, 18, 29, 67, 116, 39, 1, 174, 329, 5152]]) . def plot_confusion_matrix(matrix): &quot;&quot;&quot;If you prefer color and a colorbar&quot;&quot;&quot; fig = plt.figure(figsize=(8,8)) ax = fig.add_subplot(111) cax = ax.matshow(matrix) fig.colorbar(cax) . plt.matshow(conf_mx, cmap=plt.cm.gray) save_fig(&quot;confusion_matrix_plot&quot;, tight_layout=False) plt.show() . Saving figure confusion_matrix_plot . row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums . np.fill_diagonal(norm_conf_mx, 0) plt.matshow(norm_conf_mx, cmap=plt.cm.gray) save_fig(&quot;confusion_matrix_errors_plot&quot;, tight_layout=False) plt.show() . Saving figure confusion_matrix_errors_plot . cl_a, cl_b = 3, 5 X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)] X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)] X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)] X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)] plt.figure(figsize=(8,8)) plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5) plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5) plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5) plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5) save_fig(&quot;error_analysis_digits_plot&quot;) plt.show() . Saving figure error_analysis_digits_plot . Multilabel classification . from sklearn.neighbors import KNeighborsClassifier y_train_large = (y_train &gt;= 7) y_train_odd = (y_train % 2 == 1) y_multilabel = np.c_[y_train_large, y_train_odd] knn_clf = KNeighborsClassifier() knn_clf.fit(X_train, y_multilabel) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . knn_clf.predict([some_digit]) . array([[False, True]]) . Warning: the following cell may take a very long time (possibly hours depending on your hardware). . y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3) f1_score(y_multilabel, y_train_knn_pred, average=&quot;macro&quot;) . 0.976410265560605 . Multioutput classification . noise = np.random.randint(0, 100, (len(X_train), 784)) X_train_mod = X_train + noise noise = np.random.randint(0, 100, (len(X_test), 784)) X_test_mod = X_test + noise y_train_mod = X_train y_test_mod = X_test . some_index = 0 plt.subplot(121); plot_digit(X_test_mod[some_index]) plt.subplot(122); plot_digit(y_test_mod[some_index]) save_fig(&quot;noisy_digit_example_plot&quot;) plt.show() . Saving figure noisy_digit_example_plot . knn_clf.fit(X_train_mod, y_train_mod) clean_digit = knn_clf.predict([X_test_mod[some_index]]) plot_digit(clean_digit) save_fig(&quot;cleaned_digit_example_plot&quot;) . Saving figure cleaned_digit_example_plot . Extra material . Dummy (ie. random) classifier . from sklearn.dummy import DummyClassifier dmy_clf = DummyClassifier() y_probas_dmy = cross_val_predict(dmy_clf, X_train, y_train_5, cv=3, method=&quot;predict_proba&quot;) y_scores_dmy = y_probas_dmy[:, 1] . fprr, tprr, thresholdsr = roc_curve(y_train_5, y_scores_dmy) plot_roc_curve(fprr, tprr) . KNN classifier . from sklearn.neighbors import KNeighborsClassifier knn_clf = KNeighborsClassifier(weights=&#39;distance&#39;, n_neighbors=4) knn_clf.fit(X_train, y_train) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=4, p=2, weights=&#39;distance&#39;) . y_knn_pred = knn_clf.predict(X_test) . from sklearn.metrics import accuracy_score accuracy_score(y_test, y_knn_pred) . 0.9714 . from scipy.ndimage.interpolation import shift def shift_digit(digit_array, dx, dy, new=0): return shift(digit_array.reshape(28, 28), [dy, dx], cval=new).reshape(784) plot_digit(shift_digit(some_digit, 5, 1, new=100)) . X_train_expanded = [X_train] y_train_expanded = [y_train] for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)): shifted_images = np.apply_along_axis(shift_digit, axis=1, arr=X_train, dx=dx, dy=dy) X_train_expanded.append(shifted_images) y_train_expanded.append(y_train) X_train_expanded = np.concatenate(X_train_expanded) y_train_expanded = np.concatenate(y_train_expanded) X_train_expanded.shape, y_train_expanded.shape . ((300000, 784), (300000,)) . knn_clf.fit(X_train_expanded, y_train_expanded) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=4, p=2, weights=&#39;distance&#39;) . y_knn_expanded_pred = knn_clf.predict(X_test) . accuracy_score(y_test, y_knn_expanded_pred) . 0.9763 . ambiguous_digit = X_test[2589] knn_clf.predict_proba([ambiguous_digit]) . array([[0.24579675, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.75420325]]) . plot_digit(ambiguous_digit) . Exercise solutions . 1. An MNIST Classifier With Over 97% Accuracy . Warning: the next cell may take hours to run, depending on your hardware. . from sklearn.model_selection import GridSearchCV param_grid = [{&#39;weights&#39;: [&quot;uniform&quot;, &quot;distance&quot;], &#39;n_neighbors&#39;: [3, 4, 5]}] knn_clf = KNeighborsClassifier() grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3) grid_search.fit(X_train, y_train) . Fitting 5 folds for each of 6 candidates, totalling 30 fits [CV] n_neighbors=3, weights=uniform .................................. . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . [CV] n_neighbors=3, weights=uniform, score=0.9717617659308622, total=10.9min [CV] n_neighbors=3, weights=uniform .................................. . [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 52.2min remaining: 0.0s . [CV] n_neighbors=3, weights=uniform, score=0.9706715547408765, total=10.7min [CV] n_neighbors=3, weights=uniform .................................. . [Parallel(n_jobs=1)]: Done 2 out of 2 | elapsed: 103.2min remaining: 0.0s . [CV] n_neighbors=3, weights=uniform, score=0.9689166666666666, total=10.1min [CV] n_neighbors=3, weights=uniform .................................. [CV] n_neighbors=3, weights=uniform, score=0.968575477202634, total=11.0min [CV] n_neighbors=3, weights=uniform .................................. [CV] n_neighbors=3, weights=uniform, score=0.9704068022674225, total=11.0min [CV] n_neighbors=3, weights=distance ................................. [CV] n_neighbors=3, weights=distance, score=0.9723448563098709, total=10.9min [CV] n_neighbors=3, weights=distance ................................. [CV] n_neighbors=3, weights=distance, score=0.9716713881019831, total=11.2min [CV] n_neighbors=3, weights=distance ................................. [CV] n_neighbors=3, weights=distance, score=0.9700833333333333, total= 9.9min [CV] n_neighbors=3, weights=distance ................................. [CV] n_neighbors=3, weights=distance, score=0.9700758522964075, total=10.0min [CV] n_neighbors=3, weights=distance ................................. [CV] n_neighbors=3, weights=distance, score=0.971407135711904, total= 9.9min [CV] n_neighbors=4, weights=uniform .................................. [CV] n_neighbors=4, weights=uniform, score=0.9690129112869638, total= 9.9min [CV] n_neighbors=4, weights=uniform .................................. [CV] n_neighbors=4, weights=uniform, score=0.9682552907848692, total= 9.9min [CV] n_neighbors=4, weights=uniform .................................. [CV] n_neighbors=4, weights=uniform, score=0.9675833333333334, total= 9.9min [CV] n_neighbors=4, weights=uniform .................................. [CV] n_neighbors=4, weights=uniform, score=0.9673251646244895, total= 9.9min [CV] n_neighbors=4, weights=uniform .................................. [CV] n_neighbors=4, weights=uniform, score=0.970323441147049, total= 9.9min [CV] n_neighbors=4, weights=distance ................................. [CV] n_neighbors=4, weights=distance, score=0.9730112453144523, total= 9.9min [CV] n_neighbors=4, weights=distance ................................. [CV] n_neighbors=4, weights=distance, score=0.9722546242292951, total= 9.9min [CV] n_neighbors=4, weights=distance ................................. [CV] n_neighbors=4, weights=distance, score=0.9699166666666666, total= 9.9min [CV] n_neighbors=4, weights=distance ................................. [CV] n_neighbors=4, weights=distance, score=0.9709093940151705, total=10.5min [CV] n_neighbors=4, weights=distance ................................. [CV] n_neighbors=4, weights=distance, score=0.9719906635545181, total=10.7min [CV] n_neighbors=5, weights=uniform .................................. [CV] n_neighbors=5, weights=uniform, score=0.9697625989171179, total=10.9min [CV] n_neighbors=5, weights=uniform .................................. [CV] n_neighbors=5, weights=uniform, score=0.9701716380603232, total=10.5min [CV] n_neighbors=5, weights=uniform .................................. [CV] n_neighbors=5, weights=uniform, score=0.9694166666666667, total=10.5min [CV] n_neighbors=5, weights=uniform .................................. [CV] n_neighbors=5, weights=uniform, score=0.9681587063432525, total=10.7min [CV] n_neighbors=5, weights=uniform .................................. [CV] n_neighbors=5, weights=uniform, score=0.9689896632210737, total=10.6min [CV] n_neighbors=5, weights=distance ................................. [CV] n_neighbors=5, weights=distance, score=0.9703456892961266, total=10.6min [CV] n_neighbors=5, weights=distance ................................. [CV] n_neighbors=5, weights=distance, score=0.9713381103149475, total=11.1min [CV] n_neighbors=5, weights=distance ................................. [CV] n_neighbors=5, weights=distance, score=0.9704166666666667, total=10.4min [CV] n_neighbors=5, weights=distance ................................. [CV] n_neighbors=5, weights=distance, score=0.969409018921397, total=10.6min [CV] n_neighbors=5, weights=distance ................................. [CV] n_neighbors=5, weights=distance, score=0.9706568856285428, total=12.7min . [Parallel(n_jobs=1)]: Done 30 out of 30 | elapsed: 1523.6min finished . GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid=[{&#39;weights&#39;: [&#39;uniform&#39;, &#39;distance&#39;], &#39;n_neighbors&#39;: [3, 4, 5]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=3) . grid_search.best_params_ . {&#39;n_neighbors&#39;: 4, &#39;weights&#39;: &#39;distance&#39;} . grid_search.best_score_ . 0.9716166666666667 . from sklearn.metrics import accuracy_score y_pred = grid_search.predict(X_test) accuracy_score(y_test, y_pred) . 0.9714 . 2. Data Augmentation . from scipy.ndimage.interpolation import shift . def shift_image(image, dx, dy): image = image.reshape((28, 28)) shifted_image = shift(image, [dy, dx], cval=0, mode=&quot;constant&quot;) return shifted_image.reshape([-1]) . image = X_train[1000] shifted_image_down = shift_image(image, 0, 5) shifted_image_left = shift_image(image, -5, 0) plt.figure(figsize=(12,3)) plt.subplot(131) plt.title(&quot;Original&quot;, fontsize=14) plt.imshow(image.reshape(28, 28), interpolation=&quot;nearest&quot;, cmap=&quot;Greys&quot;) plt.subplot(132) plt.title(&quot;Shifted down&quot;, fontsize=14) plt.imshow(shifted_image_down.reshape(28, 28), interpolation=&quot;nearest&quot;, cmap=&quot;Greys&quot;) plt.subplot(133) plt.title(&quot;Shifted left&quot;, fontsize=14) plt.imshow(shifted_image_left.reshape(28, 28), interpolation=&quot;nearest&quot;, cmap=&quot;Greys&quot;) plt.show() . X_train_augmented = [image for image in X_train] y_train_augmented = [label for label in y_train] for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)): for image, label in zip(X_train, y_train): X_train_augmented.append(shift_image(image, dx, dy)) y_train_augmented.append(label) X_train_augmented = np.array(X_train_augmented) y_train_augmented = np.array(y_train_augmented) . shuffle_idx = np.random.permutation(len(X_train_augmented)) X_train_augmented = X_train_augmented[shuffle_idx] y_train_augmented = y_train_augmented[shuffle_idx] . knn_clf = KNeighborsClassifier(**grid_search.best_params_) . knn_clf.fit(X_train_augmented, y_train_augmented) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=4, p=2, weights=&#39;distance&#39;) . y_pred = knn_clf.predict(X_test) accuracy_score(y_test, y_pred) . 0.9763 . By simply augmenting the data, we got a 0.5% accuracy boost. :) . 3. Tackle the Titanic dataset . The goal is to predict whether or not a passenger survived based on attributes such as their age, sex, passenger class, where they embarked and so on. . First, login to Kaggle and go to the Titanic challenge to download train.csv and test.csv. Save them to the datasets/titanic directory. . Next, let&#39;s load the data: . import os TITANIC_PATH = os.path.join(&quot;datasets&quot;, &quot;titanic&quot;) . import pandas as pd def load_titanic_data(filename, titanic_path=TITANIC_PATH): csv_path = os.path.join(titanic_path, filename) return pd.read_csv(csv_path) . train_data = load_titanic_data(&quot;train.csv&quot;) test_data = load_titanic_data(&quot;test.csv&quot;) . The data is already split into a training set and a test set. However, the test data does not contain the labels: your goal is to train the best model you can using the training data, then make your predictions on the test data and upload them to Kaggle to see your final score. . Let&#39;s take a peek at the top few rows of the training set: . train_data.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . The attributes have the following meaning: . Survived: that&#39;s the target, 0 means the passenger did not survive, while 1 means he/she survived. | Pclass: passenger class. | Name, Sex, Age: self-explanatory | SibSp: how many siblings &amp; spouses of the passenger aboard the Titanic. | Parch: how many children &amp; parents of the passenger aboard the Titanic. | Ticket: ticket id | Fare: price paid (in pounds) | Cabin: passenger&#39;s cabin number | Embarked: where the passenger embarked the Titanic | . Let&#39;s get more info to see how much data is missing: . train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): PassengerId 891 non-null int64 Survived 891 non-null int64 Pclass 891 non-null int64 Name 891 non-null object Sex 891 non-null object Age 714 non-null float64 SibSp 891 non-null int64 Parch 891 non-null int64 Ticket 891 non-null object Fare 891 non-null float64 Cabin 204 non-null object Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.6+ KB . Okay, the Age, Cabin and Embarked attributes are sometimes null (less than 891 non-null), especially the Cabin (77% are null). We will ignore the Cabin for now and focus on the rest. The Age attribute has about 19% null values, so we will need to decide what to do with them. Replacing null values with the median age seems reasonable. . The Name and Ticket attributes may have some value, but they will be a bit tricky to convert into useful numbers that a model can consume. So for now, we will ignore them. . Let&#39;s take a look at the numerical attributes: . train_data.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . Yikes, only 38% Survived. :( That&#39;s close enough to 40%, so accuracy will be a reasonable metric to evaluate our model. | The mean Fare was Â£32.20, which does not seem so expensive (but it was probably a lot of money back then). | The mean Age was less than 30 years old. | . Let&#39;s check that the target is indeed 0 or 1: . train_data[&quot;Survived&quot;].value_counts() . 0 549 1 342 Name: Survived, dtype: int64 . Now let&#39;s take a quick look at all the categorical attributes: . train_data[&quot;Pclass&quot;].value_counts() . 3 491 1 216 2 184 Name: Pclass, dtype: int64 . train_data[&quot;Sex&quot;].value_counts() . male 577 female 314 Name: Sex, dtype: int64 . train_data[&quot;Embarked&quot;].value_counts() . S 644 C 168 Q 77 Name: Embarked, dtype: int64 . The Embarked attribute tells us where the passenger embarked: C=Cherbourg, Q=Queenstown, S=Southampton. . Note: the code below uses a mix of Pipeline, FeatureUnion and a custom DataFrameSelector to preprocess some columns differently. Since Scikit-Learn 0.20, it is preferable to use a ColumnTransformer, like in the previous chapter. . Now let&#39;s build our preprocessing pipelines. We will reuse the DataframeSelector we built in the previous chapter to select specific attributes from the DataFrame: . from sklearn.base import BaseEstimator, TransformerMixin class DataFrameSelector(BaseEstimator, TransformerMixin): def __init__(self, attribute_names): self.attribute_names = attribute_names def fit(self, X, y=None): return self def transform(self, X): return X[self.attribute_names] . Let&#39;s build the pipeline for the numerical attributes: . from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer num_pipeline = Pipeline([ (&quot;select_numeric&quot;, DataFrameSelector([&quot;Age&quot;, &quot;SibSp&quot;, &quot;Parch&quot;, &quot;Fare&quot;])), (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)), ]) . num_pipeline.fit_transform(train_data) . array([[22. , 1. , 0. , 7.25 ], [38. , 1. , 0. , 71.2833], [26. , 0. , 0. , 7.925 ], ..., [28. , 1. , 2. , 23.45 ], [26. , 0. , 0. , 30. ], [32. , 0. , 0. , 7.75 ]]) . We will also need an imputer for the string categorical columns (the regular SimpleImputer does not work on those): . # Inspired from stackoverflow.com/questions/25239958 class MostFrequentImputer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X], index=X.columns) return self def transform(self, X, y=None): return X.fillna(self.most_frequent_) . from sklearn.preprocessing import OneHotEncoder . Now we can build the pipeline for the categorical attributes: . cat_pipeline = Pipeline([ (&quot;select_cat&quot;, DataFrameSelector([&quot;Pclass&quot;, &quot;Sex&quot;, &quot;Embarked&quot;])), (&quot;imputer&quot;, MostFrequentImputer()), (&quot;cat_encoder&quot;, OneHotEncoder(sparse=False)), ]) . cat_pipeline.fit_transform(train_data) . array([[0., 0., 1., ..., 0., 0., 1.], [1., 0., 0., ..., 1., 0., 0.], [0., 0., 1., ..., 0., 0., 1.], ..., [0., 0., 1., ..., 0., 0., 1.], [1., 0., 0., ..., 1., 0., 0.], [0., 0., 1., ..., 0., 1., 0.]]) . Finally, let&#39;s join the numerical and categorical pipelines: . from sklearn.pipeline import FeatureUnion preprocess_pipeline = FeatureUnion(transformer_list=[ (&quot;num_pipeline&quot;, num_pipeline), (&quot;cat_pipeline&quot;, cat_pipeline), ]) . Cool! Now we have a nice preprocessing pipeline that takes the raw data and outputs numerical input features that we can feed to any Machine Learning model we want. . X_train = preprocess_pipeline.fit_transform(train_data) X_train . array([[22., 1., 0., ..., 0., 0., 1.], [38., 1., 0., ..., 1., 0., 0.], [26., 0., 0., ..., 0., 0., 1.], ..., [28., 1., 2., ..., 0., 0., 1.], [26., 0., 0., ..., 1., 0., 0.], [32., 0., 0., ..., 0., 1., 0.]]) . Let&#39;s not forget to get the labels: . y_train = train_data[&quot;Survived&quot;] . We are now ready to train a classifier. Let&#39;s start with an SVC: . from sklearn.svm import SVC svm_clf = SVC(gamma=&quot;auto&quot;) svm_clf.fit(X_train, y_train) . SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . Great, our model is trained, let&#39;s use it to make predictions on the test set: . X_test = preprocess_pipeline.transform(test_data) y_pred = svm_clf.predict(X_test) . And now we could just build a CSV file with these predictions (respecting the format excepted by Kaggle), then upload it and hope for the best. But wait! We can do better than hope. Why don&#39;t we use cross-validation to have an idea of how good our model is? . from sklearn.model_selection import cross_val_score svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10) svm_scores.mean() . 0.7365250822835092 . Okay, over 73% accuracy, clearly better than random chance, but it&#39;s not a great score. Looking at the leaderboard for the Titanic competition on Kaggle, you can see that you need to reach above 80% accuracy to be within the top 10% Kagglers. Some reached 100%, but since you can easily find the list of victims of the Titanic, it seems likely that there was little Machine Learning involved in their performance! ;-) So let&#39;s try to build a model that reaches 80% accuracy. . Let&#39;s try a RandomForestClassifier: . from sklearn.ensemble import RandomForestClassifier forest_clf = RandomForestClassifier(n_estimators=100, random_state=42) forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10) forest_scores.mean() . 0.8149526160481217 . That&#39;s much better! . Instead of just looking at the mean accuracy across the 10 cross-validation folds, let&#39;s plot all 10 scores for each model, along with a box plot highlighting the lower and upper quartiles, and &quot;whiskers&quot; showing the extent of the scores (thanks to Nevin Yilmaz for suggesting this visualization). Note that the boxplot() function detects outliers (called &quot;fliers&quot;) and does not include them within the whiskers. Specifically, if the lower quartile is $Q_1$ and the upper quartile is $Q_3$, then the interquartile range $IQR = Q_3 - Q_1$ (this is the box&#39;s height), and any score lower than $Q_1 - 1.5 times IQR$ is a flier, and so is any score greater than $Q3 + 1.5 times IQR$. . plt.figure(figsize=(8, 4)) plt.plot([1]*10, svm_scores, &quot;.&quot;) plt.plot([2]*10, forest_scores, &quot;.&quot;) plt.boxplot([svm_scores, forest_scores], labels=(&quot;SVM&quot;,&quot;Random Forest&quot;)) plt.ylabel(&quot;Accuracy&quot;, fontsize=14) plt.show() . To improve this result further, you could: . Compare many more models and tune hyperparameters using cross validation and grid search, | Do more feature engineering, for example: replace SibSp and Parch with their sum, | try to identify parts of names that correlate well with the Survived attribute (e.g. if the name contains &quot;Countess&quot;, then survival seems more likely), | . | try to convert numerical attributes to categorical attributes: for example, different age groups had very different survival rates (see below), so it may help to create an age bucket category and use it instead of the age. Similarly, it may be useful to have a special category for people traveling alone since only 30% of them survived (see below). | . train_data[&quot;AgeBucket&quot;] = train_data[&quot;Age&quot;] // 15 * 15 train_data[[&quot;AgeBucket&quot;, &quot;Survived&quot;]].groupby([&#39;AgeBucket&#39;]).mean() . Survived . AgeBucket . 0.0 0.576923 | . 15.0 0.362745 | . 30.0 0.423256 | . 45.0 0.404494 | . 60.0 0.240000 | . 75.0 1.000000 | . train_data[&quot;RelativesOnboard&quot;] = train_data[&quot;SibSp&quot;] + train_data[&quot;Parch&quot;] train_data[[&quot;RelativesOnboard&quot;, &quot;Survived&quot;]].groupby([&#39;RelativesOnboard&#39;]).mean() . Survived . RelativesOnboard . 0 0.303538 | . 1 0.552795 | . 2 0.578431 | . 3 0.724138 | . 4 0.200000 | . 5 0.136364 | . 6 0.333333 | . 7 0.000000 | . 10 0.000000 | . 4. Spam classifier . First, let&#39;s fetch the data: . import os import tarfile import urllib DOWNLOAD_ROOT = &quot;http://spamassassin.apache.org/old/publiccorpus/&quot; HAM_URL = DOWNLOAD_ROOT + &quot;20030228_easy_ham.tar.bz2&quot; SPAM_URL = DOWNLOAD_ROOT + &quot;20030228_spam.tar.bz2&quot; SPAM_PATH = os.path.join(&quot;datasets&quot;, &quot;spam&quot;) def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH): if not os.path.isdir(spam_path): os.makedirs(spam_path) for filename, url in ((&quot;ham.tar.bz2&quot;, HAM_URL), (&quot;spam.tar.bz2&quot;, SPAM_URL)): path = os.path.join(spam_path, filename) if not os.path.isfile(path): urllib.request.urlretrieve(url, path) tar_bz2_file = tarfile.open(path) tar_bz2_file.extractall(path=SPAM_PATH) tar_bz2_file.close() . fetch_spam_data() . Next, let&#39;s load all the emails: . HAM_DIR = os.path.join(SPAM_PATH, &quot;easy_ham&quot;) SPAM_DIR = os.path.join(SPAM_PATH, &quot;spam&quot;) ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) &gt; 20] spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) &gt; 20] . len(ham_filenames) . 2500 . len(spam_filenames) . 500 . We can use Python&#39;s email module to parse these emails (this handles headers, encoding, and so on): . import email import email.policy def load_email(is_spam, filename, spam_path=SPAM_PATH): directory = &quot;spam&quot; if is_spam else &quot;easy_ham&quot; with open(os.path.join(spam_path, directory, filename), &quot;rb&quot;) as f: return email.parser.BytesParser(policy=email.policy.default).parse(f) . ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames] spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames] . Let&#39;s look at one example of ham and one example of spam, to get a feel of what the data looks like: . print(ham_emails[1].get_content().strip()) . Martin A posted: Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the Mount Athos monastic community, was ideal for the patriotic sculpture. As well as Alexander&#39;s granite features, 240 ft high and 170 ft wide, a museum, a restored amphitheatre and car park for admiring crowds are planned So is this mountain limestone or granite? If it&#39;s limestone, it&#39;ll weather pretty fast. Yahoo! Groups Sponsor ~--&gt; 4 DVDs Free +s&amp;p Join Now http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM ~-&gt; To unsubscribe from this group, send an email to: forteana-unsubscribe@egroups.com Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/ . print(spam_emails[6].get_content().strip()) . Help wanted. We are a 14 year old fortune 500 company, that is growing at a tremendous rate. We are looking for individuals who want to work from home. This is an opportunity to make an excellent income. No experience is required. We will train you. So if you are looking to be employed from home with a career that has vast opportunities, then go: http://www.basetel.com/wealthnow We are looking for energetic and self motivated people. If that is you than click on the link and fill out the form, and one of our employement specialist will contact you. To be removed from our link simple go to: http://www.basetel.com/remove.html 4139vOLW7-758DoDY1425FRhM1-764SMFc8513fCsLl40 . Some emails are actually multipart, with images and attachments (which can have their own attachments). Let&#39;s look at the various types of structures we have: . def get_email_structure(email): if isinstance(email, str): return email payload = email.get_payload() if isinstance(payload, list): return &quot;multipart({})&quot;.format(&quot;, &quot;.join([ get_email_structure(sub_email) for sub_email in payload ])) else: return email.get_content_type() . from collections import Counter def structures_counter(emails): structures = Counter() for email in emails: structure = get_email_structure(email) structures[structure] += 1 return structures . structures_counter(ham_emails).most_common() . [(&#39;text/plain&#39;, 2408), (&#39;multipart(text/plain, application/pgp-signature)&#39;, 66), (&#39;multipart(text/plain, text/html)&#39;, 8), (&#39;multipart(text/plain, text/plain)&#39;, 4), (&#39;multipart(text/plain)&#39;, 3), (&#39;multipart(text/plain, application/octet-stream)&#39;, 2), (&#39;multipart(text/plain, text/enriched)&#39;, 1), (&#39;multipart(text/plain, application/ms-tnef, text/plain)&#39;, 1), (&#39;multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)&#39;, 1), (&#39;multipart(text/plain, video/mng)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain))&#39;, 1), (&#39;multipart(text/plain, application/x-pkcs7-signature)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)&#39;, 1), (&#39;multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))&#39;, 1), (&#39;multipart(text/plain, application/x-java-applet)&#39;, 1)] . structures_counter(spam_emails).most_common() . [(&#39;text/plain&#39;, 218), (&#39;text/html&#39;, 183), (&#39;multipart(text/plain, text/html)&#39;, 45), (&#39;multipart(text/html)&#39;, 20), (&#39;multipart(text/plain)&#39;, 19), (&#39;multipart(multipart(text/html))&#39;, 5), (&#39;multipart(text/plain, image/jpeg)&#39;, 3), (&#39;multipart(text/html, application/octet-stream)&#39;, 2), (&#39;multipart(text/plain, application/octet-stream)&#39;, 1), (&#39;multipart(text/html, text/plain)&#39;, 1), (&#39;multipart(multipart(text/html), application/octet-stream, image/jpeg)&#39;, 1), (&#39;multipart(multipart(text/plain, text/html), image/gif)&#39;, 1), (&#39;multipart/alternative&#39;, 1)] . It seems that the ham emails are more often plain text, while spam has quite a lot of HTML. Moreover, quite a few ham emails are signed using PGP, while no spam is. In short, it seems that the email structure is useful information to have. . Now let&#39;s take a look at the email headers: . for header, value in spam_emails[0].items(): print(header,&quot;:&quot;,value) . Return-Path : &lt;12a1mailbot1@web.de&gt; Delivered-To : zzzz@localhost.spamassassin.taint.org Received : from localhost (localhost [127.0.0.1]) by phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32 for &lt;zzzz@localhost&gt;; Thu, 22 Aug 2002 08:17:21 -0400 (EDT) Received : from mail.webnote.net [193.120.211.219] by localhost with POP3 (fetchmail-5.9.0) for zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST) Received : from dd_it7 ([210.97.77.167]) by webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623 for &lt;zzzz@spamassassin.taint.org&gt;; Thu, 22 Aug 2002 13:09:41 +0100 From : 12a1mailbot1@web.de Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7 with Microsoft SMTPSVC(5.5.1775.675.6); Sat, 24 Aug 2002 09:42:10 +0900 To : dcek1a1@netsgo.com Subject : Life Insurance - Why Pay More? Date : Wed, 21 Aug 2002 20:31:57 -1600 MIME-Version : 1.0 Message-ID : &lt;0103c1042001882DD_IT7@dd_it7&gt; Content-Type : text/html; charset=&#34;iso-8859-1&#34; Content-Transfer-Encoding : quoted-printable . There&#39;s probably a lot of useful information in there, such as the sender&#39;s email address (12a1mailbot1@web.de looks fishy), but we will just focus on the Subject header: . spam_emails[0][&quot;Subject&quot;] . &#39;Life Insurance - Why Pay More?&#39; . Okay, before we learn too much about the data, let&#39;s not forget to split it into a training set and a test set: . import numpy as np from sklearn.model_selection import train_test_split X = np.array(ham_emails + spam_emails) y = np.array([0] * len(ham_emails) + [1] * len(spam_emails)) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . Okay, let&#39;s start writing the preprocessing functions. First, we will need a function to convert HTML to plain text. Arguably the best way to do this would be to use the great BeautifulSoup library, but I would like to avoid adding another dependency to this project, so let&#39;s hack a quick &amp; dirty solution using regular expressions (at the risk of unÌ¨hoÍžly radianÍceÍ destroÒ‰ying all enliÌÍ„Ì‚Í„ghtenment). The following function first drops the &lt;head&gt; section, then converts all &lt;a&gt; tags to the word HYPERLINK, then it gets rid of all HTML tags, leaving only the plain text. For readability, it also replaces multiple newlines with single newlines, and finally it unescapes html entities (such as &amp;gt; or &amp;nbsp;): . import re from html import unescape def html_to_plain_text(html): text = re.sub(&#39;&lt;head.*?&gt;.*?&lt;/head&gt;&#39;, &#39;&#39;, html, flags=re.M | re.S | re.I) text = re.sub(&#39;&lt;a s.*?&gt;&#39;, &#39; HYPERLINK &#39;, text, flags=re.M | re.S | re.I) text = re.sub(&#39;&lt;.*?&gt;&#39;, &#39;&#39;, text, flags=re.M | re.S) text = re.sub(r&#39;( s* n)+&#39;, &#39; n&#39;, text, flags=re.M | re.S) return unescape(text) . Let&#39;s see if it works. This is HTML spam: . html_spam_emails = [email for email in X_train[y_train==1] if get_email_structure(email) == &quot;text/html&quot;] sample_html_spam = html_spam_emails[7] print(sample_html_spam.get_content().strip()[:1000], &quot;...&quot;) . &lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;&lt;/TITLE&gt;&lt;META http-equiv=&#34;Content-Type&#34; content=&#34;text/html; charset=windows-1252&#34;&gt;&lt;STYLE&gt;A:link {TEX-DECORATION: none}A:active {TEXT-DECORATION: none}A:visited {TEXT-DECORATION: none}A:hover {COLOR: #0033ff; TEXT-DECORATION: underline}&lt;/STYLE&gt;&lt;META content=&#34;MSHTML 6.00.2713.1100&#34; name=&#34;GENERATOR&#34;&gt;&lt;/HEAD&gt; &lt;BODY text=&#34;#000000&#34; vLink=&#34;#0033ff&#34; link=&#34;#0033ff&#34; bgColor=&#34;#CCCC99&#34;&gt;&lt;TABLE borderColor=&#34;#660000&#34; cellSpacing=&#34;0&#34; cellPadding=&#34;0&#34; border=&#34;0&#34; width=&#34;100%&#34;&gt;&lt;TR&gt;&lt;TD bgColor=&#34;#CCCC99&#34; valign=&#34;top&#34; colspan=&#34;2&#34; height=&#34;27&#34;&gt; &lt;font size=&#34;6&#34; face=&#34;Arial, Helvetica, sans-serif&#34; color=&#34;#660000&#34;&gt; &lt;b&gt;OTC&lt;/b&gt;&lt;/font&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD height=&#34;2&#34; bgcolor=&#34;#6a694f&#34;&gt; &lt;font size=&#34;5&#34; face=&#34;Times New Roman, Times, serif&#34; color=&#34;#FFFFFF&#34;&gt; &lt;b&gt;&amp;nbsp;Newsletter&lt;/b&gt;&lt;/font&gt;&lt;/TD&gt;&lt;TD height=&#34;2&#34; bgcolor=&#34;#6a694f&#34;&gt;&lt;div align=&#34;right&#34;&gt;&lt;font color=&#34;#FFFFFF&#34;&gt; &lt;b&gt;Discover Tomorrow&#39;s Winners&amp;nbsp;&lt;/b&gt;&lt;/font&gt;&lt;/div&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD height=&#34;25&#34; colspan=&#34;2&#34; bgcolor=&#34;#CCCC99&#34;&gt;&lt;table width=&#34;100%&#34; border=&#34;0&#34; ... . And this is the resulting plain text: . print(html_to_plain_text(sample_html_spam.get_content())[:1000], &quot;...&quot;) . OTC Â Newsletter Discover Tomorrow&#39;s WinnersÂ  For Immediate Release Cal-Bay (Stock Symbol: CBYI) Watch for analyst &#34;Strong Buy Recommendations&#34; and several advisory newsletters picking CBYI. CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future. Put CBYI on your watch list, acquire a position TODAY. REASONS TO INVEST IN CBYI A profitable company and is on track to beat ALL earnings estimates! One of the FASTEST growing distributors in environmental &amp; safety equipment instruments. Excellent management team, several EXCLUSIVE contracts. IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy &amp; Environmental Research. RAPIDLY GROWING INDUSTRY Industry revenues exceed $900 million, estimates indicate that there could be as much as $25 billi ... . Great! Now let&#39;s write a function that takes an email as input and returns its content as plain text, whatever its format is: . def email_to_text(email): html = None for part in email.walk(): ctype = part.get_content_type() if not ctype in (&quot;text/plain&quot;, &quot;text/html&quot;): continue try: content = part.get_content() except: # in case of encoding issues content = str(part.get_payload()) if ctype == &quot;text/plain&quot;: return content else: html = content if html: return html_to_plain_text(html) . print(email_to_text(sample_html_spam)[:100], &quot;...&quot;) . OTC Â Newsletter Discover Tomorrow&#39;s WinnersÂ  For Immediate Release Cal-Bay (Stock Symbol: CBYI) Wat ... . Let&#39;s throw in some stemming! For this to work, you need to install the Natural Language Toolkit (NLTK). It&#39;s as simple as running the following command (don&#39;t forget to activate your virtualenv first; if you don&#39;t have one, you will likely need administrator rights, or use the --user option): . $ pip3 install nltk . try: import nltk stemmer = nltk.PorterStemmer() for word in (&quot;Computations&quot;, &quot;Computation&quot;, &quot;Computing&quot;, &quot;Computed&quot;, &quot;Compute&quot;, &quot;Compulsive&quot;): print(word, &quot;=&gt;&quot;, stemmer.stem(word)) except ImportError: print(&quot;Error: stemming requires the NLTK module.&quot;) stemmer = None . Computations =&gt; comput Computation =&gt; comput Computing =&gt; comput Computed =&gt; comput Compute =&gt; comput Compulsive =&gt; compuls . We will also need a way to replace URLs with the word &quot;URL&quot;. For this, we could use hard core regular expressions but we will just use the urlextract library. You can install it with the following command (don&#39;t forget to activate your virtualenv first; if you don&#39;t have one, you will likely need administrator rights, or use the --user option): . $ pip3 install urlextract . # if running this notebook on Colab, we just pip install urlextract try: import google.colab !pip install -q -U urlextract except ImportError: pass # not running on Colab . try: import urlextract # may require an Internet connection to download root domain names url_extractor = urlextract.URLExtract() print(url_extractor.find_urls(&quot;Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s&quot;)) except ImportError: print(&quot;Error: replacing URLs requires the urlextract module.&quot;) url_extractor = None . [&#39;github.com&#39;, &#39;https://youtu.be/7Pq-S557XQU?t=3m32s&#39;] . We are ready to put all this together into a transformer that we will use to convert emails to word counters. Note that we split sentences into words using Python&#39;s split() method, which uses whitespaces for word boundaries. This works for many written languages, but not all. For example, Chinese and Japanese scripts generally don&#39;t use spaces between words, and Vietnamese often uses spaces even between syllables. It&#39;s okay in this exercise, because the dataset is (mostly) in English. . from sklearn.base import BaseEstimator, TransformerMixin class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin): def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True, replace_urls=True, replace_numbers=True, stemming=True): self.strip_headers = strip_headers self.lower_case = lower_case self.remove_punctuation = remove_punctuation self.replace_urls = replace_urls self.replace_numbers = replace_numbers self.stemming = stemming def fit(self, X, y=None): return self def transform(self, X, y=None): X_transformed = [] for email in X: text = email_to_text(email) or &quot;&quot; if self.lower_case: text = text.lower() if self.replace_urls and url_extractor is not None: urls = list(set(url_extractor.find_urls(text))) urls.sort(key=lambda url: len(url), reverse=True) for url in urls: text = text.replace(url, &quot; URL &quot;) if self.replace_numbers: text = re.sub(r&#39; d+(?: . d*(?:[eE] d+))?&#39;, &#39;NUMBER&#39;, text) if self.remove_punctuation: text = re.sub(r&#39; W+&#39;, &#39; &#39;, text, flags=re.M) word_counts = Counter(text.split()) if self.stemming and stemmer is not None: stemmed_word_counts = Counter() for word, count in word_counts.items(): stemmed_word = stemmer.stem(word) stemmed_word_counts[stemmed_word] += count word_counts = stemmed_word_counts X_transformed.append(word_counts) return np.array(X_transformed) . Let&#39;s try this transformer on a few emails: . X_few = X_train[:3] X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few) X_few_wordcounts . array([Counter({&#39;chuck&#39;: 1, &#39;murcko&#39;: 1, &#39;wrote&#39;: 1, &#39;stuff&#39;: 1, &#39;yawn&#39;: 1, &#39;r&#39;: 1}), Counter({&#39;the&#39;: 11, &#39;of&#39;: 9, &#39;and&#39;: 8, &#39;all&#39;: 3, &#39;christian&#39;: 3, &#39;to&#39;: 3, &#39;by&#39;: 3, &#39;jefferson&#39;: 2, &#39;i&#39;: 2, &#39;have&#39;: 2, &#39;superstit&#39;: 2, &#39;one&#39;: 2, &#39;on&#39;: 2, &#39;been&#39;: 2, &#39;ha&#39;: 2, &#39;half&#39;: 2, &#39;rogueri&#39;: 2, &#39;teach&#39;: 2, &#39;jesu&#39;: 2, &#39;some&#39;: 1, &#39;interest&#39;: 1, &#39;quot&#39;: 1, &#39;url&#39;: 1, &#39;thoma&#39;: 1, &#39;examin&#39;: 1, &#39;known&#39;: 1, &#39;word&#39;: 1, &#39;do&#39;: 1, &#39;not&#39;: 1, &#39;find&#39;: 1, &#39;in&#39;: 1, &#39;our&#39;: 1, &#39;particular&#39;: 1, &#39;redeem&#39;: 1, &#39;featur&#39;: 1, &#39;they&#39;: 1, &#39;are&#39;: 1, &#39;alik&#39;: 1, &#39;found&#39;: 1, &#39;fabl&#39;: 1, &#39;mytholog&#39;: 1, &#39;million&#39;: 1, &#39;innoc&#39;: 1, &#39;men&#39;: 1, &#39;women&#39;: 1, &#39;children&#39;: 1, &#39;sinc&#39;: 1, &#39;introduct&#39;: 1, &#39;burnt&#39;: 1, &#39;tortur&#39;: 1, &#39;fine&#39;: 1, &#39;imprison&#39;: 1, &#39;what&#39;: 1, &#39;effect&#39;: 1, &#39;thi&#39;: 1, &#39;coercion&#39;: 1, &#39;make&#39;: 1, &#39;world&#39;: 1, &#39;fool&#39;: 1, &#39;other&#39;: 1, &#39;hypocrit&#39;: 1, &#39;support&#39;: 1, &#39;error&#39;: 1, &#39;over&#39;: 1, &#39;earth&#39;: 1, &#39;six&#39;: 1, &#39;histor&#39;: 1, &#39;american&#39;: 1, &#39;john&#39;: 1, &#39;e&#39;: 1, &#39;remsburg&#39;: 1, &#39;letter&#39;: 1, &#39;william&#39;: 1, &#39;short&#39;: 1, &#39;again&#39;: 1, &#39;becom&#39;: 1, &#39;most&#39;: 1, &#39;pervert&#39;: 1, &#39;system&#39;: 1, &#39;that&#39;: 1, &#39;ever&#39;: 1, &#39;shone&#39;: 1, &#39;man&#39;: 1, &#39;absurd&#39;: 1, &#39;untruth&#39;: 1, &#39;were&#39;: 1, &#39;perpetr&#39;: 1, &#39;upon&#39;: 1, &#39;a&#39;: 1, &#39;larg&#39;: 1, &#39;band&#39;: 1, &#39;dupe&#39;: 1, &#39;import&#39;: 1, &#39;led&#39;: 1, &#39;paul&#39;: 1, &#39;first&#39;: 1, &#39;great&#39;: 1, &#39;corrupt&#39;: 1}), Counter({&#39;url&#39;: 5, &#39;s&#39;: 3, &#39;group&#39;: 3, &#39;to&#39;: 3, &#39;in&#39;: 2, &#39;forteana&#39;: 2, &#39;martin&#39;: 2, &#39;an&#39;: 2, &#39;and&#39;: 2, &#39;we&#39;: 2, &#39;is&#39;: 2, &#39;yahoo&#39;: 2, &#39;unsubscrib&#39;: 2, &#39;y&#39;: 1, &#39;adamson&#39;: 1, &#39;wrote&#39;: 1, &#39;for&#39;: 1, &#39;altern&#39;: 1, &#39;rather&#39;: 1, &#39;more&#39;: 1, &#39;factual&#39;: 1, &#39;base&#39;: 1, &#39;rundown&#39;: 1, &#39;on&#39;: 1, &#39;hamza&#39;: 1, &#39;career&#39;: 1, &#39;includ&#39;: 1, &#39;hi&#39;: 1, &#39;belief&#39;: 1, &#39;that&#39;: 1, &#39;all&#39;: 1, &#39;non&#39;: 1, &#39;muslim&#39;: 1, &#39;yemen&#39;: 1, &#39;should&#39;: 1, &#39;be&#39;: 1, &#39;murder&#39;: 1, &#39;outright&#39;: 1, &#39;know&#39;: 1, &#39;how&#39;: 1, &#39;unbias&#39;: 1, &#39;memri&#39;: 1, &#39;don&#39;: 1, &#39;t&#39;: 1, &#39;html&#39;: 1, &#39;rob&#39;: 1, &#39;sponsor&#39;: 1, &#39;number&#39;: 1, &#39;dvd&#39;: 1, &#39;free&#39;: 1, &#39;p&#39;: 1, &#39;join&#39;: 1, &#39;now&#39;: 1, &#39;from&#39;: 1, &#39;thi&#39;: 1, &#39;send&#39;: 1, &#39;email&#39;: 1, &#39;your&#39;: 1, &#39;use&#39;: 1, &#39;of&#39;: 1, &#39;subject&#39;: 1})], dtype=object) . This looks about right! . Now we have the word counts, and we need to convert them to vectors. For this, we will build another transformer whose fit() method will build the vocabulary (an ordered list of the most common words) and whose transform() method will use the vocabulary to convert word counts to vectors. The output is a sparse matrix. . from scipy.sparse import csr_matrix class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin): def __init__(self, vocabulary_size=1000): self.vocabulary_size = vocabulary_size def fit(self, X, y=None): total_count = Counter() for word_count in X: for word, count in word_count.items(): total_count[word] += min(count, 10) most_common = total_count.most_common()[:self.vocabulary_size] self.most_common_ = most_common self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)} return self def transform(self, X, y=None): rows = [] cols = [] data = [] for row, word_count in enumerate(X): for word, count in word_count.items(): rows.append(row) cols.append(self.vocabulary_.get(word, 0)) data.append(count) return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1)) . vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10) X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts) X_few_vectors . &lt;3x11 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39; with 20 stored elements in Compressed Sparse Row format&gt; . X_few_vectors.toarray() . array([[ 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [99, 11, 9, 8, 1, 3, 3, 1, 3, 2, 3], [65, 0, 1, 2, 5, 3, 1, 2, 0, 1, 0]], dtype=int64) . What does this matrix mean? Well, the 99 in the second row, first column, means that the second email contains 99 words that are not part of the vocabulary. The 11 next to it means that the first word in the vocabulary is present 11 times in this email. The 9 next to it means that the second word is present 9 times, and so on. You can look at the vocabulary to know which words we are talking about. The first word is &quot;the&quot;, the second word is &quot;of&quot;, etc. . vocab_transformer.vocabulary_ . {&#39;the&#39;: 1, &#39;of&#39;: 2, &#39;and&#39;: 3, &#39;url&#39;: 4, &#39;to&#39;: 5, &#39;all&#39;: 6, &#39;in&#39;: 7, &#39;christian&#39;: 8, &#39;on&#39;: 9, &#39;by&#39;: 10} . We are now ready to train our first spam classifier! Let&#39;s transform the whole dataset: . from sklearn.pipeline import Pipeline preprocess_pipeline = Pipeline([ (&quot;email_to_wordcount&quot;, EmailToWordCounterTransformer()), (&quot;wordcount_to_vector&quot;, WordCounterToVectorTransformer()), ]) X_train_transformed = preprocess_pipeline.fit_transform(X_train) . Note: to be future-proof, we set solver=&quot;lbfgs&quot; since this will be the default value in Scikit-Learn 0.22. . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score log_clf = LogisticRegression(solver=&quot;lbfgs&quot;, random_state=42) score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3) score.mean() . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations. &#34;of iterations.&#34;, ConvergenceWarning) [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 0.1s remaining: 0.0s . [CV] ................................................................ [CV] .................................... , score=0.985, total= 0.1s [CV] ................................................................ [CV] .................................... , score=0.985, total= 0.1s [CV] ................................................................ [CV] ................................... , score=0.9925, total= 0.1s . /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations. &#34;of iterations.&#34;, ConvergenceWarning) [Parallel(n_jobs=1)]: Done 2 out of 2 | elapsed: 0.2s remaining: 0.0s /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations. &#34;of iterations.&#34;, ConvergenceWarning) [Parallel(n_jobs=1)]: Done 3 out of 3 | elapsed: 0.4s finished . 0.9874999999999999 . Over 98.7%, not bad for a first try! :) However, remember that we are using the &quot;easy&quot; dataset. You can try with the harder datasets, the results won&#39;t be so amazing. You would have to try multiple models, select the best ones and fine-tune them using cross-validation, and so on. . But you get the picture, so let&#39;s stop now, and just print out the precision/recall we get on the test set: . from sklearn.metrics import precision_score, recall_score X_test_transformed = preprocess_pipeline.transform(X_test) log_clf = LogisticRegression(solver=&quot;lbfgs&quot;, random_state=42) log_clf.fit(X_train_transformed, y_train) y_pred = log_clf.predict(X_test_transformed) print(&quot;Precision: {:.2f}%&quot;.format(100 * precision_score(y_test, y_pred))) print(&quot;Recall: {:.2f}%&quot;.format(100 * recall_score(y_test, y_pred))) . Precision: 95.88% Recall: 97.89% . /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations. &#34;of iterations.&#34;, ConvergenceWarning) .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/2020/03/09/_classification.html",
            "relUrl": "/2020/03/09/_classification.html",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Autoencoders and GANs",
            "content": "This notebook contains all the sample code in chapter 17. . Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20 and TensorFlow â‰¥2.0. . # Python â‰¥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn â‰¥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x IS_COLAB = True except Exception: IS_COLAB = False # TensorFlow â‰¥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; if not tf.test.is_gpu_available(): print(&quot;No GPU was detected. LSTMs and CNNs can be very slow without a GPU.&quot;) if IS_COLAB: print(&quot;Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.&quot;) # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) tf.random.set_seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;autoencoders&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . A couple utility functions to plot grayscale 28x28 image: . def plot_image(image): plt.imshow(image, cmap=&quot;binary&quot;) plt.axis(&quot;off&quot;) . PCA with a linear Autoencoder . Build 3D dataset: . np.random.seed(4) def generate_3d_data(m, w1=0.1, w2=0.3, noise=0.1): angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5 data = np.empty((m, 3)) data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2 data[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2 data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * np.random.randn(m) return data X_train = generate_3d_data(60) X_train = X_train - X_train.mean(axis=0, keepdims=0) . Now let&#39;s build the Autoencoder... . np.random.seed(42) tf.random.set_seed(42) encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])]) decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])]) autoencoder = keras.models.Sequential([encoder, decoder]) autoencoder.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(lr=1.5)) . history = autoencoder.fit(X_train, X_train, epochs=20) . Train on 60 samples Epoch 1/20 60/60 [==============================] - 0s 1ms/sample - loss: 0.2648 Epoch 2/20 60/60 [==============================] - 0s 49us/sample - loss: 0.1317 Epoch 3/20 60/60 [==============================] - 0s 50us/sample - loss: 0.0778 Epoch 4/20 60/60 [==============================] - 0s 46us/sample - loss: 0.0655 Epoch 5/20 60/60 [==============================] - 0s 51us/sample - loss: 0.0748 Epoch 6/20 60/60 [==============================] - 0s 47us/sample - loss: 0.1039 Epoch 7/20 60/60 [==============================] - 0s 50us/sample - loss: 0.1262 Epoch 8/20 60/60 [==============================] - 0s 52us/sample - loss: 0.0536 Epoch 9/20 60/60 [==============================] - 0s 51us/sample - loss: 0.0208 Epoch 10/20 60/60 [==============================] - 0s 52us/sample - loss: 0.0146 Epoch 11/20 60/60 [==============================] - 0s 52us/sample - loss: 0.0097 Epoch 12/20 60/60 [==============================] - 0s 48us/sample - loss: 0.0076 Epoch 13/20 60/60 [==============================] - 0s 43us/sample - loss: 0.0067 Epoch 14/20 60/60 [==============================] - 0s 49us/sample - loss: 0.0070 Epoch 15/20 60/60 [==============================] - 0s 58us/sample - loss: 0.0061 Epoch 16/20 60/60 [==============================] - 0s 53us/sample - loss: 0.0055 Epoch 17/20 60/60 [==============================] - 0s 63us/sample - loss: 0.0056 Epoch 18/20 60/60 [==============================] - 0s 58us/sample - loss: 0.0055 Epoch 19/20 60/60 [==============================] - 0s 64us/sample - loss: 0.0054 Epoch 20/20 60/60 [==============================] - 0s 55us/sample - loss: 0.0055 . codings = encoder.predict(X_train) . fig = plt.figure(figsize=(4,3)) plt.plot(codings[:,0], codings[:, 1], &quot;b.&quot;) plt.xlabel(&quot;$z_1$&quot;, fontsize=18) plt.ylabel(&quot;$z_2$&quot;, fontsize=18, rotation=0) plt.grid(True) save_fig(&quot;linear_autoencoder_pca_plot&quot;) plt.show() . Saving figure linear_autoencoder_pca_plot . Stacked Autoencoders . Let&#39;s use MNIST: . (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() X_train_full = X_train_full.astype(np.float32) / 255 X_test = X_test.astype(np.float32) / 255 X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:] y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:] . Train all layers at once . Let&#39;s build a stacked Autoencoder with 3 hidden layers and 1 output layer (i.e., 2 stacked Autoencoders). . def rounded_accuracy(y_true, y_pred): return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred)) . tf.random.set_seed(42) np.random.seed(42) stacked_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(100, activation=&quot;selu&quot;), keras.layers.Dense(30, activation=&quot;selu&quot;), ]) stacked_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[30]), keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;), keras.layers.Reshape([28, 28]) ]) stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder]) stacked_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1.5), metrics=[rounded_accuracy]) history = stacked_ae.fit(X_train, X_train, epochs=20, validation_data=[X_valid, X_valid]) . WARNING: Logging before flag parsing goes to stderr. W0610 11:04:16.485157 140735810999168 deprecation.py:323] From /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4149: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where . Train on 55000 samples, validate on 5000 samples Epoch 1/20 55000/55000 [==============================] - 4s 72us/sample - loss: 0.3386 - rounded_accuracy: 0.8866 - val_loss: 0.3118 - val_rounded_accuracy: 0.9128 Epoch 2/20 55000/55000 [==============================] - 4s 64us/sample - loss: 0.3055 - rounded_accuracy: 0.9153 - val_loss: 0.3030 - val_rounded_accuracy: 0.9200 Epoch 3/20 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2986 - rounded_accuracy: 0.9214 - val_loss: 0.2982 - val_rounded_accuracy: 0.9249 Epoch 4/20 55000/55000 [==============================] - 4s 67us/sample - loss: 0.2946 - rounded_accuracy: 0.9251 - val_loss: 0.2938 - val_rounded_accuracy: 0.9284 Epoch 5/20 55000/55000 [==============================] - 4s 70us/sample - loss: 0.2921 - rounded_accuracy: 0.9273 - val_loss: 0.2922 - val_rounded_accuracy: 0.9302 Epoch 6/20 55000/55000 [==============================] - 4s 69us/sample - loss: 0.2904 - rounded_accuracy: 0.9289 - val_loss: 0.2917 - val_rounded_accuracy: 0.9304 Epoch 7/20 55000/55000 [==============================] - 4s 72us/sample - loss: 0.2889 - rounded_accuracy: 0.9303 - val_loss: 0.2901 - val_rounded_accuracy: 0.9313 Epoch 8/20 55000/55000 [==============================] - 4s 66us/sample - loss: 0.2878 - rounded_accuracy: 0.9311 - val_loss: 0.2884 - val_rounded_accuracy: 0.9324 Epoch 9/20 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2869 - rounded_accuracy: 0.9319 - val_loss: 0.2879 - val_rounded_accuracy: 0.9321 Epoch 10/20 55000/55000 [==============================] - 4s 69us/sample - loss: 0.2860 - rounded_accuracy: 0.9326 - val_loss: 0.2874 - val_rounded_accuracy: 0.9328 Epoch 11/20 55000/55000 [==============================] - 4s 71us/sample - loss: 0.2854 - rounded_accuracy: 0.9331 - val_loss: 0.2873 - val_rounded_accuracy: 0.9313 Epoch 12/20 55000/55000 [==============================] - 4s 72us/sample - loss: 0.2847 - rounded_accuracy: 0.9336 - val_loss: 0.2872 - val_rounded_accuracy: 0.9299 Epoch 13/20 55000/55000 [==============================] - 4s 65us/sample - loss: 0.2841 - rounded_accuracy: 0.9341 - val_loss: 0.2863 - val_rounded_accuracy: 0.9311 Epoch 14/20 55000/55000 [==============================] - 4s 67us/sample - loss: 0.2837 - rounded_accuracy: 0.9344 - val_loss: 0.2846 - val_rounded_accuracy: 0.9348 Epoch 15/20 55000/55000 [==============================] - 4s 65us/sample - loss: 0.2832 - rounded_accuracy: 0.9348 - val_loss: 0.2842 - val_rounded_accuracy: 0.9344 Epoch 16/20 55000/55000 [==============================] - 4s 66us/sample - loss: 0.2827 - rounded_accuracy: 0.9352 - val_loss: 0.2850 - val_rounded_accuracy: 0.9359 Epoch 17/20 55000/55000 [==============================] - 4s 65us/sample - loss: 0.2823 - rounded_accuracy: 0.9355 - val_loss: 0.2841 - val_rounded_accuracy: 0.9363 Epoch 18/20 55000/55000 [==============================] - 4s 65us/sample - loss: 0.2820 - rounded_accuracy: 0.9357 - val_loss: 0.2832 - val_rounded_accuracy: 0.9355 Epoch 19/20 55000/55000 [==============================] - 4s 71us/sample - loss: 0.2817 - rounded_accuracy: 0.9360 - val_loss: 0.2858 - val_rounded_accuracy: 0.9361 Epoch 20/20 55000/55000 [==============================] - 4s 76us/sample - loss: 0.2814 - rounded_accuracy: 0.9363 - val_loss: 0.2835 - val_rounded_accuracy: 0.9370 . This function processes a few test images through the autoencoder and displays the original images and their reconstructions: . def show_reconstructions(model, images=X_valid, n_images=5): reconstructions = model.predict(images[:n_images]) fig = plt.figure(figsize=(n_images * 1.5, 3)) for image_index in range(n_images): plt.subplot(2, n_images, 1 + image_index) plot_image(images[image_index]) plt.subplot(2, n_images, 1 + n_images + image_index) plot_image(reconstructions[image_index]) . show_reconstructions(stacked_ae) save_fig(&quot;reconstruction_plot&quot;) . Saving figure reconstruction_plot . Visualizing Fashion MNIST . np.random.seed(42) from sklearn.manifold import TSNE X_valid_compressed = stacked_encoder.predict(X_valid) tsne = TSNE() X_valid_2D = tsne.fit_transform(X_valid_compressed) X_valid_2D = (X_valid_2D - X_valid_2D.min()) / (X_valid_2D.max() - X_valid_2D.min()) . plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=&quot;tab10&quot;) plt.axis(&quot;off&quot;) plt.show() . Let&#39;s make this diagram a bit prettier: . # adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html plt.figure(figsize=(10, 8)) cmap = plt.cm.tab10 plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=cmap) image_positions = np.array([[1., 1.]]) for index, position in enumerate(X_valid_2D): dist = np.sum((position - image_positions) ** 2, axis=1) if np.min(dist) &gt; 0.02: # if far enough from other images image_positions = np.r_[image_positions, [position]] imagebox = mpl.offsetbox.AnnotationBbox( mpl.offsetbox.OffsetImage(X_valid[index], cmap=&quot;binary&quot;), position, bboxprops={&quot;edgecolor&quot;: cmap(y_valid[index]), &quot;lw&quot;: 2}) plt.gca().add_artist(imagebox) plt.axis(&quot;off&quot;) save_fig(&quot;fashion_mnist_visualization_plot&quot;) plt.show() . Saving figure fashion_mnist_visualization_plot . Tying weights . It is common to tie the weights of the encoder and the decoder, by simply using the transpose of the encoder&#39;s weights as the decoder weights. For this, we need to use a custom layer. . class DenseTranspose(keras.layers.Layer): def __init__(self, dense, activation=None, **kwargs): self.dense = dense self.activation = keras.activations.get(activation) super().__init__(**kwargs) def build(self, batch_input_shape): self.biases = self.add_weight(name=&quot;bias&quot;, shape=[self.dense.input_shape[-1]], initializer=&quot;zeros&quot;) super().build(batch_input_shape) def call(self, inputs): z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True) return self.activation(z + self.biases) . keras.backend.clear_session() tf.random.set_seed(42) np.random.seed(42) dense_1 = keras.layers.Dense(100, activation=&quot;selu&quot;) dense_2 = keras.layers.Dense(30, activation=&quot;selu&quot;) tied_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), dense_1, dense_2 ]) tied_decoder = keras.models.Sequential([ DenseTranspose(dense_2, activation=&quot;selu&quot;), DenseTranspose(dense_1, activation=&quot;sigmoid&quot;), keras.layers.Reshape([28, 28]) ]) tied_ae = keras.models.Sequential([tied_encoder, tied_decoder]) tied_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1.5), metrics=[rounded_accuracy]) history = tied_ae.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 4s 80us/sample - loss: 0.3213 - rounded_accuracy: 0.8996 - val_loss: 0.3038 - val_rounded_accuracy: 0.9154 Epoch 2/10 55000/55000 [==============================] - 4s 74us/sample - loss: 0.2967 - rounded_accuracy: 0.9216 - val_loss: 0.2931 - val_rounded_accuracy: 0.9268 Epoch 3/10 55000/55000 [==============================] - 4s 70us/sample - loss: 0.2916 - rounded_accuracy: 0.9263 - val_loss: 0.2929 - val_rounded_accuracy: 0.9254 Epoch 4/10 55000/55000 [==============================] - 4s 64us/sample - loss: 0.2889 - rounded_accuracy: 0.9287 - val_loss: 0.2905 - val_rounded_accuracy: 0.9316 Epoch 5/10 55000/55000 [==============================] - 4s 70us/sample - loss: 0.2871 - rounded_accuracy: 0.9303 - val_loss: 0.2917 - val_rounded_accuracy: 0.9307 Epoch 6/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2858 - rounded_accuracy: 0.9316 - val_loss: 0.2870 - val_rounded_accuracy: 0.9332 Epoch 7/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2847 - rounded_accuracy: 0.9327 - val_loss: 0.2865 - val_rounded_accuracy: 0.9336 Epoch 8/10 55000/55000 [==============================] - 4s 71us/sample - loss: 0.2840 - rounded_accuracy: 0.9334 - val_loss: 0.2859 - val_rounded_accuracy: 0.9349 Epoch 9/10 55000/55000 [==============================] - 4s 70us/sample - loss: 0.2834 - rounded_accuracy: 0.9339 - val_loss: 0.2864 - val_rounded_accuracy: 0.9338 Epoch 10/10 55000/55000 [==============================] - 4s 72us/sample - loss: 0.2828 - rounded_accuracy: 0.9345 - val_loss: 0.2839 - val_rounded_accuracy: 0.9338 . show_reconstructions(tied_ae) plt.show() . Training one Autoencoder at a Time . def train_autoencoder(n_neurons, X_train, X_valid, loss, optimizer, n_epochs=10, output_activation=None, metrics=None): n_inputs = X_train.shape[-1] encoder = keras.models.Sequential([ keras.layers.Dense(n_neurons, activation=&quot;selu&quot;, input_shape=[n_inputs]) ]) decoder = keras.models.Sequential([ keras.layers.Dense(n_inputs, activation=output_activation), ]) autoencoder = keras.models.Sequential([encoder, decoder]) autoencoder.compile(optimizer, loss, metrics=metrics) autoencoder.fit(X_train, X_train, epochs=n_epochs, validation_data=[X_valid, X_valid]) return encoder, decoder, encoder(X_train), encoder(X_valid) . tf.random.set_seed(42) np.random.seed(42) K = keras.backend X_train_flat = K.batch_flatten(X_train) # equivalent to .reshape(-1, 28 * 28) X_valid_flat = K.batch_flatten(X_valid) enc1, dec1, X_train_enc1, X_valid_enc1 = train_autoencoder( 100, X_train_flat, X_valid_flat, &quot;binary_crossentropy&quot;, keras.optimizers.SGD(lr=1.5), output_activation=&quot;sigmoid&quot;, metrics=[rounded_accuracy]) enc2, dec2, _, _ = train_autoencoder( 30, X_train_enc1, X_valid_enc1, &quot;mse&quot;, keras.optimizers.SGD(lr=0.05), output_activation=&quot;selu&quot;) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 4s 73us/sample - loss: 0.3446 - rounded_accuracy: 0.8874 - val_loss: 0.3122 - val_rounded_accuracy: 0.9147 Epoch 2/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.3039 - rounded_accuracy: 0.9204 - val_loss: 0.3006 - val_rounded_accuracy: 0.9241 Epoch 3/10 55000/55000 [==============================] - 4s 69us/sample - loss: 0.2949 - rounded_accuracy: 0.9286 - val_loss: 0.2933 - val_rounded_accuracy: 0.9319 Epoch 4/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2890 - rounded_accuracy: 0.9343 - val_loss: 0.2887 - val_rounded_accuracy: 0.9362 Epoch 5/10 55000/55000 [==============================] - 4s 72us/sample - loss: 0.2853 - rounded_accuracy: 0.9379 - val_loss: 0.2856 - val_rounded_accuracy: 0.9390 Epoch 6/10 55000/55000 [==============================] - 4s 67us/sample - loss: 0.2826 - rounded_accuracy: 0.9404 - val_loss: 0.2833 - val_rounded_accuracy: 0.9410 Epoch 7/10 55000/55000 [==============================] - 4s 69us/sample - loss: 0.2806 - rounded_accuracy: 0.9424 - val_loss: 0.2816 - val_rounded_accuracy: 0.9430 Epoch 8/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2791 - rounded_accuracy: 0.9439 - val_loss: 0.2802 - val_rounded_accuracy: 0.9448 Epoch 9/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.2778 - rounded_accuracy: 0.9451 - val_loss: 0.2790 - val_rounded_accuracy: 0.9454 Epoch 10/10 55000/55000 [==============================] - 4s 65us/sample - loss: 0.2768 - rounded_accuracy: 0.9461 - val_loss: 0.2781 - val_rounded_accuracy: 0.9462 Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 2s 35us/sample - loss: 0.5678 - val_loss: 0.2887 Epoch 2/10 55000/55000 [==============================] - 2s 30us/sample - loss: 0.2633 - val_loss: 0.2512 Epoch 3/10 55000/55000 [==============================] - 2s 33us/sample - loss: 0.2237 - val_loss: 0.2115 Epoch 4/10 55000/55000 [==============================] - 2s 33us/sample - loss: 0.2025 - val_loss: 0.1967 Epoch 5/10 55000/55000 [==============================] - 2s 31us/sample - loss: 0.1909 - val_loss: 0.1864 Epoch 6/10 55000/55000 [==============================] - 2s 29us/sample - loss: 0.1824 - val_loss: 0.1734 Epoch 7/10 55000/55000 [==============================] - 2s 31us/sample - loss: 0.1750 - val_loss: 0.1696 Epoch 8/10 55000/55000 [==============================] - 2s 31us/sample - loss: 0.1732 - val_loss: 0.1719 Epoch 9/10 55000/55000 [==============================] - 2s 30us/sample - loss: 0.1711 - val_loss: 0.1917 Epoch 10/10 55000/55000 [==============================] - 2s 29us/sample - loss: 0.1704 - val_loss: 0.1687 . stacked_ae_1_by_1 = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), enc1, enc2, dec2, dec1, keras.layers.Reshape([28, 28]) ]) . show_reconstructions(stacked_ae_1_by_1) plt.show() . stacked_ae_1_by_1.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=0.1), metrics=[rounded_accuracy]) history = stacked_ae_1_by_1.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 5s 83us/sample - loss: 0.2853 - rounded_accuracy: 0.9359 - val_loss: 0.2868 - val_rounded_accuracy: 0.9361 Epoch 2/10 55000/55000 [==============================] - 4s 70us/sample - loss: 0.2849 - rounded_accuracy: 0.9363 - val_loss: 0.2866 - val_rounded_accuracy: 0.9364 Epoch 3/10 55000/55000 [==============================] - 4s 66us/sample - loss: 0.2847 - rounded_accuracy: 0.9365 - val_loss: 0.2864 - val_rounded_accuracy: 0.9362 Epoch 4/10 55000/55000 [==============================] - 4s 76us/sample - loss: 0.2846 - rounded_accuracy: 0.9366 - val_loss: 0.2863 - val_rounded_accuracy: 0.9367 Epoch 5/10 55000/55000 [==============================] - 4s 74us/sample - loss: 0.2844 - rounded_accuracy: 0.9368 - val_loss: 0.2862 - val_rounded_accuracy: 0.9369 Epoch 6/10 55000/55000 [==============================] - 4s 66us/sample - loss: 0.2843 - rounded_accuracy: 0.9369 - val_loss: 0.2861 - val_rounded_accuracy: 0.9368 Epoch 7/10 55000/55000 [==============================] - 4s 67us/sample - loss: 0.2842 - rounded_accuracy: 0.9370 - val_loss: 0.2860 - val_rounded_accuracy: 0.9368 Epoch 8/10 55000/55000 [==============================] - 4s 66us/sample - loss: 0.2841 - rounded_accuracy: 0.9371 - val_loss: 0.2859 - val_rounded_accuracy: 0.9369 Epoch 9/10 55000/55000 [==============================] - 4s 67us/sample - loss: 0.2840 - rounded_accuracy: 0.9372 - val_loss: 0.2858 - val_rounded_accuracy: 0.9368 Epoch 10/10 55000/55000 [==============================] - 4s 66us/sample - loss: 0.2839 - rounded_accuracy: 0.9373 - val_loss: 0.2857 - val_rounded_accuracy: 0.9371 . show_reconstructions(stacked_ae_1_by_1) plt.show() . Using Convolutional Layers Instead of Dense Layers . Let&#39;s build a stacked Autoencoder with 3 hidden layers and 1 output layer (i.e., 2 stacked Autoencoders). . tf.random.set_seed(42) np.random.seed(42) conv_encoder = keras.models.Sequential([ keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]), keras.layers.Conv2D(16, kernel_size=3, padding=&quot;SAME&quot;, activation=&quot;selu&quot;), keras.layers.MaxPool2D(pool_size=2), keras.layers.Conv2D(32, kernel_size=3, padding=&quot;SAME&quot;, activation=&quot;selu&quot;), keras.layers.MaxPool2D(pool_size=2), keras.layers.Conv2D(64, kernel_size=3, padding=&quot;SAME&quot;, activation=&quot;selu&quot;), keras.layers.MaxPool2D(pool_size=2) ]) conv_decoder = keras.models.Sequential([ keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=&quot;VALID&quot;, activation=&quot;selu&quot;, input_shape=[3, 3, 64]), keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=&quot;SAME&quot;, activation=&quot;selu&quot;), keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=&quot;SAME&quot;, activation=&quot;sigmoid&quot;), keras.layers.Reshape([28, 28]) ]) conv_ae = keras.models.Sequential([conv_encoder, conv_decoder]) conv_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1.0), metrics=[rounded_accuracy]) history = conv_ae.fit(X_train, X_train, epochs=5, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/5 55000/55000 [==============================] - 40s 734us/sample - loss: 0.3017 - accuracy: 0.5064 - val_loss: 0.2842 - val_accuracy: 0.5058 Epoch 2/5 55000/55000 [==============================] - 39s 712us/sample - loss: 0.2756 - accuracy: 0.5088 - val_loss: 0.2739 - val_accuracy: 0.5058 Epoch 3/5 55000/55000 [==============================] - 39s 715us/sample - loss: 0.2709 - accuracy: 0.5092 - val_loss: 0.2720 - val_accuracy: 0.5059 Epoch 4/5 55000/55000 [==============================] - 39s 707us/sample - loss: 0.2682 - accuracy: 0.5094 - val_loss: 0.2685 - val_accuracy: 0.5063 Epoch 5/5 55000/55000 [==============================] - 39s 706us/sample - loss: 0.2665 - accuracy: 0.5095 - val_loss: 0.2671 - val_accuracy: 0.5066 . conv_encoder.summary() conv_decoder.summary() . Model: &#34;sequential_16&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= reshape_3 (Reshape) (None, 28, 28, 1) 0 _________________________________________________________________ conv2d (Conv2D) (None, 28, 28, 16) 160 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 14, 14, 16) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 14, 14, 32) 4640 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 7, 7, 64) 18496 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64) 0 ================================================================= Total params: 23,296 Trainable params: 23,296 Non-trainable params: 0 _________________________________________________________________ Model: &#34;sequential_17&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_transpose (Conv2DTran (None, 7, 7, 32) 18464 _________________________________________________________________ conv2d_transpose_1 (Conv2DTr (None, 14, 14, 16) 4624 _________________________________________________________________ conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1) 145 _________________________________________________________________ reshape_4 (Reshape) (None, 28, 28) 0 ================================================================= Total params: 23,233 Trainable params: 23,233 Non-trainable params: 0 _________________________________________________________________ . show_reconstructions(conv_ae) plt.show() . Recurrent Autoencoders . recurrent_encoder = keras.models.Sequential([ keras.layers.LSTM(100, return_sequences=True, input_shape=[28, 28]), keras.layers.LSTM(30) ]) recurrent_decoder = keras.models.Sequential([ keras.layers.RepeatVector(28, input_shape=[30]), keras.layers.LSTM(100, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(28, activation=&quot;sigmoid&quot;)) ]) recurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder]) recurrent_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(0.1), metrics=[rounded_accuracy]) . history = recurrent_ae.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 79s 1ms/sample - loss: 0.5165 - rounded_accuracy: 0.7363 - val_loss: 0.4489 - val_rounded_accuracy: 0.8137 Epoch 2/10 55000/55000 [==============================] - 78s 1ms/sample - loss: 0.4049 - rounded_accuracy: 0.8415 - val_loss: 0.3762 - val_rounded_accuracy: 0.8650 Epoch 3/10 55000/55000 [==============================] - 80s 1ms/sample - loss: 0.3662 - rounded_accuracy: 0.8703 - val_loss: 0.3626 - val_rounded_accuracy: 0.8730 Epoch 4/10 55000/55000 [==============================] - 80s 1ms/sample - loss: 0.3505 - rounded_accuracy: 0.8808 - val_loss: 0.3483 - val_rounded_accuracy: 0.8838 Epoch 5/10 55000/55000 [==============================] - 82s 1ms/sample - loss: 0.3398 - rounded_accuracy: 0.8881 - val_loss: 0.3345 - val_rounded_accuracy: 0.8941 Epoch 6/10 55000/55000 [==============================] - 93s 2ms/sample - loss: 0.3328 - rounded_accuracy: 0.8930 - val_loss: 0.3372 - val_rounded_accuracy: 0.8914 Epoch 7/10 55000/55000 [==============================] - 94s 2ms/sample - loss: 0.3280 - rounded_accuracy: 0.8962 - val_loss: 0.3261 - val_rounded_accuracy: 0.8980 Epoch 8/10 55000/55000 [==============================] - 95s 2ms/sample - loss: 0.3244 - rounded_accuracy: 0.8988 - val_loss: 0.3226 - val_rounded_accuracy: 0.9030 Epoch 9/10 55000/55000 [==============================] - 92s 2ms/sample - loss: 0.3215 - rounded_accuracy: 0.9010 - val_loss: 0.3239 - val_rounded_accuracy: 0.8958 Epoch 10/10 55000/55000 [==============================] - 90s 2ms/sample - loss: 0.3190 - rounded_accuracy: 0.9030 - val_loss: 0.3206 - val_rounded_accuracy: 0.9015 . &lt;tensorflow.python.keras.callbacks.History at 0x1a5b98fa20&gt; . show_reconstructions(recurrent_ae) plt.show() . Stacked denoising Autoencoder . Using Gaussian noise: . tf.random.set_seed(42) np.random.seed(42) denoising_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.GaussianNoise(0.2), keras.layers.Dense(100, activation=&quot;selu&quot;), keras.layers.Dense(30, activation=&quot;selu&quot;) ]) denoising_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[30]), keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;), keras.layers.Reshape([28, 28]) ]) denoising_ae = keras.models.Sequential([denoising_encoder, denoising_decoder]) denoising_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1.0), metrics=[rounded_accuracy]) history = denoising_ae.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 5s 82us/sample - loss: 0.3508 - rounded_accuracy: 0.8768 - val_loss: 0.3231 - val_rounded_accuracy: 0.9065 Epoch 2/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.3125 - rounded_accuracy: 0.9093 - val_loss: 0.3077 - val_rounded_accuracy: 0.9153 Epoch 3/10 55000/55000 [==============================] - 4s 74us/sample - loss: 0.3061 - rounded_accuracy: 0.9149 - val_loss: 0.3034 - val_rounded_accuracy: 0.9190 Epoch 4/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.3025 - rounded_accuracy: 0.9181 - val_loss: 0.3007 - val_rounded_accuracy: 0.9195 Epoch 5/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.2998 - rounded_accuracy: 0.9203 - val_loss: 0.2980 - val_rounded_accuracy: 0.9230 Epoch 6/10 55000/55000 [==============================] - 4s 78us/sample - loss: 0.2979 - rounded_accuracy: 0.9220 - val_loss: 0.2987 - val_rounded_accuracy: 0.9193 Epoch 7/10 55000/55000 [==============================] - 4s 74us/sample - loss: 0.2965 - rounded_accuracy: 0.9233 - val_loss: 0.2945 - val_rounded_accuracy: 0.9269 Epoch 8/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.2953 - rounded_accuracy: 0.9243 - val_loss: 0.2946 - val_rounded_accuracy: 0.9286 Epoch 9/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.2943 - rounded_accuracy: 0.9251 - val_loss: 0.2927 - val_rounded_accuracy: 0.9283 Epoch 10/10 55000/55000 [==============================] - 4s 77us/sample - loss: 0.2935 - rounded_accuracy: 0.9258 - val_loss: 0.2920 - val_rounded_accuracy: 0.9291 . tf.random.set_seed(42) np.random.seed(42) noise = keras.layers.GaussianNoise(0.2) show_reconstructions(denoising_ae, noise(X_valid, training=True)) plt.show() . Using dropout: . tf.random.set_seed(42) np.random.seed(42) dropout_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dropout(0.5), keras.layers.Dense(100, activation=&quot;selu&quot;), keras.layers.Dense(30, activation=&quot;selu&quot;) ]) dropout_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[30]), keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;), keras.layers.Reshape([28, 28]) ]) dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder]) dropout_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1.0), metrics=[rounded_accuracy]) history = dropout_ae.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 5s 83us/sample - loss: 0.3564 - accuracy: 0.4969 - val_loss: 0.3206 - val_accuracy: 0.5011 Epoch 2/10 55000/55000 [==============================] - 4s 73us/sample - loss: 0.3182 - accuracy: 0.5034 - val_loss: 0.3113 - val_accuracy: 0.5014 Epoch 3/10 55000/55000 [==============================] - 4s 74us/sample - loss: 0.3130 - accuracy: 0.5042 - val_loss: 0.3079 - val_accuracy: 0.5012 Epoch 4/10 55000/55000 [==============================] - 4s 73us/sample - loss: 0.3091 - accuracy: 0.5048 - val_loss: 0.3037 - val_accuracy: 0.5026 Epoch 5/10 55000/55000 [==============================] - 4s 76us/sample - loss: 0.3066 - accuracy: 0.5052 - val_loss: 0.3032 - val_accuracy: 0.5016 Epoch 6/10 55000/55000 [==============================] - 4s 78us/sample - loss: 0.3047 - accuracy: 0.5054 - val_loss: 0.3001 - val_accuracy: 0.5032 Epoch 7/10 55000/55000 [==============================] - 4s 79us/sample - loss: 0.3033 - accuracy: 0.5056 - val_loss: 0.2987 - val_accuracy: 0.5033 Epoch 8/10 55000/55000 [==============================] - 4s 76us/sample - loss: 0.3021 - accuracy: 0.5057 - val_loss: 0.2976 - val_accuracy: 0.5033 Epoch 9/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.3012 - accuracy: 0.5058 - val_loss: 0.2976 - val_accuracy: 0.5033 Epoch 10/10 55000/55000 [==============================] - 4s 76us/sample - loss: 0.3004 - accuracy: 0.5059 - val_loss: 0.2958 - val_accuracy: 0.5033 . tf.random.set_seed(42) np.random.seed(42) dropout = keras.layers.Dropout(0.5) show_reconstructions(dropout_ae, dropout(X_valid, training=True)) save_fig(&quot;dropout_denoising_plot&quot;, tight_layout=False) . Saving figure dropout_denoising_plot . Sparse Autoencoder . Let&#39;s build a simple stacked autoencoder, so we can compare it to the sparse autoencoders we will build. This time we will use the sigmoid activation function for the coding layer, to ensure that the coding values range from 0 to 1: . tf.random.set_seed(42) np.random.seed(42) simple_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(100, activation=&quot;selu&quot;), keras.layers.Dense(30, activation=&quot;sigmoid&quot;), ]) simple_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[30]), keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;), keras.layers.Reshape([28, 28]) ]) simple_ae = keras.models.Sequential([simple_encoder, simple_decoder]) simple_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1.), metrics=[rounded_accuracy]) history = simple_ae.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 4s 78us/sample - loss: 0.4331 - accuracy: 0.4906 - val_loss: 0.3778 - val_accuracy: 0.4911 Epoch 2/10 55000/55000 [==============================] - 4s 67us/sample - loss: 0.3610 - accuracy: 0.4976 - val_loss: 0.3510 - val_accuracy: 0.4972 Epoch 3/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.3405 - accuracy: 0.5006 - val_loss: 0.3359 - val_accuracy: 0.4990 Epoch 4/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.3276 - accuracy: 0.5027 - val_loss: 0.3248 - val_accuracy: 0.5003 Epoch 5/10 55000/55000 [==============================] - 4s 72us/sample - loss: 0.3206 - accuracy: 0.5035 - val_loss: 0.3206 - val_accuracy: 0.5007 Epoch 6/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.3172 - accuracy: 0.5038 - val_loss: 0.3176 - val_accuracy: 0.5010 Epoch 7/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.3149 - accuracy: 0.5041 - val_loss: 0.3154 - val_accuracy: 0.5013 Epoch 8/10 55000/55000 [==============================] - 4s 69us/sample - loss: 0.3128 - accuracy: 0.5045 - val_loss: 0.3133 - val_accuracy: 0.5014 Epoch 9/10 55000/55000 [==============================] - 4s 68us/sample - loss: 0.3108 - accuracy: 0.5049 - val_loss: 0.3118 - val_accuracy: 0.5023 Epoch 10/10 55000/55000 [==============================] - 4s 71us/sample - loss: 0.3088 - accuracy: 0.5053 - val_loss: 0.3092 - val_accuracy: 0.5023 . show_reconstructions(simple_ae) plt.show() . Let&#39;s create a couple functions to print nice activation histograms: . def plot_percent_hist(ax, data, bins): counts, _ = np.histogram(data, bins=bins) widths = bins[1:] - bins[:-1] x = bins[:-1] + widths / 2 ax.bar(x, counts / len(data), width=widths*0.8) ax.xaxis.set_ticks(bins) ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter( lambda y, position: &quot;{}%&quot;.format(int(np.round(100 * y))))) ax.grid(True) . def plot_activations_histogram(encoder, height=1, n_bins=10): X_valid_codings = encoder(X_valid).numpy() activation_means = X_valid_codings.mean(axis=0) mean = activation_means.mean() bins = np.linspace(0, 1, n_bins + 1) fig, [ax1, ax2] = plt.subplots(figsize=(10, 3), nrows=1, ncols=2, sharey=True) plot_percent_hist(ax1, X_valid_codings.ravel(), bins) ax1.plot([mean, mean], [0, height], &quot;k--&quot;, label=&quot;Overall Mean = {:.2f}&quot;.format(mean)) ax1.legend(loc=&quot;upper center&quot;, fontsize=14) ax1.set_xlabel(&quot;Activation&quot;) ax1.set_ylabel(&quot;% Activations&quot;) ax1.axis([0, 1, 0, height]) plot_percent_hist(ax2, activation_means, bins) ax2.plot([mean, mean], [0, height], &quot;k--&quot;) ax2.set_xlabel(&quot;Neuron Mean Activation&quot;) ax2.set_ylabel(&quot;% Neurons&quot;) ax2.axis([0, 1, 0, height]) . Let&#39;s use these functions to plot histograms of the activations of the encoding layer. The histogram on the left shows the distribution of all the activations. You can see that values close to 0 or 1 are more frequent overall, which is consistent with the saturating nature of the sigmoid function. The histogram on the right shows the distribution of mean neuron activations: you can see that most neurons have a mean activation close to 0.5. Both histograms tell us that each neuron tends to either fire close to 0 or 1, with about 50% probability each. However, some neurons fire almost all the time (right side of the right histogram). . plot_activations_histogram(simple_encoder, height=0.35) plt.show() . Now let&#39;s add $ ell_1$ regularization to the coding layer: . tf.random.set_seed(42) np.random.seed(42) sparse_l1_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(100, activation=&quot;selu&quot;), keras.layers.Dense(300, activation=&quot;sigmoid&quot;), keras.layers.ActivityRegularization(l1=1e-3) # Alternatively, you could add # activity_regularizer=keras.regularizers.l1(1e-3) # to the previous layer. ]) sparse_l1_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[300]), keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;), keras.layers.Reshape([28, 28]) ]) sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder]) sparse_l1_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1.0), metrics=[rounded_accuracy]) history = sparse_l1_ae.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 5s 98us/sample - loss: 0.4306 - accuracy: 0.4947 - val_loss: 0.3819 - val_accuracy: 0.4897 Epoch 2/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.3689 - accuracy: 0.4971 - val_loss: 0.3639 - val_accuracy: 0.4940 Epoch 3/10 55000/55000 [==============================] - 5s 86us/sample - loss: 0.3553 - accuracy: 0.4987 - val_loss: 0.3513 - val_accuracy: 0.4970 Epoch 4/10 55000/55000 [==============================] - 4s 78us/sample - loss: 0.3443 - accuracy: 0.5003 - val_loss: 0.3428 - val_accuracy: 0.4964 Epoch 5/10 55000/55000 [==============================] - 4s 76us/sample - loss: 0.3379 - accuracy: 0.5009 - val_loss: 0.3372 - val_accuracy: 0.4979 Epoch 6/10 55000/55000 [==============================] - 4s 76us/sample - loss: 0.3332 - accuracy: 0.5015 - val_loss: 0.3329 - val_accuracy: 0.4980 Epoch 7/10 55000/55000 [==============================] - 4s 78us/sample - loss: 0.3286 - accuracy: 0.5025 - val_loss: 0.3306 - val_accuracy: 0.4981 Epoch 8/10 55000/55000 [==============================] - 4s 76us/sample - loss: 0.3249 - accuracy: 0.5032 - val_loss: 0.3254 - val_accuracy: 0.5000 Epoch 9/10 55000/55000 [==============================] - 4s 80us/sample - loss: 0.3223 - accuracy: 0.5036 - val_loss: 0.3244 - val_accuracy: 0.4995 Epoch 10/10 55000/55000 [==============================] - 4s 75us/sample - loss: 0.3205 - accuracy: 0.5039 - val_loss: 0.3212 - val_accuracy: 0.5014 . show_reconstructions(sparse_l1_ae) . plot_activations_histogram(sparse_l1_encoder, height=1.) plt.show() . Let&#39;s use the KL Divergence loss instead to ensure sparsity, and target 10% sparsity rather than 0%: . p = 0.1 q = np.linspace(0.001, 0.999, 500) kl_div = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q)) mse = (p - q)**2 mae = np.abs(p - q) plt.plot([p, p], [0, 0.3], &quot;k:&quot;) plt.text(0.05, 0.32, &quot;Target nsparsity&quot;, fontsize=14) plt.plot(q, kl_div, &quot;b-&quot;, label=&quot;KL divergence&quot;) plt.plot(q, mae, &quot;g--&quot;, label=r&quot;MAE ($ ell_1$)&quot;) plt.plot(q, mse, &quot;r--&quot;, linewidth=1, label=r&quot;MSE ($ ell_2$)&quot;) plt.legend(loc=&quot;upper left&quot;, fontsize=14) plt.xlabel(&quot;Actual sparsity&quot;) plt.ylabel(&quot;Cost&quot;, rotation=0) plt.axis([0, 1, 0, 0.95]) save_fig(&quot;sparsity_loss_plot&quot;) . Saving figure sparsity_loss_plot . K = keras.backend kl_divergence = keras.losses.kullback_leibler_divergence class KLDivergenceRegularizer(keras.regularizers.Regularizer): def __init__(self, weight, target=0.1): self.weight = weight self.target = target def __call__(self, inputs): mean_activities = K.mean(inputs, axis=0) return self.weight * ( kl_divergence(self.target, mean_activities) + kl_divergence(1. - self.target, 1. - mean_activities)) . tf.random.set_seed(42) np.random.seed(42) kld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1) sparse_kl_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(100, activation=&quot;selu&quot;), keras.layers.Dense(300, activation=&quot;sigmoid&quot;, activity_regularizer=kld_reg) ]) sparse_kl_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[300]), keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;), keras.layers.Reshape([28, 28]) ]) sparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder]) sparse_kl_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1.0), metrics=[rounded_accuracy]) history = sparse_kl_ae.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 6s 103us/sample - loss: 0.4151 - rounded_accuracy: 0.8121 - val_loss: 0.3714 - val_rounded_accuracy: 0.8560 Epoch 2/10 55000/55000 [==============================] - 4s 81us/sample - loss: 0.3532 - rounded_accuracy: 0.8762 - val_loss: 0.3442 - val_rounded_accuracy: 0.8842 Epoch 3/10 55000/55000 [==============================] - 5s 83us/sample - loss: 0.3340 - rounded_accuracy: 0.8919 - val_loss: 0.3292 - val_rounded_accuracy: 0.8976 Epoch 4/10 55000/55000 [==============================] - 5s 84us/sample - loss: 0.3224 - rounded_accuracy: 0.9018 - val_loss: 0.3213 - val_rounded_accuracy: 0.9040 Epoch 5/10 55000/55000 [==============================] - 5s 85us/sample - loss: 0.3170 - rounded_accuracy: 0.9062 - val_loss: 0.3170 - val_rounded_accuracy: 0.9075 Epoch 6/10 55000/55000 [==============================] - 5s 82us/sample - loss: 0.3134 - rounded_accuracy: 0.9093 - val_loss: 0.3140 - val_rounded_accuracy: 0.9105 Epoch 7/10 55000/55000 [==============================] - 5s 85us/sample - loss: 0.3107 - rounded_accuracy: 0.9116 - val_loss: 0.3114 - val_rounded_accuracy: 0.9121 Epoch 8/10 55000/55000 [==============================] - 5s 83us/sample - loss: 0.3084 - rounded_accuracy: 0.9136 - val_loss: 0.3094 - val_rounded_accuracy: 0.9145 Epoch 9/10 55000/55000 [==============================] - 5s 83us/sample - loss: 0.3064 - rounded_accuracy: 0.9154 - val_loss: 0.3074 - val_rounded_accuracy: 0.9166 Epoch 10/10 55000/55000 [==============================] - 5s 84us/sample - loss: 0.3044 - rounded_accuracy: 0.9170 - val_loss: 0.3053 - val_rounded_accuracy: 0.9174 . show_reconstructions(sparse_kl_ae) . plot_activations_histogram(sparse_kl_encoder) save_fig(&quot;sparse_autoencoder_plot&quot;) plt.show() . Saving figure sparse_autoencoder_plot . Variational Autoencoder . class Sampling(keras.layers.Layer): def call(self, inputs): mean, log_var = inputs return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean . tf.random.set_seed(42) np.random.seed(42) codings_size = 10 inputs = keras.layers.Input(shape=[28, 28]) z = keras.layers.Flatten()(inputs) z = keras.layers.Dense(150, activation=&quot;selu&quot;)(z) z = keras.layers.Dense(100, activation=&quot;selu&quot;)(z) codings_mean = keras.layers.Dense(codings_size)(z) codings_log_var = keras.layers.Dense(codings_size)(z) codings = Sampling()([codings_mean, codings_log_var]) variational_encoder = keras.models.Model( inputs=[inputs], outputs=[codings_mean, codings_log_var, codings]) decoder_inputs = keras.layers.Input(shape=[codings_size]) x = keras.layers.Dense(100, activation=&quot;selu&quot;)(decoder_inputs) x = keras.layers.Dense(150, activation=&quot;selu&quot;)(x) x = keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;)(x) outputs = keras.layers.Reshape([28, 28])(x) variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs]) _, _, codings = variational_encoder(inputs) reconstructions = variational_decoder(codings) variational_ae = keras.models.Model(inputs=[inputs], outputs=[reconstructions]) latent_loss = -0.5 * K.sum( 1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean), axis=-1) variational_ae.add_loss(K.mean(latent_loss) / 784.) variational_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;, metrics=[rounded_accuracy]) history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/25 55000/55000 [==============================] - 5s 84us/sample - loss: 0.3889 - rounded_accuracy: 0.8608 - val_loss: 0.3592 - val_rounded_accuracy: 0.8840 Epoch 2/25 55000/55000 [==============================] - 3s 60us/sample - loss: 0.3429 - rounded_accuracy: 0.8974 - val_loss: 0.3369 - val_rounded_accuracy: 0.8982 Epoch 3/25 55000/55000 [==============================] - 3s 53us/sample - loss: 0.3329 - rounded_accuracy: 0.9050 - val_loss: 0.3356 - val_rounded_accuracy: 0.9022 Epoch 4/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.3275 - rounded_accuracy: 0.9092 - val_loss: 0.3255 - val_rounded_accuracy: 0.9105 Epoch 5/25 55000/55000 [==============================] - 3s 59us/sample - loss: 0.3243 - rounded_accuracy: 0.9119 - val_loss: 0.3232 - val_rounded_accuracy: 0.9169 Epoch 6/25 55000/55000 [==============================] - 3s 58us/sample - loss: 0.3219 - rounded_accuracy: 0.9138 - val_loss: 0.3236 - val_rounded_accuracy: 0.9149 Epoch 7/25 55000/55000 [==============================] - 3s 55us/sample - loss: 0.3204 - rounded_accuracy: 0.9150 - val_loss: 0.3194 - val_rounded_accuracy: 0.9176 Epoch 8/25 55000/55000 [==============================] - 3s 56us/sample - loss: 0.3190 - rounded_accuracy: 0.9162 - val_loss: 0.3195 - val_rounded_accuracy: 0.9146 Epoch 9/25 55000/55000 [==============================] - 3s 58us/sample - loss: 0.3180 - rounded_accuracy: 0.9169 - val_loss: 0.3197 - val_rounded_accuracy: 0.9151 Epoch 10/25 55000/55000 [==============================] - 3s 60us/sample - loss: 0.3172 - rounded_accuracy: 0.9178 - val_loss: 0.3169 - val_rounded_accuracy: 0.9192 Epoch 11/25 55000/55000 [==============================] - 3s 57us/sample - loss: 0.3165 - rounded_accuracy: 0.9183 - val_loss: 0.3197 - val_rounded_accuracy: 0.9177 Epoch 12/25 55000/55000 [==============================] - 3s 58us/sample - loss: 0.3159 - rounded_accuracy: 0.9188 - val_loss: 0.3168 - val_rounded_accuracy: 0.9185 Epoch 13/25 55000/55000 [==============================] - 3s 62us/sample - loss: 0.3154 - rounded_accuracy: 0.9193 - val_loss: 0.3175 - val_rounded_accuracy: 0.9178 Epoch 14/25 55000/55000 [==============================] - 4s 64us/sample - loss: 0.3150 - rounded_accuracy: 0.9197 - val_loss: 0.3170 - val_rounded_accuracy: 0.9201 Epoch 15/25 55000/55000 [==============================] - 3s 60us/sample - loss: 0.3145 - rounded_accuracy: 0.9199 - val_loss: 0.3177 - val_rounded_accuracy: 0.9202 Epoch 16/25 55000/55000 [==============================] - 3s 58us/sample - loss: 0.3141 - rounded_accuracy: 0.9202 - val_loss: 0.3161 - val_rounded_accuracy: 0.9206 Epoch 17/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.3138 - rounded_accuracy: 0.9206 - val_loss: 0.3164 - val_rounded_accuracy: 0.9173 Epoch 18/25 55000/55000 [==============================] - 3s 58us/sample - loss: 0.3135 - rounded_accuracy: 0.9209 - val_loss: 0.3160 - val_rounded_accuracy: 0.9174 Epoch 19/25 55000/55000 [==============================] - 3s 58us/sample - loss: 0.3132 - rounded_accuracy: 0.9211 - val_loss: 0.3160 - val_rounded_accuracy: 0.9216 Epoch 20/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.3129 - rounded_accuracy: 0.9213 - val_loss: 0.3155 - val_rounded_accuracy: 0.9212 Epoch 21/25 55000/55000 [==============================] - 3s 61us/sample - loss: 0.3127 - rounded_accuracy: 0.9215 - val_loss: 0.3163 - val_rounded_accuracy: 0.9174 Epoch 22/25 55000/55000 [==============================] - 3s 60us/sample - loss: 0.3125 - rounded_accuracy: 0.9217 - val_loss: 0.3145 - val_rounded_accuracy: 0.9215 Epoch 23/25 55000/55000 [==============================] - 3s 53us/sample - loss: 0.3122 - rounded_accuracy: 0.9219 - val_loss: 0.3158 - val_rounded_accuracy: 0.9201 Epoch 24/25 55000/55000 [==============================] - 3s 56us/sample - loss: 0.3121 - rounded_accuracy: 0.9222 - val_loss: 0.3136 - val_rounded_accuracy: 0.9211 Epoch 25/25 55000/55000 [==============================] - 3s 54us/sample - loss: 0.3118 - rounded_accuracy: 0.9223 - val_loss: 0.3133 - val_rounded_accuracy: 0.9228 . show_reconstructions(variational_ae) plt.show() . Generate Fashion Images . def plot_multiple_images(images, n_cols=None): n_cols = n_cols or len(images) n_rows = (len(images) - 1) // n_cols + 1 if images.shape[-1] == 1: images = np.squeeze(images, axis=-1) plt.figure(figsize=(n_cols, n_rows)) for index, image in enumerate(images): plt.subplot(n_rows, n_cols, index + 1) plt.imshow(image, cmap=&quot;binary&quot;) plt.axis(&quot;off&quot;) . Let&#39;s generate a few random codings, decode them and plot the resulting images: . tf.random.set_seed(42) codings = tf.random.normal(shape=[12, codings_size]) images = variational_decoder(codings).numpy() plot_multiple_images(images, 4) save_fig(&quot;vae_generated_images_plot&quot;, tight_layout=False) . Saving figure vae_generated_images_plot . Now let&#39;s perform semantic interpolation between these images: . tf.random.set_seed(42) np.random.seed(42) codings_grid = tf.reshape(codings, [1, 3, 4, codings_size]) larger_grid = tf.image.resize(codings_grid, size=[5, 7]) interpolated_codings = tf.reshape(larger_grid, [-1, codings_size]) images = variational_decoder(interpolated_codings).numpy() plt.figure(figsize=(7, 5)) for index, image in enumerate(images): plt.subplot(5, 7, index + 1) if index%7%2==0 and index//7%2==0: plt.gca().get_xaxis().set_visible(False) plt.gca().get_yaxis().set_visible(False) else: plt.axis(&quot;off&quot;) plt.imshow(image, cmap=&quot;binary&quot;) save_fig(&quot;semantic_interpolation_plot&quot;, tight_layout=False) . Saving figure semantic_interpolation_plot . Generative Adversarial Networks . np.random.seed(42) tf.random.set_seed(42) codings_size = 30 generator = keras.models.Sequential([ keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[codings_size]), keras.layers.Dense(150, activation=&quot;selu&quot;), keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;), keras.layers.Reshape([28, 28]) ]) discriminator = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(150, activation=&quot;selu&quot;), keras.layers.Dense(100, activation=&quot;selu&quot;), keras.layers.Dense(1, activation=&quot;sigmoid&quot;) ]) gan = keras.models.Sequential([generator, discriminator]) . discriminator.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;) discriminator.trainable = False gan.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;) . batch_size = 32 dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000) dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1) . def train_gan(gan, dataset, batch_size, codings_size, n_epochs=50): generator, discriminator = gan.layers for epoch in range(n_epochs): print(&quot;Epoch {}/{}&quot;.format(epoch + 1, n_epochs)) # not shown in the book for X_batch in dataset: # phase 1 - training the discriminator noise = tf.random.normal(shape=[batch_size, codings_size]) generated_images = generator(noise) X_fake_and_real = tf.concat([generated_images, X_batch], axis=0) y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size) discriminator.trainable = True discriminator.train_on_batch(X_fake_and_real, y1) # phase 2 - training the generator noise = tf.random.normal(shape=[batch_size, codings_size]) y2 = tf.constant([[1.]] * batch_size) discriminator.trainable = False gan.train_on_batch(noise, y2) plot_multiple_images(generated_images, 8) # not shown plt.show() # not shown . train_gan(gan, dataset, batch_size, codings_size, n_epochs=1) . Epoch 1/1 . tf.random.set_seed(42) np.random.seed(42) noise = tf.random.normal(shape=[batch_size, codings_size]) generated_images = generator(noise) plot_multiple_images(generated_images, 8) save_fig(&quot;gan_generated_images_plot&quot;, tight_layout=False) . Saving figure gan_generated_images_plot . train_gan(gan, dataset, batch_size, codings_size) . Epoch 1/50 . Epoch 2/50 . Epoch 3/50 . Epoch 4/50 . Epoch 5/50 . Epoch 6/50 . Epoch 7/50 . Epoch 8/50 . Epoch 9/50 . Epoch 10/50 . Epoch 11/50 . Epoch 12/50 . Epoch 13/50 . Epoch 14/50 . Epoch 15/50 . Epoch 16/50 . Epoch 17/50 . Epoch 18/50 . Epoch 19/50 . Epoch 20/50 . Epoch 21/50 . Epoch 22/50 . Epoch 23/50 . Epoch 24/50 . Epoch 25/50 . Epoch 26/50 . Epoch 27/50 . Epoch 28/50 . Epoch 29/50 . Epoch 30/50 . Epoch 31/50 . Epoch 32/50 . Epoch 33/50 . Epoch 34/50 . Epoch 35/50 . Epoch 36/50 . Epoch 37/50 . Epoch 38/50 . Epoch 39/50 . Epoch 40/50 . Epoch 41/50 . Epoch 42/50 . Epoch 43/50 . Epoch 44/50 . Epoch 45/50 . Epoch 46/50 . Epoch 47/50 . Epoch 48/50 . Epoch 49/50 . Epoch 50/50 . Deep Convolutional GAN . tf.random.set_seed(42) np.random.seed(42) codings_size = 100 generator = keras.models.Sequential([ keras.layers.Dense(7 * 7 * 128, input_shape=[codings_size]), keras.layers.Reshape([7, 7, 128]), keras.layers.BatchNormalization(), keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=&quot;SAME&quot;, activation=&quot;selu&quot;), keras.layers.BatchNormalization(), keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding=&quot;SAME&quot;, activation=&quot;tanh&quot;), ]) discriminator = keras.models.Sequential([ keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=&quot;SAME&quot;, activation=keras.layers.LeakyReLU(0.2), input_shape=[28, 28, 1]), keras.layers.Dropout(0.4), keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=&quot;SAME&quot;, activation=keras.layers.LeakyReLU(0.2)), keras.layers.Dropout(0.4), keras.layers.Flatten(), keras.layers.Dense(1, activation=&quot;sigmoid&quot;) ]) gan = keras.models.Sequential([generator, discriminator]) . discriminator.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;) discriminator.trainable = False gan.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;) . X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale . batch_size = 32 dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan) dataset = dataset.shuffle(1000) dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1) . train_gan(gan, dataset, batch_size, codings_size) . Epoch 1/50 Saving figure gan_generated_images_plot . Epoch 2/50 . Epoch 3/50 . Epoch 4/50 . Epoch 5/50 . Epoch 6/50 . Epoch 7/50 . Epoch 8/50 . Epoch 9/50 . Epoch 10/50 . Epoch 11/50 . Epoch 12/50 . Epoch 13/50 . Epoch 14/50 . Epoch 15/50 . Epoch 16/50 . Epoch 17/50 . Epoch 18/50 . Epoch 19/50 . Epoch 20/50 . Epoch 21/50 . Epoch 22/50 . Epoch 23/50 . Epoch 24/50 . Epoch 25/50 . Epoch 26/50 . Epoch 27/50 . Epoch 28/50 . Epoch 29/50 . Epoch 30/50 . Epoch 31/50 . Epoch 32/50 . Epoch 33/50 . Epoch 34/50 . Epoch 35/50 . Epoch 36/50 . Epoch 37/50 . Epoch 38/50 . Epoch 39/50 . Epoch 40/50 . Epoch 41/50 . Epoch 42/50 . Epoch 43/50 . Epoch 44/50 . Epoch 45/50 . Epoch 46/50 . Epoch 47/50 . Epoch 48/50 . Epoch 49/50 . Epoch 50/50 . tf.random.set_seed(42) np.random.seed(42) noise = tf.random.normal(shape=[batch_size, codings_size]) generated_images = generator(noise) plot_multiple_images(generated_images, 8) save_fig(&quot;dcgan_generated_images_plot&quot;, tight_layout=False) . Saving figure dcgan_generated_images_plot . Exercise Solutions . Unsupervised pretraining . Let&#39;s create a small neural network for MNIST classification: . tf.random.set_seed(42) np.random.seed(42) X_train_small = X_train[:500] y_train_small = y_train[:500] classifier = keras.models.Sequential([ keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]), keras.layers.Conv2D(16, kernel_size=3, padding=&quot;SAME&quot;, activation=&quot;selu&quot;), keras.layers.MaxPool2D(pool_size=2), keras.layers.Conv2D(32, kernel_size=3, padding=&quot;SAME&quot;, activation=&quot;selu&quot;), keras.layers.MaxPool2D(pool_size=2), keras.layers.Conv2D(64, kernel_size=3, padding=&quot;SAME&quot;, activation=&quot;selu&quot;), keras.layers.MaxPool2D(pool_size=2), keras.layers.Flatten(), keras.layers.Dense(20, activation=&quot;selu&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) classifier.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=0.02), metrics=[&quot;accuracy&quot;]) history = classifier.fit(X_train_small, y_train_small, epochs=20, validation_data=[X_valid, y_valid]) . Train on 500 samples, validate on 5000 samples Epoch 1/20 500/500 [==============================] - 1s 3ms/sample - loss: 2.1965 - accuracy: 0.2480 - val_loss: 2.0234 - val_accuracy: 0.3148 Epoch 2/20 500/500 [==============================] - 1s 2ms/sample - loss: 1.7927 - accuracy: 0.5180 - val_loss: 1.5677 - val_accuracy: 0.6280 Epoch 3/20 500/500 [==============================] - 1s 2ms/sample - loss: 1.3931 - accuracy: 0.6360 - val_loss: 1.2556 - val_accuracy: 0.5482 Epoch 4/20 500/500 [==============================] - 1s 2ms/sample - loss: 1.1168 - accuracy: 0.6620 - val_loss: 0.9990 - val_accuracy: 0.6892 Epoch 5/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.9421 - accuracy: 0.7360 - val_loss: 1.1235 - val_accuracy: 0.6208 Epoch 6/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.8392 - accuracy: 0.7240 - val_loss: 0.8985 - val_accuracy: 0.6778 Epoch 7/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.7738 - accuracy: 0.7400 - val_loss: 0.7833 - val_accuracy: 0.7296 Epoch 8/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.7472 - accuracy: 0.7380 - val_loss: 0.7364 - val_accuracy: 0.7396 Epoch 9/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.6908 - accuracy: 0.7580 - val_loss: 0.8782 - val_accuracy: 0.6802 Epoch 10/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.6740 - accuracy: 0.7640 - val_loss: 0.7064 - val_accuracy: 0.7454 Epoch 11/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.6431 - accuracy: 0.7700 - val_loss: 0.8587 - val_accuracy: 0.6848 Epoch 12/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.6301 - accuracy: 0.7740 - val_loss: 0.6704 - val_accuracy: 0.7584 Epoch 13/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5834 - accuracy: 0.8040 - val_loss: 0.7229 - val_accuracy: 0.7302 Epoch 14/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5612 - accuracy: 0.8220 - val_loss: 0.6370 - val_accuracy: 0.7734 Epoch 15/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5845 - accuracy: 0.7960 - val_loss: 0.6511 - val_accuracy: 0.7592 Epoch 16/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5488 - accuracy: 0.8080 - val_loss: 0.7779 - val_accuracy: 0.7014 Epoch 17/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5362 - accuracy: 0.8160 - val_loss: 0.6632 - val_accuracy: 0.7636 Epoch 18/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5064 - accuracy: 0.8180 - val_loss: 0.7703 - val_accuracy: 0.6954 Epoch 19/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5147 - accuracy: 0.8240 - val_loss: 0.6980 - val_accuracy: 0.7390 Epoch 20/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5151 - accuracy: 0.8160 - val_loss: 0.7014 - val_accuracy: 0.7374 . import pandas as pd pd.DataFrame(history.history).plot() plt.show() . tf.random.set_seed(42) np.random.seed(42) conv_encoder_clone = keras.models.clone_model(conv_encoder) pretrained_clf = keras.models.Sequential([ conv_encoder_clone, keras.layers.Flatten(), keras.layers.Dense(20, activation=&quot;selu&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) . conv_encoder_clone.trainable = False pretrained_clf.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=0.02), metrics=[&quot;accuracy&quot;]) history = pretrained_clf.fit(X_train_small, y_train_small, epochs=30, validation_data=[X_valid, y_valid]) . Train on 500 samples, validate on 5000 samples Epoch 1/30 500/500 [==============================] - 1s 3ms/sample - loss: 2.3174 - accuracy: 0.1820 - val_loss: 2.2350 - val_accuracy: 0.2156 Epoch 2/30 500/500 [==============================] - 1s 2ms/sample - loss: 2.1829 - accuracy: 0.2760 - val_loss: 2.1267 - val_accuracy: 0.3650 Epoch 3/30 500/500 [==============================] - 1s 2ms/sample - loss: 2.0852 - accuracy: 0.3880 - val_loss: 2.0370 - val_accuracy: 0.4378 Epoch 4/30 500/500 [==============================] - 1s 2ms/sample - loss: 1.9953 - accuracy: 0.4500 - val_loss: 1.9513 - val_accuracy: 0.5028 Epoch 5/30 500/500 [==============================] - 1s 2ms/sample - loss: 1.9117 - accuracy: 0.5860 - val_loss: 1.8742 - val_accuracy: 0.5610 Epoch 6/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.8310 - accuracy: 0.6180 - val_loss: 1.7963 - val_accuracy: 0.6242 Epoch 7/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.7526 - accuracy: 0.6760 - val_loss: 1.7218 - val_accuracy: 0.6440 Epoch 8/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.6823 - accuracy: 0.6760 - val_loss: 1.6525 - val_accuracy: 0.6682 Epoch 9/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.6132 - accuracy: 0.7020 - val_loss: 1.5936 - val_accuracy: 0.6430 Epoch 10/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.5521 - accuracy: 0.6960 - val_loss: 1.5257 - val_accuracy: 0.6844 Epoch 11/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.4915 - accuracy: 0.7180 - val_loss: 1.4718 - val_accuracy: 0.6688 Epoch 12/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.4381 - accuracy: 0.7200 - val_loss: 1.4196 - val_accuracy: 0.6832 Epoch 13/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.3849 - accuracy: 0.7180 - val_loss: 1.3708 - val_accuracy: 0.6798 Epoch 14/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.3376 - accuracy: 0.7180 - val_loss: 1.3270 - val_accuracy: 0.6852 Epoch 15/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.2971 - accuracy: 0.7320 - val_loss: 1.2876 - val_accuracy: 0.6846 Epoch 16/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.2556 - accuracy: 0.7380 - val_loss: 1.2488 - val_accuracy: 0.6976 Epoch 17/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.2174 - accuracy: 0.7240 - val_loss: 1.2141 - val_accuracy: 0.6938 Epoch 18/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.1855 - accuracy: 0.7300 - val_loss: 1.1859 - val_accuracy: 0.6938 Epoch 19/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.1551 - accuracy: 0.7400 - val_loss: 1.1562 - val_accuracy: 0.6982 Epoch 20/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.1256 - accuracy: 0.7300 - val_loss: 1.1256 - val_accuracy: 0.7016 Epoch 21/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.0985 - accuracy: 0.7520 - val_loss: 1.0996 - val_accuracy: 0.7064 Epoch 22/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.0735 - accuracy: 0.7400 - val_loss: 1.0756 - val_accuracy: 0.7142 Epoch 23/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.0494 - accuracy: 0.7560 - val_loss: 1.0562 - val_accuracy: 0.7066 Epoch 24/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.0259 - accuracy: 0.7480 - val_loss: 1.0377 - val_accuracy: 0.7034 Epoch 25/30 500/500 [==============================] - 1s 1ms/sample - loss: 1.0079 - accuracy: 0.7440 - val_loss: 1.0139 - val_accuracy: 0.7174 Epoch 26/30 500/500 [==============================] - 1s 1ms/sample - loss: 0.9860 - accuracy: 0.7420 - val_loss: 0.9959 - val_accuracy: 0.7224 Epoch 27/30 500/500 [==============================] - 1s 1ms/sample - loss: 0.9691 - accuracy: 0.7440 - val_loss: 0.9819 - val_accuracy: 0.7180 Epoch 28/30 500/500 [==============================] - 1s 1ms/sample - loss: 0.9526 - accuracy: 0.7600 - val_loss: 0.9638 - val_accuracy: 0.7238 Epoch 29/30 500/500 [==============================] - 1s 1ms/sample - loss: 0.9365 - accuracy: 0.7600 - val_loss: 0.9512 - val_accuracy: 0.7236 Epoch 30/30 500/500 [==============================] - 1s 1ms/sample - loss: 0.9203 - accuracy: 0.7600 - val_loss: 0.9364 - val_accuracy: 0.7244 . conv_encoder_clone.trainable = True pretrained_clf.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=0.02), metrics=[&quot;accuracy&quot;]) history = pretrained_clf.fit(X_train_small, y_train_small, epochs=20, validation_data=[X_valid, y_valid]) . Train on 500 samples, validate on 5000 samples Epoch 1/20 500/500 [==============================] - 1s 3ms/sample - loss: 0.8479 - accuracy: 0.7360 - val_loss: 0.8023 - val_accuracy: 0.7154 Epoch 2/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.7508 - accuracy: 0.7480 - val_loss: 0.7908 - val_accuracy: 0.7062 Epoch 3/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.6956 - accuracy: 0.7700 - val_loss: 0.8156 - val_accuracy: 0.7006 Epoch 4/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.6806 - accuracy: 0.7700 - val_loss: 0.7408 - val_accuracy: 0.7244 Epoch 5/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.6563 - accuracy: 0.7700 - val_loss: 0.6731 - val_accuracy: 0.7540 Epoch 6/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.6262 - accuracy: 0.7920 - val_loss: 0.7332 - val_accuracy: 0.7316 Epoch 7/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.6039 - accuracy: 0.7860 - val_loss: 0.6458 - val_accuracy: 0.7592 Epoch 8/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5875 - accuracy: 0.7800 - val_loss: 0.8370 - val_accuracy: 0.6970 Epoch 9/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5720 - accuracy: 0.8000 - val_loss: 0.6247 - val_accuracy: 0.7724 Epoch 10/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5601 - accuracy: 0.8140 - val_loss: 0.6436 - val_accuracy: 0.7524 Epoch 11/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5256 - accuracy: 0.8300 - val_loss: 0.6169 - val_accuracy: 0.7738 Epoch 12/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5029 - accuracy: 0.8260 - val_loss: 0.6318 - val_accuracy: 0.7672 Epoch 13/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.4956 - accuracy: 0.8340 - val_loss: 0.6539 - val_accuracy: 0.7548 Epoch 14/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.4754 - accuracy: 0.8360 - val_loss: 0.6640 - val_accuracy: 0.7598 Epoch 15/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.5025 - accuracy: 0.8240 - val_loss: 0.6049 - val_accuracy: 0.7762 Epoch 16/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.4462 - accuracy: 0.8440 - val_loss: 0.5851 - val_accuracy: 0.7882 Epoch 17/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.4607 - accuracy: 0.8400 - val_loss: 0.6206 - val_accuracy: 0.7706 Epoch 18/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.4216 - accuracy: 0.8580 - val_loss: 0.6025 - val_accuracy: 0.7800 Epoch 19/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.4308 - accuracy: 0.8460 - val_loss: 0.6109 - val_accuracy: 0.7702 Epoch 20/20 500/500 [==============================] - 1s 2ms/sample - loss: 0.4044 - accuracy: 0.8580 - val_loss: 0.5820 - val_accuracy: 0.7902 . Hashing Using a Binary Autoencoder . tf.random.set_seed(42) np.random.seed(42) hashing_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(100, activation=&quot;selu&quot;), keras.layers.GaussianNoise(15.), keras.layers.Dense(16, activation=&quot;sigmoid&quot;), ]) hashing_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[16]), keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;), keras.layers.Reshape([28, 28]) ]) hashing_ae = keras.models.Sequential([hashing_encoder, hashing_decoder]) hashing_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(lr=1.0), metrics=[rounded_accuracy]) history = hashing_ae.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid]) . Train on 55000 samples, validate on 5000 samples Epoch 1/10 55000/55000 [==============================] - 4s 77us/sample - loss: 0.4999 - accuracy: 0.4830 - val_loss: 0.4866 - val_accuracy: 0.4815 Epoch 2/10 55000/55000 [==============================] - 4s 69us/sample - loss: 0.4780 - accuracy: 0.4892 - val_loss: 0.4768 - val_accuracy: 0.4540 Epoch 3/10 55000/55000 [==============================] - 4s 69us/sample - loss: 0.4284 - accuracy: 0.4889 - val_loss: 0.4229 - val_accuracy: 0.4757 Epoch 4/10 55000/55000 [==============================] - 4s 70us/sample - loss: 0.4088 - accuracy: 0.4895 - val_loss: 0.4195 - val_accuracy: 0.4752 Epoch 5/10 55000/55000 [==============================] - 4s 70us/sample - loss: 0.4018 - accuracy: 0.4900 - val_loss: 0.4166 - val_accuracy: 0.4751 Epoch 6/10 55000/55000 [==============================] - 4s 69us/sample - loss: 0.3971 - accuracy: 0.4905 - val_loss: 0.4170 - val_accuracy: 0.4746 Epoch 7/10 55000/55000 [==============================] - 4s 69us/sample - loss: 0.3933 - accuracy: 0.4909 - val_loss: 0.4106 - val_accuracy: 0.4763 Epoch 8/10 55000/55000 [==============================] - 4s 71us/sample - loss: 0.3902 - accuracy: 0.4912 - val_loss: 0.4038 - val_accuracy: 0.4794 Epoch 9/10 55000/55000 [==============================] - 4s 65us/sample - loss: 0.3877 - accuracy: 0.4917 - val_loss: 0.4049 - val_accuracy: 0.4782 Epoch 10/10 55000/55000 [==============================] - 4s 66us/sample - loss: 0.3858 - accuracy: 0.4917 - val_loss: 0.4002 - val_accuracy: 0.4793 . show_reconstructions(hashing_ae) plt.show() . plot_activations_histogram(hashing_encoder) plt.show() . hashes = np.round(hashing_encoder.predict(X_valid)).astype(np.int32) hashes *= np.array([[2**bit for bit in range(16)]]) hashes = hashes.sum(axis=1) for h in hashes[:5]: print(&quot;{:016b}&quot;.format(h)) print(&quot;...&quot;) . 0000100100000001 0000100100000000 0000100100000001 0000100000000000 0000100000100000 ... . n_bits = 4 n_images = 8 plt.figure(figsize=(n_images, n_bits)) for bit_index in range(n_bits): in_bucket = (hashes &amp; 2**bit_index != 0) for index, image in zip(range(n_images), X_valid[in_bucket]): plt.subplot(n_bits, n_images, bit_index * n_images + index + 1) plt.imshow(image, cmap=&quot;binary&quot;) plt.axis(&quot;off&quot;) .",
            "url": "https://machine-learning-apps.github.io/hands-on-ml2/17_autoencoders_and_gans",
            "relUrl": "/17_autoencoders_and_gans",
            "date": " â€¢ Mar 9, 2020"
        }
        
    
  

  
  

  

  
  

  

  
  

  
  

  
  

}